submissionId,paperId,paperTitle,paperAbstract,conferenceJournalId,conferenceJournalTitle,proceedingsVolume,decision,finalPaperId,proceedingVolumeYear,acceptanceDate
s1,p1,High-Dimensional Probability: An Introduction with Applications in Data Science,"© 2018, Cambridge University Press Let us summarize our findings. A random projection of a set T in R n onto an m-dimensional subspace approximately preserves the geometry of T if m ⪆ d ( T ) . For...",c14,International Conference on Exploring Services Science,cp14,accepted,f0,2016,2016-05-08
s3,p3,AutoDS: Towards Human-Centered Automation of Data Science,"Data science (DS) projects often follow a lifecycle that consists of laborious tasks for data scientists and domain experts (e.g., data exploration, model training, etc.). Only till recently, machine learning(ML) researchers have developed promising automation techniques to aid data workers in these tasks. This paper introduces AutoDS, an automated machine learning (AutoML) system that aims to leverage the latest ML automation techniques to support data science projects. Data workers only need to upload their dataset, then the system can automatically suggest ML configurations, preprocess data, select algorithm, and train the model. These suggestions are presented to the user via a web-based graphical user interface and a notebook-based programming user interface. Our goal is to offer a systematic investigation of user interaction and perceptions of using an AutoDS system in solving a data science task. We studied AutoDS with 30 professional data scientists, where one group used AutoDS, and the other did not, to complete a data science project. As expected, AutoDS improves productivity; Yet surprisingly, we find that the models produced by the AutoDS group have higher quality and less errors, but lower human confidence scores. We reflect on the findings by presenting design implications for incorporating automation techniques into human work in the data science lifecycle.",c0,International Conference on Human Factors in Computing Systems,cp0,accepted,f1,2019,2019-10-29
s4,p4,"Data Science and Analytics: An Overview from Data-Driven Smart Computing, Decision-Making and Applications Perspective",Abstract content goes here ...,j2,SN Computer Science,jv2,accepted,f2,2014,2014-11-03
s5,p5,Computational Optimal Transport: With Applications to Data Science,"The goal of Optimal Transport (OT) is to define geometric tools that are useful to compare probability distributions. Their use dates back to 1781. Recent years have witnessed a new revolution in the spread of OT, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This monograph reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications. Computational Optimal Transport presents an overview of the main theoretical insights that support the practical effectiveness of OT before explaining how to turn these insights into fast computational schemes. Written for readers at all levels, the authors provide descriptions of foundational theory at two-levels. Generally accessible to all readers, more advanced readers can read the specially identified more general mathematical expositions of optimal transport tailored for discrete measures. Furthermore, several chapters deal with the interplay between continuous and discrete measures, and are thus targeting a more mathematically-inclined audience. This monograph will be a valuable reference for researchers and students wishing to get a thorough understanding of Computational Optimal Transport, a mathematical gem at the interface of probability, analysis and optimization.",c44,International Workshop on Green and Sustainable Software,cp44,accepted,f3,2008,2008-04-21
s7,p7,Data Science,"Data Science refers to an arising district of work regard the accumulation, arrangement, reasoning, imagination, administration, and protection of abundant groups of facts. Although the name Data Science appears to combine most powerfully accompanying extents to a degree databases and information technology, many various types of abilities containing nonmathematical abilities are still wanted attending. Data Science is much in addition to plainly analysing data. The main aim of Data Science search out turn big sets of two together unorganized and organized data into valuable news that can help organisations to create strong data-compelled resolutions. At a extreme level, data erudition maybe defined as a set of fundamental law unavoidable for profitable ancestry of news from data. Since we accumulate data continually and about everything, its use is various. The most average request is in healthcare, travel, e - trade, sports, management, public publishing, etc. The aim concerning this paper search out present data erudition and to present allure benefits and request in differing fields.",j4,"International Journal of Advanced Research in Science, Communication and Technology",jv4,accepted,f4,2003,2003-09-05
s8,p8,Eleven grand challenges in single-cell data science,Abstract content goes here ...,j5,Genome Biology,jv5,accepted,f5,2020,2020-05-14
s9,p9,Computing competencies for undergraduate data science curricula,Abstract content goes here ...,c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f6,2008,2008-03-26
s11,p11,Foundations of Data Science,"Computer science as an academic discipline began in the 1960’s. Emphasis was on programming languages, compilers, operating systems, and the mathematical theory that supported these areas. Courses in theoretical computer science covered finite automata, regular expressions, context-free languages, and computability. In the 1970’s, the study of algorithms was added as an important component of theory. The emphasis was on making computers useful. Today, a fundamental change is taking place and the focus is more on applications. There are many reasons for this change. The merging of computing and communications has played an important role. The enhanced ability to observe, collect, and store data in the natural sciences, in commerce, and in other fields calls for a change in our understanding of data and how to handle it in the modern setting. The emergence of the web and social networks as central aspects of daily life presents both opportunities and challenges for theory.",c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f7,2021,2021-09-17
s12,p12,Data science: a game changer for science and innovation,Abstract content goes here ...,j7,International Journal of Data Science and Analysis,jv7,accepted,f8,2001,2001-12-18
s13,p13,"QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science","We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science.",c10,Big Data,cp10,accepted,f9,2021,2021-11-03
s14,p14,Cybersecurity data science: an overview from machine learning perspective,Abstract content goes here ...,j8,Journal of Big Data,jv8,accepted,f10,2019,2019-12-30
s15,p15,Leveraging Data Science to Combat COVID-19: A Comprehensive Review,"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.",j9,IEEE Transactions on Artificial Intelligence,jv9,accepted,f11,2011,2011-03-29
s16,p16,"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",c74,IEEE International Conference on Tools with Artificial Intelligence,cp74,accepted,f12,2003,2003-01-15
s17,p17,"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.",c84,The Web Conference,cp84,accepted,f13,2006,2006-06-22
s18,p18,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",j10,Chemical Reviews,jv10,accepted,f14,2021,2021-12-03
s19,p19,Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework.",c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f15,2020,2020-09-13
s22,p22,Data science in economics: comprehensive review of advanced machine learning and deep learning methods,"This paper provides a state-of-the-art investigation of advances in data science in emerging economic applications. The analysis was performed on novel data science methods in four individual classes of deep learning models, hybrid deep learning models, hybrid machine learning, and ensemble models. Application domains include a wide and diverse range of economics research from the stock market, marketing, and e-commerce to corporate banking and cryptocurrency. Prisma method, a systematic literature review methodology, was used to ensure the quality of the survey. The findings reveal that the trends follow the advancement of hybrid models, which, based on the accuracy metric, outperform other learning algorithms. It is further expected that the trends will converge toward the advancements of sophisticated hybrid deep learning models.",c100,ACM SIGMOD Conference,cp100,accepted,f16,2010,2010-03-07
s23,p23,A new paradigm for accelerating clinical data science at Stanford Medicine,"Stanford Medicine is building a new data platform for our academic research community to do better clinical data science. Hospitals have a large amount of patient data and researchers have demonstrated the ability to reuse that data and AI approaches to derive novel insights, support patient care, and improve care quality. However, the traditional data warehouse and Honest Broker approaches that are in current use, are not scalable. We are establishing a new secure Big Data platform that aims to reduce time to access and analyze data. In this platform, data is anonymized to preserve patient data privacy and made available preparatory to Institutional Review Board (IRB) submission. Furthermore, the data is standardized such that analysis done at Stanford can be replicated elsewhere using the same analytical code and clinical concepts. Finally, the analytics data warehouse integrates with a secure data science computational facility to support large scale data analytics. The ecosystem is designed to bring the modern data science community to highly sensitive clinical data in a secure and collaborative big data analytics environment with a goal to enable bigger, better and faster science.",c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f17,2011,2011-09-04
s24,p24,Opening practice: supporting reproducibility and critical spatial data science,Abstract content goes here ...,j12,Journal of Geographical Systems,jv12,accepted,f18,2003,2003-10-26
s25,p25,Fixed Point Strategies in Data Science,"The goal of this article is to promote the use of fixed point strategies in data science by showing that they provide a simplifying and unifying framework to model, analyze, and solve a great variety of problems. They are seen to constitute a natural environment to explain the behavior of advanced convex optimization methods as well as of recent nonlinear methods in data science which are formulated in terms of paradigms that go beyond minimization concepts and involve constructs such as Nash equilibria or monotone inclusions. We review the pertinent tools of fixed point theory and describe the main state-of-the-art algorithms for provenly convergent fixed point construction. We also incorporate additional ingredients such as stochasticity, block-implementations, and non-Euclidean metrics, which provide further enhancements. Applications to signal and image processing, machine learning, statistics, neural networks, and inverse problems are discussed.",j13,IEEE Transactions on Signal Processing,jv13,accepted,f19,2013,2013-11-07
s26,p26,Heidelberg colorectal data set for surgical data science in the sensor operating room,Abstract content goes here ...,j14,Scientific Data,jv14,accepted,f20,2020,2020-02-27
s27,p27,Making data science systems work,"How are data science systems made to work? It may seem that whether a system works is a function of its technical design, but it is also accomplished through ongoing forms of discretionary work by many actors. Based on six months of ethnographic fieldwork with a corporate data science team, we describe how actors involved in a corporate project negotiated what work the system should do, how it should work, and how to assess whether it works. These negotiations laid the foundation for how, why, and to what extent the system ultimately worked. We describe three main findings. First, how already-existing technologies are essential reference points to determine how and whether systems work. Second, how the situated resolution of development challenges continually reshapes the understanding of how and whether systems work. Third, how business goals, and especially their negotiated balance with data science imperatives, affect a system’s working. We conclude with takeaways for critical data studies, orienting researchers to focus on the organizational and cultural aspects of data science, the third-party platforms underlying data science systems, and ways to engage with practitioners’ imagination of how systems can and should work.",c87,European Conference on Computer Vision,cp87,accepted,f21,2014,2014-11-27
s28,p28,The Role of Academia in Data Science Education,"As the demand for data scientists continues to grow, universities are trying to figure out how to best contribute to the training of a workforce. However, there does not appear to be a consensus on the fundamental principles, expertise, skills, or knowledge-base needed to define an academic discipline. We argue that data science is not a discipline but rather an umbrella term used to describe a complex process involving not one data scientist possessing all the necessary expertise, but a team of data scientists with nonoverlapping complementary skills. We provide some recommendations for how to take this into account when designing data science academic programs.Keywords: applied statistics, data science, data science curriculum, data wrangling, machine learning, software engineering",c41,Software Product Lines Conference,cp41,accepted,f22,2002,2002-07-21
s29,p29,Data Science: Challenges and Directions,"While data science has emerged as a contentious new scientific field, enormous debates and discussions have been made on it why we need data science and what makes it as a science. However, only a limited number of discussions are about intrinsic complexities and intelligence embedded in data science problems, and the gaps and opportunities for disciplinary directions. After a comprehensive review [5, 6, 9, 10, 12, 15, 18] of hundreds of literature that directly incorporates data science in their scopes, we make the following observations of the big data buzz and data science debate:",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f23,2017,2017-01-08
s30,p30,Glossary for public health surveillance in the age of data science,"Public health surveillance is the ongoing systematic collection, analysis and interpretation of data, closely integrated with the timely dissemination of the resulting information to those responsible for preventing and controlling disease and injury. With the rapid development of data science, encompassing big data and artificial intelligence, and with the exponential growth of accessible and highly heterogeneous health-related data, from healthcare providers to user-generated online content, the field of surveillance and health monitoring is changing rapidly. It is, therefore, the right time for a short glossary of key terms in public health surveillance, with an emphasis on new data-science developments in the field.",j15,Journal of Epidemiology and Community Health,jv15,accepted,f24,2012,2012-05-30
s32,p32,The Democratization of Data Science Education,"Abstract Over the last three decades, data have become ubiquitous and cheap. This transition has accelerated over the last five years and training in statistics, machine learning, and data analysis has struggled to keep up. In April 2014, we launched a program of nine courses, the Johns Hopkins Data Science Specialization, which has now had more than 4 million enrollments over the past five years. Here, the program is described and compared to standard data science curricula as they were organized in 2014 and 2015. We show that novel pedagogical and administrative decisions introduced in our program are now standard in online data science programs. The impact of the Data Science Specialization on data science education in the U.S. is also discussed. Finally, we conclude with some thoughts about the future of data science education in a data democratized world.",j16,American Statistician,jv16,accepted,f25,2010,2010-03-06
s33,p33,A Fresh Look at Introductory Data Science,"ABSTRACT The proliferation of vast quantities of available datasets that are large and complex in nature has challenged universities to keep up with the demand for graduates trained in both the statistical and the computational set of skills required to effectively plan, acquire, manage, analyze, and communicate the findings of such data. To keep up with this demand, attracting students early on to data science as well as providing them a solid foray into the field becomes increasingly important. We present a case study of an introductory undergraduate course in data science that is designed to address these needs. Offered at Duke University, this course has no prerequisites and serves a wide audience of aspiring statistics and data science majors as well as humanities, social sciences, and natural sciences students. We discuss the unique set of challenges posed by offering such a course, and in light of these challenges, we present a detailed discussion into the pedagogical design elements, content, structure, computational infrastructure, and the assessment methodology of the course. We also offer a repository containing all teaching materials that are open-source, along with supplementary materials and the R code for reproducing the figures found in the article.",j17,Journal of Statistics and Data Science Education,jv17,accepted,f26,2020,2020-07-16
s35,p35,Data science applications to string theory,Abstract content goes here ...,c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f27,2022,2022-03-03
s37,p37,Statistical Foundations of Data Science,Abstract content goes here ...,c109,International Conference on Mobile Data Management,cp109,accepted,f28,2014,2014-02-08
s39,p39,"How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation","With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.",c0,International Conference on Human Factors in Computing Systems,cp0,accepted,f29,2019,2019-11-23
s40,p40,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",c97,Interspeech,cp97,accepted,f30,2004,2004-12-07
s41,p41,Process Mining for Python (PM4Py): Bridging the Gap Between Process- and Data Science,"Process mining, i.e., a sub-field of data science focusing on the analysis of event data generated during the execution of (business) processes, has seen a tremendous change over the past two decades. Starting off in the early 2000's, with limited to no tool support, nowadays, several software tools, i.e., both open-source, e.g., ProM and Apromore, and commercial, e.g., Disco, Celonis, ProcessGold, etc., exist. The commercial process mining tools provide limited support for implementing custom algorithms. Moreover, both commercial and open-source process mining tools are often only accessible through a graphical user interface, which hampers their usage in large-scale experimental settings. Initiatives such as RapidProM provide process mining support in the scientific workflow-based data science suite RapidMiner. However, these offer limited to no support for algorithmic customization. In the light of the aforementioned, in this paper, we present a novel process mining library, i.e. Process Mining for Python (PM4Py) that aims to bridge this gap, providing integration with state-of-the-art data science libraries, e.g., pandas, numpy, scipy and scikit-learn. We provide a global overview of the architecture and functionality of PM4Py, accompanied by some representative examples of its usage.",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f31,2018,2018-08-27
s43,p43,A Systematic Review on Supervised and Unsupervised Machine Learning Algorithms for Data Science,Abstract content goes here ...,c75,International Conference on Machine Learning,cp75,accepted,f32,2005,2005-07-28
s44,p44,Veridical data science,"Significance Predictability, computability, and stability (PCS) are three core principles of data science. They embed the scientific principles of prediction and replication in data-driven decision making while recognizing the central role of computation. Based on these principles, we propose the PCS framework, including workflow and documentation (in R Markdown or Jupyter Notebook). The PCS framework aims at responsible, reliable, reproducible, and transparent analysis across fields of science, social science, engineering, business, and government. It can be used as a recommendation system for scientific hypothesis generation and experimental design. In particular, we propose (basic) PCS inference for reliability measures on data results, extending statistical inference to a much broader scope as current data science practice entails. Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.",j20,Proceedings of the National Academy of Sciences of the United States of America,jv20,accepted,f33,2006,2006-12-09
s45,p45,Outbreak analytics: a developing data science for informing the response to emerging pathogens,"Despite continued efforts to improve health systems worldwide, emerging pathogen epidemics remain a major public health concern. Effective response to such outbreaks relies on timely intervention, ideally informed by all available sources of data. The collection, visualization and analysis of outbreak data are becoming increasingly complex, owing to the diversity in types of data, questions and available methods to address them. Recent advances have led to the rise of outbreak analytics, an emerging data science focused on the technological and methodological aspects of the outbreak data pipeline, from collection to analysis, modelling and reporting to inform outbreak response. In this article, we assess the current state of the field. After laying out the context of outbreak response, we critically review the most common analytics components, their inter-dependencies, data requirements and the type of information they can provide to inform operations in real time. We discuss some challenges and opportunities and conclude on the potential role of outbreak analytics for improving our understanding of, and response to outbreaks of emerging pathogens. This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control‘. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.",j21,Philosophical Transactions of the Royal Society of London. Biological Sciences,jv21,accepted,f34,2010,2010-11-22
s46,p46,"Practitioners Teaching Data Science in Industry and Academia: Expectations, Workflows, and Challenges","Data science has been growing in prominence across both academia and industry, but there is still little formal consensus about how to teach it. Many people who currently teach data science are practitioners such as computational researchers in academia or data scientists in industry. To understand how these practitioner-instructors pass their knowledge onto novices and how that contrasts with teaching more traditional forms of programming, we interviewed 20 data scientists who teach in settings ranging from small-group workshops to large online courses. We found that: 1) they must empathize with a diverse array of student backgrounds and expectations, 2) they teach technical workflows that integrate authentic practices surrounding code, data, and communication, 3) they face challenges involving authenticity versus abstraction in software setup, finding and curating pedagogically-relevant datasets, and acclimating students to live with uncertainty in data analysis. These findings can point the way toward better tools for data science education and help bring data literacy to more people around the world.",c0,International Conference on Human Factors in Computing Systems,cp0,accepted,f35,2019,2019-10-08
s48,p48,Data science ethical considerations: a systematic literature review and proposed project framework,Abstract content goes here ...,j23,Ethics and Information Technology,jv23,accepted,f36,2018,2018-06-29
s49,p49,Toward collaborative open data science in metabolomics using Jupyter Notebooks and cloud computing,Abstract content goes here ...,j24,Metabolomics,jv24,accepted,f37,2002,2002-10-15
s50,p50,Health Care and Precision Medicine Research: Analysis of a Scalable Data Science Platform,"Background Health care data are increasing in volume and complexity. Storing and analyzing these data to implement precision medicine initiatives and data-driven research has exceeded the capabilities of traditional computer systems. Modern big data platforms must be adapted to the specific demands of health care and designed for scalability and growth. Objective The objectives of our study were to (1) demonstrate the implementation of a data science platform built on open source technology within a large, academic health care system and (2) describe 2 computational health care applications built on such a platform. Methods We deployed a data science platform based on several open source technologies to support real-time, big data workloads. We developed data-acquisition workflows for Apache Storm and NiFi in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results Emerging data management approaches, along with open source technologies such as Hadoop, can be used to create integrated data lakes to store large, real-time datasets. This infrastructure also provides a robust analytics platform where health care and biomedical research data can be analyzed in near real time for precision medicine and computational health care use cases. Conclusions The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional datasets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to health care data for computational health care and precision medicine research.",j25,Journal of Medical Internet Research,jv25,accepted,f38,2008,2008-07-02
s52,p52,Human-Centered Study of Data Science Work Practices,"With the rise of big data, there has been an increasing need to understand who is working in data science and how they are doing their work. HCI and CSCW researchers have begun to examine these questions. In this workshop, we invite researchers to share their observations, experiences, hypotheses, and insights, in the hopes of developing a taxonomy of work practices and open issues in the behavioral and social study of data science and data science workers.",c100,ACM SIGMOD Conference,cp100,accepted,f39,2010,2010-03-05
s53,p53,Data Science,"The presence of new science does not necessarily occur just like that. Every science starts from interests, discussion, and looks for a basic foundation, but in general the main foundation of science is mathematics. Data science includes structured and systematic knowledge about data. However, many other sciences that has a relationship with the data in question, ranging from statistics to computer science. This paper aims to reveal the obstacle and limitations of other science into a data science completely, on that basis the definition of data sciences needs to be elaborated, then confirm data science as new science and not depend directly on several other sciences.",c90,Computer Vision and Pattern Recognition,cp90,accepted,f40,2008,2008-12-26
s54,p54,50 Years of Data Science,"ABSTRACT More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a $100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f41,2011,2011-01-11
s55,p55,Data science in data librarianship: Core competencies of a data librarian,"Currently, data are stored in an always-on condition, and can be globally accessed at any point, by any user. Data librarianship has its origins in the social sciences. In particular, the creation of data services and data archives, in the United Kingdom (Data Archives Services) and in the United States and Canada (Data Library Services), is a key factor for the emergence of data librarianship. The focus of data librarianship nowadays is on the creation of new library services. Data librarians are concerned with the proposition of services for data management and curation in academic libraries and other research organizations. The purpose of this paper is to understand how the complexity of the data can serve as the basis for identifying the technical skills required by data librarians. This essay is systematically divided, first introducing the concepts of data and research data in data librarianship, followed by an overview of data science as a theory, method, and technology to assess data. Next, the identification of the competencies and skills required by data scientists and data librarians are discussed. Our final remarks highlight that data librarians should understand that the complexity and novelty associated with data science praxis. Data science provides new methods and practices for data librarianship. A data librarian need not become a programmer, statistician, or database manager, but should be interested in learning about the languages and programming logic of computers, databases, and information retrieval tools. We believe that numerous kinds of scientific data research provide opportunities for a data librarian to engage with data science.",j27,Journal of Library and Information Sciences,jv27,accepted,f42,2003,2003-02-10
s56,p56,A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",j28,CHANCE : New Directions for Statistics and Computing,jv28,accepted,f43,2014,2014-09-01
s57,p57,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",j29,British Journal of Educational Technology,jv29,accepted,f44,2007,2007-03-30
s58,p58,Genomics and data science: an application within an umbrella,Abstract content goes here ...,j5,Genome Biology,jv5,accepted,f45,2020,2020-03-18
s60,p60,Process Mining: Data Science in Action,"This is the second edition of Wil van der Aalsts seminal book on process mining, which now discusses the field also in the broader context of data science and big data approaches. It includes several additions and updates, e.g. on inductive mining techniques, the notion of alignments, a considerably expanded section on software tools and a completely new chapter of process mining in the large. It is self-contained, while at the same time covering the entire process-mining spectrum from process discovery to predictive analytics. After a general introduction to data science and process mining in Part I, Part II provides the basics of business process modeling and data mining necessary to understand the remainder of the book. Next, Part III focuses on process discovery as the most important process mining task, while Part IV moves beyond discovering the control flow of processes, highlighting conformance checking, and organizational and time perspectives. Part V offers a guide to successfully applying process mining in practice, including an introduction to the widely used open-source tool ProM and several commercial products. Lastly, Part VI takes a step back, reflecting on the material presented and the key open challenges. Overall, this book provides a comprehensive overview of the state of the art in process mining. It is intended for business process analysts, business consultants, process managers, graduate students, and BPM researchers.",c6,Americas Conference on Information Systems,cp6,accepted,f46,2007,2007-03-17
s61,p61,Upscaling urban data science for global climate solutions,"Non-technical summary Manhattan, Berlin and New Delhi all need to take action to adapt to climate change and to reduce greenhouse gas emissions. While case studies on these cities provide valuable insights, comparability and scalability remain sidelined. It is therefore timely to review the state-of-the-art in data infrastructures, including earth observations, social media data, and how they could be better integrated to advance climate change science in cities and urban areas. We present three routes for expanding knowledge on global urban areas: mainstreaming data collections, amplifying the use of big data and taking further advantage of computational methods to analyse qualitative data to gain new insights. These data-based approaches have the potential to upscale urban climate solutions and effect change at the global scale. Technical summary Cities have an increasingly integral role in addressing climate change. To gain a common understanding of solutions, we require adequate and representative data of urban areas, including data on related greenhouse gas emissions, climate threats and of socio-economic contexts. Here, we review the current state of urban data science in the context of climate change, investigating the contribution of urban metabolism studies, remote sensing, big data approaches, urban economics, urban climate and weather studies. We outline three routes for upscaling urban data science for global climate solutions: 1) Mainstreaming and harmonizing data collection in cities worldwide; 2) Exploiting big data and machine learning to scale solutions while maintaining privacy; 3) Applying computational techniques and data science methods to analyse published qualitative information for the systematization and understanding of first-order climate effects and solutions. Collaborative efforts towards a joint data platform and integrated urban services would provide the quantitative foundations of the emerging global urban sustainability science.",c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f47,2005,2005-10-01
s62,p62,The Challenge of Big Data and Data Science,"Big data and data science are transforming the world in ways that spawn new concerns for social scientists, such as the impacts of the internet on citizens and the media, the repercussions of smart cities, the possibilities of cyber-warfare and cyber-terrorism, the implications of precision medicine, and the consequences of artificial intelligence and automation. Along with these changes in society, powerful new data science methods support research using administrative, internet, textual, and sensor-audio-video data. Burgeoning data and innovative methods facilitate answering previously hard-to-tackle questions about society by offering new ways to form concepts from data, to do descriptive inference, to make causal inferences, and to generate predictions. They also pose challenges as social scientists must grasp the meaning of concepts and predictions generated by convoluted algorithms, weigh the relative value of prediction versus causal inference, and cope with ethical challenges as their methods, such as algorithms for mobilizing voters or determining bail, are adopted by policy makers.",j30,"Annual review of political science (Palo Alto, Calif. Print)",jv30,accepted,f48,2019,2019-04-14
s63,p63,INTRODUCTION TO DATA SCIENCE,データサイエンスの概説 データ獲得法 国と統計データ データの形式と分析の準備 データの変換と加工 データをグラフに表す 統計量によるデータの要約 ２次元の量的データの記述 回帰分析の基礎 質的データの分析 社会現象データの分析事例 データ解析のソフトウェア,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f49,2005,2005-01-25
s64,p64,Data Science,"Program of Study The technological revolution has led to an explosion of data in domains of knowledge including medicine, policy, social sciences, commerce, and the natural sciences. Petabytes of data are being collected from a myriad of instruments, like sequencing machines for genomics and mobile devices for quantifying social interactions. In addition to driving research, data are shaping the way people work, live, and communicate. Correspondingly, new methodologies have emerged to power intelligent systems, make more accurate predictions, and gain new insight using the large volumes of data generated by scientists, entrepreneurs, and analysts.",c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f50,2019,2019-08-25
s65,p65,"Our data, our society, our health: A vision for inclusive and transparent health data science in the United Kingdom and beyond","The last 6 years have seen sustained investment in health data science in the United Kingdom and beyond, which should result in a data science community that is inclusive of all stakeholders, working together to use data to benefit society through the improvement of public health and well‐being.",c0,International Conference on Human Factors in Computing Systems,cp0,accepted,f51,2019,2019-09-19
s66,p66,Situating Data Science: Exploring How Relationships to Data Shape Learning,"The emerging field of Data Science has had a large impact on science and society. This has led to over a decade of calls to establish a corresponding field of Data Science Education. There is still a need, however, to more deeply conceptualize what a field of Data Science Education might entail in terms of scope, responsibility, and execution. This special issue explores how one distinguishing feature of Data Science—its focus on data collected from social and environmental contexts within which learners often find themselves deeply embedded—suggests serious implications for learning and education. The learning sciences is uniquely positioned to investigate how such contextual embeddings impact learners’ engagement with data including conceptual, experiential, communal, racialized, spatial, and political dimensions. This special issue demonstrates the richly layered relationships learners build with data and reveals them to be not merely utilitarian mechanisms for learning about data, but a critical part of navigating data as social text and understanding Data Science as a discipline. Together, the contributions offer a vision of how the learning sciences can contribute to a more expansive, agentive and socially aware Data Science Education.",j31,The Journal of the Learning Sciences,jv31,accepted,f52,2019,2019-09-14
s67,p67,A Data Science Framework for Movement,"Author(s): Dodge, S | Abstract: © 2019 The Ohio State University Movement is the driving force behind the form and function of many ecological and human systems. Identification and analysis of movement patterns that may relate to the behavior of individuals and their interactions is a fundamental first step in understanding these systems. With advances in IoT and the ubiquity of smart connected sensors to collect movement and contextual data, we now have access to a wealth of geo-enriched high-resolution tracking data. These data promise new forms of knowledge and insight into movement of humans, animals, and goods, and hence can increase our understanding of complex spatiotemporal processes such as disease outbreak, urban mobility, migration, and human-species interaction. To take advantage of the evolution in our data, we need a revolution in how we visualize, model, and analyze movement as a multidimensional process that involves space, time, and context. This paper introduces a data science paradigm with the aim of advancing research on movement.",j32,Geographical Analysis,jv32,accepted,f53,2015,2015-12-09
s69,p69,Geographic Data Science,"It is widely acknowledged that the emergence of “Big Data” is having a profound and often controversial impact on the production of knowledge. In this context, Data Science has developed as an interdisciplinary approach that turns such “Big Data” into information. This article argues for the positive role that Geography can have on Data Science when being applied to spatially explicit problems; and inversely, makes the case that there is much that Geography and Geographical Analysis could learn from Data Science. We propose a deeper integration through an ambitious research agenda, including systems engineering, new methodological development, and work toward addressing some acute challenges around epistemology. We argue that such issues must be resolved in order to realize a Geographic Data Science, and that such goal would be a desirable one.",j32,Geographical Analysis,jv32,accepted,f54,2015,2015-03-23
s70,p70,Trinity: An Extensible Synthesis Framework for Data Science,"In this demo paper, we introduce Trinity, a general-purpose framework that can be used to quickly build domain-specific program synthesizers for automating many tedious tasks that arise in data science. We illustrate how Trinity can be used by three different users: First, we show how endusers can use Trinity’s built-in synthesizers to automate data wrangling tasks. Second, we show how advanced users can easily extend existing synthesizers to support additional functionalities. Third, we show how synthesis experts can change the underlying search engine in Trinity. Overall, this paper is intended to demonstrate how users can quickly use, modify, and extend the Trinity framework with the goal of automating many tasks that are considered to be the “janitor” work of data science. PVLDB Reference Format: Ruben Martins, Jia Chen, Yanju Chen, Yu Feng, and Isil Dillig. Trinity: An Extensible Synthesis Framework for Data Science. PVLDB, 12(12): 1914-1917, 2019. DOI: https://doi.org/10.14778/3352063.3352098",j34,Proceedings of the VLDB Endowment,jv34,accepted,f55,2001,2001-03-16
s72,p72,Data Science and Machine Learning,"The purpose of Data Science and Machine Learning: Mathematical and Statistical Methods is to provide an accessible, yet comprehensive textbook intended for students interested in gaining a better understanding of the mathematics and statistics that underpin the rich variety of ideas and machine learning algorithms in data science.",c64,Experimental Software Engineering Network,cp64,accepted,f56,2014,2014-06-26
s73,p73,Data science from a library and information science perspective,"
Purpose
Data science is a relatively new field which has gained considerable attention in recent years. This new field requires a wide range of knowledge and skills from different disciplines including mathematics and statistics, computer science and information science. The purpose of this paper is to present the results of the study that explored the field of data science from the library and information science (LIS) perspective.


Design/methodology/approach
Analysis of research publications on data science was made on the basis of papers published in the Web of Science database. The following research questions were proposed: What are the main tendencies in publication years, document types, countries of origin, source titles, authors of publications, affiliations of the article authors and the most cited articles related to data science in the field of LIS? What are the main themes discussed in the publications from the LIS perspective?


Findings
The highest contribution to data science comes from the computer science research community. The contribution of information science and library science community is quite small. However, there has been continuous increase in articles from the year 2015. The main document types are journal articles, followed by conference proceedings and editorial material. The top three journals that publish data science papers from the LIS perspective are the Journal of the American Medical Informatics Association, the International Journal of Information Management and the Journal of the Association for Information Science and Technology. The top five countries publishing are USA, China, England, Australia and India. The most cited article has got 112 citations. The analysis revealed that the data science field is quite interdisciplinary by nature. In addition to the field of LIS the papers belonged to several other research areas. The reviewed articles belonged to the six broad categories: data science education and training; knowledge and skills of the data professional; the role of libraries and librarians in the data science movement; tools, techniques and applications of data science; data science from the knowledge management perspective; and data science from the perspective of health sciences.


Research limitations/implications
The limitations of this research are that this study only analyzed research papers in the Web of Science database and therefore only covers a certain amount of scientific papers published in the field of LIS. In addition, only publications with the term “data science” in the topic area of the Web of Science database were analyzed. Therefore, several relevant studies are not discussed in this paper that are not reflected in the Web of Science database or were related to other keywords such as “e-science,” “e-research,” “data service,” “data curation” or “research data management.”


Originality/value
The field of data science has not been explored using bibliographic analysis of publications from the perspective of the LIS. This paper helps to better understand the field of data science and the perspectives for information professionals.
",c76,International Conference on Artificial Neural Networks,cp76,accepted,f57,2013,2013-03-17
s74,p74,A First Course in Data Science,"Abstract Data science is a discipline that provides principles, methodology, and guidelines for the analysis of data for tools, values, or insights. Driven by a huge workforce demand, many academic institutions have started to offer degrees in data science, with many at the graduate, and a few at the undergraduate level. Curricula may differ at different institutions, because of varying levels of faculty expertise, and different disciplines (such as mathematics, computer science, and business) in developing the curriculum. The University of Massachusetts Dartmouth started offering degree programs in data science from Fall 2015, at both the undergraduate and the graduate level. Quite a few articles have been published that deal with graduate data science courses, much less so dealing with undergraduate ones. Our discussion will focus on undergraduate course structure and function, and specifically, a first course in data science. Our design of this course centers around a concept called the data science life cycle. That is, we view tasks or steps in the practice of data science as forming a process, consisting of states that indicate how it comes into life, how different tasks in data science depend on or interact with others until the birth of a data product or a conclusion. Naturally, different pieces of the data science life cycle then form individual parts of the course. Details of each piece are filled up by concepts, techniques, or skills that are popular in industry. Consequently, the design of our course is both “principled” and practical. A significant feature of our course philosophy is that, in line with activity theory, the course is based on the use of tools to transform real data to answer strongly motivated questions related to the data.",j36,Journal of Statistics Education,jv36,accepted,f58,2007,2007-09-12
s75,p75,ACM Task Force on Data Science Education: Draft Report and Opportunity for Feedback,"The ACM Data Science Task Force was established by the ACM Education Council and tasked with articulating the role of computing discipline-specific contributions to this emerging field. This special session seeks to introduce the work of the ACM Data Science Task Force as well as to engage the SIGCSE community in this effort. Members of the task force will introduce key components of a draft report, including a summary of data science curricular efforts to date, results of ACM academic and industry surveys on data science, as well as the initial articulation of computing competencies for undergraduate programs in data science. This session should be of interest to all SIGCSE attendees, but especially faculty developing college-level curricula in Data Science.",c1,Technical Symposium on Computer Science Education,cp1,accepted,f59,2002,2002-09-11
s77,p77,What is responsible and sustainable data science?,"In the expansion of health ecosystems, issues of responsibility and sustainability of the data science involved are central. The idea that these values should be central to the practice of data science is increasingly gaining traction, yet there is no agreement on what exactly makes data science responsible or sustainable because these concepts prove slippery when applied to a global field involving commercial, academic and governmental actors. This lack of clarity is causing problems in setting goals and boundaries for data scientific practice, and risks fundamental disagreement on governance principles for this emerging field. We will argue in this commentary for a commons analytical framework as one approach to this problem, since it offers useful signposts for how to establish governance principles for shared resources.",j38,Big Data & Society,jv38,accepted,f60,2019,2019-07-31
s79,p79,Data Science Support at the Academic Library,"Abstract Data science is a rapidly growing field with applications across all scientific domains. The demand for support in data science literacy is outpacing available resources at college campuses. The academic library is uniquely positioned to provide training and guidance in a number of areas relevant to data science. The University of Arizona Libraries has built a successful data science support program, focusing on computational literacy, geographic information systems, and reproducible science. Success of the program has largely been due to the strength of library personnel and strategic partnerships with units outside of the library. Academic libraries can support campus data science needs through professional development of current staff and recruitment of new personnel with expertise in data-intensive domains.",j39,Journal of Library Administration,jv39,accepted,f61,2007,2007-09-18
s80,p80,Data Science for Local Government,"The Data Science for Local Government project was about understanding how the growth of ‘data science’ is changing the way that local government works in the UK. We define data science as a dual shift which involves both bringing in new decision making and analytical techniques to local government work (e.g. machine learning and predictive analytics, artificial intelligence and A/B testing) and also expanding the types of data local government makes use of (for example, by repurposing administrative data, harvesting social media data, or working with mobile phone companies). The emergence of data science is facilitated by the growing availability of free, open-source tools for both collecting data and performing analysis. Based on extensive documentary review, a nationwide survey of local authorities, and in-depth interviews with over 30 practitioners, we have sought to produce a comprehensive guide to the different types of data science being undertaken in the UK, the types of opportunities and benefits created, and also some of the challenges and difficulties being encountered. Our aim was to provide a basis for people working in local government to start on their own data science projects, both by providing a library of dozens of ideas which have been tried elsewhere and also by providing hints and tips for overcoming key problems and challenges.",j40,Social Science Research Network,jv40,accepted,f62,2017,2017-10-25
s81,p81,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",j41,Journal of Physics: Condensed Matter,jv41,accepted,f63,2019,2019-10-31
s82,p82,Data Science as Political Action: Grounding Data Science in a Politics of Justice,"In response to recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics. Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people's lives. I justify this notion in two parts: first, by articulating why data scientists must recognize themselves as political actors, and second, by describing how the field can evolve toward a deliberative and rigorous grounding in a politics of social justice. Part 1 responds to three arguments that are commonly invoked by data scientists when they are challenged to take political positions regarding their work. In confronting these arguments, I will demonstrate why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why the field's current attempts to promote ""social good"" dangerously rely on vague and unarticulated political assumptions. Part 2 proposes a framework for what a politically-engaged data science could look like and how to achieve it, recognizing the challenge of reforming the field in this manner. I conceptualize the process of incorporating politics into data science in four stages: becoming interested in directly addressing social issues, recognizing the politics underlying these issues, redirecting existing methods toward new applications, and, finally, developing new practices and methods that orient data science around a mission of social justice. The path ahead does not require data scientists to abandon their technical expertise, but it does entail expanding their notions of what problems to work on and how to engage with society.",j42,Journal of Social Computing,jv42,accepted,f64,2011,2011-11-22
s84,p84,Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data,"Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.",j1,IEEE Transactions on Knowledge and Data Engineering,jv1,accepted,f65,2020,2020-07-20
s86,p86,Trust in Data Science,"The trustworthiness of data science systems in applied and real-world settings emerges from the resolution of specific tensions through situated, pragmatic, and ongoing forms of work. Drawing on research in CSCW, critical data studies, and history and sociology of science, and six months of immersive ethnographic fieldwork with a corporate data science team, we describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantification, but also on negotiation and translation. We conclude by discussing the implications of our findings for data science research and practice, both within and beyond CSCW.",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f66,2017,2017-12-23
s87,p87,Data Science: the impact of statistics,Abstract content goes here ...,j7,International Journal of Data Science and Analysis,jv7,accepted,f67,2001,2001-02-19
s88,p88,Big Data and Data Science in Critical Care.,Abstract content goes here ...,j43,Chest,jv43,accepted,f68,2002,2002-01-06
s89,p89,Smart Blockchain Badges for Data Science Education,"Blockchain technology has the potential to revolutionise education in a number of ways. In this paper, we explore the applications of Smart Blockchain Badges on data science education. In particular, we investigate how Smart Blockchain Badges can support learners that want to advance their careers in data science, by offering them personalised recommendations based on their learning achievements. This work aims at enhancing data science accreditation by introducing a robust system based on the Blockchain technology. Learners will benefit from a sophisticated, open and transparent accreditation system, as well as from receiving job recommendations that match their skills and can potentially progress their careers. As a result, this work contributes towards closing the data science skills gap by linking data science education to the industry.",c3,Frontiers in Education Conference,cp3,accepted,f69,2016,2016-07-19
s90,p90,Searching for Hidden Perovskite Materials for Photovoltaic Systems by Combining Data Science and First Principle Calculations,"Undiscovered perovskite materials for applications in capturing solar lights are explored through the implementation of data science. In particular, 15000 perovskite materials data is analyzed where visualization of the data reveals hidden trends and clustering of data. Random forest classification within machine learning is used in order to predict the band gap of perovskite materials where 18 physical descriptors are revealed to determine the band gap. With trained random forest, 9328 perovskite materials with potential for applications in solar cell materials are predicted. The selected Li and Na based perovskite materials within predicted 9328 perovskite materials are evaluated with first principle calculations where 11 undiscovered Li(Na) based perovskite materials fall into the ideal band gap and formation energy ranges for solar cell applications. Thus, the implementation of data science accelerates the discovery of hidden perovskite materials and the approach can be applied to the materials scienc...",c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f70,2014,2014-05-22
s91,p91,Data Science as Machinic Neoplatonism,Abstract content goes here ...,c69,International Conference on Parallel Processing,cp69,accepted,f71,2010,2010-12-31
s92,p92,Network embedding in biomedical data science,"Owning to the rapid development of computer technologies, an increasing number of relational data have been emerging in modern biomedical research. Many network-based learning methods have been proposed to perform analysis on such data, which provide people a deep understanding of topology and knowledge behind the biomedical networks and benefit a lot of applications for human healthcare. However, most network-based methods suffer from high computational and space cost. There remain challenges on handling high dimensionality and sparsity of the biomedical networks. The latest advances in network embedding technologies provide new effective paradigms to solve the network analysis problem. It converts network into a low-dimensional space while maximally preserves structural properties. In this way, downstream tasks such as link prediction and node classification can be done by traditional machine learning methods. In this survey, we conduct a comprehensive review of the literature on applying network embedding to advance the biomedical domain. We first briefly introduce the widely used network embedding models. After that, we carefully discuss how the network embedding approaches were performed on biomedical networks as well as how they accelerated the downstream tasks in biomedical science. Finally, we discuss challenges the existing network embedding applications in biomedical domains are faced with and suggest several promising future directions for a better improvement in human healthcare.",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f72,2018,2018-10-08
s93,p93,"Situating Ecology as a Big-Data Science: Current Advances, Challenges, and Solutions","Ecology has joined a world of big data. Two complementary frameworks define big data: data that exceed the analytical capacities of individuals or disciplines or the “Four Vs” axes of volume, variety, veracity, and velocity. Variety predominates in ecoinformatics and limits the scalability of ecological science. Volume varies widely. Ecological velocity is low but growing as data throughput and societal needs increase. Ecological big-data systems include in situ and remote sensors, community data resources, biodiversity databases, citizen science, and permanent stations. Technological solutions include the development of open code- and data-sharing platforms, flexible statistical models that can handle heterogeneous data and sources of uncertainty, and cloud-computing delivery of high-velocity computing to large-volume analytics. Cultural solutions include training targeted to early and current scientific workforce and strengthening collaborations among ecologists and data scientists. The broader goal is to maximize the power, scalability, and timeliness of ecological insights and forecasting.",j44,BioScience,jv44,accepted,f73,2022,2022-02-05
s94,p94,Fundamentals of Clinical Data Science,Abstract content goes here ...,c65,Formal Concept Analysis,cp65,accepted,f74,2008,2008-06-30
s95,p95,Data science,"While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.",j37,Communications of the ACM,jv37,accepted,f75,2011,2011-03-18
s96,p96,Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science,"As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.",c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f76,2019,2019-11-19
s99,p99,Data Science for Undergraduates,Abstract content goes here ...,c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f77,2014,2014-04-24
s100,p100,A Position Statement on Population Data Science: The Science of Data about People,"Information is increasingly digital, creating opportunities to respond to pressing issues about human populations using linked datasets that are large, complex, and diverse. The potential social and individual benefits that can come from data-intensive science are large, but raise challenges of balancing individual privacy and the public good, building appropriate socio-technical systems to support data-intensive science, and determining whether defining a new field of inquiry might help move those collective interests and activities forward. A combination of expert engagement, literature review, and iterative conversations led to our conclusion that defining the field of Population Data Science (challenge 3) will help address the other two challenges as well. We define Population Data Science succinctly as the science of data about people and note that it is related to but distinct from the fields of data science and informatics. A broader definition names four characteristics of: data use for positive impact on citizens and society; bringing together and analyzing data from multiple sources; finding population-level insights; and developing safe, privacy-sensitive and ethical infrastructure to support research. One implication of these characteristics is that few people possess all of the requisite knowledge and skills of Population Data Science, so this is by nature a multi-disciplinary field. Other implications include the need to advance various aspects of science, such as data linkage technology, various forms of analytics, and methods of public engagement. These implications are the beginnings of a research agenda for Population Data Science, which if approached as a collective field, can catalyze significant advances in our understanding of trends in society, health, and human behavior.",j45,International Journal of Population Data Science,jv45,accepted,f78,2019,2019-11-12
s101,p101,Progressive Data Science: Potential and Challenges,"Data science requires time-consuming iterative manual activities. In particular, activities such as data selection, preprocessing, transformation, and mining, highly depend on iterative trial-and-error processes that could be sped up significantly by providing quick feedback on the impact of changes. The idea of progressive data science is to compute the results of changes in a progressive manner, returning a first approximation of results quickly and allow iterative refinements until converging to a final result. Enabling the user to interact with the intermediate results allows an early detection of erroneous or suboptimal choices, the guided definition of modifications to the pipeline and their quick assessment. In this paper, we discuss the progressiveness challenges arising in different steps of the data science pipeline. We describe how changes in each step of the pipeline impact the subsequent steps and outline why progressive data science will help to make the process more effective. Computing progressive approximations of outcomes resulting from changes creates numerous research challenges, especially if the changes are made in the early steps of the pipeline. We discuss these challenges and outline first steps towards progressiveness, which, we argue, will ultimately help to significantly speed-up the overall data science process.",c15,International Conference on Conceptual Structures,cp15,accepted,f79,2011,2011-07-07
s102,p102,Milo: A visual programming environment for Data Science Education,"Most courses on Data Science offered at universities or online require students to have familiarity with at least one programming language. In this paper, we present, “Milo”, a web-based visual programming environment for Data Science Education, designed as a pedagogical tool that can be used by students without prior-programming experience. To that end, Milo uses graphical blocks as abstractions of language specific implementations of Data Science and Machine Learning(ML) concepts along with creation of interactive visualizations. Using block definitions created by a user, Milo generates equivalent source code in JavaScript to run entirely in the browser. Based on a preliminary user study with a focus group of undergraduate computer science students, Milo succeeds as an effective tool for novice learners in the field of Data Science.",c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f80,2001,2001-08-09
s103,p103,Data science for finite strain mechanical science of ductile materials,Abstract content goes here ...,j46,Computational Mechanics,jv46,accepted,f81,2002,2002-03-05
s104,p104,Environmental Data Science,Abstract content goes here ...,j47,Environmental Modelling & Software,jv47,accepted,f82,2002,2002-10-25
s105,p105,Twinning data science with information science in schools of library and information science,"As an emerging discipline, data science represents a vital new current of school of library and information science (LIS) education. However, it remains unclear how it relates to information science within LIS schools. The purpose of this paper is to clarify this issue.,Mission statement and nature of both data science and information science are analyzed by reviewing existing work in the two disciplines and drawing DIKW hierarchy. It looks at the ways in which information science theories bring new insights and shed new light on fundamentals of data science.,Data science and information science are twin disciplines by nature. The mission, task and nature of data science are consistent with those of information science. They greatly overlap and share similar concerns. Furthermore, they can complement each other. LIS school should integrate both sciences and develop organizational ambidexterity. Information science can make unique contributions to data science research, including conception of data, data quality control, data librarianship and theory dualism. Document theory, as a promising direction of unified information science, should be introduced to data science to solve the disciplinary divide.,The results of this paper may contribute to the integration of data science and information science within LIS schools and iSchools. It has particular value for LIS school development and reform in the age of big data.",c63,IEEE International Software Metrics Symposium,cp63,accepted,f83,2008,2008-06-09
s106,p106,Exploring Project Management Methodologies Used Within Data Science Teams,"There are many reasons data science teams should use a well-defined process to manage and coordinate their efforts, such as improved collaboration, efficiency and stakeholder communication. This paper explores the current methodology data science teams use to manage and coordinate their efforts. Unfortunately, based on our survey results, most data science teams currently use an ad hoc project management approach. In fact, 82% of the data scientists surveyed did not follow an explicit process. However, it is encouraging to note that 85% of the respondents thought that adopting an improved process methodology would improve the teams’ outcomes. Based on these results, we described six possible process methodologies teams could use. To conclude, we outlined plans to describe best practices for data science team processes and to develop a process evaluation framework.",c6,Americas Conference on Information Systems,cp6,accepted,f84,2007,2007-12-07
s108,p108,Shifting to Data Savvy: The Future of Data Science In Libraries,"The Data Science in Libraries Project is funded by the Institute for Museum and Library Services (IMLS) and led by Matt Burton and Liz Lyon, School of Computing & Information, University of Pittsburgh; Chris Erdmann, North Carolina State University; and Bonnie Tijerina, Data & Society. The project explores the challenges associated with implementing data science within diverse library environments by examining two specific perspectives framed as ‘the skills gap,’ i.e. where librarians are perceived to lack the technical skills to be effective in a data-rich research environment; and ‘the management gap,’ i.e. the ability of library managers to understand and value the benefits of in-house data science skills and to provide organizational and managerial support. 
 
This report primarily presents a synthesis of the discussions, findings, and reflections from an international, two-day workshop held in May 2017 in Pittsburgh, where community members participated in a program with speakers, group discussions, and activities to drill down into the challenges of successfully implementing data science in libraries. Participants came from funding organizations, academic and public libraries, nonprofits, and commercial organizations with most of the discussions focusing on academic libraries and library schools.",c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,cp20,accepted,f85,2014,2014-02-15
s109,p109,Drafting a Data Science Curriculum for Secondary Schools,"Data science as the art of generating information and knowledge from data is increasingly becoming an important part of most operational processes. But up to now, data science is hardly an issue in German computer science education at secondary schools. For this reason, we are developing a data science curriculum for German secondary schools, which first guidelines and ideas we present in this paper. The curriculum is designed as interdisciplinary approach between maths and computer science education, with also a strong focus on societal aspects. After a brief discussion of important concepts and challenges in data science, a first draft of the curriculum and an outline of a data science course for upper secondary schools accompanying the development are presented.",c7,European Conference on Modelling and Simulation,cp7,accepted,f86,2015,2015-02-07
s111,p111,Care and the Practice of Data Science for Social Good,"Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of ""good"" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like.",c8,The Compass,cp8,accepted,f87,2016,2016-12-04
s112,p112,Defining Data Science by a Data-Driven Quantification of the Community,"Data science is a new academic field that has received much attention in recent years. One reason for this is that our increasingly digitalized society generates more and more data in all areas of our lives and science and we are desperately seeking for solutions to deal with this problem. In this paper, we investigate the academic roots of data science. We are using data of scientists and their citations from Google Scholar, who have an interest in data science, to perform a quantitative analysis of the data science community. Furthermore, for decomposing the data science community into its major defining factors corresponding to the most important research fields, we introduce a statistical regression model that is fully automatic and robust with respect to a subsampling of the data. This statistical model allows us to define the ‘importance’ of a field as its predictive abilities. Overall, our method provides an objective answer to the question ‘What is data science?’.",c49,International Symposium on Search Based Software Engineering,cp49,accepted,f88,2012,2012-06-28
s113,p113,Key Concepts for a Data Science Ethics Curriculum,"Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.",c1,Technical Symposium on Computer Science Education,cp1,accepted,f89,2002,2002-09-02
s114,p114,What makes Data Science different? A discussion involving Statistics2.0 and Computational Sciences,Abstract content goes here ...,j7,International Journal of Data Science and Analysis,jv7,accepted,f90,2001,2001-05-29
s115,p115,How data science can advance mental health research,Abstract content goes here ...,j48,Nature Human Behaviour,jv48,accepted,f91,2019,2019-09-09
s116,p116,Counter‐mapping data science,"Counter-mapping is a combination of critical ideas and practices for social change that offers a productive and promising approach for grassroots data science initiatives. Current information technologies collect, store, and analyze data with new degrees of size, speed, heterogeneity, and detail. While much work utilizing data science technologies is dedicated to generating profit or to national security, some data science projects explicitly attempt to facilitate new social relations, though with inconsistent results and consequences. This paper reviews counter-mapping's particular combination of theory and practice as a potential point of reference for such initiatives. Counter-mapping takes the tools of institutional map-making at government agencies and corporations and applies them in situated, bottom-up ways. Moreover, counter-mapping's multiple theoretical approaches and polyglot practices offer a variety of inspirations and avenues for future work in identifying and realizing alternative, ideally better, possibilities. This paper defines counter-mapping; outlines its multiple theorizations; briefly describes three relevant case studies, The Detroit Geographical Expedition and Institute, Mapping Police Violence, and the Counter-Cartographies Collective; and concludes with a few hard-learned considerations from counter-mapping that are directly pertinent for data-oriented projects focused on change.",c102,International Conference on Biometrics,cp102,accepted,f92,2022,2022-04-24
s117,p117,Teaching Stats for Data Science,"ABSTRACT “Data science” is a useful catchword for methods and concepts original to the field of statistics, but typically being applied to large, multivariate, observational records. Such datasets call for techniques not often part of an introduction to statistics: modeling, consideration of covariates, sophisticated visualization, and causal reasoning. This article re-imagines introductory statistics as an introduction to data science and proposes a sequence of 10 blocks that together compose a suitable course for extracting information from contemporary data. Recent extensions to the mosaic packages for R together with tools from the “tidyverse” provide a concise and readable notation for wrangling, visualization, model-building, and model interpretation: the fundamental computational tasks of data science.",c9,Pacific Symposium on Biocomputing,cp9,accepted,f93,2009,2009-01-02
s118,p118,Data Science,"The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.",j49,ACM Computing Surveys,jv49,accepted,f94,2012,2012-02-09
s119,p119,Democratizing data science through data science training,"The biomedical sciences have experienced an explosion of data which promises to overwhelm many current practitioners. Without easy access to data science training resources, biomedical researchers may find themselves unable to wrangle their own datasets. In 2014, to address the challenges posed such a data onslaught, the National Institutes of Health (NIH) launched the Big Data to Knowledge (BD2K) initiative. To this end, the BD2K Training Coordinating Center (TCC; bigdatau.org) was funded to facilitate both in-person and online learning, and open up the concepts of data science to the widest possible audience. Here, we describe the activities of the BD2K TCC and its focus on the construction of the Educational Resource Discovery Index (ERuDIte), which identifies, collects, describes, and organizes online data science materials from BD2K awardees, open online courses, and videos from scientific lectures and tutorials. ERuDIte now indexes over 9,500 resources. Given the richness of online training materials and the constant evolution of biomedical data science, computational methods applying information retrieval, natural language processing, and machine learning techniques are required - in effect, using data science to inform training in data science. In so doing, the TCC seeks to democratize novel insights and discoveries brought forth via large-scale data science training.",c9,Pacific Symposium on Biocomputing,cp9,accepted,f95,2009,2009-05-13
s120,p120,"STS, Meet Data Science, Once Again","Science and technology studies (STS) and the emerging field of data science share surprising elective affinities. At the growing intersections of these fields, there will be many opportunities and not a few thorny difficulties for STS scholars. First, I discuss how both fields frame the rollout of data science as a simultaneously social and technical endeavor, even if in distinct ways and for diverging purposes. Second, I discuss the logic of domains in contemporary computer, information, and data science circles. While STS is often agnostic about the borders between the sciences or with industry and state—occasionally taking those boundaries as an object of study—data science takes those boundaries as its target to overcome. These two elective affinities present analytic and practical challenges for STS but also opportunities for engagement. Overall, in addition to these typifications, I urge STS scholars to strategically position themselves to investigate and contribute to the breadth of transformations that seek to touch virtually every science and newly bind spheres of academy, industry, and state.",j50,"Science, Technology and Human Values",jv50,accepted,f96,2013,2013-05-11
s121,p121,Data Science,"Data science is the study of how analytics techniques can be applied to large and diverse data sets. This field is emerging because of the availability of massive data sets in both consumer and health sectors, new machine learning and other analytics requiring large-scale computation, and the vital need to identify risk factors, trends, and other relationships not apparent when applying traditional analytics methods to smaller structured data sets. In some organizations, the primary role of a clinical informatics professional no longer is focused on how electronic health records are used in healthcare delivery but instead is focused on how patient encounter information can be collected efficiently, aggregated with information from other encounters or sources, and analyzed to improve our understanding of how population studies can improve the care of individuals. Such an understanding is critical to improving care quality and lowering healthcare costs.",c33,International Conference on Agile Software Development,cp33,accepted,f97,2022,2022-06-18
s122,p122,Curriculum Guidelines for Undergraduate Programs in Data Science,"The Park City Math Institute (PCMI) 2016 Summer Undergraduate Faculty Program met for the purpose of composing guidelines for undergraduate programs in Data Science. The group consisted of 25 undergraduate faculty from a variety of institutions in the U.S., primarily from the disciplines of mathematics, statistics and computer science. These guidelines are meant to provide some structure for institutions planning for or revising a major in Data Science.",c14,International Conference on Exploring Services Science,cp14,accepted,f98,2016,2016-12-13
s123,p123,"Computer Age Statistical Inference: Algorithms, Evidence, and Data Science","The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.",c17,International Conference on Enterprise Information Systems,cp17,accepted,f99,2008,2008-04-24
s125,p125,Data Science in Action,Abstract content goes here ...,c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f100,2015,2015-08-07
s126,p126,Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science,"Abstract What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, “data for good” projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies.",c10,Big Data,cp10,accepted,f101,2021,2021-11-06
s127,p127,Automating Biomedical Data Science Through Tree-Based Pipeline Optimization,Abstract content goes here ...,c91,Workshop on Algorithms and Models for the Web-Graph,cp91,accepted,f102,2022,2022-04-08
s128,p128,Big Data and Data Science Methods for Management Research,"The recent advent of remote sensing, mobile technologies, novel transaction systems, and highperformance computing offers opportunities to understand trends, behaviors, and actions in a manner that has not been previously possible. Researchers can thus leverage “big data” that are generated from a plurality of sources including mobile transactions, wearable technologies, social media, ambient networks, andbusiness transactions.An earlierAcademy of Management Journal (AMJ) editorial explored the potential implications for data science inmanagement research and highlighted questions for management scholarship as well as the attendant challenges of data sharing and privacy (George, Haas, & Pentland, 2014). This nascent field is evolving rapidly and at a speed that leaves scholars and practitioners alike attempting to make sense of the emergent opportunities that big datahold.With thepromiseof bigdata comequestions about the analytical value and thus relevance of these data for theory development—including concerns over the context-specific relevance, its reliability and its validity. To address this challenge, data science is emerging as an interdisciplinary field that combines statistics, data mining, machine learning, and analytics to understand and explainhowwecan generate analytical insights and prediction models from structured and unstructured big data. Data science emphasizes the systematic study of the organization, properties, and analysis of data and their role in inference, including our confidence in the inference (Dhar, 2013).Whereas both big data and data science terms are often used interchangeably, “big data” refer to large and varied data that can be collected and managed, whereas “data science” develops models that capture, visualize, andanalyze theunderlyingpatterns in thedata. In this editorial, we address both the collection and handling of big data and the analytical tools provided by data science for management scholars. At the current time, practitioners suggest that data science applications tackle the three core elements of big data: volume, velocity, and variety (McAfee & Brynjolfsson, 2012; Zikopoulos & Eaton, 2011). “Volume” represents the sheer size of the dataset due to the aggregation of a large number of variables and an even larger set of observations for each variable. “Velocity” reflects the speed atwhich these data are collected and analyzed, whether in real time or near real time from sensors, sales transactions, social media posts, and sentiment data for breaking news and social trends. “Variety” in big data comes from the plurality of structured and unstructured data sources such as text, videos, networks, and graphics among others. The combinations of volume, velocity, and variety reveal the complex task of generating knowledge from big data, which often runs into millions of observations, and deriving theoretical contributions from such data. In this editorial, we provide a primer or a “starter kit” for potential data science applications inmanagement research. We do so with a caveat that emerging fields outdate and improve uponmethodologies while often supplanting them with new applications. Nevertheless, this primer can guide management scholars who wish to use data science techniques to reach better answers to existing questions or explore completely new research questions.",c85,International Conference on Graph Transformation,cp85,accepted,f103,2007,2007-11-09
s129,p129,"R for Data Science: Import, Tidy, Transform, Visualize, and Model Data","Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible. Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. Youll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what youve learned along the way. Youll learn how to: Wrangletransform your datasets into a form convenient for analysisProgramlearn powerful R tools for solving data problems with greater clarity and easeExploreexamine your data, generate hypotheses, and quickly test themModelprovide a low-dimensional summary that captures true ""signals"" in your datasetCommunicatelearn R Markdown for integrating prose, code, and results",c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f104,2012,2012-06-07
s131,p131,Big Data Science: Opportunities and Challenges to Address Minority Health and Health Disparities in the 21st Century.,"Addressing minority health and health disparities has been a missing piece of the puzzle in Big Data science. This article focuses on three priority opportunities that Big Data science may offer to the reduction of health and health care disparities. One opportunity is to incorporate standardized information on demographic and social determinants in electronic health records in order to target ways to improve quality of care for the most disadvantaged populations over time. A second opportunity is to enhance public health surveillance by linking geographical variables and social determinants of health for geographically defined populations to clinical data and health outcomes. Third and most importantly, Big Data science may lead to a better understanding of the etiology of health disparities and understanding of minority health in order to guide intervention development. However, the promise of Big Data needs to be considered in light of significant challenges that threaten to widen health disparities. Care must be taken to incorporate diverse populations to realize the potential benefits. Specific recommendations include investing in data collection on small sample populations, building a diverse workforce pipeline for data science, actively seeking to reduce digital divides, developing novel ways to assure digital data privacy for small populations, and promoting widespread data sharing to benefit under-resourced minority-serving institutions and minority researchers. With deliberate efforts, Big Data presents a dramatic opportunity for reducing health disparities but without active engagement, it risks further widening them.",j51,Ethnicity & Disease,jv51,accepted,f105,2011,2011-06-25
s132,p132,Knowledge-based biomedical Data Science,"Computational manipulation of knowledge is an important, and often under-appreciated, aspect of biomedical Data Science. The first Data Science initiative from the US National Institutes of Health was entitled “Big Data to Knowledge (BD2K).” The main emphasis of the more than $200M allocated to that program has been on “Big Data;” the “Knowledge” component has largely been the implicit assumption that the work will lead to new biomedical knowledge. However, there is long-standing and highly productive work in computational knowledge representation and reasoning, and computational processing of knowledge has a role in the world of Data Science. Knowledge-based biomedical Data Science involves the design and implementation of computer systems that act as if they knew about biomedicine. There are many ways in which a computational approach might act as if it knew something: for example, it might be able to answer a natural language question about a biomedical topic, or pass an exam; it might be able to use existing biomedical knowledge to rank or evaluate hypotheses; it might explain or interpret data in light of prior knowledge, either in a Bayesian or other sort of framework. These are all examples of automated reasoning that act on computational representations of knowledge. After a brief survey of existing approaches to knowledge-based data science, this position paper argues that such research is ripe for expansion, and expanded application.",j52,Data Science,jv52,accepted,f106,2020,2020-09-22
s133,p133,Data Science,Abstract content goes here ...,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",cp38,accepted,f107,2015,2015-03-11
s134,p134,Data science for building energy management: A review,Abstract content goes here ...,c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f108,2008,2008-12-21
s135,p135,Data science in education: Big data and learning analytics,This paper considers the data science and the summaries significance of Big Data and Learning Analytics in education. The widespread platform of making high‐quality benefits that could be achieved by exhausting big data techniques in the field of education is considered. One principal architecture framework to support education research is proposed.,j53,Computer Applications in Engineering Education,jv53,accepted,f109,2021,2021-10-25
s136,p136,Process-Structure Linkages Using a Data Science Approach: Application to Simulated Additive Manufacturing Data,Abstract content goes here ...,c77,Networks,cp77,accepted,f110,2019,2019-03-06
s138,p138,Materials Knowledge Systems in Python—a Data Science Framework for Accelerated Development of Hierarchical Materials,Abstract content goes here ...,c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f111,2006,2006-11-19
s139,p139,Surgical data science for next-generation interventions,Abstract content goes here ...,j54,Nature Biomedical Engineering,jv54,accepted,f112,2006,2006-08-11
s140,p140,Our path to better science in less time using open data science tools,Abstract content goes here ...,c31,International Conference on Evaluation & Assessment in Software Engineering,cp31,accepted,f113,2022,2022-03-26
s142,p142,Comparing Data Science Project Management Methodologies via a Controlled Experiment,"Data Science is an emerging field with a significant research focus on improving the techniques available to analyze data. However, there has been much less focus on how people should work together on a data science project. In this paper, we report on the results of an experiment comparing four different methodologies to manage and coordinate a data science project. We first introduce a model to compare different project management methodologies and then report on the results of our experiment. The results from our experiment demonstrate that there are significant differences based on the methodology used, with an Agile Kanban methodology being the most effective and surprisingly, an Agile Scrum methodology being the least effective.",c11,Hawaii International Conference on System Sciences,cp11,accepted,f114,2006,2006-12-19
s143,p143,Data science: Data science tutorials,Abstract content goes here ...,c12,International Conference on Statistical and Scientific Database Management,cp12,accepted,f115,2002,2002-02-25
s144,p144,Data Science,Abstract content goes here ...,c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f116,2014,2014-08-17
s145,p145,Hack weeks as a model for data science education and collaboration,"Significance As scientific disciplines grapple with more datasets of rapidly increasing complexity and size, new approaches are urgently required to introduce new statistical and computational tools into research communities and improve the cross-disciplinary exchange of ideas. In this paper, we introduce a type of scientific workshop, called a hack week, which allows for fast dissemination of new methodologies into scientific communities and fosters exchange and collaboration within and between disciplines. We present implementations of this concept in astronomy, neuroscience, and geoscience and show that hack weeks produce positive learning outcomes, foster lasting collaborations, yield scientific results, and promote positive attitudes toward open science. Across many scientific disciplines, methods for recording, storing, and analyzing data are rapidly increasing in complexity. Skillfully using data science tools that manage this complexity requires training in new programming languages and frameworks as well as immersion in new modes of interaction that foster data sharing, collaborative software development, and exchange across disciplines. Learning these skills from traditional university curricula can be challenging because most courses are not designed to evolve on time scales that can keep pace with rapidly shifting data science methods. Here, we present the concept of a hack week as an effective model offering opportunities for networking and community building, education in state-of-the-art data science methods, and immersion in collaborative project work. We find that hack weeks are successful at cultivating collaboration and facilitating the exchange of knowledge. Participants self-report that these events help them in both their day-to-day research as well as their careers. Based on our results, we conclude that hack weeks present an effective, easy-to-implement, fairly low-cost tool to positively impact data analysis literacy in academic disciplines, foster collaboration, and cultivate best practices.",j20,Proceedings of the National Academy of Sciences of the United States of America,jv20,accepted,f117,2006,2006-11-01
s148,p148,Python Data Science Handbook: Essential Tools for Working with Data,"For many researchers, Python is a first-class tool mainly because of its libraries for storing, manipulating, and gaining insight from data. Several resources exist for individual pieces of this data science stack, but only with the Python Data Science Handbook do you get them all IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and other related tools. Working scientists and data crunchers familiar with reading and writing Python code will find this comprehensive desk reference ideal for tackling day-to-day issues: manipulating, transforming, and cleaning data; visualizing different types of data; and using data to build statistical or machine learning models. Quite simply, this is the must-have reference for scientific computing in Python. With this handbook, youll learn how to use:IPython and Jupyter: provide computational environments for data scientists using PythonNumPy: includes the ndarray for efficient storage and manipulation of dense data arrays in PythonPandas: features the DataFrame for efficient storage and manipulation of labeled/columnar data in PythonMatplotlib: includes capabilities for a flexible range of data visualizations in PythonScikit-Learn: for efficient and clean Python implementations of the most important and established machine learning algorithms",c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f118,2019,2019-01-03
s149,p149,"Editorial - Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research","We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems IS community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.",j55,Information systems research,jv55,accepted,f119,2006,2006-02-22
s150,p150,Educational data science in massive open online courses,"The current massive open online course (MOOC) euphoria is revolutionizing online education. Despite its expediency, there is considerable skepticism over various concerns. In order to resolve some of these problems, educational data science (EDS) has been used with success. MOOCs provide a wealth of information about the way in which a large number of learners interact with educational platforms and engage with the courses offered. This extensive amount of data provided by MOOCs concerning students' usage information is a gold mine for EDS. This paper aims to provide the reader with a complete and comprehensive review of the existing literature that helps us understand the application of EDS in MOOCs. The main works in this area are described and grouped by task or issue to be solved, along with the techniques used. WIREs Data Mining Knowl Discov 2017, 7:e1187. doi: 10.1002/widm.1187",c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f120,2006,2006-09-07
s151,p151,"A review and future direction of agile, business intelligence, analytics and data science",Abstract content goes here ...,j56,International Journal of Information Management,jv56,accepted,f121,2004,2004-12-08
s152,p152,Big data and data science: what should we teach?,"The era of big data has arrived. Big data bring us the data‐driven paradigm and enlighten us to challenge new classes of problems we were not able to solve in the past. We are beginning to see the impacts of big data in every aspect of our lives and society. We need a science that can address these big data problems. Data science is a new emerging discipline that was termed to address challenges that we are facing and going to face in the big data era. Thus, education in data science is the key to success, and we need concrete strategies and approaches to better educate future data scientists. In this paper, we discuss general concepts on big data, data science, and data scientists and show the results of an extensive survey on current data science education in United States. Finally, we propose various approaches that data science education should aim to accomplish.",c27,ACM-SIAM Symposium on Discrete Algorithms,cp27,accepted,f122,2022,2022-10-05
s153,p153,R – Data Science,Abstract content goes here ...,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f123,2018,2018-06-11
s154,p154,Surgical Data Science: Enabling Next-Generation Surgery,Abstract content goes here ...,c8,The Compass,cp8,accepted,f124,2016,2016-07-31
s156,p156,Locating ethics in data science: responsibility and accountability in global and distributed knowledge production systems,"The distributed and global nature of data science creates challenges for evaluating the quality, import and potential impact of the data and knowledge claims being produced. This has significant consequences for the management and oversight of responsibilities and accountabilities in data science. In particular, it makes it difficult to determine who is responsible for what output, and how such responsibilities relate to each other; what ‘participation’ means and which accountabilities it involves, with regard to data ownership, donation and sharing as well as data analysis, re-use and authorship; and whether the trust placed on automated tools for data mining and interpretation is warranted (especially as data processing strategies and tools are often developed separately from the situations of data use where ethical concerns typically emerge). To address these challenges, this paper advocates a participative, reflexive management of data practices. Regulatory structures should encourage data scientists to examine the historical lineages and ethical implications of their work at regular intervals. They should also foster awareness of the multitude of skills and perspectives involved in data science, highlighting how each perspective is partial and in need of confrontation with others. This approach has the potential to improve not only the ethical oversight for data science initiatives, but also the quality and reliability of research outputs. This article is part of the themed issue ‘The ethical impact of data science’.",c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f125,2014,2014-04-12
s159,p159,Introducing Data Science to School Kids,"Data-driven decision making is fast becoming a necessary skill in jobs across the board. The industry today uses analytics and machine learning to get useful insights from a wealth of digital information in order to make decisions. With data science becoming an important skill needed in varying degrees of complexity by the workforce of the near future, we felt the need to expose school-goers to its power through a hands-on exercise. We organized a half-day long data science tutorial for kids in grades 5 through 9 (10-15 years old). Our aim was to expose them to the full cycle of a typical supervised learning approach - data collection, data entry, data visualization, feature engineering, model building, model testing and data permissions. We discuss herein the design choices made while developing the dataset, the method and the pedagogy for the tutorial. These choices aimed to maximize student engagement while ensuring minimal pre-requisite knowledge. This was a challenging task given that we limited the pre-requisites for the kids to the knowledge of counting, addition, percentages, comparisons and a basic exposure to operating computers. By designing an exercise with the stated principles, we were able to provide to kids an exciting, hands-on introduction to data science, as confirmed by their experiences. To the best of the authors' knowledge, the tutorial was the first of its kind. Considering the positive reception of such a tutorial, we hope that educators across the world are encouraged to introduce data science in their respective curricula for high-schoolers and are able to use the principles laid out in this work to build full-fledged courses.",c1,Technical Symposium on Computer Science Education,cp1,accepted,f126,2002,2002-04-08
s160,p160,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.",c10,Big Data,cp10,accepted,f127,2021,2021-07-09
s162,p162,"Big Metadata, Smart Metadata, and Metadata Capital: Toward Greater Synergy Between Data Science and Metadata","Abstract Purpose The purpose of the paper is to provide a framework for addressing the disconnect between metadata and data science. Data science cannot progress without metadata research. This paper takes steps toward advancing the synergy between metadata and data science, and identifies pathways for developing a more cohesive metadata research agenda in data science. Design/methodology/approach This paper identifies factors that challenge metadata research in the digital ecosystem, defines metadata and data science, and presents the concepts big metadata, smart metadata, and metadata capital as part of a metadata lingua franca connecting to data science. Findings The “utilitarian nature” and “historical and traditional views” of metadata are identified as two intersecting factors that have inhibited metadata research. Big metadata, smart metadata, and metadata capital are presented as part of a metadata lingua franca to help frame research in the data science research space. Research limitations There are additional, intersecting factors to consider that likely inhibit metadata research, and other significant metadata concepts to explore. Practical implications The immediate contribution of this work is that it may elicit response, critique, revision, or, more significantly, motivate research. The work presented can encourage more researchers to consider the significance of metadata as a research worthy topic within data science and the larger digital ecosystem. Originality/value Although metadata research has not kept pace with other data science topics, there is little attention directed to this problem. This is surprising, given that metadata is essential for data science endeavors. This examination synthesizes original and prior scholarship to provide new grounding for metadata research in data science.",j58,Journal of Data and Information Science,jv58,accepted,f128,2012,2012-11-13
s163,p163,Big Data and Data Science: Opportunities and Challenges of iSchools,"Abstract Due to the recent explosion of big data, our society has been rapidly going through digital transformation and entering a new world with numerous eye-opening developments. These new trends impact the society and future jobs, and thus student careers. At the heart of this digital transformation is data science, the discipline that makes sense of big data. With many rapidly emerging digital challenges ahead of us, this article discusses perspectives on iSchools’ opportunities and suggestions in data science education. We argue that iSchools should empower their students with “information computing” disciplines, which we define as the ability to solve problems and create values, information, and knowledge using tools in application domains. As specific approaches to enforcing information computing disciplines in data science education, we suggest the three foci of user-based, tool-based, and application-based. These three foci will serve to differentiate the data science education of iSchools from that of computer science or business schools. We present a layered Data Science Education Framework (DSEF) with building blocks that include the three pillars of data science (people, technology, and data), computational thinking, data-driven paradigms, and data science lifecycles. Data science courses built on the top of this framework should thus be executed with user-based, tool-based, and application-based approaches. This framework will help our students think about data science problems from the big picture perspective and foster appropriate problem-solving skills in conjunction with broad perspectives of data science lifecycles. We hope the DSEF discussed in this article will help fellow iSchools in their design of new data science curricula.",j58,Journal of Data and Information Science,jv58,accepted,f129,2012,2012-12-06
s165,p165,An Introduction to Data Science,"An Introduction to Data Scienceby Jeffrey S. Saltz and Jeffrey M. Stanton is an easy-to-read, gentle introduction for people with a wide range of backgrounds into the world of data science. Needing no prior coding experience or a deep understanding of statistics, this book uses the R programming language and RStudio platform to make data science welcoming and accessible for all learners. After introducing the basics of data science, the book builds on each previous concept to explain R programming from the ground up. Readers will learn essential skills in data science through demonstrations of how to use data to construct models, predict outcomes, and visualize data.",c59,British Computer Society Conference on Human-Computer Interaction,cp59,accepted,f130,2019,2019-12-11
s166,p166,Responsible Data Science,Abstract content goes here ...,j57,Business & Information Systems Engineering,jv57,accepted,f131,2020,2020-02-25
s167,p167,Predicting data science sociotechnical execution challenges by categorizing data science projects,"The challenge in executing a data science project is more than just identifying the best algorithm and tool set to use. Additional sociotechnical challenges include items such as how to define the project goals and how to ensure the project is effectively managed. This paper reports on a set of case studies where researchers were embedded within data science teams and where the researcher observations and analysis was focused on the attributes that can help describe data science projects and the challenges faced by the teams executing these projects, as opposed to the algorithms and technologies that were used to perform the analytics. Based on our case studies, we identified 14 characteristics that can help describe a data science project. We then used these characteristics to create a model that defines two key dimensions of the project. Finally, by clustering the projects within these two dimensions, we identified four types of data science projects, and based on the type of project, we identified some of the sociotechnical challenges that project teams should expect to encounter when executing data science projects.",c62,International Conference on Software Reuse,cp62,accepted,f132,2006,2006-12-27
s168,p168,"Introducing Data Science: Big Data, Machine Learning, and more, using Python tools","Summary Introducing Data Science teaches you how to accomplish the fundamental tasks that occupy data scientists. Using the Python language and common Python libraries, you'll experience firsthand the challenges of dealing with data at scale and gain a solid foundation in data science. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Many companies need developers with data science skills to work on projects ranging from social media marketing to machine learning. Discovering what you need to learn to begin a career as a data scientist can seem bewildering. This book is designed to help you get started. About the BookIntroducing Data Science Introducing Data Science explains vital data science concepts and teaches you how to accomplish the fundamental tasks that occupy data scientists. Youll explore data visualization, graph databases, the use of NoSQL, and the data science process. Youll use the Python language and common Python libraries as you experience firsthand the challenges of dealing with data at scale. Discover how Python allows you to gain insights from data sets so big that they need to be stored on multiple machines, or from data moving so quickly that no single machine can handle it. This book gives you hands-on experience with the most popular Python data science libraries, Scikit-learn and Stats Models. After reading this book, youll have the solid foundation you need to start a career in data science. Whats Inside Handling large data Introduction to machine learning Using Python to work with data Writing data science algorithms About the ReaderThis book assumes you're comfortable reading code in Python or a similar language, such as C, Ruby, or JavaScript. No prior experience with data science is required. About the Authors Davy Cielen, Arno D. B. Meysman, and Mohamed Ali are the founders and managing partners of Optimately and Maiton, where they focus on developing data science projects and solutions in various sectors.",c41,Software Product Lines Conference,cp41,accepted,f133,2002,2002-05-16
s169,p169,Ushering in a New Frontier in Geospace Through Data Science,"Our understanding and specification of solar‐terrestrial interactions benefit from taking advantage of comprehensive data‐intensive approaches. These data‐driven methods are taking on new importance in light of the shifting data landscape of the geospace system, which extends from the near Earth space environment, through the magnetosphere and interplanetary space, to the Sun. The space physics community faces both an exciting opportunity and an important imperative to create a new frontier built at the intersection of traditional approaches and state‐of‐the‐art data‐driven sciences and technologies. This brief commentary addresses the current paradigm of geospace science and the emerging need for data science innovation, discusses the meaning of data science in the context of geospace, and highlights community efforts to respond to the changing landscape.",c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f134,2019,2019-11-10
s170,p170,Transdisciplinary Foundations of Geospatial Data Science,"Recent developments in data mining and machine learning approaches have brought lots of excitement in providing solutions for challenging tasks (e.g., computer vision). However, many approaches have limited interpretability, so their success and failure modes are difficult to understand and their scientific robustness is difficult to evaluate. Thus, there is an urgent need for better understanding of the scientific reasoning behind data mining and machine learning approaches. This requires taking a transdisciplinary view of data science and recognizing its foundations in mathematics, statistics, and computer science. Focusing on the geospatial domain, we apply this crucial transdisciplinary perspective to five common geospatial techniques (hotspot detection, colocation detection, prediction, outlier detection and teleconnection detection). We also describe challenges and opportunities for future advancement.",c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f135,2006,2006-08-16
s171,p171,Fides: Towards a Platform for Responsible Data Science,"Issues of responsible data analysis and use are coming to the forefront of the discourse in data science research and practice, with most significant efforts to date on the part of the data mining, machine learning, and security and privacy communities. In these fields, the research has been focused on analyzing the fairness, accountability and transparency (FAT) properties of specific algorithms and their outputs. Although these issues are most apparent in the social sciences where fairness is interpreted in terms of the distribution of resources across protected groups, management of bias in source data affects a variety of fields. Consider climate change studies that require representative data from geographically diverse regions, or supply chain analyses that require data that represents the diversity of products and customers. Any domain that involves sparse or sampled data has exposure to potential bias. In this vision paper, we argue that FAT properties must be considered as database system issues, further upstream in the data science lifecycle: bias in source data goes unnoticed, and bias may be introduced during pre-processing (fairness), spurious correlations lead to reproducibility problems (accountability), and assumptions made during pre-processing have invisible but significant effects on decisions (transparency). As machine learning methods continue to be applied broadly by non-experts, the potential for misuse increases. We see a need for a data sharing and collaborative analytics platform with features to encourage (and in some cases, enforce) best practices at all stages of the data science lifecycle. We describe features of such a platform, which we term Fides, in the context of urban analytics, outlining a systems research agenda in responsible data science.",c12,International Conference on Statistical and Scientific Database Management,cp12,accepted,f136,2002,2002-08-31
s172,p172,Applications of Python to evaluate environmental data science problems,"There is a significant convergence of interests in the research community efforts to advance the development and application of software resources (capable of handling the relevant mathematical algorithms to provide scalable information) for solving data science problems. Anaconda is one of the many open source platforms that facilitate the use of open source programming languages (R, Python) for large‐scale data processing, predictive analytics, and scientific computing. The environmental research community may choose to adapt the use of either of the R or the Python programming languages for analyzing the data science problems on the Anaconda platform. This study demonstrated the applications of using Scikit‐learn (a Python machine learning library package) on Anaconda platform for analyzing the in‐bus carbon dioxide concentrations by (i) importing the data into Spyder (Python 3.6) in Anaconda, (ii) performing an exploratory data analysis, (iii) performing dimensionality reduction through RandomForestRegressor feature selection, (iv) developing statistical regression models, and (v) generating regression decision tree models with DecisionTreeRegressor feature. The readers may adopt the methods (inclusive of the Python coding) discussed in this article to successfully address their own data science problems. © 2017 American Institute of Chemical Engineers Environ Prog, 36: 1580–1586, 2017",c18,Conference on Innovative Data Systems Research,cp18,accepted,f137,2012,2012-05-29
s173,p173,Data science vs. statistics: two cultures?,Abstract content goes here ...,c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f138,2018,2018-03-10
s174,p174,"Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications",Abstract content goes here ...,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f139,2001,2001-01-26
s175,p175,A Guide to Teaching Data Science,"ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.",j16,American Statistician,jv16,accepted,f140,2010,2010-04-19
s176,p176,The ambiguity of data science team roles and the need for a data science workforce framework,"This paper first reviews the benefits of well-defined roles and then discusses the current lack of standardized roles within the data science community, perhaps due to the newness of the field. Specifically, the paper reports on five case studies exploring five different attempts to define a standard set of roles. These case studies explore the usage of roles from an industry perspective as well as from national standard big data committee efforts. The paper then leverages the results of these case studies to explore the use of data science roles within online job postings. While some roles appeared frequently, such as data scientist and data engineer, no role was consistently used across all five case studies. Hence, the paper concludes by noting the need to create a data science workforce framework that could be used by students, employers, and academic institutions. This framework would enable organizations to staff their data science teams more accurately with the desired skillsets.",c2,International Symposium on Intelligent Data Analysis,cp2,accepted,f141,2014,2014-08-24
s177,p177,Building the biomedical data science workforce,"This article describes efforts at the National Institutes of Health (NIH) from 2013 to 2016 to train a national workforce in biomedical data science. We provide an analysis of the Big Data to Knowledge (BD2K) training program strengths and weaknesses with an eye toward future directions aimed at any funder and potential funding recipient worldwide. The focus is on extramurally funded programs that have a national or international impact rather than the training of NIH staff, which was addressed by the NIH’s internal Data Science Workforce Development Center. From its inception, the major goal of BD2K was to narrow the gap between needed and existing biomedical data science skills. As biomedical research increasingly relies on computational, mathematical, and statistical thinking, supporting the training and education of the workforce of tomorrow requires new emphases on analytical skills. From 2013 to 2016, BD2K jump-started training in this area for all levels, from graduate students to senior researchers.",j60,PLoS Biology,jv60,accepted,f142,2001,2001-10-04
s179,p179,Data science: Accelerating innovation and discovery in chemical engineering,"A ll of science and engineering, including chemical engineering, is being transformed by new sources of data from high-throughput experiments, observational studies, and simulation. In this new era of data-enabled science and engineering, discovery is no longer limited by the collection and processing of data but by data management, knowledge extraction, and the visualization of information. The term data science has become increasingly popular across industry, and academic disciplines to refer to the combination of strategies and tools for addressing the oncoming deluge of data. The term data scientist is a common descriptor of an engineer or scientist from any disciplinary background who is equipped to seamlessly process, analyze, and communicate in this data-intensive context. The core areas of data science are often identified as data management, statistical and machine learning, and visualization. In this Perspective, we present an overview of these core areas, discuss application areas from within chemical engineering research, and conclude with perspectives on how data science principles can be included in our training. As has been noted for several years, chemical engineers of all varieties, from the practicing process engineer to the academic researcher, are being asked more and more often to manipulate, transform, and analyze complex data sets. The complexity often stems from the size of the data set itself, but this is not the only factor. For example, the stream of information available to an engineer in a modern plant is tremendous because of the proliferation of inexpensive instrumentation and the nearly ubiquitous high bandwidth and low-latency connectivity. In the area of research and discovery, a student or researcher conducting data-intensive experiments, such as high-resolution particle tracking, might generate more data in an afternoon than a student from a previous decade in the entire time spent earning his or her Ph.D. For those conducting mathematical modeling and computer simulations, advanced algorithms and hardware now give simulators unprecedented resolution but at the cost of massive increases in the data set. Underlying all of these examples is the cheap (near free) cost of data storage and the ubiquitous availability of our data from cloud-based services. The aforementioned examples may appear to be vastly different from the outset. However, common themes in the limitation of our current approaches quickly emerge. Because our training of new chemical engineers (at all levels) has not kept pace with the explosion of data, each chemical engineer in the previous examples will likely approach her or his work in the same manner: manual searching for relationships in the data, classical visualization of monovariate or bivariate correlations in features, and a hypothesis-driven approach to science reminiscent of a data-poor era when the researcher or engineer could essentially manipulate relevant data in their mind. Simply put, without knowledge about and training on how to handle data skillfully, most of the information from our plants and refineries, our data-intensive experiments, and our computer simulations is thrown away, simply because we do not know how to extract knowledge from it. Fortunately, there is a potential solution on the horizon. Through the lens of the nascent field of data science, we can see an emergent (and limited) set of tasks needed by all of the previous chemical engineers: (1) to manage a huge data set consisting of ensembles of spatiotemporal data, (2) to sensibly read the data in a computationally scalable manner, and (3) to extract knowledge from this pile of information with robust techniques whose statistical reliability can be quantified. It also goes without saying that data science itself is not a panacea. Chemical engineering fundamentals are of the utmost importance, and Correspondence concerning this article should be addressed to D. A. C. Beck at dacb@uw.edu and J. Pfaendtner at jpfaendt@uw.edu.",c88,Symposium on the Theory of Computing,cp88,accepted,f143,2014,2014-04-16
s182,p182,The Quantified Self: Fundamental Disruption in Big Data Science and Biological Discovery,"A key contemporary trend emerging in big data science is the quantified self (QS)-individuals engaged in the self-tracking of any kind of biological, physical, behavioral, or environmental information as n=1 individuals or in groups. There are opportunities for big data scientists to develop new models to support QS data collection, integration, and analysis, and also to lead in defining open-access database resources and privacy standards for how personal data is used. Next-generation QS applications could include tools for rendering QS data meaningful in behavior change, establishing baselines and variability in objective metrics, applying new kinds of pattern recognition techniques, and aggregating multiple self-tracking data streams from wearable electronics, biosensors, mobile phones, genomic data, and cloud-based services. The long-term vision of QS activity is that of a systemic monitoring approach where an individual's continuous personal information climate provides real-time performance optimization suggestions. There are some potential limitations related to QS activity-barriers to widespread adoption and a critique regarding scientific soundness-but these may be overcome. One interesting aspect of QS activity is that it is fundamentally a quantitative and qualitative phenomenon since it includes both the collection of objective metrics data and the subjective experience of the impact of these data. Some of this dynamic is being explored as the quantified self is becoming the qualified self in two new ways: by applying QS methods to the tracking of qualitative phenomena such as mood, and by understanding that QS data collection is just the first step in creating qualitative feedback loops for behavior change. In the long-term future, the quantified self may become additionally transformed into the extended exoself as data quantification and self-tracking enable the development of new sense capabilities that are not possible with ordinary senses. The individual body becomes a more knowable, calculable, and administrable object through QS activity, and individuals have an increasingly intimate relationship with data as it mediates the experience of reality.",c10,Big Data,cp10,accepted,f144,2021,2021-08-05
s183,p183,"Data Science, Predictive Analytics, and Big Data in Supply Chain Management: Current State and Future Potential","While data science, predictive analytics, and big data have been frequently used buzzwords, rigorous academic investigations into these areas are just emerging. In this forward thinking article, we discuss the results of a recent large-scale survey on these topics among supply chain management (SCM) professionals, complemented with our experiences in developing, implementing, and administering one of the first master's degree programs in predictive analytics. As such, we effectively provide an assessment of the current state of the field via a large-scale survey, and offer insight into its future potential via the discussion of how a research university is training next-generation data scientists. Specifically, we report on the current use of predictive analytics in SCM and the underlying motivations, as well as perceived benefits and barriers. In addition, we highlight skills desired for successful data scientists, and provide illustrations of how predictive analytics can be implemented in the curriculum. Relying on one of the largest data sets of predictive analytics users in SCM collected to date and our experiences with one of the first master's degree programs in predictive analytics, it is our intent to provide a timely assessment of the field, illustrate its future potential, and motivate additional research and pedagogical advancements in this domain.",c81,IEEE Annual Symposium on Foundations of Computer Science,cp81,accepted,f145,2004,2004-09-07
s184,p184,Data Science in Libraries,"EDITOR'S SUMMARY 
 
The new field of data science involves advanced knowledge in statistics and computer science, combined with copious amounts of data. A report from the Big Data and Research Initiative under the Obama Administration, The Federal Big Data Research and Development Strategic Plan, calls attention to the roles that librarians will play in the future of data science. However, there are skills and management gaps librarians face that inhibit their ability to move forward in data science. A number of educational programs are now offered to remedy this problem, such as the Data and Visualization Institute for Librarians from North Carolina State University, the volunteer-led Library Carpentry program, and most recently, the Data Sciences in Libraries Project, funded by the IMLS. This project aims to get librarians and library managers together to discuss the world of data science and create a roadmap for strategic planning.",c33,International Conference on Agile Software Development,cp33,accepted,f146,2022,2022-11-19
s185,p185,From Data Science to Value Creation,Abstract content goes here ...,c14,International Conference on Exploring Services Science,cp14,accepted,f147,2016,2016-05-24
s186,p186,EDISON Data Science Framework: A Foundation for Building Data Science Profession for Research and Industry,"Data Science is an emerging field of science, which requires a multi-disciplinary approach and should be built with a strong link to emerging Big Data and data driven technologies, and consequently needs re-thinking and re-design of both traditional educational models and existing courses. The education and training of Data Scientists currently lacks a commonly accepted, harmonized instructional model that reflects by design the whole lifecycle of data handling in modern, data driven research and the digital economy. This paper presents the EDISON Data Science Framework (EDSF) that is intended to create a foundation for the Data Science profession definition. The EDSF includes the following core components: Data Science Competence Framework (CF-DS), Data Science Body of Knowledge (DS-BoK), Data Science Model Curriculum (MC-DS), and Data Science Professional profiles (DSP profiles). The MC-DS is built based on CF-DS and DS-BoK, where Learning Outcomes are defined based on CF-DS competences and Learning Units are mapped to Knowledge Units in DS-BoK. In its own turn, Learning Units are defined based on the ACM Classification of Computer Science (CCS2012) and reflect typical courses naming used by universities in their current programmes. The paper provides example how the proposed EDSF can be used for designing effective Data Science curricula and reports the experience of implementing EDSF by the Champion Universities that cooperate with the EDISON project.",c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f148,2014,2014-01-26
s187,p187,Data science: supporting decision-making,"Abstract Data science is a new academic trans-discipline that builds on 60 years of research about supporting decision-making in organisations. It is an important and potentially significant concept and practice. Contemplating the need for data scientists encourages academics and managers to examine issues of decision-maker rationality, data and data analysis needs, analytical tools, job skills and academic preparation. This article explores data science and the data professionals who will use new data streams and analytics to support decision-making. It also examines the dimensions that are changing in the data stream and the skills needed by data scientists to analyse the new data streams. Organisations need data scientists, but academics need to understand the new data science jobs to prepare more people to support decision-making.",j61,Journal of Decision Systems,jv61,accepted,f149,2018,2018-04-13
s189,p189,Developing a Research Agenda for Human-Centered Data Science,"The study and analysis of large and complex data sets offer a wealth of insights in a variety of applications. Computational approaches provide researchers access to broad assemblages of data, but the insights extracted may lack the rich detail that qualitative approaches have brought to the understanding of sociotechnical phenomena. How do we preserve the richness associated with traditional qualitative methods while utilizing the power of large data sets? How do we uncover social nuances or consider ethics and values in data use? These and other questions are explored by human-centered data science, an emerging field at the intersection of human-computer interaction (HCI), computer-supported cooperative work (CSCW), human computation, and the statistical and computational techniques of data science. This workshop, the first of its kind at CSCW, seeks to bring together researchers interested in human-centered approaches to data science to collaborate, define a research agenda, and form a community.",c33,International Conference on Agile Software Development,cp33,accepted,f150,2022,2022-06-29
s190,p190,Introduction to HPC with MPI for Data Science,Abstract content goes here ...,c19,ACM Conference on Economics and Computation,cp19,accepted,f151,2002,2002-07-23
s191,p191,Big data analytics and big data science: a survey,"Big data has attracted much attention from academia and industry. But the discussion of big data is disparate, fragmented and distributed among different outlets. This paper conducts a systematic and extensive review on 186 journal publications about big data from 2011 to 2015 in the Science Citation Index (SCI) and the Social Science Citation Index (SSCI) database aiming to provide scholars and practitioners with a comprehensive overview and big picture about research on big data. The selected papers are grouped into 20 research categories. The contents of the paper(s) in each research category are summarized. Research directions for each category are outlined as well. The results in this study indicate that the selected papers were mainly published between 2013 and 2015 and focus on technological issues regarding big data. Diverse new approaches, methods, frameworks and systems are proposed for data collection, storage, transport, processing and analysis in the selected papers. Possible directions for f...",c16,Knowledge Discovery and Data Mining,cp16,accepted,f152,2003,2003-05-10
s193,p193,Thinking by classes in data science: the symbolic data analysis paradigm,"Data Science, considered as a science by itself, is in general terms, the extraction of knowledge from data. Symbolic data analysis (SDA) gives a new way of thinking in Data Science by extending the standard input to a set of classes of individual entities. Hence, classes of a given population are considered to be units of a higher level population to be studied. Such classes often represent the real units of interest. In order to take variability between the members of each class into account, classes are described by intervals, distributions, set of categories or numbers sometimes weighted and the like. In that way, we obtain new kinds of data, called ‘symbolic’ as they cannot be reduced to numbers without losing much information. The first step in SDA is to build the symbolic data table where the rows are classes and the variables can take symbolic values. The second step is to study and extract new knowledge from these new kinds of data by at least an extension of Computer Statistics and Data Mining to symbolic data. SDA is a new paradigm which opens up a vast domain of research and applications by giving complementary results to classical methods applied to standard data. SDA also gives answers to big data and complex data challenges as big data can be reduced and summarized by classes and as complex data with multiple unstructured data tables and unpaired variables can be transformed into a structured data table with paired symbolic‐valued variables. WIREs Comput Stat 2016, 8:172–205. doi: 10.1002/wics.1384",c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",cp38,accepted,f153,2015,2015-12-04
s194,p194,Role of materials data science and informatics in accelerated materials innovation,"The goal of the Materials Genome Initiative is to substantially reduce the time and cost of materials design and deployment. Achieving this goal requires taking advantage of the recent advances in data and information sciences. This critical need has impelled the emergence of a new discipline, called materials data science and informatics. This emerging new discipline not only has to address the core scientific/technological challenges related to datafication of materials science and engineering, but also, a number of equally important challenges around data-driven transformation of the current culture, practices, and workflows employed for materials innovation. A comprehensive effort that addresses both of these aspects in a synergistic manner is likely to succeed in realizing the vision of scaled-up materials innovation. Key toolsets needed for the successful adoption of materials data science and informatics in materials innovation are identified and discussed in this article. Prototypical examples of emerging novel toolsets and their functionality are described along with select case studies.",c100,ACM SIGMOD Conference,cp100,accepted,f154,2010,2010-06-06
s195,p195,A Practical and Sustainable Model for Learning and Teaching Data Science,"This paper details our experiences with design and implementation of data science curriculum at University at Buffalo (UB). We discuss (i) briefly the history of project, (ii) a certificate program that we created, (iii) a data-intensive computing course that forms the core of the curriculum and (iv) some of the challenges we faced and how we addressed them. Major goal of the project was to improve the preparedness of our workforce for the emerging data-intensive computing area. We measured this through assessment of student learning on various concepts and topics related to data-intensive computing. We also discuss the best practices in building a data science program. We highlight the importance of external funding support and multi-disciplinary collaborations in the success of the project. The pedagogical resources created for the project are freely available to help educators and other learners navigate the path to learning data science. We expect this paper about our experience will provide a road map for educators who desire to introduce data science in their curriculum.",c1,Technical Symposium on Computer Science Education,cp1,accepted,f155,2002,2002-06-04
s197,p197,Teaching Data Science,Abstract content goes here ...,c15,International Conference on Conceptual Structures,cp15,accepted,f156,2011,2011-11-23
s198,p198,Deep learning and process understanding for data-driven Earth system science,Abstract content goes here ...,j62,Nature,jv62,accepted,f157,2017,2017-05-30
s200,p200,Providing data science support for systems pharmacology and its implications to drug discovery,"ABSTRACT Introduction: The conventional one-drug-one-target-one-disease drug discovery process has been less successful in tracking multi-genic, multi-faceted complex diseases. Systems pharmacology has emerged as a new discipline to tackle the current challenges in drug discovery. The goal of systems pharmacology is to transform huge, heterogeneous, and dynamic biological and clinical data into interpretable and actionable mechanistic models for decision making in drug discovery and patient treatment. Thus, big data technology and data science will play an essential role in systems pharmacology. Areas covered: This paper critically reviews the impact of three fundamental concepts of data science on systems pharmacology: similarity inference, overfitting avoidance, and disentangling causality from correlation. The authors then discuss recent advances and future directions in applying the three concepts of data science to drug discovery, with a focus on proteome-wide context-specific quantitative drug target deconvolution and personalized adverse drug reaction prediction. Expert opinion: Data science will facilitate reducing the complexity of systems pharmacology modeling, detecting hidden correlations between complex data sets, and distinguishing causation from correlation. The power of data science can only be fully realized when integrated with mechanism-based multi-scale modeling that explicitly takes into account the hierarchical organization of biological systems from nucleic acid to proteins, to molecular interaction networks, to cells, to tissues, to patients, and to populations.",j63,Expert Opinion on Drug Discovery,jv63,accepted,f158,2012,2012-03-18
s201,p201,A Case for Data Commons: Toward Data Science as a Service,"Data commons collocate data, storage, and computing infrastructure with core services and commonly used tools and applications for managing, analyzing, and sharing data to create an interoperable resource for the research community. An architecture for data commons is described, as well as some lessons learned from operating several large-scale data commons.",c7,European Conference on Modelling and Simulation,cp7,accepted,f159,2015,2015-06-05
s203,p203,Data Science: Nature and Pitfalls,Data science is creating exciting trends as well as significant controversy. A critical matter for the healthy development of data science in its early stages is to deeply understand the nature of data and data science and discuss the various pitfalls. These important issues motivate the discussions in this article.,j64,IEEE Intelligent Systems,jv64,accepted,f160,2003,2003-09-20
s204,p204,Integrating Systems Modelling and Data Science: The Joint Future of Simulation and 'Big Data' Science,"Although System Dynamics modelling is sometimes referred to as data-poor modelling, it often is -or could be-applied in a data-rich manner. However, more can be done in the era of 'big data'. Big data refers here to situations with much more available data than was until recently manageable. The field of data science makes bigger data manageable. This paper provides a perspective on the future of System Dynamics with a prominent place for bigger data and data science. It discusses different approaches for dealing with bigger data. It reviews methods, techniques and tools for dealing with bigger data in System Dynamics, and sheds light on the modelling phases for which data science is most useful. Finally, it provides several examples of current applications in which big data, data science, and System Dynamics modelling and simulation are being merged.",j65,International Journal of System Dynamics Applications,jv65,accepted,f161,2013,2013-10-11
s205,p205,Data Science from Scratch: First Principles with Python,"Data science libraries, frameworks, modules, and toolkits are great for doing data science, but they're also a good way to dive into the discipline without actually understanding data science. In this book, you'll learn how many of the most fundamental data science tools and algorithms work by implementing them from scratch. If you have an aptitude for mathematics and some programming skills, author Joel Grus will help you get comfortable with the math and statistics at the core of data science, and with hacking skills you need to get started as a data scientist. Today's messy glut of data holds answers to questions no one's even thought to ask. This book provides you with the know-how to dig those answers out.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f162,2011,2011-04-26
s206,p206,"Responsible Data Science: Using Event Data in a ""People Friendly"" Manner",Abstract content goes here ...,c17,International Conference on Enterprise Information Systems,cp17,accepted,f163,2008,2008-01-05
s207,p207,Survey on data science with population-based algorithms,Abstract content goes here ...,c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f164,2018,2018-12-23
s208,p208,"Data science on the ground: Hype, criticism, and everyday work","Modern organizations often employ data scientists to improve business processes using diverse sets of data. Researchers and practitioners have both touted the benefits and warned of the drawbacks associated with data science and big data approaches, but few studies investigate how data science is carried out “on the ground.” In this paper, we first review the hype and criticisms surrounding data science and big data approaches. We then present the findings of semistructured interviews with 18 data analysts from various industries and organizational roles. Using qualitative coding techniques, we evaluated these interviews in light of the hype and criticisms surrounding data science in the popular discourse. We found that although the data analysts we interviewed were sensitive to both the allure and the potential pitfalls of data science, their motivations and evaluations of their work were more nuanced. We conclude by reflecting on the relationship between data analysts' work and the discourses around data science and big data, suggesting how future research can better account for the everyday practices of this profession.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f165,2007,2007-12-20
s209,p209,Towards Data Science,"Currently, a huge amount of data is being rapidly generated in cyberspace. Datanature (all data in cyberspace) is forming due to a data explosion. Exploring the patterns and rules in datanature is necessary but difficult. A new discipline called Data Science is coming. It provides a type of novel research method (a data-intensive method) for natural and social sciences and goes beyond computer science in researching data. This paper presents the challenges presented by data and discusses what differentiates data science from the established sciences, data technologies, and big data. Our goal is to encourage data related researchers to transfer their focus towards this new science.",j66,Data Science Journal,jv66,accepted,f166,2015,2015-12-28
s210,p210,"Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data By EMC Education Services Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you: Become a contributor on a data science team ●",c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f167,2006,2006-10-08
s211,p211,The Role of Data Science in Web Science,"Web science relies on an interdisciplinary approach that seeks to go beyond what any one subject can say about the World Wide Web. By incorporating numerous disciplinary perspectives and relying heavily on domain knowledge and expertise, data science has emerged as an important new area that integrates statistics with computational knowledge, data collection, cleaning and processing, analysis methods, and visualization to produce actionable insights from big data. As a discipline to use within Web science research, data science offers significant opportunities for uncovering trends in large Web-based datasets. A Web science observatory exemplifies this relationship by offering an online platform of tools for carrying out Web science research, allowing users to carry out data science techniques to produce insights into Web science issues such as community development, online behavior, and information propagation. The authors outline the similarities and differences of these two growing subject areas to demonstrate the important relationship developing between them.",j64,IEEE Intelligent Systems,jv64,accepted,f168,2003,2003-11-08
s212,p212,Recent Activities in Earth Data Science [Technical Committees],"Recent trends on big Earth-observing (EO) data lead to some questions that the Earth science community needs to address. Are we experiencing a paradigm shift in Earth science research now? How can we better utilize the explosion of technology maturation to create new forms of EO data processing? Can we summarize the existing methodologies and technologies scaling to big EO data as a new field named earth data science? Big data technologies are being widely practiced in Earth sciences and remote sensing communities to support EO data access, processing, and knowledge discovery. The data-intensive scientific discovery, named the fourth paradigm, leads to data science in the big data era [1]. According to the definition by the U.S. National Institute of Standards and Technology, the data science paradigm is the ""extraction of actionable knowledge directly from data through a process of discovery, hypothesis, and hypothesis testing"" [2]. Earth data science is the art and science of applying the data science paradigm to EO data.",j67,IEEE Geoscience and Remote Sensing Magazine,jv67,accepted,f169,2019,2019-03-21
s214,p214,Perspectives on Data Science for Software Engineering,Abstract content goes here ...,c106,Chinese Conference on Biometric Recognition,cp106,accepted,f170,2016,2016-09-27
s215,p215,Data science and analytics: a new era,Abstract content goes here ...,j7,International Journal of Data Science and Analysis,jv7,accepted,f171,2001,2001-07-12
s216,p216,A Data Science Course for Undergraduates: Thinking With Data,"Data science is an emerging interdisciplinary field that combines elements of mathematics, statistics, computer science, and knowledge in a particular application domain for the purpose of extracting meaningful information from the increasingly sophisticated array of data available in many settings. These data tend to be nontraditional, in the sense that they are often live, large, complex, and/or messy. A first course in statistics at the undergraduate level typically introduces students to a variety of techniques to analyze small, neat, and clean datasets. However, whether they pursue more formal training in statistics or not, many of these students will end up working with data that are considerably more complex, and will need facility with statistical computing techniques. More importantly, these students require a framework for thinking structurally about data. We describe an undergraduate course in a liberal arts environment that provides students with the tools necessary to apply data science. The course emphasizes modern, practical, and useful skills that cover the full data analysis spectrum, from asking an interesting question to acquiring, managing, manipulating, processing, querying, analyzing, and visualizing data, as well communicating findings in written, graphical, and oral forms. Supplementary materials for this article are available online. [Received June 2014. Revised July 2015.]",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f172,2008,2008-01-03
s217,p217,Materials Data Science: Current Status and Future Outlook,"The field of materials science and engineering is on the cusp of a digital data revolution. After reviewing the nature of data science and Big Data, we discuss the features of materials data that distinguish them from data in other fields. We introduce the concept of process-structure-property (PSP) linkages and illustrate how the determination of PSPs is one of the main objectives of materials data science. Then we review a selection of materials databases, as well as important aspects of materials data management, such as storage hardware, archiving strategies, and data access strategies. We introduce the emerging field of materials data analytics, which focuses on data-driven approaches to extract and curate materials knowledge from available data sets. The critical need for materials e-collaboration platforms is highlighted, and we conclude the article with a number of suggestions regarding the near-term future of the materials data science field.",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f173,2018,2018-11-24
s218,p218,Data science ethics in government,"Data science can offer huge opportunities for government. With the ability to process larger and more complex datasets than ever before, it can provide better insights for policymakers and make services more tailored and efficient. As with all new technologies, there is a risk that we do not take up its opportunities and miss out on its enormous potential. We want people to feel confident to innovate with data. So, over the past 18 months, the Government Data Science Partnership has taken an open, evidence-based and user-centred approach to creating an ethical framework. It is a practical document that brings all the legal guidance together in one place, and is written in the context of new data science capabilities. As part of its development, we ran a public dialogue on data science ethics, including deliberative workshops, an experimental conjoint survey and an online engagement tool. The research supported the principles set out in the framework as well as provided useful insight into how we need to communicate about data science. It found that people had a low awareness of the term ‘data science’, but that showing data science examples can increase broad support for government exploring innovative uses of data. But people's support is highly context driven. People consider acceptability on a case-by-case basis, first thinking about the overall policy goals and likely intended outcome, and then weighing up privacy and unintended consequences. The ethical framework is a crucial start, but it does not solve all the challenges it highlights, particularly as technology is creating new challenges and opportunities every day. Continued research is needed into data minimization and anonymization, robust data models, algorithmic accountability, and transparency and data security. It also has revealed the need to set out a renewed deal between the citizen and state on data, to maintain and solidify trust in how we use people's data for social good. This article is part of the themed issue ‘The ethical impact of data science’.",c58,Australian Software Engineering Conference,cp58,accepted,f174,2021,2021-10-14
s220,p220,Processes Meet Big Data: Connecting Data Science with Process Science,"As more and more companies are embracing Big data, it has become apparent that the ultimate challenge is to relate massive amounts of event data to processes that are highly dynamic. To unleash the value of event data, events need to be tightly connected to the control and management of operational processes. However, the primary focus of Big data technologies is currently on storage, processing, and rather simple analytical tasks. Big data initiatives rarely focus on the improvement of end-to-end processes. To address this mismatch, we advocate a better integration of data science, data technology and process science. Data science approaches tend to be process agonistic whereas process science approaches tend to be model-driven without considering the “evidence” hidden in the data. Process mining aims to bridge this gap. This editorial discusses the interplay between data science and process science and relates process mining to Big data technologies, service orientation, and cloud computing.",j69,IEEE Transactions on Services Computing,jv69,accepted,f175,2002,2002-10-07
s221,p221,Data science and cyberinfrastructure: critical enablers for accelerated development of hierarchical materials,"Abstract The slow pace of new/improved materials development and deployment has been identified as the main bottleneck in the innovation cycles of most emerging technologies. Much of the continuing discussion in the materials development community is therefore focused on the creation of novel materials innovation ecosystems designed to dramatically accelerate materials development efforts, while lowering the overall cost involved. In this paper, it is argued that the recent advances in data science can be leveraged suitably to address this challenge by effectively mediating between the seemingly disparate, inherently uncertain, multiscale and multimodal measurements and computations involved in the current materials’ development efforts. Proper utilisation of modern data science in the materials’ development efforts can lead to a new generation of data-driven decision support tools for guiding effort investment (for both measurements and computations) at various stages of the materials development. It should also be recognised that the success of such ecosystems is predicated on the creation and utilisation of integration platforms for promoting intimate, synchronous collaborations between cross-disciplinary and distributed team members (i.e. cyberinfrastructure). Indeed, data sciences and cyberinfrastructure form the two main pillars of the emerging new discipline broadly referred to as materials informatics (MI). This paper provides a summary of current capabilities in this emerging new field as they relate to the accelerated development of advanced hierarchical materials (the internal structure plays a dominant role in controlling overall properties/performance in these materials) and identifies specific directions of research that offer the most promising avenues.",c69,International Conference on Parallel Processing,cp69,accepted,f176,2010,2010-11-02
s223,p223,Mining the Quantified Self: Personal Knowledge Discovery as a Challenge for Data Science,"The last several years have seen an explosion of interest in wearable computing, personal tracking devices, and the so-called quantified self (QS) movement. Quantified self involves ordinary people recording and analyzing numerous aspects of their lives to understand and improve themselves. This is now a mainstream phenomenon, attracting a great deal of attention, participation, and funding. As more people are attracted to the movement, companies are offering various new platforms (hardware and software) that allow ever more aspects of daily life to be tracked. Nearly every aspect of the QS ecosystem is advancing rapidly, except for analytic capabilities, which remain surprisingly primitive. With increasing numbers of qualified self participants collecting ever greater amounts and types of data, many people literally have more data than they know what to do with. This article reviews the opportunities and challenges posed by the QS movement. Data science provides well-tested techniques for knowledge discovery. But making these useful for the QS domain poses unique challenges that derive from the characteristics of the data collected as well as the specific types of actionable insights that people want from the data. Using a small sample of QS time series data containing information about personal health we provide a formulation of the QS problem that connects data to the decisions of interest to the user.",c10,Big Data,cp10,accepted,f177,2021,2021-09-07
s224,p224,Statistics and computing: the genesis of data science,Abstract content goes here ...,j70,Statistics and computing,jv70,accepted,f178,2005,2005-01-07
s225,p225,Data Science and Digital Art History,"I present a number of core concepts from data science that are relevant to digital art history and the use of quantitative methods to study any cultural artifacts or processes in general. These concepts are objects, features, data, feature space, and dimension reduction. These concepts enable computational exploration of both large and small visual cultural data. We can analyze relations between works on a single artist, many artists, all digitized production from a whole historical period, holdings in museum collections, collection metadata, or writings about art. The same concepts allow us to study contemporary vernacular visual media using massive social media content. (In our lab, we analyzed works by van Gogh, Mondrian, and Rothko, 6000 paintings by French Impressionists, 20,000 photographs from MoMA photo­graphy collection, one million manga pages from manga books, one million artworks of contemporary non-professional artists, and over 13 million Instagram images from 16 global cities.) While data science techniques do not replace other art historical methods, they allow us to see familiar art historical material in new ways, and also to study contemporary digital visual culture.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f179,2010,2010-07-02
s226,p226,Exploration of data science techniques to predict fatigue strength of steel from composition and processing parameters,Abstract content goes here ...,c60,IEEE International Conference on Software Engineering and Formal Methods,cp60,accepted,f180,2011,2011-05-23
s227,p227,Data Science,Abstract content goes here ...,j71,Communications in Computer and Information Science,jv71,accepted,f181,2015,2015-11-19
s228,p228,Data Science in Statistics Curricula: Preparing Students to “Think with Data”,"A growing number of students are completing undergraduate degrees in statistics and entering the workforce as data analysts. In these positions, they are expected to understand how to use databases and other data warehouses, scrape data from Internet sources, program solutions to complex problems in multiple languages, and think algorithmically as well as statistically. These data science topics have not traditionally been a major component of undergraduate programs in statistics. Consequently, a curricular shift is needed to address additional learning outcomes. The goal of this article is to motivate the importance of data science proficiency and to provide examples and resources for instructors to implement data science in their own statistics curricula. We provide case studies from seven institutions. These varied approaches to teaching data science demonstrate curricular innovations to address new needs. Also included here are examples of assignments designed for courses that foster engagement of undergraduates with data and data science. [Received November 2014. Revised July 2015.]",c1,Technical Symposium on Computer Science Education,cp1,accepted,f182,2002,2002-07-02
s229,p229,A Big Data Guide to Understanding Climate Change: The Case for Theory-Guided Data Science,"Global climate change and its impact on human life has become one of our era's greatest challenges. Despite the urgency, data science has had little impact on furthering our understanding of our planet in spite of the abundance of climate data. This is a stark contrast from other fields such as advertising or electronic commerce where big data has been a great success story. This discrepancy stems from the complex nature of climate data as well as the scientific questions climate science brings forth. This article introduces a data science audience to the challenges and opportunities to mine large climate datasets, with an emphasis on the nuanced difference between mining climate data and traditional big data approaches. We focus on data, methods, and application challenges that must be addressed in order for big data to fulfill their promise with regard to climate science applications. More importantly, we highlight research showing that solely relying on traditional big data techniques results in dubious findings, and we instead propose a theory-guided data science paradigm that uses scientific theory to constrain both the big data techniques as well as the results-interpretation process to extract accurate insight from large climate data.",c10,Big Data,cp10,accepted,f183,2021,2021-05-17
s230,p230,HEALTH BANK - A Workbench for Data Science Applications in Healthcare,"The enormous amounts of data that are generated in the healthcare process and stored in electronic health record (EHR) systems are an underutilized resource that, with the use of data science applica- tions, can be exploited to improve healthcare. To foster the development and use of data science applications in healthcare, there is a fundamen- tal need for access to EHR data, which is typically not readily available to researchers and developers. A relatively rare exception is the large EHR database, the Stockholm EPR Corpus, comprising data from more than two million patients, that has been been made available to a lim- ited group of researchers at Stockholm University. Here, we describe a number of data science applications that have been developed using this database, demonstrating the potential reuse of EHR data to support healthcare and public health activities, as well as facilitate medical re- search. However, in order to realize the full potential of this resource, it needs to be made available to a larger community of researchers, as well as to industry actors. To that end, we envision the provision of an in- frastructure around this database called HEALTH BANK – the Swedish Health Record Research Bank. It will function both as a workbench for the development of data science applications and as a data explo- ration tool, allowing epidemiologists, pharmacologists and other medical researchers to generate and evaluate hypotheses. Aggregated data will be fed into a pipeline for open e-access, while non-aggregated data will be provided to researchers within an ethical permission framework. We believe that HEALTH BANK has the potential to promote a growing industry around the development of data science applications that will ultimately increase the efficiency and effectiveness of healthcare.",c72,Intelligent Systems in Molecular Biology,cp72,accepted,f184,2003,2003-02-18
s231,p231,Statistics: a data science for the 21st century,"The rise of data science could be seen as a potental threat to the long‐term status of the statistics discipline. I first argue that, although there is a threat, there is also a much greater opportunity to re‐emphasize the universal relevance of statistical method to the interpretation of data, and I give a short historical outline of the increasingly important links between statistics and information technology. The core of the paper is a summary of several recent research projects, through which I hope to demonstrate that statistics makes an essential, but incomplete, contribution to the emerging field of ‘electronic health’ research. Finally, I offer personal thoughts on how statistics might best be organized in a research‐led university, on what we should teach our students and on some issues broadly related to data science where the Royal Statistical Society can take a lead.",c37,Asia-Pacific Software Engineering Conference,cp37,accepted,f185,2008,2008-02-27
s232,p232,Nursing Knowledge: Big Data Science—Implications for Nurse Leaders,"The integration of Big Data from electronic health records and other information systems within and across health care enterprises provides an opportunity to develop actionable predictive models that can increase the confidence of nursing leaders' decisions to improve patient outcomes and safety and control costs. As health care shifts to the community, mobile health applications add to the Big Data available. There is an evolving national action plan that includes nursing data in Big Data science, spearheaded by the University of Minnesota School of Nursing. For the past 3 years, diverse stakeholders from practice, industry, education, research, and professional organizations have collaborated through the “Nursing Knowledge: Big Data Science” conferences to create and act on recommendations for inclusion of nursing data, integrated with patient-generated, interprofessional, and contextual data. It is critical for nursing leaders to understand the value of Big Data science and the ways to standardize data and workflow processes to take advantage of newer cutting edge analytics to support analytic methods to control costs and improve patient quality and safety.",j72,Nursing Administration Quarterly,jv72,accepted,f186,2016,2016-09-21
s233,p233,Taking a Chance in the Classroom: Setting the Stage for Data Science: Integration of Data Management Skills in Introductory and Second Courses in Statistics,"Many have argued that statistics students need additional facility to express statistical computations. By introducing students to commonplace tools for data management, visualization, and reproducible analysis in data science and applying these to real-world scenarios, we prepare them to think statistically. In an era of increasingly big data, it is imperative that students develop data-related capacities, beginning with the introductory course. We believe that the integration of these precursors to data science into our curricula-early and often-will help statisticians be part of the dialogue regarding ""Big Data"" and ""Big Questions"".",c19,ACM Conference on Economics and Computation,cp19,accepted,f187,2002,2002-11-29
s234,p234,Practical Data Science with R,"Summary Practical Data Science with R lives up to its name. It explains basic principles without the theoretical mumbo-jumbo and jumps right to the real use cases you'll face as you collect, curate, and analyze the data crucial to the success of your business. You'll apply the R programming language and statistical analysis techniques to carefully explained examples based in marketing, business intelligence, and decision support. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Book Business analysts and developers are increasingly collecting, curating, analyzing, and reporting on crucial business data. The R language and its associated tools provide a straightforward way to tackle day-to-day data science tasks without a lot of academic theory or advanced mathematics. Practical Data Science with R shows you how to apply the R programming language and useful statistical techniques to everyday business situations. Using examples from marketing, business intelligence, and decision support, it shows you how to design experiments (such as A/B tests), build predictive models, and present results to audiences of all levels. This book is accessible to readers without a background in data science. Some familiarity with basic statistics, R, or another scripting language is assumed. What's Inside Data science for the business professional Statistical analysis using the R language Project lifecycle, from planning to delivery Numerous instantly familiar use cases Keys to effective data presentations About the Authors Nina Zumel and John Mount are cofounders of a San Francisco-based data science consulting firm. Both hold PhDs from Carnegie Mellon and blog on statistics, probability, and computer science at win-vector.com.",c64,Experimental Software Engineering Network,cp64,accepted,f188,2014,2014-02-21
s235,p235,An undergraduate degree in data science: curriculum and a decade of implementation experience,"We describe Data Science, a four-year undergraduate program in predictive analytics, machine learning, and data mining implemented at the College of Charleston, Charleston, South Carolina, USA. We present a ten-year status report detailing the program's origins, successes, and challenges. Our experience demonstrates that education and training for big data concepts are possible and practical at the undergraduate level. The development of this program parallels the growing demand for finding utility in data sets and streaming data. The curriculum is a seventy-seven credit-hour program that has been successfully implemented in a liberal arts and sciences institution by the faculties of computer science and mathematics.",c1,Technical Symposium on Computer Science Education,cp1,accepted,f189,2002,2002-08-15
s236,p236,Intelligent services for Big Data science,Abstract content goes here ...,j73,Future generations computer systems,jv73,accepted,f190,2020,2020-04-26
s237,p237,DataHub: Collaborative Data Science & Dataset Version Management at Scale,"Relational databases have limited support for data collaboration, where teams collaboratively curate and analyze large datasets. Inspired by software version control systems like git, we propose (a) a dataset version control system, giving users the ability to create, branch, merge, difference and search large, divergent collections of datasets, and (b) a platform, DATAHUB, that gives users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale.",c18,Conference on Innovative Data Systems Research,cp18,accepted,f191,2012,2012-11-17
s238,p238,Data science: An action plan for expanding the technical areas of the field of statistics,"An action plan to expand the technical areas of statistics focuses on the data analyst. The plan sets out six technical areas of work for a university department and advocates a specific allocation of resources devoted to research in each area and to courses in each area. The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly. The plan is also applicable to government research labs and corporate research organizations.",j74,Statistical analysis and data mining,jv74,accepted,f192,2015,2015-04-17
s239,p239,"Big Data, Big Problems: Emerging Issues in the Ethics of Data Science and Journalism","As big data techniques become widespread in journalism, both as the subject of reporting and as newsgathering tools, the ethics of data science must inform and be informed by media ethics. This article explores emerging problems in ethical research using big data techniques. It does so using the duty-based framework advanced by W.D. Ross, who has significantly influenced both research science and media ethics. A successful framework must provide stability and flexibility. Without stability, ethical precommitments will vanish as technology rapidly shifts costs. Without flexibility, traditional approaches will rapidly become obsolete in the face of technological change. The article concludes that Ross's duty-based approach both provides stability in the face of rapid technological change and flexibility to innovate to achieve the original purpose of basic ethical principles.",c105,Biometrics and Identity Management,cp105,accepted,f193,2006,2006-04-07
s240,p240,Philosophy of Big Data: Expanding the Human-Data Relation with Big Data Science Services,"Big data is growing as an area of information technology, service, and science, and so too is the need for its intellectual understanding and interpretation from a theoretical, philosophical, and societal perspective. The Philosophy of Big Data is the branch of philosophy concerned with the foundations, methods, and implications of big data, the definitions, meaning, conceptualization, knowledge possibilities, truth standards, and practices in situations involving very-large data sets that are big in volume, velocity, variety, veracity, and variability. The Philosophy of Big Data is evolving into a discipline at two levels, one internal to the field as a generalized articulation of the concepts, theory, and systems that comprise the overall conduct of big data science. The other is external to the field, as a consideration of the impact of big data science more broadly on individuals, society, and the world. Methods, tools, and concepts are evaluated at both the level of industry practice theory and social impact. Three aspects are considered: what might constitute a Philosophy of Big Data, how the disciplines of the Philosophy of Information and the Philosophy of Big Data are developing, and an example of the Philosophy of Big Data in application in the data-intensive science field of Synthetic Biology. Overall a Philosophy of Big Data might helpful in conceptualizing and realizing big data science as a service practice, and also in transitioning to data-rich futures with human and data entities more productively co-existing in mutual growth and collaboration.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f194,2011,2011-09-14
s241,p241,Mechanism design for data science,"The promise of data science is that if data from a system can be recorded and understood then this understanding can potentially be utilized to improve the system. Behavioral and economic data, however, is different from scientific data in that it is subjective to the system. Behavior changes when the system changes, and to predict behavior for any given system change or to optimize over system changes, the behavioral model that generates the data must be inferred from the data. The ease with which this inference can be performed generally also depends on the system. Trivially, a system that ignores behavior does not admit any inference of a behavior generating model that can be used to predict behavior in a system that is responsive to behavior. To realize the promise of data science in economic systems, a theory for the design of such systems must also incorporate the desired inference properties. Consider as an example the revenue-maximizing auctioneer. If the auctioneer has knowledge of the distribution of bidder values then she can run the first-price auction with a reserve price that is tuned to the distribution. Under some mild distributional assumptions, with the appropriate reserve price the first-price auction is revenue optimal [Myerson 1981]. Notice that the historical bid data for the first-price auction with a reserve price will in most cases not have bids for bidders whose values are below the reserve. Therefore, there is no data analysis that the auctioneer can perform that will enable properties of the distribution of bidder values below the reserve price to be inferred. It could be, nonetheless, that over time the population of potential bidders evolves and the optimal reserve price lowers. This change could go completely unnoticed in the auctioneer's data. The two main tools for optimizing revenue in an auction are reserve prices (as above) and ironing. Both of these tools cause pooling behavior (i.e., bidders with distinct values take the same action) and economic inference cannot thereafter differentiate these pooled bidders. In order to maintain the distributional knowledge necessary to be able to run a good auction in the long term, the auctioneer must sacrifice the short-term revenue by running a non-revenue-optimal auction.",c19,ACM Conference on Economics and Computation,cp19,accepted,f195,2002,2002-03-28
s242,p242,Analytics in a Big Data World: The Essential Guide to Data Science and its Applications,"The guide to targeting and leveraging business opportunities using big data & analytics By leveraging big data & analytics, businesses create the potential to better understand, manage, and strategically exploiting the complex dynamics of customer behavior. Analytics in a Big Data World reveals how to tap into the powerful tool of data analytics to create a strategic advantage and identify new business opportunities. Designed to be an accessible resource, this essential book does not include exhaustive coverage of all analytical techniques, instead focusing on analytics techniques that really provide added value in business environments. The book draws on author Bart Baesens' expertise on the topics of big data, analytics and its applications in e.g. credit risk, marketing, and fraud to provide a clear roadmap for organizations that want to use data analytics to their advantage, but need a good starting point. Baesens has conducted extensive research on big data, analytics, customer relationship management, web analytics, fraud detection, and credit risk management, and uses this experience to bring clarity to a complex topic. * Includes numerous case studies on risk management, fraud detection, customer relationship management, and web analytics * Offers the results of research and the author's personal experience in banking, retail, and government * Contains an overview of the visionary ideas and current developments on the strategic use of analytics for business * Covers the topic of data analytics in easy-to-understand terms without an undo emphasis on mathematics and the minutiae of statistical analysis For organizations looking to enhance their capabilities via data analytics, this resource is the go-to reference for leveraging data to enhance business capabilities.",c102,International Conference on Biometrics,cp102,accepted,f196,2022,2022-04-30
s243,p243,Data Science,Abstract content goes here ...,j75,Lecture Notes in Computer Science,jv75,accepted,f197,2015,2015-06-11
s245,p245,A survey of open source data science tools,"Purpose – Data science is the study of the generalizable extraction of knowledge from data. It includes a variety of components and develops on methods and concepts from many domains, containing mathematics, probability models, machine learning, statistical learning, computer programming, data engineering, pattern recognition and learning, visualization and data warehousing aiming to extract value from data. The purpose of this paper is to provide an overview of open source (OS) data science tools, proposing a classification scheme that can be used to study OS data science software. Design/methodology/approach – The proposed classification scheme is based on general characteristics, project activity, operational characteristics and data mining characteristics. The authors then use the proposed scheme to examine 70 identified Open Source Software. From this the authors provide insight about the current status of OS data science tools and reveal the state-of-the-art tools. Findings – The features of 70 OS t...",j77,International Journal of Intelligent Computing and Cybernetics,jv77,accepted,f198,2006,2006-12-26
s246,p246,Dealing with Data: Science Librarians' Participation in Data Management at Association of Research Libraries Institutions,"As long as empirical research has existed, researchers have been doing “data management” in one form or another. However, funding agency mandates for doing formal data management are relatively recent, and academic libraries’ involvement has been concentrated mainly in the last few years. The National Science Foundation implemented a new mandate in January 2011, requiring researchers to include a data management plan with their proposals for funding. This has prompted many academic libraries to work more actively than before in data management, and science librarians in particular are uniquely poised to step into new roles to meet researchers’ data management needs. This study, a survey of science librarians at institutions affiliated with the Association of Research Libraries, investigates science librarians’ awareness of and involvement in institutional repositories, data repositories, and data management support services at their institutions. The study also explores the roles and responsibilities, both new and traditional, that science librarians have assumed related to data management, and the skills that science librarians believe are necessary to meet the demands of data management work. The results reveal themes of both uncertainty and optimism—uncertainty about the roles of librarians, libraries, and other campus entities; uncertainty about the skills that will be required; but also optimism about applying “traditional” librarian skills to this emerging field of academic librarianship.",j78,College and Research Libraries,jv78,accepted,f199,2012,2012-12-05
s247,p247,Editor’s comments: the business of business data science in IS journals,"Data science is at the core of a host of ongoing business transformations and disruptive technologies. The application of data science methods to new and old business problems presents a wealth of research opportunities upon which the information systems (IS) data science community is uniquely positioned to focus. Strong demand for business data science education by both students and recruiters has also given rise to new business analytics programs, often lead by IS groups. This mission will be well served by an active business data science research within IS. While contributions by IS data science scholars are being published in the premier reference discipline journals, and although such a relationship with the reference disciplines is immensely important to maintain, it may be difficult to sustain IS data science research without a healthy presence in the leading IS journals. Furthermore, IS journals are arguably best positioned to publish research at the nexus of data science, business, and society. The objective of this editorial is to inspire a discussion on opportunities to facilitate IS data science reviewing and publication in top IS journals, and to capitalize on these opportunities effectively as a community. This discussion is based on the understanding that both writing and reviewing practices are fundamental to our ability to assess the quality of data science contributions and to draw upon and publish high-quality and impactful research. These practices also affect the potential to sustain and grow an impactful data science community. Toward that end, this editorial also hopes to initiate a discussion on sustaining and growing a data science community within IS.",c85,International Conference on Graph Transformation,cp85,accepted,f200,2007,2007-03-24
s248,p248,Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking,"Written by renowned data science experts Foster Provost and Tom Fawcett, Data Science for Business introduces the fundamental principles of data science, and walks you through the ""data-analytic thinking"" necessary for extracting useful knowledge and business value from the data you collect. This guide also helps you understand the many data-mining techniques in use today. Based on an MBA course Provost has taught at New York University over the past ten years, Data Science for Business provides examples of real-world business problems to illustrate these principles. Youll not only learn how to improve communication between business stakeholders and data scientists, but also how participate intelligently in your companys data science projects. Youll also discover how to think data-analytically, and fully appreciate how data science methods can support business decision-making.Understand how data science fits in your organizationand how you can use it for competitive advantage Treat data as a business asset that requires careful investment if youre to gain real value Approach business problems data-analytically, using the data-mining process to gather good data in the most appropriate way Learn general concepts for actually extracting knowledge from data Apply data science principles when interviewing data science job candidates",c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f201,2012,2012-05-06
s249,p249,Theory-Guided Data Science for Climate Change,"To adequately address climate change, we need novel data-science methods that account for the spatiotemporal and physical nature of climate phenomena. Only then will we be able to move from statistical analysis to scientific insights.",j79,Computer,jv79,accepted,f202,2014,2014-12-15
s250,p250,Data Science For Dummies,"Discover how data science can help you gain in-depth insight into your business the easy way! Jobs in data science abound, but few people have the data science skills needed to fill these increasingly important roles in organizations. Data Science For Dummies is the perfect starting point for IT professionals and students interested in making sense of their organization s massive data sets and applying their findings to real-world business scenarios. From uncovering rich data sources to managing large amounts of data within hardware and software limitations, ensuring consistency in reporting, merging various data sources, and beyond, you ll develop the know-how you need to effectively interpret data and tell a story that can be understood by anyone in your organization. * Provides a background in data science fundamentals before moving on to working with relational databases and unstructured data and preparing your data for analysis * Details different data visualization techniques that can be used to showcase and summarize your data * Explains both supervised and unsupervised machine learning, including regression, model validation, and clustering techniques * Includes coverage of big data processing tools like MapReduce, Hadoop, Dremel, Storm, and Spark It s a big, big data world out there let Data Science For Dummies help you harness its power and gain a competitive edge for your organization.",c22,International Conference on Data Technologies and Applications,cp22,accepted,f203,2020,2020-08-03
s251,p251,A Data Science Solution for Mining Interesting Patterns from Uncertain Big Data,"Nowadays, high volumes of valuable uncertain data can be easily collected or generated at high velocity in many real-life applications. Mining these uncertain Big data is computationally intensive due to the presence of existential probability values associated with items in every transaction in the uncertain data. Each existential probability value expresses the likelihood of that item to be present in a particular transaction in the Big data. In some situations, users may be interested in mining all frequent patterns from these uncertain Big data, in other situations, users may be interested in only a tiny portion of these mined patterns. To reduce the computation and to focus the mining for the latter situations, we propose a tree-based algorithm that (i) allows users to express the patterns to be mined according to their intention via the use of constraints and (ii) uses MapReduce to mine uncertain Big data for only those frequent patterns that satisfy user-specified constraints. Experimental results show the effectiveness of our algorithm in mining interesting patterns from uncertain Big data.",c77,Networks,cp77,accepted,f204,2019,2019-04-05
s252,p252,"Intrinsic Relations between Data Science, Big Data, Business Analytics and Datafication","Data recording and storage have evolved over the past decades from manual gathering of data by using simple writing materials to the automation of data collection. Data storage has evolved significantly in the past decades and today databases no longer suffice as the only medium for the storage and management of data. This is due to the emergence of the Big Data and Data Science concepts. Previous studies have indicated that the multiplication of processing power of computers and the availability of larger data storage at reduced cost are part of the catalysts for the volume and rate at which data is now made available and captured.
 In this paper, the concepts of Big Data, Data Science and Business Analytics are reviewed. This paper discusses datafication of different aspects of life as the fundamental concept behind the growth of Big Data and Data Science. A review of the characteristics and value of Big Data and Data Science suggests that these emerging concepts will bring a paradigm change to a number of areas. Big Data was described as the basis for Data Science and Business Analytics which are tools employed in Data Science. Because these fields are still developing, there are diverse opinions, especially on the definition of Data Science. This paper provides a revised definition of Data Science, based on the review of available literature and proposes a schematic representation of the concepts.",c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,cp20,accepted,f205,2014,2014-10-16
s253,p253,Divide and recombine (D&R): Data science for large complex data,"The need for deep analysis of large complex data has brought a focus to data science. The reasoning is simple. Data science consists of all technical areas that come into play in the analysis of data, and deep analysis of large complex data challenges all of the technical areas, from statistical theory to the architecture of clusters designed specifically for data. What is more, research in the technical areas needs to be tightly integrated.",j74,Statistical analysis and data mining,jv74,accepted,f206,2015,2015-10-21
s254,p254,Doing Data Science: Straight Talk from the Frontline,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field thats so clouded in hype? This insightful book, based on Columbia Universitys Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If youre familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include:Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy ONeil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",c26,PS,cp26,accepted,f207,2010,2010-11-15
s255,p255,Data science as an undergraduate degree,The purpose of this panel is to discuss the creation and implementation of a data science degree program at the undergraduate level. The panel includes representatives from three different universities that each offers an undergraduate degree in Data Science as of fall 2013. We plan to share information on the logistics of how the data science programs came to exist at each of our schools as well as encourage a robust interactive discussion about the future of data science education at the undergraduate level.,c1,Technical Symposium on Computer Science Education,cp1,accepted,f208,2002,2002-03-04
s256,p256,Data science for business,"Written by renowned data science experts Foster Provost and Tom Fawcett, Data Science for Business introduces the fundamental principles of data science, and walks you through the ""data-analytic thinking"" necessary for extracting useful knowledge and business value from the data you collect. This guide also helps you understand the many data-mining techniques in use today. Based on an MBA course Provost has taught at New York University over the past ten years, Data Science for Business provides examples of real-world business problems to illustrate these principles. You'll not only learn how to improve communication between business stakeholders and data scientists, but also how participate intelligently in your company's data science projects. You'll also discover how to think data-analytically, and fully appreciate how data science methods can support business decision-making. Understand how data science fits in your organization - and how you can use it for competitive advantage Treat data as a business asset that requires careful investment if you're to gain real value Approach business problems data-analytically, using the data-mining process to gather good data in the most appropriate way Learn general concepts for actually extracting knowledge from data Apply data science principles when interviewing data science job candidates",c64,Experimental Software Engineering Network,cp64,accepted,f209,2014,2014-10-07
s257,p257,The Science of Data Science,Abstract content goes here ...,c10,Big Data,cp10,accepted,f210,2021,2021-10-11
s258,p258,Data science and prediction,Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.,c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f211,2006,2006-05-13
s259,p259,Doing Data Science,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field that's so clouded in hype? This insightful book, based on Columbia University's Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If you're familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include: Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy O'Neil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,cp79,accepted,f212,2020,2020-11-15
s260,p260,"Color Science: Concepts and Methods, Quantitative Data and Formulae, 2nd Edition",Physical Data. The Eye. Colorimetry. Photometry. Visual Equivalence and Visual Matching. Uniform Color Scales. Visual Thresholds. Theories and Models of Color Vision. Appendix. References. Author and Subject Indexes.,c16,Knowledge Discovery and Data Mining,cp16,accepted,f213,2003,2003-08-03
s261,p261,Data Smart: Using Data Science to Transform Information into Insight,"Data Science gets thrown around in the press like it's magic. Major retailers are predicting everything from when their customers are pregnant to when they want a new pair of Chuck Taylors. It's a brave new world where seemingly meaningless data can be transformed into valuable insight to drive smart business decisions.But how does one exactly do data science? Do you have to hire one of these priests of the dark arts, the ""data scientist,"" to extract this gold from your data? Nope.Data science is little more than using straight-forward steps to process raw data into actionable insight. And inData Smart, author and data scientist John Foreman will show you how that's done within the familiar environment of a spreadsheet.",c9,Pacific Symposium on Biocomputing,cp9,accepted,f214,2009,2009-03-19
s262,p262,Computing: A vision for data science,Abstract content goes here ...,j62,Nature,jv62,accepted,f215,2017,2017-10-29
s263,p263,Sjplot - Data Visualization For Statistics In Social Science.,"New functions


 tab_model() as replacement for sjt.lm() , sjt.glm() , sjt.lmer() and sjt.glmer() . Furthermore, tab_model() is designed to work with the same model-objects as plot_model() .
 New colour scales for ggplot-objects: scale_fill_sjplot() and scale_color_sjplot() . These provide predifined colour palettes from this package.
 show_sjplot_pals() to show all predefined colour palettes provided by this package.
 sjplot_pal() to return colour values of a specific palette.


Deprecated

Following functions are now deprecated:


 sjp.lm() , sjp.glm() , sjp.lmer() , sjp.glmer() and sjp.int() . Please use plot_model() instead.
 sjt.frq() . Please use sjmisc::frq(out = ""v"") instead.


Removed / Defunct

Following functions are now defunct:


 sjt.grpmean() , sjt.mwu() and sjt.df() . The replacements are sjstats::grpmean() , sjstats::mwu() and tab_df() resp. tab_dfs() .


Changes to functions


 plot_model() and plot_models() get a prefix.labels -argument, to prefix automatically retrieved term labels with either the related variable name or label.
 plot_model() gets a show.zeroinf -argument to show or hide the zero-inflation-part of models in the plot.
 plot_model() gets a jitter -argument to add some random variation to data points for those plot types that accept show.data = TRUE .
 plot_model() gets a legend.title -argument to define the legend title for plots that display a legend.
 plot_model() now passes more arguments in ... down to ggeffects::plot() for marginal effects plots.
 plot_model() now plots the zero-inflated part of the model for brmsfit -objects.
 plot_model() now plots multivariate response models, i.e. models with multiple outcomes.
 Diagnostic plots in plot_model() ( type = ""diag"" ) can now also be used with brmsfit -objects.
 Axis limits of diagnostic plots in plot_model() ( type = ""diag"" ) for Stan-models ( brmsfit or stanreg resp. stanfit ) can now be set with the axis.lim -argument.
 The grid.breaks -argument for plot_model() and plot_models() now also takes a vector of values to directly define the grid breaks for the plot.
 Better default calculation for grid breaks in plot_model() and plot_models() when the grid.breaks -argument is of length one.
 The terms -argument for plot_model() now also allows the specification of a range of numeric values in square brackets for marginal effects plots, e.g. terms = ""age [30:50]"" or terms = ""age [pretty]"" .
 For coefficient-plots, the terms - and rm.terms -arguments for plot_model() now also allows specification of factor levels for categorical terms. Coefficients for the indicted factor levels are kept resp. removed (see ?plot_model for details).
 plot_model() now supports clmm -objects (package ordinal).
 plot_model(type = ""diag"") now also shows random-effects QQ-plots for glmmTMB -models, and also plots random-effects QQ-plots for all random effects (if model has more than one random effect term).


Bug fixes


 plot_model(type = ""re"") now supports standard errors and confidence intervals for glmmTMB -objects.
 Fixed typo for glmmTMB -tidier, which may have returned wrong data for zero-inflation part of model.
 Multiple random intercepts for multilevel models fitted with brms area now shown in each own facet per intercept.
 Remove unnecessary warning in sjp.likert() for uneven category count when neutral category is specified.
 plot_model(type = ""int"") could not automatically select mdrt.values properly for non-integer variables.
 sjp.grpfrq() now correctly uses the complete space in facets when facet.grid = TRUE .
 sjp.grpfrq(type = ""boxplot"") did not correctly label the x-axis when one category had no elements in a vector.
 Problems with German umlauts when printing HTML tables were fixed.",c25,International Conference on Contemporary Computing,cp25,accepted,f216,2014,2014-05-08
s264,p264,Network analysis of multivariate data in psychological science,Abstract content goes here ...,j80,Nature Reviews Methods Primers,jv80,accepted,f217,2011,2011-11-09
s265,p265,"Data-driven science and engineering: machine learning, dynamical systems, and control",Abstract content goes here ...,c11,Hawaii International Conference on System Sciences,cp11,accepted,f218,2006,2006-03-27
s266,p266,"Scopus as a curated, high-quality bibliometric data source for academic research in quantitative science studies","Scopus is among the largest curated abstract and citation databases, with a wide global and regional coverage of scientific journals, conference proceedings, and books, while ensuring only the highest quality data are indexed through rigorous content selection and re-evaluation by an independent Content Selection and Advisory Board. Additionally, extensive quality assurance processes continuously monitor and improve all data elements in Scopus. Besides enriched metadata records of scientific articles, Scopus offers comprehensive author and institution profiles, obtained from advanced profiling algorithms and manual curation, ensuring high precision and recall. The trustworthiness of Scopus has led to its use as bibliometric data source for large-scale analyses in research assessments, research landscape studies, science policy evaluations, and university rankings. Scopus data have been offered for free for selected studies by the academic research community, such as through application programming interfaces, which have led to many publications employing Scopus data to investigate topics such as researcher mobility, network visualizations, and spatial bibliometrics. In June 2019, the International Center for the Study of Research was launched, with an advisory board consisting of bibliometricians, aiming to work with the scientometric research community and offering a virtual laboratory where researchers will be able to utilize Scopus data.",j81,Quantitative Science Studies,jv81,accepted,f219,2018,2018-10-03
s267,p267,Web of Science as a data source for research on scientific and scholarly activity,"Web of Science (WoS) is the world’s oldest, most widely used and authoritative database of research publications and citations. Based on the Science Citation Index, founded by Eugene Garfield in 1964, it has expanded its selective, balanced, and complete coverage of the world’s leading research to cover around 34,000 journals today. A wide range of use cases are supported by WoS from daily search and discovery by researchers worldwide through to the supply of analytical data sets and the provision of specialized access to raw data for bibliometric partners. A long- and well-established network of such partners enables the Institute for Scientific Information (ISI) to continue to work closely with bibliometric groups around the world to the benefit of both the community and the services that the company provides to researchers and analysts.",j81,Quantitative Science Studies,jv81,accepted,f220,2018,2018-05-12
s268,p268,Numerical data and functional relationships in science and technology,Abstract content goes here ...,c60,IEEE International Conference on Software Engineering and Formal Methods,cp60,accepted,f221,2011,2011-12-07
s269,p269,The data science education dilemma,"The need for people fluent in working with data is growing rapidly and enormously, but U.S. K–12 education does not provide meaningful learning experiences designed to develop understanding of data science concepts or a fluency with data science skills. Data science is inherently inter- disciplinary, so it makes sense to integrate it with existing content areas, but difficulties abound. Consideration of the work involved in doing data science and the habits of mind that lie behind it leads to a way of thinking about integrating data science with mathematics and science. Examples drawn from current activity development in the Data Games project shed some light on what technology-based, data-driven might be like. The project’s ongoing research on learners’ conceptions of organizing data and the relevance to data science education is explained.",c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f222,2019,2019-06-16
s270,p270,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",j20,Proceedings of the National Academy of Sciences of the United States of America,jv20,accepted,f223,2006,2006-07-19
s271,p271,"Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic","We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.",j81,Quantitative Science Studies,jv81,accepted,f224,2018,2018-08-30
s272,p272,Harnessing the GPS Data Explosion for Interdisciplinary Science,"More GPS stations, faster data delivery, and better data processing provide an abundance of information for all kinds of Earth scientists.",j82,EOS,jv82,accepted,f225,2017,2017-09-04
s273,p273,Data-Driven Science and Engineering,Abstract content goes here ...,c41,Software Product Lines Conference,cp41,accepted,f226,2002,2002-05-24
s275,p275,TENDL: Complete Nuclear Data Library for Innovative Nuclear Science and Technology,Abstract content goes here ...,j84,Nuclear Data Sheets,jv84,accepted,f227,2013,2013-07-15
s276,p276,"ENDF/B-VII.1 Nuclear Data for Science and Technology: Cross Sections, Covariances, Fission Product Yields and Decay Data",Abstract content goes here ...,c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f228,2006,2006-07-05
s277,p277,Perspective: Materials informatics and big data: Realization of the “fourth paradigm” of science in materials science,"Our ability to collect “big data” has greatly surpassed our capability to analyze it, underscoring the emergence of the fourth paradigm of science, which is data-driven discovery. The need for data informatics is also emphasized by the Materials Genome Initiative (MGI), further boosting the emerging field of materials informatics. In this article, we look at how data-driven techniques are playing a big role in deciphering processing-structure-property-performance relationships in materials, with illustrative examples of both forward models (property prediction) and inverse models (materials discovery). Such analytics can significantly reduce time-to-insight and accelerate cost-effective materials discovery, which is the goal of MGI.",c56,European Conference on Software Process Improvement,cp56,accepted,f229,2016,2016-10-10
s279,p279,ImageJ2: ImageJ for the next generation of scientific image data,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f230,2007,2007-10-05
s280,p280,What Is Data Science,"Program of Study The technological revolution has led to an explosion of data in domains of knowledge including medicine, policy, social sciences, commerce, and the natural sciences. Petabytes of data are being collected from a myriad of instruments, like sequencing machines for genomics and mobile devices for quantifying social interactions. In addition to driving research, data are shaping the way people work, live, and communicate. Correspondingly, new methodologies have emerged to power intelligent systems, make more accurate predictions, and gain new insight using the large volumes of data generated by scientists, entrepreneurs, and analysts.",c15,International Conference on Conceptual Structures,cp15,accepted,f231,2011,2011-08-27
s282,p282,Social media data for conservation science: A methodological overview,Abstract content goes here ...,j87,Biological Conservation,jv87,accepted,f232,2014,2014-06-12
s283,p283,Engagement in science through citizen science: Moving beyond data collection,"""To date, most studies of citizen science engagement focus on quantifiable measures related to the contribution of data or other output measures. Few studies have attempted to qualitatively characterize citizen science engagement across multiple projects and from the perspective of the participants. Building on pertinent literature and sociocultural learning theories, this study operationalizes engagement in citizen science through an analysis of interviews of 72 participants from six different environmentally based projects. We document engagement in citizen science through an examination of cognitive, affective, social, behavioral, and motivational dimensions. We assert that engagement in citizen science is enhanced by acknowledging these multiple dimensions and creating opportunities for volunteers to find personal relevance in their work with scientists. A Dimensions of Engagement framework is presented that can facilitate the innovation of new questions and methodologies for studying engagement in citizen science and other forms of informal science education.""",j88,Science Education,jv88,accepted,f233,2021,2021-07-05
s284,p284,Machine intelligence and the data-driven future of marine science,"
 Oceans constitute over 70% of the earth's surface, and the marine environment and ecosystems are central to many global challenges. Not only are the oceans an important source of food and other resources, but they also play a important roles in the earth's climate and provide crucial ecosystem services. To monitor the environment and ensure sustainable exploitation of marine resources, extensive data collection and analysis efforts form the backbone of management programmes on global, regional, or national levels. Technological advances in sensor technology, autonomous platforms, and information and communications technology now allow marine scientists to collect data in larger volumes than ever before. But our capacity for data analysis has not progressed comparably, and the growing discrepancy is becoming a major bottleneck for effective use of the available data, as well as an obstacle to scaling up data collection further. Recent years have seen rapid advances in the fields of artificial intelligence and machine learning, and in particular, so-called deep learning systems are now able to solve complex tasks that previously required human expertise. This technology is directly applicable to many important data analysis problems and it will provide tools that are needed to solve many complex challenges in marine science and resource management. Here we give a brief review of recent developments in deep learning, and highlight the many opportunities and challenges for effective adoption of this technology across the marine sciences.",j89,ICES Journal of Marine Science,jv89,accepted,f234,2005,2005-02-08
s285,p285,FAIR Data and Services in Biodiversity Science and Geoscience,"We examine the intersection of the FAIR principles (Findable, Accessible, Interoperable and Reusable), the challenges and opportunities presented by the aggregation of widely distributed and heterogeneous data about biological and geological specimens, and the use of the Digital Object Architecture (DOA) data model and components as an approach to solving those challenges that offers adherence to the FAIR principles as an integral characteristic. This approach will be prototyped in the Distributed System of Scientific Collections (DiSSCo) project, the pan-European Research Infrastructure which aims to unify over 110 natural science collections across 21 countries. We take each of the FAIR principles, discuss them as requirements in the creation of a seamless virtual collection of bio/geo specimen data, and map those requirements to Digital Object components and facilities such as persistent identification, extended data typing, and the use of an additional level of abstraction to normalize existing heterogeneous data structures. The FAIR principles inform and motivate the work and the DO Architecture provides the technical vision to create the seamless virtual collection vitally needed to address scientific questions of societal importance.",j90,Data Intelligence,jv90,accepted,f235,2002,2002-03-11
s286,p286,Big data of materials science: critical role of the descriptor.,"Statistical learning of materials properties or functions so far starts with a largely silent, nonchallenged step: the choice of the set of descriptive parameters (termed descriptor). However, when the scientific connection between the descriptor and the actuating mechanisms is unclear, the causality of the learned descriptor-property relation is uncertain. Thus, a trustful prediction of new promising materials, identification of anomalies, and scientific advancement are doubtful. We analyze this issue and define requirements for a suitable descriptor. For a classic example, the energy difference of zinc blende or wurtzite and rocksalt semiconductors, we demonstrate how a meaningful descriptor can be found systematically.",j91,Physical Review Letters,jv91,accepted,f236,2006,2006-12-04
s287,p287,Creating the CIPRES Science Gateway for inference of large phylogenetic trees,"Understanding the evolutionary history of living organisms is a central problem in biology. Until recently the ability to infer evolutionary relationships was limited by the amount of DNA sequence data available, but new DNA sequencing technologies have largely removed this limitation. As a result, DNA sequence data are readily available or obtainable for a wide spectrum of organisms, thus creating an unprecedented opportunity to explore evolutionary relationships broadly and deeply across the Tree of Life. Unfortunately, the algorithms used to infer evolutionary relationships are NP-hard, so the dramatic increase in available DNA sequence data has created a commensurate increase in the need for access to powerful computational resources. Local laptop or desktop machines are no longer viable for analysis of the larger data sets available today, and progress in the field relies upon access to large, scalable high-performance computing resources. This paper describes development of the CIPRES Science Gateway, a web portal designed to provide researchers with transparent access to the fastest available community codes for inference of phylogenetic relationships, and implementation of these codes on scalable computational resources. Meeting the needs of the community has included developing infrastructure to provide access, working with the community to improve existing community codes, developing infrastructure to insure the portal is scalable to the entire systematics community, and adopting strategies that make the project sustainable by the community. The CIPRES Science Gateway has allowed more than 1800 unique users to run jobs that required 2.5 million Service Units since its release in December 2009. (A Service Unit is a CPU-hour at unit priority).",c21,Grid Computing Environments,cp21,accepted,f237,2005,2005-09-09
s288,p288,LSST: From Science Drivers to Reference Design and Anticipated Data Products,"We describe here the most ambitious survey currently planned in the optical, the Large Synoptic Survey Telescope (LSST). The LSST design is driven by four main science themes: probing dark energy and dark matter, taking an inventory of the solar system, exploring the transient optical sky, and mapping the Milky Way. LSST will be a large, wide-field ground-based system designed to obtain repeated images covering the sky visible from Cerro Pachón in northern Chile. The telescope will have an 8.4 m (6.5 m effective) primary mirror, a 9.6 deg2 field of view, a 3.2-gigapixel camera, and six filters (ugrizy) covering the wavelength range 320–1050 nm. The project is in the construction phase and will begin regular survey operations by 2022. About 90% of the observing time will be devoted to a deep-wide-fast survey mode that will uniformly observe a 18,000 deg2 region about 800 times (summed over all six bands) during the anticipated 10 yr of operations and will yield a co-added map to r ∼ 27.5. These data will result in databases including about 32 trillion observations of 20 billion galaxies and a similar number of stars, and they will serve the majority of the primary science programs. The remaining 10% of the observing time will be allocated to special projects such as Very Deep and Very Fast time domain surveys, whose details are currently under discussion. We illustrate how the LSST science drivers led to these choices of system parameters, and we describe the expected data products and their characteristics.",j92,Astrophysical Journal,jv92,accepted,f238,2021,2021-08-29
s289,p289,A data ecosystem to support machine learning in materials science,"Facilitating the application of machine learning to materials science problems will require enhancing the data ecosystem to enable discovery and collection of data from many sources, automated dissemination of new data across the ecosystem, and the connecting of data with materials-specific machine learning models. Here, we present two projects, the Materials Data Facility (MDF) and the Data and Learning Hub for Science (DLHub), that address these needs. We use examples to show how MDF and DLHub capabilities can be leveraged to link data with machine learning models and how users can access those capabilities through web and programmatic interfaces.",j93,MRS Communications,jv93,accepted,f239,2021,2021-11-13
s291,p291,ENDF/B-VII.0: Next Generation Evaluated Nuclear Data Library for Nuclear Science and Technology,Abstract content goes here ...,c90,Computer Vision and Pattern Recognition,cp90,accepted,f240,2008,2008-11-11
s292,p292,"Open Science, Open Data, and Open Scholarship: European Policies to Make Science Fit for the Twenty-First Century","Open science will make science more efficient, reliable, and responsive to societal challenges. The European Commission has sought to advance open science policy from its inception in a holistic and integrated way, covering all aspects of the research cycle from scientific discovery and review to sharing knowledge, publishing, and outreach. We present the steps taken with a forward-looking perspective on the challenges laying ahead, in particular the necessary change of the rewards and incentives system for researchers (for which various actors are co-responsible and which goes beyond the mandate of the European Commission). Finally, we discuss the role of artificial intelligence (AI) within an open science perspective.",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f241,2008,2008-07-01
s293,p293,NOMAD: The FAIR concept for big data-driven materials science,"<jats:p><jats:fig position=""anchor""><jats:graphic xmlns:xlink=""http://www.w3.org/1999/xlink"" orientation=""portrait"" mime-subtype=""jpeg"" mimetype=""image"" position=""float"" xlink:type=""simple"" xlink:href=""S0883769418002087_figAb"" /></jats:fig></jats:p>",j94,MRS bulletin,jv94,accepted,f242,2007,2007-07-03
s294,p294,Data-driven modeling and learning in science and engineering,Abstract content goes here ...,j95,Comptes rendus. Mecanique,jv95,accepted,f243,2019,2019-04-27
s295,p295,Using Smartphones to Collect Behavioral Data in Psychological Science,"Smartphones now offer the promise of collecting behavioral data unobtrusively, in situ, as it unfolds in the course of daily life. Data can be collected from the onboard sensors and other phone logs embedded in today’s off-the-shelf smartphone devices. These data permit fine-grained, continuous collection of people’s social interactions (e.g., speaking rates in conversation, size of social groups, calls, and text messages), daily activities (e.g., physical activity and sleep), and mobility patterns (e.g., frequency and duration of time spent at various locations). In this article, we have drawn on the lessons from the first wave of smartphone-sensing research to highlight areas of opportunity for psychological research, present practical considerations for designing smartphone studies, and discuss the ongoing methodological and ethical challenges associated with research in this domain. It is our hope that these practical guidelines will facilitate the use of smartphones as a behavioral observation tool in psychological science.",j96,Perspectives on Psychological Science,jv96,accepted,f244,2014,2014-10-02
s296,p296,The Science Of Real Time Data Capture Self Reports In Health Research,"The National Cancer Institute (NCI) has designated the topic of real-time data capture as an important and innovative research area. As such, the NCI sponsored a national meeting of distinguished research scientists to discuss the state of the science in this emerging and burgeoning field. This book reflects the findings of the conference and discusses the state of the science of real-time data capture and its application to health and cancer research. It provides a conceptual framework for minute-by-minute data captureecological momentary assessments (EMA)and discusses health-related topics where these assessements have been applied. In addition, future directions in real-time data capture assessment, interventions, methodology, and technology are discussed.",c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,cp20,accepted,f245,2014,2014-11-16
s297,p297,Data-driven predictions in the science of science,"The desire to predict discoveries—to have some idea, in advance, of what will be discovered, by whom, when, and where—pervades nearly all aspects of modern science, from individual scientists to publishers, from funding agencies to hiring committees. In this Essay, we survey the emerging and interdisciplinary field of the “science of science” and what it teaches us about the predictability of scientific discovery. We then discuss future opportunities for improving predictions derived from the science of science and its potential impact, positive and negative, on the scientific community.",j97,Science,jv97,accepted,f246,2012,2012-08-29
s298,p298,Earth Observation Open Science: Enhancing Reproducible Science Using Data Cubes,"Earth Observation Data Cubes (EODC) have emerged as a promising solution to efficiently and effectively handle Big Earth Observation (EO) Data generated by satellites and made freely and openly available from different data repositories. The aim of this Special Issue, “Earth Observation Data Cube”, in Data, is to present the latest advances in EODC development and implementation, including innovative approaches for the exploitation of satellite EO data using multi-dimensional (e.g., spatial, temporal, spectral) approaches. This Special Issue contains 14 articles covering a wide range of topics such as Synthetic Aperture Radar (SAR), Analysis Ready Data (ARD), interoperability, thematic applications (e.g., land cover, snow cover mapping), capacity development, semantics, processing techniques, as well as national implementations and best practices. These papers made significant contributions to the advancement of a more Open and Reproducible Earth Observation Science, reducing the gap between users’ expectations for decision-ready products and current Big Data analytical capabilities, and ultimately unlocking the information power of EO data by transforming them into actionable knowledge.",c22,International Conference on Data Technologies and Applications,cp22,accepted,f247,2020,2020-01-09
s299,p299,ON PATIENT FLOW IN HOSPITALS: A DATA-BASED QUEUEING-SCIENCE PERSPECTIVE,"Hospitals are complex systems with essential societal benefits and huge mounting costs. These costs are exacerbated by inefficiencies in hospital processes, which are often manifested by congestion and long delays in patient care. Thus, a queueing-network view of patient flow in hospitals is natural for studying and improving its performance. The goal of our research is to explore patient flow data through the lens of a queueing scientist. The means is exploratory data analysis (EDA) in a large Israeli hospital, which reveals important features that are not readily explainable by existing models. Questions raised by our EDA include: Can a simple (parsimonious) queueing model usefully capture the complex operational reality of the Emergency Department (ED)? What time scales and operational regimes are relevant for modeling patient length of stay in the Internal Wards (IWs)? How do protocols of patient transfer between the ED and the IWs influence patient delay, workload division and fairness? EDA also unde...",c91,Workshop on Algorithms and Models for the Web-Graph,cp91,accepted,f248,2022,2022-07-13
s301,p301,Statistics for citizen science: extracting signals of change from noisy ecological data,"Policy‐makers increasingly demand robust measures of biodiversity change over short time periods. Long‐term monitoring schemes provide high‐quality data, often on an annual basis, but are taxonomically and geographically restricted. By contrast, opportunistic biological records are relatively unstructured but vast in quantity. Recently, these data have been applied to increasingly elaborate science and policy questions, using a range of methods. At present, we lack a firm understanding of which methods, if any, are capable of delivering unbiased trend estimates on policy‐relevant time‐scales. We identified a set of candidate methods that employ data filtering criteria and/or correction factors to deal with variation in recorder activity. We designed a computer simulation to compare the statistical properties of these methods under a suite of realistic data collection scenarios. We measured the Type I error rates of each method–scenario combination, as well as the power to detect genuine trends. We found that simple methods produce biased trend estimates, and/or had low power. Most methods are robust to variation in sampling effort, but biases in spatial coverage, sampling effort per visit, and detectability, as well as turnover in community composition, all induced some methods to fail. No method was wholly unaffected by all forms of variation in recorder activity, although some performed well enough to be useful. We warn against the use of simple methods. Sophisticated methods that model the data collection process offer the greatest potential to estimate timely trends, notably Frescalo and occupancy–detection models. The potential of these methods and the value of opportunistic data would be further enhanced by assessing the validity of model assumptions and by capturing small amounts of information about sampling intensity at the point of data collection.",c12,International Conference on Statistical and Scientific Database Management,cp12,accepted,f249,2002,2002-09-13
s302,p302,Opening the archive: How free data has enabled the science and monitoring promise of Landsat,Abstract content goes here ...,c87,European Conference on Computer Vision,cp87,accepted,f250,2014,2014-03-21
s303,p303,MedRec: Using Blockchain for Medical Data Access and Permission Management,"Years of heavy regulation and bureaucratic inefficiency have slowed innovation for electronic medical records (EMRs). We now face a critical need for such innovation, as personalization and data science prompt patients to engage in the details of their healthcare and restore agency over their medical data. In this paper, we propose MedRec: a novel, decentralized record management system to handle EMRs, using blockchain technology. Our system gives patients a comprehensive, immutable log and easy access to their medical information across providers and treatment sites. Leveraging unique blockchain properties, MedRec manages authentication, confidentiality, accountability and data sharing- crucial considerations when handling sensitive information. A modular design integrates with providers' existing, local data storage solutions, facilitating interoperability and making our system convenient and adaptable. We incentivize medical stakeholders (researchers, public health authorities, etc.) to participate in the network as blockchain “miners”. This provides them with access to aggregate, anonymized data as mining rewards, in return for sustaining and securing the network via Proof of Work. MedRec thus enables the emergence of data economics, supplying big data to empower researchers while engaging patients and providers in the choice to release metadata. The purpose of this short paper is to expose, prior to field tests, a working prototype through which we analyze and discuss our approach.",c23,International Conference on Open and Big Data,cp23,accepted,f251,2012,2012-07-19
s304,p304,Measurement and Data Analysis for Engineering and Science,"Fundamentals of Experimentation Introduction Experiments Chapter Overview Experimental Approach Role of Experiments The Experiment Classification of Experiments Plan for Successful Experimentation Hypothesis Testing* Design of Experiments* Factorial Design* Problems Bibliography Fundamental Electronics Chapter Overview Concepts and Definitions Circuit Elements RLC Combinations Elementary DC Circuit Analysis Elementary AC Circuit Analysis Equivalent Circuits* Meters* Impedance Matching and Loading Error* Electrical Noise* Problems Bibliography Measurement Systems: Sensors and Transducers Chapter Overview Measurement System Overview Sensor Domains Sensor Characteristics Physical Principles of Sensors Electric Piezoelectric Fluid Mechanic Optic Photoelastic Thermoelectric Electrochemical Sensor Scaling* Problems Bibliography Measurement Systems: Other Components Chapter Overview Signal Conditioning, Processing, and Recording Amplifiers Filters Analog-to-Digital Converters Smart Measurement Systems Other Example Measurement Systems Problems Bibliography Measurement Systems: Calibration and Response Chapter Overview Static Response Characterization by Calibration Dynamic Response Characterization Zero-Order System Dynamic Response First-Order System Dynamic Response Second-Order System Dynamic Response Measurement System Dynamic Response Problems Bibliography Measurement Systems: Design-Stage Uncertainty Chapter Overview Design-Stage Uncertainty Analysis Design-Stage Uncertainty Estimate of a Measurand Design-Stage Uncertainty Estimate of a Result Problems Bibliography Signal Characteristics Chapter Overview Signal Classification Signal Variables Signal Statistical Parameters Problems Bibliography The Fourier Transform Chapter Overview Fourier Series of a Periodic Signal Complex Numbers and Waves Exponential Fourier Series Spectral Representations Continuous Fourier Transform Continuous Fourier Transform Properties* Discrete Fourier Transform Fast Fourier Transform Problems Bibliography Digital Signal Analysis Chapter Overview Digital Sampling Digital Sampling Errors Windowing* Determining a Sample Period Problems Bibliography Probability Chapter Overview Relation to Measurements Basic Probability Concepts Sample versus Population Plotting Statistical Information Probability Density Function Various Probability Density Functions Central Moments Probability Distribution Function Problems Bibliography Statistics Chapter Overview Normal Distribution Normalized Variables Student's t Distribution Rejection of Data Standard Deviation of the Means Chi-Square Distribution Pooling Samples* Problems Bibliography Uncertainty Analysis Chapter Overview Modeling and Experimental Uncertainties Probabilistic Basis of Uncertainty Identifying Sources of Error Systematic and Random Errors Quantifying Systematic and Random Errors Measurement Uncertainty Analysis Uncertainty Analysis of a Multiple-Measurement Result Uncertainty Analyses for Other Measurement Situations Uncertainty Analysis Summary Finite-Difference Uncertainties* Uncertainty Based upon Interval Statistics* Problems Bibliography Regression and Correlation Chapter Overview Least-Squares Approach Least-Squares Regression Analysis Linear Analysis Higher-Order Analysis* Multi-Variable Linear Analysis* Determining the Appropriate Fit Regression Confidence Intervals Regression Parameters Linear Correlation Analysis Signal Correlations in Time* Problems Bibliography Units and Significant Figures Chapter Overview English and Metric Systems Systems of Units SI Standards Technical English and SI Conversion Factors Prefixes Significant Figures Problems Bibliography Technical Communication Chapter Overview Guidelines for Writing Technical Memo Technical Report Oral Technical Presentation Problems Bibliography A Glossary B Symbols C Review Problem Answers Index",c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f252,2016,2016-10-10
s305,p305,rioja: Analysis of Quaternary Science Data,Abstract content goes here ...,c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f253,2019,2019-12-07
s308,p308,The Materials Data Facility: Data Services to Advance Materials Science Research,Abstract content goes here ...,c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f254,2002,2002-06-28
s310,p310,The role of administrative data in the big data revolution in social science research.,Abstract content goes here ...,j98,Social Science Research,jv98,accepted,f255,2018,2018-12-27
s311,p311,"AIRS/AMSU/HSB on the Aqua mission: design, science objectives, data products, and processing systems","The Atmospheric Infrared Sounder (AIRS), the Advanced Microwave Sounding Unit (AMSU), and the Humidity Sounder for Brazil (HSB) form an integrated cross-track scanning temperature and humidity sounding system on the Aqua satellite of the Earth Observing System (EOS). AIRS is an infrared spectrometer/radiometer that covers the 3.7-15.4-/spl mu/m spectral range with 2378 spectral channels. AMSU is a 15-channel microwave radiometer operating between 23 and 89 GHz. HSB is a four-channel microwave radiometer that makes measurements between 150 and 190 GHz. In addition to supporting the National Aeronautics and Space Administration's interest in process study and climate research, AIRS is the first hyperspectral infrared radiometer designed to support the operational requirements for medium-range weather forecasting of the National Ocean and Atmospheric Administration's National Centers for Environmental Prediction (NCEP) and other numerical weather forecasting centers. AIRS, together with the AMSU and HSB microwave radiometers, will achieve global retrieval accuracy of better than 1 K in the lower troposphere under clear and partly cloudy conditions. This paper presents an overview of the science objectives, AIRS/AMSU/HSB data products, retrieval algorithms, and the ground-data processing concepts. The EOS Aqua was launched on May 4, 2002 from Vandenberg AFB, CA, into a 705-km-high, sun-synchronous orbit. Based on the excellent radiometric and spectral performance demonstrated by AIRS during prelaunch testing, which has by now been verified during on-orbit testing, we expect the assimilation of AIRS data into the numerical weather forecast to result in significant forecast range and reliability improvements.",j99,IEEE Transactions on Geoscience and Remote Sensing,jv99,accepted,f256,2013,2013-10-15
s312,p312,Gaia Data Release 2,"Context. The Gaia second Data Release (DR2) presents a first mapping of full-sky RR Lyrae stars and Cepheids observed by the spacecraft during the initial 22 months of science operations.
Aims. The Specific Objects Study (SOS) pipeline, developed to validate and fully characterise Cepheids and RR Lyrae stars (SOS Cep&RRL) observed by Gaia, has been presented in the documentation and papers accompanying the Gaia first Data Release. Here we describe how the SOS pipeline was modified to allow for processing the Gaia multi-band (G, GBP, and GRP) time-series photometry of all-sky candidate variables and produce specific results for confirmed RR Lyrae stars and Cepheids that are published in the DR2 catalogue.
Methods. The SOS Cep&RRL processing uses tools such as the period–amplitude and the period–luminosity relations in the G band. For the analysis of the Gaia DR2 candidates we also used tools based on the GBP and GRP photometry, such as the period–Wesenheit relation in (G, GRP).
Results. Multi-band time-series photometry and characterisation by the SOS Cep&RRL pipeline are published in Gaia DR2 for 150 359 such variables (9575 classified as Cepheids and 140 784 as RR Lyrae stars) distributed throughout the sky. The sample includes variables in 87 globular clusters and 14 dwarf galaxies (the Magellanic Clouds, 5 classical and 7 ultra-faint dwarfs). To the best of our knowledge, as of 25 April 2018, the variability of 50 570 of these sources (350 Cepheids and 50 220 RR Lyrae stars) has not been reported before in the literature, therefore they are likely new discoveries by Gaia. An estimate of the interstellar absorption is published for 54 272 fundamental-mode RR Lyrae stars from a relation based on the G-band amplitude and the pulsation period. Metallicities derived from the Fourier parameters of the light curves are also released for 64 932 RR Lyrae stars and 3738 fundamental-mode classical Cepheids with periods shorter than 6.3 days.",j100,Astronomy & Astrophysics,jv100,accepted,f257,2002,2002-11-12
s313,p313,Educational data mining and learning analytics: An updated survey,"This survey is an updated and improved version of the previous one published in 2013 in this journal with the title “data mining in education”. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data‐Driven Education, Data‐Driven Decision‐Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f258,2011,2011-10-13
s314,p314,ProteomeXchange provides globally co-ordinated proteomics data submission and dissemination,Abstract content goes here ...,j0,Nature Biotechnology,jv0,accepted,f259,2006,2006-08-05
s315,p315,A survey of data provenance in e-science,"Data management is growing in complexity as large-scale applications take advantage of the loosely coupled resources brought together by grid middleware and by abundant storage capacity. Metadata describing the data products used in and generated by these applications is essential to disambiguate the data and enable reuse. Data provenance, one kind of metadata, pertains to the derivation history of a data product starting from its original sources.In this paper we create a taxonomy of data provenance characteristics and apply it to current research efforts in e-science, focusing primarily on scientific workflow approaches. The main aspect of our taxonomy categorizes provenance systems based on why they record provenance, what they describe, how they represent and store provenance, and ways to disseminate it. The survey culminates with an identification of open research problems in the field.",c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f260,2005,2005-10-14
s316,p316,The Accuracy of Citizen Science Data: A Quantitative Review,"Author(s): Aceves-Bueno, Erendira; Adeleye, Adeyemi S; Feraud, Marina; Huang, Yuxiong; Tao, Mengya; Yang, Yi; Anderson, Sarah E",c72,Intelligent Systems in Molecular Biology,cp72,accepted,f261,2003,2003-04-23
s317,p317,Named data networking,"Named Data Networking (NDN) is one of five projects funded by the U.S. National Science Foundation under its Future Internet Architecture Program. NDN has its roots in an earlier project, Content-Centric Networking (CCN), which Van Jacobson first publicly presented in 2006. The NDN project investigates Jacobson's proposed evolution from today's host-centric network architecture (IP) to a data-centric network architecture (NDN). This conceptually simple shift has far-reaching implications for how we design, develop, deploy, and use networks and applications. We describe the motivation and vision of this new architecture, and its basic components and operations. We also provide a snapshot of its current design, development status, and research challenges. More information about the project, including prototype implementations, publications, and annual reports, is available on named-data.net.",c33,International Conference on Agile Software Development,cp33,accepted,f262,2022,2022-08-04
s318,p318,Deep learning applications and challenges in big data analytics,Abstract content goes here ...,j8,Journal of Big Data,jv8,accepted,f263,2019,2019-08-18
s319,p319,Principles and methods of scaling geospatial Earth science data,Abstract content goes here ...,j101,Earth-Science Reviews,jv101,accepted,f264,2020,2020-02-29
s320,p320,TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data,"The Cancer Genome Atlas (TCGA) research network has made public a large collection of clinical and molecular phenotypes of more than 10 000 tumor patients across 33 different tumor types. Using this cohort, TCGA has published over 20 marker papers detailing the genomic and epigenomic alterations associated with these tumor types. Although many important discoveries have been made by TCGA's research network, opportunities still exist to implement novel methods, thereby elucidating new biological pathways and diagnostic markers. However, mining the TCGA data presents several bioinformatics challenges, such as data retrieval and integration with clinical data and other molecular data types (e.g. RNA and DNA methylation). We developed an R/Bioconductor package called TCGAbiolinks to address these challenges and offer bioinformatics solutions by using a guided workflow to allow users to query, download and perform integrative analyses of TCGA data. We combined methods from computer science and statistics into the pipeline and incorporated methodologies developed in previous TCGA marker studies and in our own group. Using four different TCGA tumor types (Kidney, Brain, Breast and Colon) as examples, we provide case studies to illustrate examples of reproducibility, integrative analysis and utilization of different Bioconductor packages to advance and accelerate novel discoveries.",j102,Nucleic Acids Research,jv102,accepted,f265,2002,2002-04-09
s321,p321,Understanding the paradigm shift to computational social science in the presence of big data,Abstract content goes here ...,c24,Decision Support Systems,cp24,accepted,f266,2013,2013-11-29
s322,p322,"The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd Edition",Abstract content goes here ...,c11,Hawaii International Conference on System Sciences,cp11,accepted,f267,2006,2006-12-06
s323,p323,Emerging problems of data quality in citizen science,"The role of citizen science in research and natural resource monitoring and management is increasing, as evidenced by the growing number of peer-reviewed publications (including a special section in this journal) and calls for involving citizens in monitoring and governance (through, for example, “participatory research” [Danielsen et al. 2014] and “participatory monitoring” [Kennett et al. 2015]). Citizen science projects can be targeted to a specific research question (and thus involve very specific data-collection protocols) or can be more open-ended (giving rise to a need to collect data for which the uses may be unknown or changing) (Wiersma 2010). Advances in online content production and sharing technologies (i.e., Web 2.0), mobile computing, and sensor-equipped devices have contributed to a dramatic rise in online citizen science projects, in which citizens contribute sightings (e.g., eBird [Sullivan et al. 2009]), transcribe data (e.g., Old Weather [Eveleigh et al. 2013]), or classify phenomena (e.g., Galaxy Zoo [Hopkin 2007]). It is these online projects, also referred to as crowdsourcing (Franzoni & Sauermann 2014), which have been the focus of our research and that inform the opinions presented here. Galaxy Zoo exemplifies an initiative that began as a targeted project in which citizens were engaged in the relatively simple task of classifying images of galaxies as one of 3 shapes (Hopkin 2007). The goal was to distribute a large workload among a large number of people. Citizen participation grew quickly, which led project sponsors to create an online forum to accommodate the large volume of comments and questions. Through this forum, a number of unanticipated categories of celestial bodies arose, including 2 from Dutch school teacher Hanny Van Arkel, who noted the “green peas” phenomena (Cardamone et al. 2009) and a new body that became known as “Hanny’s Voorwerp” (Lintott et al. 2009). The Galaxy Zoo story provides an example of the different dimensions of data quality in citizen science. The researchers anticipated a small, fixed set of categories of galaxy shapes and designed the data-collection interface accordingly. One dimension of data quality (Lewandowski & Specht 2015) is data accuracy; others include data completeness and timeliness. (For a complete discussion of the many dimensions of data quality, see Wang and Strong [1996]). In the case of Galaxy Zoo, data accuracy is measured as the proportion of images correctly classified by galaxy shape. Had it not been for the attentiveness of one person who went beyond the task of classifying galaxies into predetermined categories and was able to communicate this to the researchers via the online forum, what turned out to be important new phenomena might have gone undiscovered. Failure to discover these phenomena would have affected the data-quality dimension of completeness because not all celestial bodies in the images would have been cataloged. Thus, the data quality would be diminished. Lewandowski and Specht (2015) describe 4 dimensions of data quality in their broad review of biologythemed citizen science: data accuracy and precision; sufficient sample size; and standardized sampling procedures (including sufficient spatial and temporal representation). These dimensions are congruent with good scientific practice and thus suggest that the criteria used to measure the quality of citizens’ data should fit the standards of professional science. In this sense, citizen science amounts to asking citizens to fill in the blanks in a story written by scientists. Although it is helpful for citizen scientists to adhere to standards of scientific practice, the process of doing science includes more than simply collecting and processing data. As Stevens et al. (2014:21) admitted: “Often . . . participants might be viewed as sensors or data collectors, but they’re rarely invited to decide what data to collect or to contribute to the data analysis or interpretation, even though they . . . might have valuable insights,” a view echoed in a recent Nature commentary by Kennett et al. (2015). The online forum created by the Galaxy Zoo project manifests a design decision that allowed for participants to provide valuable new insights and contribute beyond simply classifying images. Because discoveries resulted from one individual going beyond the assigned task, an open question is how many discoveries went undetected because other participants failed to notice particular features (given the prescribed task) or noticed but failed to post on the site’s forum. Based on examples such as this one, we argue that data quality in citizen science is much more than data accuracy. Because citizens generally lack formal scientific training, they view problems and issues in light of their own knowledge and interests, creating fertile ground for discoveries. This perspective – that citizen scientists view problems differently than scientists – means that the quality of data should be defined as more than simply",j103,Conservation Biology,jv103,accepted,f268,2006,2006-06-25
s324,p324,Prospects and challenges for social media data in conservation science,"Social media data have been extensively used in numerous fields of science, but examples of their use in conservation science are still very limited. In this paper, we propose a framework on how social media data could be useful for conservation science and practice. We present the commonly used social media platforms and discuss how their content could be providing new data and information for conservation science. Based on this, we discuss how future work in conservation science and practice would benefit from social media data.",j26,Frontiers in Environmental Science,jv26,accepted,f269,2019,2019-06-29
s325,p325,The journal coverage of Web of Science and Scopus: a comparative analysis,Abstract content goes here ...,j104,Scientometrics,jv104,accepted,f270,2010,2010-02-13
s326,p326,The Role of Anomalous Data in Knowledge Acquisition: A Theoretical Framework and Implications for Science Instruction,"Understanding how science students respond to anomalous data is essential to understanding knowledge acquisition in science classrooms. This article presents a detailed analysis of the ways in which scientists and science students respond to such data. We postulate that there are seven distinct forms of response to anomalous data, only one of which is to accept the data and change theories. The other six responses involve discounting the data in various ways in order to protect the preinstructional theory. We analyze the factors that influence which of these seven forms of response a scientist or student will choose, giving special attention to the factors that make theory change more likely. Finally, we discuss the implications of our framework for science instruction.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f271,2007,2007-05-03
s327,p327,TurkPrime.com: A versatile crowdsourcing data acquisition platform for the behavioral sciences,Abstract content goes here ...,j105,Behavior Research Methods,jv105,accepted,f272,2008,2008-01-07
s328,p328,Science Mapping: A Systematic Review of the Literature,"Abstract Purpose We present a systematic review of the literature concerning major aspects of science mapping to serve two primary purposes: First, to demonstrate the use of a science mapping approach to perform the review so that researchers may apply the procedure to the review of a scientific domain of their own interest, and second, to identify major areas of research activities concerning science mapping, intellectual milestones in the development of key specialties, evolutionary stages of major specialties involved, and the dynamics of transitions from one specialty to another. Design/methodology/approach We first introduce a theoretical framework of the evolution of a scientific specialty. Then we demonstrate a generic search strategy that can be used to construct a representative dataset of bibliographic records of a domain of research. Next, progressively synthesized co-citation networks are constructed and visualized to aid visual analytic studies of the domain’s structural and dynamic patterns and trends. Finally, trajectories of citations made by particular types of authors and articles are presented to illustrate the predictive potential of the analytic approach. Findings The evolution of the science mapping research involves the development of a number of interrelated specialties. Four major specialties are discussed in detail in terms of four evolutionary stages: conceptualization, tool construction, application, and codification. Underlying connections between major specialties are also explored. The predictive analysis demonstrates citations trajectories of potentially transformative contributions. Research limitations The systematic review is primarily guided by citation patterns in the dataset retrieved from the literature. The scope of the data is limited by the source of the retrieval, i.e. the Web of Science, and the composite query used. An iterative query refinement is possible if one would like to improve the data quality, although the current approach serves our purpose adequately. More in-depth analyses of each specialty would be more revealing by incorporating additional methods such as citation context analysis and studies of other aspects of scholarly publications. Practical implications The underlying analytic process of science mapping serves many practical needs, notably bibliometric mapping, knowledge domain visualization, and visualization of scientific literature. In order to master such a complex process of science mapping, researchers often need to develop a diverse set of skills and knowledge that may span multiple disciplines. The approach demonstrated in this article provides a generic method for conducting a systematic review. Originality/value Incorporating the evolutionary stages of a specialty into the visual analytic study of a research domain is innovative. It provides a systematic methodology for researchers to achieve a good understanding of how scientific fields evolve, to recognize potentially insightful patterns from visually encoded signs, and to synthesize various information so as to capture the state of the art of the domain.",j58,Journal of Data and Information Science,jv58,accepted,f273,2012,2012-11-19
s330,p330,Ethical Issues Relating to Scientific Discovery in Exercise Science.,"This work aims to present concepts related to ethical issues in conducting and reporting scientific research in a clear and straightforward manner. Considerations around research design including authorship, sound research practices, non-discrimination in subject recruitment, objectivity, respect for intellectual property, and financial interests are detailed. Further, concepts relating to the conducting of research including the competency of the researcher, conflicts of interest, accurately representing data, and ethical practices in human and animal research are presented. Attention pertaining to the dissemination of research including plagiarism, duplicate submission, redundant publication, and figure manipulation is offered. Other considerations including responsible mentoring, respect for colleagues, and social responsibility are set forth. The International Journal of Exercise Science will now require a statement in all subsequent published manuscripts that the authors have complied with each of the ethics statements contained in this work.",j106,International Journal of Exercise Science,jv106,accepted,f274,2002,2002-02-20
s331,p331,General data protection regulation,"Presentacio sobre l'Oficina de Proteccio de Dades Personals de la UAB i la politica Open Science. Va formar part de la conferencia ""Les politiques d'Open Data / Open Acces: Implicacions a la recerca"" orientada a investigadors i gestors de projectes europeus que va tenir lloc el 20 de setembre de 2018 a la Universitat Autonoma de Barcelona",c76,International Conference on Artificial Neural Networks,cp76,accepted,f275,2013,2013-09-09
s332,p332,The misuse of colour in science communication,Abstract content goes here ...,j107,Nature Communications,jv107,accepted,f276,2019,2019-10-01
s334,p334,Big Data and Social Science: A Practical Guide to Methods and Tools,"Both Traditional Students and Working Professionals Acquire the Skills to Analyze Social Problems. Big Data and Social Science: A Practical Guide to Methods and Tools shows how to apply data science to real-world problems in both research and the practice. The book provides practical guidance on combining methods and tools from computer science, statistics, and social science. This concrete approach is illustrated throughout using an important national problem, the quantitative study of innovation. The text draws on the expertise of prominent leaders in statistics, the social sciences, data science, and computer science to teach students how to use modern social science research principles as well as the best analytical and computational tools. It uses a real-world challenge to introduce how these tools are used to identify and capture appropriate data, apply data science models and tools to that data, and recognize and respond to data errors and limitations. For more information, including sample chapters and news, please visit the author's website.",c92,Advances in Soft Computing,cp92,accepted,f277,2009,2009-06-21
s335,p335,Tree-Based Models for Political Science Data,"Political scientists often find themselves analyzing data sets with a large number of observations, a large number of variables, or both. Yet, traditional statistical techniques fail to take full advantage of the opportunities inherent in “big data,” as they are too rigid to recover nonlinearities and do not facilitate the easy exploration of interactions in high-dimensional data sets. In this article, we introduce a family of tree-based nonparametric techniques that may, in some circumstances, be more appropriate than traditional methods for confronting these data challenges. In particular, tree models are very effective for detecting nonlinearities and interactions, even in data sets with many (potentially irrelevant) covariates. We introduce the basic logic of tree-based models, provide an overview of the most prominent methods in the literature, and conduct three analyses that illustrate how the methods can be implemented while highlighting both their advantages and limitations. Replication Materials: The data, code, and any additional materials required to replicate all analyses in this article are available on the American Journal of Political Science Dataverse within the Harvard Dataverse Network at: https://doi.org/10.7910/DVN/8ZJBLI. Social science scholars often work with data sets containing a large number of observations, many potential covariates, or (increasingly) both. Indeed, political scientists now regularly analyze data with levels of complexity unimaginable just two decades ago. Widely used surveys, for instance, interview tens of thousands of respondents about hundreds of topics. Scholars of institutions can quickly assemble data sets with thousands of observations using resources like the Comparative Agendas Project. Moreover, new measurement methods, such as text analysis, have combined with data sources, such as Twitter, to generate databases of almost unmanageable sizes. It is clear that political science, like all areas of the social sciences, will increasingly have access to a deluge of data so vast that it will dwarf everything that has come before. What statistical methods are needed in this datasaturated world? Surely, there is no one correct answer. Yet, just as surely, traditional statistical models are not always equipped to take full advantage of new data sources. Traditional models—largely variants of linear regressions—are ideal for evaluating theories that imply specific functional forms relating outcomes to predictors. In particular, they excel in their ability to leverage assumptions about the data-generating process, or DGP (additivity, linearity in the parameters, homoskedasticity, Jacob M. Montgomery is Associate Professor, Department of Political Science, Washington University in St. Louis, Campus Box 1063, One Brookings Drive, St. Louis, MO 63130 (jacob.montgomery@wustl.edu). Santiago Olivella is Assistant Professor, Department of Political Science, University of North Carolina at Chapel Hill, Hamilton Hall 361, CB 3265, Chapel Hill, NC 27599 (olivella@unc.edu). etc.) to make valid inferences despite inherent data limitations. Although appropriate when testing theories that conform with these assumptions, standard models are often insufficiently flexible to capture nuances in the data—such as complex nonlinear functional forms and deep interactions—when no clear a priori expectations exist. In this article, we introduce a family of tree-based nonparametric techniques from the machine learning literature. We argue that, under specific circumstances, regression and classification tree models are an appropriate standard choice for analyzing high-dimensional data sets. In particular, past research has shown tree-based methods to be very useful for making accurate predictions when the underlying DGP includes nonlinearities, discontinuities, and interactions among many covariates. Further, tree models require few assumptions. Rather than imposing a presumed structure on the DGP, tree-based methods allow the data to “speak for themselves.” Thus, our goal in this article is to introduce political scientists to this promising family of methods, which are well suited for today’s data analysis demands. In the next sections, we discuss the promise and perils of high-dimensional, “large”-N data sets and introduce the basic logic of tree models. We then provide an overview of the most prominent methods in the literature. American Journal of Political Science, Vol. 62, No. 3, July 2018, Pp. 729–744 C ©2018, Midwest Political Science Association DOI: 10.1111/ajps.12361",j109,American Journal of Political Science,jv109,accepted,f278,2004,2004-04-08
s336,p336,Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts,"Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.",j110,Political Analysis,jv110,accepted,f279,2021,2021-10-15
s338,p338,"Big Data, new epistemologies and paradigm shifts","This article examines how the availability of Big Data, coupled with new data analytics, challenges established epistemologies across the sciences, social sciences and humanities, and assesses the extent to which they are engendering paradigm shifts across multiple disciplines. In particular, it critically explores new forms of empiricism that declare ‘the end of theory’, the creation of data-driven rather than knowledge-driven science, and the development of digital humanities and computational social sciences that propose radically different ways to make sense of culture, history, economy and society. It is argued that: (1) Big Data and new data analytics are disruptive innovations which are reconfiguring in many instances how research is conducted; and (2) there is an urgent need for wider critical reflection within the academy on the epistemological implications of the unfolding data revolution, a task that has barely begun to be tackled despite the rapid changes in research practices presently taking place. After critically reviewing emerging epistemological positions, it is contended that a potentially fruitful approach would be the development of a situated, reflexive and contextually nuanced epistemology.",c39,International Conference on Global Software Engineering,cp39,accepted,f280,2020,2020-05-12
s339,p339,Novel methods improve prediction of species' distributions from occurrence data,"Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.",c72,Intelligent Systems in Molecular Biology,cp72,accepted,f281,2003,2003-12-05
s341,p341,Brief introduction of medical database and data mining technology in big data era,"Data mining technology can search for potentially valuable knowledge from a large amount of data, mainly divided into data preparation and data mining, and expression and analysis of results. It is a mature information processing technology and applies database technology. Database technology is a software science that researches manages, and applies databases. The data in the database are processed and analyzed by studying the underlying theory and implementation methods of the structure, storage, design, management, and application of the database. We have introduced several databases and data mining techniques to help a wide range of clinical researchers better understand and apply database technology.",j111,Journal of Evidence-Based Medicine,jv111,accepted,f282,2019,2019-01-20
s342,p342,Strategies Employed by Citizen Science Programs to Increase the Credibility of Their Data,"The success of citizen science in producing important and unique data is attracting interest from scientists and resource managers. Nonetheless, questions remain about the credibility of citizen science data. Citizen science programs desire to meet the same standards of credibility as academic science, but they usually work within a different context, for example, training and managing significant numbers of volunteers with limited resources. We surveyed the credibility-building strategies of 30 citizen science programs that monitor environmental aspects of the California coast. We identified a total of twelve strategies: Three that are applied during training and planning; four that are applied during data collection; and five that are applied during data analysis and program evaluation. Variation in the application of these strategies by program is related to factors such as the number of participants, the focus on group or individual work, and the time commitment required of volunteers. The structure of each program and available resources require program designers to navigate tradeoffs in the choices of their credibility strategies. Our results illustrate those tradeoffs and provide a framework for the necessary discussions between citizen science programs and potential users of their data—including scientists and decision makers—about shared expectations for credibility and practical approaches for meeting those expectations. This article has been corrected here: http://dx.doi.org/10.5334/cstp.91",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f283,2007,2007-03-21
s343,p343,Data Mining and Analysis: Fundamental Concepts and Algorithms,"The fundamental algorithms in data mining and analysis form the basis for the emerging field of data science, which includes automated methods to analyze patterns and models for all kinds of data, with applications ranging from scientific discovery to business intelligence and analytics. This textbook for senior undergraduate and graduate data mining courses provides a broad yet in-depth overview of data mining, integrating related concepts from machine learning and statistics. The main parts of the book include exploratory data analysis, pattern mining, clustering, and classification. The book lays the basic foundations of these tasks, and also covers cutting-edge topics such as kernel methods, high-dimensional data analysis, and complex graphs and networks. With its comprehensive coverage, algorithmic perspective, and wealth of examples, this book offers solid guidance in data mining for students, researchers, and practitioners alike. Key features: Covers both core methods and cutting-edge research Algorithmic approach with open-source implementations Minimal prerequisites: all key mathematical concepts are presented, as is the intuition behind the formulas Short, self-contained chapters with class-tested examples and exercises allow for flexibility in designing a course and for easy reference Supplementary website with lecture slides, videos, project ideas, and more",c17,International Conference on Enterprise Information Systems,cp17,accepted,f284,2008,2008-06-01
s344,p344,Defining Computational Thinking for Mathematics and Science Classrooms,Abstract content goes here ...,c98,North American Chapter of the Association for Computational Linguistics,cp98,accepted,f285,2009,2009-07-21
s345,p345,OpenML: networked science in machine learning,"Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.",c58,Australian Software Engineering Conference,cp58,accepted,f286,2021,2021-04-02
s346,p346,Learning from Imbalanced Data Sets,Abstract content goes here ...,j112,Cambridge International Law Journal,jv112,accepted,f287,2012,2012-06-21
s347,p347,Next Steps for Citizen Science,"Strategic investments and coordination are needed for citizen science to reach its full potential. Around the globe, thousands of research projects are engaging millions of individuals—many of whom are not trained as scientists—in collecting, categorizing, transcribing, or analyzing scientific data. These projects, known as citizen science, cover a breadth of topics from microbiomes to native bees to water quality to galaxies. Most projects obtain or manage scientific information at scales or resolutions unattainable by individual researchers or research teams, whether enrolling thousands of individuals collecting data across several continents, enlisting small armies of participants in categorizing vast quantities of online data, or organizing small groups of volunteers to tackle local problems.",j97,Science,jv97,accepted,f288,2012,2012-12-13
s348,p348,"Google Scholar, Scopus and the Web of Science: a longitudinal and cross-disciplinary comparison",Abstract content goes here ...,j104,Scientometrics,jv104,accepted,f289,2010,2010-07-09
s350,p350,"Big Data, Digital Media, and Computational Social Science",forecasts and misrepresent,c10,Big Data,cp10,accepted,f290,2021,2021-02-03
s351,p351,Liberating field science samples and data,"Promote reproducibility by moving beyond “available upon request” Transparency and reproducibility enhance the integrity of research results for scientific and public uses and empower novel research applications. Access to data, samples, methods, and reagents used to conduct research and analysis, as well as to the code used to analyze and process data and samples, is a fundamental requirement for transparency and reproducibility. The field sciences (e.g., geology, ecology, and archaeology), where each study is temporally (and often spatially) unique, provide exemplars for the importance of preserving data and samples for further analysis. Yet field sciences, if they even address such access, commonly do so by simply noting “data and samples available upon request.” They lag behind some laboratory sciences in making data and samples available to the broader research community. It is time for this to change. We discuss cultural, financial, and technical barriers to change and ways in which funders, publishers, scientific societies, and others are responding.",j97,Science,jv97,accepted,f291,2012,2012-01-05
s352,p352,Consistent Covariance Matrix Estimation with Spatially Dependent Panel Data,"Many panel data sets encountered in macroeconomics, international economics, regional science, and finance are characterized by cross-sectional or spatial dependence. Standard techniques that fail to account for this dependence will result in inconsistently estimated standard errors. In this paper we present conditions under which a simple extension of common nonparametric covariance matrix estimation techniques yields standard error estimates that are robust to very general forms of spatial and temporal dependence as the time dimension becomes large. We illustrate the relevance of this approach using Monte Carlo simulations and a number of empirical examples.",j113,Review of Economics and Statistics,jv113,accepted,f292,2006,2006-12-02
s354,p354,Data mining with big data,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",j1,IEEE Transactions on Knowledge and Data Engineering,jv1,accepted,f293,2020,2020-10-11
s355,p355,Explaining Fixed Effects: Random Effects Modeling of Time-Series Cross-Sectional and Panel Data*,"This article challenges Fixed Effects (FE) modeling as the ‘default’ for time-series-cross-sectional and panel data. Understanding different within and between effects is crucial when choosing modeling strategies. The downside of Random Effects (RE) modeling—correlated lower-level covariates and higher-level residuals—is omitted-variable bias, solvable with Mundlak's (1978a) formulation. Consequently, RE can provide everything that FE promises and more, as confirmed by Monte-Carlo simulations, which additionally show problems with Plümper and Troeger's FE Vector Decomposition method when data are unbalanced. As well as incorporating time-invariant variables, RE models are readily extendable, with random coefficients, cross-level interactions and complex variance functions. We argue not simply for technical solutions to endogeneity, but for the substantive importance of context/heterogeneity, modeled using RE. The implications extend beyond political science to all multilevel datasets. However, omitted variables could still bias estimated higher-level variable effects; as with any model, care is required in interpretation.",j115,Political Science Research and Methods,jv115,accepted,f294,2002,2002-12-27
s356,p356,"Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython","Get complete instructions for manipulating, processing, cleaning, and crunching datasets in Python. Updated for Python 3.6, the second edition of this hands-on guide is packed with practical case studies that show you how to solve a broad set of data analysis problems effectively. Youll learn the latest versions of pandas, NumPy, IPython, and Jupyter in the process. Written by Wes McKinney, the creator of the Python pandas project, this book is a practical, modern introduction to data science tools in Python. Its ideal for analysts new to Python and for Python programmers new to data science and scientific computing. Data files and related material are available on GitHub. Use the IPython shell and Jupyter notebook for exploratory computing Learn basic and advanced features in NumPy (Numerical Python)Get started with data analysis tools in the pandas library Use flexible tools to load, clean, transform, merge, and reshape data Create informative visualizations with matplotlib Apply the pandas groupby facility to slice, dice, and summarize datasets Analyze and manipulate regular and irregular time series dataLearn how to solve real-world data analysis problems with thorough, detailed examples",c113,International Conference on Image Analysis and Processing,cp113,accepted,f295,2002,2002-05-18
s357,p357,Taking a ‘Big Data’ approach to data quality in a citizen science project,Abstract content goes here ...,j116,Ambio,jv116,accepted,f296,2013,2013-10-06
s358,p358,Possible artifacts of data biases in the recent global surface warming hiatus,"Walking back talk of the end of warming Previous analyses of global temperature trends during the first decade of the 21st century seemed to indicate that warming had stalled. This allowed critics of the idea of global warming to claim that concern about climate change was misplaced. Karl et al. now show that temperatures did not plateau as thought and that the supposed warming “hiatus” is just an artifact of earlier analyses. Warming has continued at a pace similar to that of the last half of the 20th century, and the slowdown was just an illusion. Science, this issue p. 1469 Updated global surface temperature data do not support the notion of a global warming “hiatus.” Much study has been devoted to the possible causes of an apparent decrease in the upward trend of global surface temperatures since 1998, a phenomenon that has been dubbed the global warming “hiatus.” Here, we present an updated global surface temperature analysis that reveals that global trends are higher than those reported by the Intergovernmental Panel on Climate Change, especially in recent decades, and that the central estimate for the rate of warming during the first 15 years of the 21st century is at least as great as the last half of the 20th century. These results do not support the notion of a “slowdown” in the increase of global surface temperature.",j97,Science,jv97,accepted,f297,2012,2012-10-14
s359,p359,Data science as an academic discipline,"I recall being a proud young academic about 1970; I had just received a research grant to build and study a scientific database, and I had joined CODATA. I was looking forward to the future in this new exciting discipline when the head of my department, an internationally known professor, advised me that data was “a low level activity” not suitable for an academic. I recall my dismay. What can we do to ensure that this does not happen again and that data science is universally recognized as a worthwhile academic activity? Incidentally, I did not take that advice, or I would not be writing this essay, but moved into computer science. I will use my experience to draw comparisons between the problems computer science had to become academically recognized and those faced by data science.",j66,Data Science Journal,jv66,accepted,f298,2015,2015-05-14
s361,p361,"Calibration of the Computer Science and Applications, Inc. accelerometer.","PURPOSE
We established accelerometer count ranges for the Computer Science and Applications, Inc. (CSA) activity monitor corresponding to commonly employed MET categories.


METHODS
Data were obtained from 50 adults (25 males, 25 females) during treadmill exercise at three different speeds (4.8, 6.4, and 9.7 km x h(-1)).


RESULTS
Activity counts and steady-state oxygen consumption were highly correlated (r = 0.88), and count ranges corresponding to light, moderate, hard, and very hard intensity levels were < or = 1951, 1952-5724, 5725-9498, > or = 9499 cnts x min(-1), respectively. A model to predict energy expenditure from activity counts and body mass was developed using data from a random sample of 35 subjects (r2 = 0.82, SEE = 1.40 kcal x min(-1)). Cross validation with data from the remaining 15 subjects revealed no significant differences between actual and predicted energy expenditure at any treadmill speed (SEE = 0.50-1.40 kcal x min(-1)).


CONCLUSIONS
These data provide a template on which patterns of activity can be classified into intensity levels using the CSA accelerometer.",j117,Medicine & Science in Sports & Exercise,jv117,accepted,f299,2020,2020-06-15
s362,p362,Using Twitter for Demographic and Social Science Research: Tools for Data Collection and Processing,"Despite recent and growing interest in using Twitter to examine human behavior and attitudes, there is still significant room for growth regarding the ability to leverage Twitter data for social science research. In particular, gleaning demographic information about Twitter users—a key component of much social science research—remains a challenge. This article develops an accurate and reliable data processing approach for social science researchers interested in using Twitter data to examine behaviors and attitudes, as well as the demographic characteristics of the populations expressing or engaging in them. Using information gathered from Twitter users who state an intention to not vote in the 2012 presidential election, we describe and evaluate a method for processing data to retrieve demographic information reported by users that is not encoded as text (e.g., details of images) and evaluate the reliability of these techniques. We end by assessing the challenges of this data collection strategy and discussing how large-scale social media data may benefit demographic researchers.",j118,Sociological Methods & Research,jv118,accepted,f300,2014,2014-01-28
s364,p364,Voronoi diagrams—a survey of a fundamental geometric data structure,"Computational geometry is concerned with the design and analysis of algorithms for geometrical problems. In addition, other more practically oriented, areas of computer science— such as computer graphics, computer-aided design, robotics, pattern recognition, and operations research—give rise to problems that inherently are geometrical. This is one reason computational geometry has attracted enormous research interest in the past decade and is a well-established area today. (For standard sources, we refer to the survey article by Lee and Preparata [19841 and to the textbooks by Preparata and Shames [1985] and Edelsbrunner [1987bl.) Readers familiar with the literature of computational geometry will have noticed, especially in the last few years, an increasing interest in a geometrical construct called the Voronoi diagram. This trend can also be observed in combinatorial geometry and in a considerable number of articles in natural science journals that address the Voronoi diagram under different names specific to the respective area. Given some number of points in the plane, their Voronoi diagram divides the plane according to the nearest-neighbor",c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f301,2006,2006-03-04
s365,p365,Big Data and Clinicians: A Review on the State of the Science,"Background In the past few decades, medically related data collection saw a huge increase, referred to as big data. These huge datasets bring challenges in storage, processing, and analysis. In clinical medicine, big data is expected to play an important role in identifying causality of patient symptoms, in predicting hazards of disease incidence or reoccurrence, and in improving primary-care quality. Objective The objective of this review was to provide an overview of the features of clinical big data, describe a few commonly employed computational algorithms, statistical methods, and software toolkits for data manipulation and analysis, and discuss the challenges and limitations in this realm. Methods We conducted a literature review to identify studies on big data in medicine, especially clinical medicine. We used different combinations of keywords to search PubMed, Science Direct, Web of Knowledge, and Google Scholar for literature of interest from the past 10 years. Results This paper reviewed studies that analyzed clinical big data and discussed issues related to storage and analysis of this type of data. Conclusions Big data is becoming a common feature of biological and clinical studies. Researchers who use clinical big data face multiple challenges, and the data itself has limitations. It is imperative that methodologies for data analysis keep pace with our ability to collect and store data.",j119,JMIR Medical Informatics,jv119,accepted,f302,2003,2003-03-02
s367,p367,"Color Science, Concepts and Methods. Quantitative Data and Formulas","G. Wyszecki and W. S. Stiles London: John Wiley. 1967. Pp. xiv + 628. Price £11. This remarkable and unusual book is by two outstanding authorities on the science of colour: Dr. Stiles, for many years a senior member of the Light Division at the National Physical Laboratory, and Dr. Wyszecki, currently in charge of the Radiation Optics Section of the Canadian National Research Council. The authors' aim has been to provide a comprehensive source book of data required by the practical and theoretical worker in the field of colour and they have achieved this aim so successfully that their book is likely to become the standard work on the subject and to remain so for a good many years.",c92,Advances in Soft Computing,cp92,accepted,f303,2009,2009-08-15
s368,p368,The Extent and Consequences of P-Hacking in Science,"A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.",j60,PLoS Biology,jv60,accepted,f304,2001,2001-06-03
s369,p369,Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar,"The Institute for Scientific Information's (ISI, now Thomson Scientific, Philadelphia, PA) citation databases have been used for decades as a starting point and often as the only tools for locating citations andsor conducting citation analyses. The ISI databases (or Web of Science [WoS]), however, may no longer be sufficient because new databases and tools that allow citation searching are now available. Using citations to the work of 25 library and information science (LIS) faculty members as a case study, the authors examine the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. Overall, more than 10,000 citing and purportedly citing documents were examined. Results show that Scopus significantly alters the relative ranking of those scholars that appear in the middle of the rankings and that GS stands out in its coverage of conference proceedings as well as international, non-English language journals. The use of Scopus and GS, in addition to WoS, helps reveal a more accurate and comprehensive picture of the scholarly impact of authors. The WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours. © 2007 Wiley Periodicals, Inc.",c33,International Conference on Agile Software Development,cp33,accepted,f305,2022,2022-05-31
s370,p370,Human neuroimaging as a “Big Data” science,Abstract content goes here ...,j120,Brain Imaging and Behavior,jv120,accepted,f306,2012,2012-03-22
s371,p371,The Electric and Magnetic Field Instrument Suite and Integrated Science (EMFISIS) on RBSP,Abstract content goes here ...,c58,Australian Software Engineering Conference,cp58,accepted,f307,2021,2021-02-26
s372,p372,Data Science and Classification,Abstract content goes here ...,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f308,2021,2021-01-26
s373,p373,The inevitable application of big data to health care.,"THE AMOUNT OF DATA BEING DIGITALLY COLLECTED AND stored is vast and expanding rapidly. As a result, the science of data management and analysis is also advancing to enable organizations to convert this vast resource into information and knowledge that helps them achieve their objectives. Computer scientists have invented the term big data to describe this evolving technology. Big data has been successfully used in astronomy (eg, the Sloan Digital Sky Survey of telescopic information), retail sales (eg, Walmart’s expansive number of transactions), search engines (eg, Google’s customization of individual searches based on previous web data), and politics (eg, a campaign’s focus of political advertisements on people most likely to support their candidate based on web searches). In this Viewpoint, we discuss the application of big data to health care, using an economic framework to highlight the opportunities it will offer and the roadblocks to implementation. We suggest that leveraging the collection of patient and practitioner data could be an important way to improve quality and efficiency of health care delivery. Widespread uptake of electronic health records (EHRs) has generated massive data sets. A survey by the American Hospital Association showed that adoption of EHRs has doubled from 2009 to 2011, partly a result of funding provided by the Health Information Technology for Economic and Clinical Health Act of 2009. Most EHRs now contain quantitative data (eg, laboratory values), qualitative data (eg, text-based documents and demographics), and transactional data (eg, a record of medication delivery). However, much of this rich data set is currently perceived as a byproduct of health care delivery, rather than a central asset to improve its efficiency. The transition of data from refuse to riches has been key in the big data revolution of other industries. Advances in analytic techniques in the computer sciences, especially in machine learning, have been a major catalyst for dealing with these large information sets. These analytic techniques are in contrast to traditional statistical methods (derived from the social and physical sciences), which are largely not useful for analysis of unstructured data such as text-based documents that do not fit into relational tables. One estimate suggests that 80% of business-related data exist in an unstructured format. The same could probably be said for health care data, a large proportion of which is text-based. In contrast to most consumer service industries, medicine adopted a practice of generating evidence from experimental (randomized trials) and quasi-experimental studies to inform patients and clinicians. The evidence-based movement is founded on the belief that scientific inquiry is superior to expert opinion and testimonials. In this way, medicine was ahead of many other industries in terms of recognizing the value of data and information guiding rational decision making. However, health care has lagged in uptake of newer techniques to leverage the rich information contained in EHRs. There are 4 ways big data may advance the economic mission of health care delivery by improving quality and efficiency. First, big data may greatly expand the capacity to generate new knowledge. The cost of answering many clinical questions prospectively, and even retrospectively, by collecting structured data is prohibitive. Analyzing the unstructured data contained within EHRs using computational techniques (eg, natural language processing to extract medical concepts from free-text documents) permits finer data acquisition in an automated fashion. For instance, automated identification within EHRs using natural language processing was superior in detecting postoperative complications compared with patient safety indicators based on discharge coding. Big data offers the potential to create an observational evidence base for clinical questions that would otherwise not be possible and may be especially helpful with issues of generalizability. The latter issue limits the application of conclusions derived from randomized trials performed on a narrow spectrum of participants to patients who exhibit very different characteristics. Second, big data may help with knowledge dissemination. Most physicians struggle to stay current with the latest evidence guiding clinical practice. The digitization of medical literature has greatly improved access; however, the sheer",c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f309,2014,2014-03-24
s374,p374,"Data Mining: Concepts and Techniques, 3rd edition","The book Knowledge Discovery in Databases, edited by Piatetsky-Shapiro and Frawley [PSF91], is an early collection of research papers on knowledge discovery from data. The book Advances in Knowledge Discovery and Data Mining, edited by Fayyad, Piatetsky-Shapiro, Smyth, and Uthurusamy [FPSSe96], is a collection of later research results on knowledge discovery and data mining. There have been many data mining books published in recent years, including Predictive Data Mining by Weiss and Indurkhya [WI98], Data Mining Solutions: Methods and Tools for Solving Real-World Problems by Westphal and Blaxton [WB98], Mastering Data Mining: The Art and Science of Customer Relationship Management by Berry and Linofi [BL99], Building Data Mining Applications for CRM by Berson, Smith, and Thearling [BST99], Data Mining: Practical Machine Learning Tools and Techniques by Witten and Frank [WF05], Principles of Data Mining (Adaptive Computation and Machine Learning) by Hand, Mannila, and Smyth [HMS01], The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman [HTF01], Data Mining: Introductory and Advanced Topics by Dunham, and Data Mining: Multimedia, Soft Computing, and Bioinformatics by Mitra and Acharya [MA03]. There are also books containing collections of papers on particular aspects of knowledge discovery, such as Machine Learning and Data Mining: Methods and Applications edited by Michalski, Brakto, and Kubat [MBK98], and Relational Data Mining edited by Dzeroski and Lavrac [De01], as well as many tutorial notes on data mining in major database, data mining and machine learning conferences.",c105,Biometrics and Identity Management,cp105,accepted,f310,2006,2006-02-04
s375,p375,Qualitative Descriptive Methods in Health Science Research,"Objective: The purpose of this methodology paper is to describe an approach to qualitative design known as qualitative descriptive that is well suited to junior health sciences researchers because it can be used with a variety of theoretical approaches, sampling techniques, and data collection strategies. Background: It is often difficult for junior qualitative researchers to pull together the tools and resources they need to embark on a high-quality qualitative research study and to manage the volumes of data they collect during qualitative studies. This paper seeks to pull together much needed resources and provide an overview of methods. Methods: A step-by-step guide to planning a qualitative descriptive study and analyzing the data is provided, utilizing exemplars from the authors’ research. Results: This paper presents steps to conducting a qualitative descriptive study under the following headings: describing the qualitative descriptive approach, designing a qualitative descriptive study, steps to data analysis, and ensuring rigor of findings. Conclusions: The qualitative descriptive approach results in a summary in everyday, factual language that facilitates understanding of a selected phenomenon across disciplines of health science researchers.",c100,ACM SIGMOD Conference,cp100,accepted,f311,2010,2010-07-11
s376,p376,Big Data and Science: Myths and Reality,Abstract content goes here ...,j121,Big Data Research,jv121,accepted,f312,2007,2007-11-04
s377,p377,The Science DMZ: A network design pattern for data-intensive science,"The ever-increasing scale of scientific data has become a significant challenge for researchers that rely on networks to interact with remote computing systems and transfer results to collaborators worldwide. Despite the availability of high-capacity connections, scientists struggle with inadequate cyberinfrastructure that cripples data transfer performance, and impedes scientific progress. The Science DMZ paradigm comprises a proven set of network design patterns that collectively address these problems for scientists. We explain the Science DMZ model, including network architecture, system configuration, cybersecurity, and performance tools, that creates an optimized network environment for science. We describe use cases from universities, supercomputing centers and research laboratories, highlighting the effectiveness of the Science DMZ model in diverse operational settings. In all, the Science DMZ model is a solid platform that supports any science workflow, and flexibly accommodates emerging network technologies. As a result, the Science DMZ vastly improves collaboration, accelerating scientific discovery.",c8,The Compass,cp8,accepted,f313,2016,2016-03-18
s378,p378,Mapping citizen science contributions to the UN sustainable development goals,Abstract content goes here ...,j122,Sustainability Science,jv122,accepted,f314,2016,2016-06-11
s379,p379,"Materials Cloud, a platform for open computational science",Abstract content goes here ...,j14,Scientific Data,jv14,accepted,f315,2020,2020-04-11
s380,p380,A survey of machine learning for big data processing,Abstract content goes here ...,j123,EURASIP Journal on Advances in Signal Processing,jv123,accepted,f316,2006,2006-02-13
s381,p381,Secondary Data Analysis: A Method of which the Time Has Come,"Technological advances have led to vast amounts of data that has been collected, compiled, and archived, and that is now easily accessible for research. As a result, utilizing existing data for research is becoming more prevalent, and therefore secondary data analysis. While secondary analysis is flexible and can be utilized in several ways, it is also an empirical exercise and a systematic method with procedural and evaluative steps, just as in collecting and evaluating primary data. This paper asserts that secondary data analysis is a viable method to utilize in the process of inquiry when a systematic procedure is followed and presents an illustrative research application utilizing secondary data analysis in library and information science research.",c11,Hawaii International Conference on System Sciences,cp11,accepted,f317,2006,2006-12-19
s383,p383,What is Data Science ? Fundamental Concepts and a Heuristic Example,Abstract content goes here ...,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f318,2005,2005-09-22
s385,p385,Citizen Science and Volunteered Geographic Information: Overview and Typology of Participation,Abstract content goes here ...,c56,European Conference on Software Process Improvement,cp56,accepted,f319,2016,2016-07-17
s386,p386,Data Management and Analysis Methods,"This chapter is about methods for managing and analyzing qualitative data. By qualitative data the authors mean text: newspapers, movies, sitcoms, e-mail traffic, folktales, life histories. They also mean narratives--narratives about getting divorced, about being sick, about surviving hand-to-hand combat, about selling sex, about trying to quit smoking. In fact, most of the archaeologically recoverable information about human thought and human behavior is text, the good stuff of social science.",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f320,2005,2005-02-04
s387,p387,Outlier Detection for Temporal Data: A Survey,"In the statistics community, outlier detection for time series data has been studied for decades. Recently, with advances in hardware and software technology, there has been a large body of work on temporal outlier detection from a computational perspective within the computer science community. In particular, advances in hardware technology have enabled the availability of various forms of temporal data collection mechanisms, and advances in software technology have enabled a variety of data management mechanisms. This has fueled the growth of different kinds of data sets such as data streams, spatio-temporal data, distributed streams, temporal networks, and time series data, generated by a multitude of applications. There arises a need for an organized and detailed study of the work done in the area of outlier detection with respect to such temporal datasets. In this survey, we provide a comprehensive and structured overview of a large set of interesting outlier definitions for various forms of temporal data, novel techniques, and application scenarios in which specific definitions and techniques have been widely used.",j1,IEEE Transactions on Knowledge and Data Engineering,jv1,accepted,f321,2020,2020-05-16
s388,p388,Data Streams - Models and Algorithms,Abstract content goes here ...,c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f322,2011,2011-11-11
s389,p389,scmap: projection of single-cell RNA-seq data across data sets,Abstract content goes here ...,j6,Nature Methods,jv6,accepted,f323,2010,2010-06-17
s391,p391,Educational Data Mining and Learning Analytics,Abstract content goes here ...,c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f324,2004,2004-08-22
s392,p392,"Big data: Issues, challenges, tools and Good practices","Big data is defined as large amount of data which requires new technologies and architectures so that it becomes possible to extract value from it by capturing and analysis process. Due to such large size of data it becomes very difficult to perform effective analysis using the existing traditional techniques. Big data due to its various properties like volume, velocity, variety, variability, value and complexity put forward many challenges. Since Big data is a recent upcoming technology in the market which can bring huge benefits to the business organizations, it becomes necessary that various challenges and issues associated in bringing and adapting to this technology are brought into light. This paper introduces the Big data technology along with its importance in the modern world and existing projects which are effective and important in changing the concept of science into big science and society too. The various challenges and issues in adapting and accepting Big data technology, its tools (Hadoop) are also discussed in detail along with the problems Hadoop is facing. The paper concludes with the Good Big data practices to be followed.",c25,International Conference on Contemporary Computing,cp25,accepted,f325,2014,2014-11-04
s393,p393,A Bibliometric Analysis and Visualization of Medical Big Data Research,"With the rapid development of “Internet plus”, medical care has entered the era of big data. However, there is little research on medical big data (MBD) from the perspectives of bibliometrics and visualization. The substantive research on the basic aspects of MBD itself is also rare. This study aims to explore the current status of medical big data through visualization analysis on the journal papers related to MBD. We analyze a total of 988 references which were downloaded from the Science Citation Index Expanded and the Social Science Citation Index databases from Web of Science and the time span was defined as “all years”. The GraphPad Prism 5, VOSviewer and CiteSpace softwares are used for analysis. Many results concerning the annual trends, the top players in terms of journal and institute levels, the citations and H-index in terms of country level, the keywords distribution, the highly cited papers, the co-authorship status and the most influential journals and authors are presented in this paper. This study points out the development status and trends on MBD. It can help people in the medical profession to get comprehensive understanding on the state of the art of MBD. It also has reference values for the research and application of the MBD visualization methods.",c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,cp20,accepted,f326,2014,2014-02-05
s394,p394,The natural selection of bad science,"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.",j124,Royal Society Open Science,jv124,accepted,f327,2006,2006-09-16
s395,p395,Can citizen science enhance public understanding of science?,"Over the past 20 years, thousands of citizen science projects engaging millions of participants in collecting and/or processing data have sprung up around the world. Here we review documented outcomes from four categories of citizen science projects which are defined by the nature of the activities in which their participants engage – Data Collection, Data Processing, Curriculum-based, and Community Science. We find strong evidence that scientific outcomes of citizen science are well documented, particularly for Data Collection and Data Processing projects. We find limited but growing evidence that citizen science projects achieve participant gains in knowledge about science knowledge and process, increase public awareness of the diversity of scientific research, and provide deeper meaning to participants’ hobbies. We also find some evidence that citizen science can contribute positively to social well-being by influencing the questions that are being addressed and by giving people a voice in local environmental decision making. While not all citizen science projects are intended to achieve a greater degree of public understanding of science, social change, or improved science -society relationships, those projects that do require effort and resources in four main categories: (1) project design, (2) outcomes measurement, (3) engagement of new audiences, and (4) new directions for research.",j125,Public Understanding of Science,jv125,accepted,f328,2013,2013-10-16
s398,p398,Advances in data science and classification,Abstract content goes here ...,c29,International Conference on Software Engineering,cp29,accepted,f329,2015,2015-07-06
s399,p399,Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",j11,International Journal of Digital Earth,jv11,accepted,f330,2011,2011-12-24
s401,p401,Astronomical Data Analysis Software and Systems XV,"Until 23 Nov 2003, no total solar eclipse (TSE) had ever been observed from the Antarctic. Yet, interest in securing observations of that event, visible only from the Antarctic, was extremely high and provided the impetus for breaking that paradigm of elusivity in the historical record of science and exploration. The execution of a lunar shadow intercept and the conduction of an observing program from a Boeing 747-400 ER aircraft over the Antarctic interior permitted the previously unobtainable to be accomplished. The unique computational and navigational requirements for this flight are discussed from the enabling perspective of control and data acquisition S/W specifically developed for this task.",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f331,2015,2015-12-26
s402,p402,Openness in Political Science: Data Access and Research Transparency,"In 2012, the American Political Science Association (APSA) Council adopted new policies guiding data access and research transparency in political science. The policies appear as a revision to APSA's Guide to Professional Ethics in Political Science. The revisions were the product of an extended and broad consultation with a variety of APSA committees and the association's membership.",c26,PS,cp26,accepted,f332,2010,2010-02-03
s403,p403,"""Big Data"" : big gaps of knowledge in the field of internet science","Research on so-called ‘Big Data’ has received a considerable momentum and is expected to grow in the future. One very interesting stream of research on Big Data analyzes online networks. Many online networks are known to have some typical macro-characteristics, such as ‘small world’ properties. Much less is known about underlying micro-processes leading to these properties. The models used by Big Data researchers usually are inspired by mathematical ease of exposition. We propose to follow in addition a different strategy that leads to knowledge about micro-processes that match with actual online behavior. This knowledge can then be used for the selection of mathematically-tractable models of online network formation and evolution. Insight from social and behavioral research is needed for pursuing this strategy of knowledge generation about micro-processes. Accordingly, our proposal points to a unique role that social scientists could play in Big Data research.",c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f333,2001,2001-08-03
s404,p404,Sharing Data and Materials in Psychological Science,Abstract content goes here ...,j126,Psychology Science,jv126,accepted,f334,2015,2015-03-30
s405,p405,Next Generation Science Standards,"Science and Engineering Practices that connect to garden-based education (all 8): • Asking questions (for science) and defining problems (for engineering) • Developing and using models • Planning and carrying out investigations • Analyzing and interpreting data • Using mathematics and computational thinking • Constructing explanations (for science) and designing solutions (for engineering) • Engaging in argument from evidence • Obtaining, evaluating, and communicating information",c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f335,2006,2006-11-24
s406,p406,Comment on “Estimating the reproducibility of psychological science”,"A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.",j97,Science,jv97,accepted,f336,2012,2012-12-05
s407,p407,Ecoinformatics: supporting ecology as a data-intensive science.,Abstract content goes here ...,j127,Trends in Ecology & Evolution,jv127,accepted,f337,2011,2011-04-09
s408,p408,Big healthcare data: preserving security and privacy,Abstract content goes here ...,j8,Journal of Big Data,jv8,accepted,f338,2019,2019-11-11
s409,p409,Open Science - Practical Issues in Open Research Data,"The term âopen dataâ refers to information that has been made technically and legally available for reuse. In our research, we focus on the particular case of open research data. We conducted a literature review in order to determine what are the motivations to release open research data and what are the issues related to the development of open research data. Our research allowed to identify seven motivations for researchers to open research data and discuss seven issues. The paper highlights the lack of data infrastructure dedicated to open research data and the need for developing the researcherâs ability to animate online communities.",c22,International Conference on Data Technologies and Applications,cp22,accepted,f339,2020,2020-10-01
s410,p410,Concrete mathematics - a foundation for computer science,"From the Publisher: 
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. 
 
Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. ""More concretely,"" the authors explain, ""it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems."" The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. 
 
Major topics include: 
 
Sums 
Recurrences 
Integer functions 
Elementary number theory 
Binomial coefficients 
Generating functions 
Discrete probability 
Asymptotic methods 
 
 
This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.",c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f340,2006,2006-07-08
s411,p411,Topology and data,"An important feature of modern science and engineering is that data of various kinds is being produced at an unprecedented rate. This is so in part because of new experimental methods, and in part because of the increase in the availability of high powered computing technology. It is also clear that the nature of the data we are obtaining is significantly different. For example, it is now often the case that we are given data in the form of very long vectors, where all but a few of the coordinates turn out to be irrelevant to the questions of interest, and further that we don’t necessarily know which coordinates are the interesting ones. A related fact is that the data is often very high-dimensional, which severely restricts our ability to visualize it. The data obtained is also often much noisier than in the past and has more missing information (missing data). This is particularly so in the case of biological data, particularly high throughput data from microarray or other sources. Our ability to analyze this data, both in terms of quantity and the nature of the data, is clearly not keeping pace with the data being produced. In this paper, we will discuss how geometry and topology can be applied to make useful contributions to the analysis of various kinds of data. Geometry and topology are very natural tools to apply in this direction, since geometry can be regarded as the study of distance functions, and what one often works with are distance functions on large finite sets of data. The mathematical formalism which has been developed for incorporating geometric and topological techniques deals with point clouds, i.e. finite sets of points equipped with a distance function. It then adapts tools from the various branches of geometry to the study of point clouds. The point clouds are intended to be thought of as finite samples taken from a geometric object, perhaps with noise. Here are some of the key points which come up when applying these geometric methods to data analysis. • Qualitative information is needed: One important goal of data analysis is to allow the user to obtain knowledge about the data, i.e. to understand how it is organized on a large scale. For example, if we imagine that we are looking at a data set constructed somehow from diabetes patients, it would be important to develop the understanding that there are two types of the disease, namely the juvenile and adult onset forms. Once that is established, one of course wants to develop quantitative methods for distinguishing them, but the first insight about the distinct forms of the disease is key.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f341,2011,2011-02-07
s412,p412,"Statistics for experimenters : an introduction to design, data analysis, and model building","Science and Statistics. COMPARING TWO TREATMENTS. Use of External Reference Distribution to Compare Two Means. Random Sampling and the Declaration of Independence. Randomization and Blocking with Paired Comparisons. Significance Tests and Confidence Intervals for Means, Variances, Proportions and Frequences. COMPARING MORE THAN TWO TREATMENTS. Experiments to Compare k Treatment Means. Randomized Block and Two--Way Factorial Designs. Designs with More Than One Blocking Variable. MEASURING THE EFFECTS OF VARIABLES. Empirical Modeling. Factorial Designs at Two Levels. More Applications of Factorial Designs. Fractional Factorial Designs at Two Levels. More Applications of Fractional Factorial Designs. BUILDING MODELS AND USING THEM. Simple Modeling with Least Squares (Regression Analysis). Response Surface Methods. Mechanistic Model Building. Study of Variation. Modeling Dependence: Times Series. Appendix Tables. Index.",c27,ACM-SIAM Symposium on Discrete Algorithms,cp27,accepted,f342,2022,2022-01-26
s413,p413,The Fourth Paradigm: Data-Intensive Scientific Discovery,Abstract content goes here ...,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f343,2018,2018-10-23
s414,p414,Data-intensive science applied to broad-scale citizen science.,Abstract content goes here ...,j127,Trends in Ecology & Evolution,jv127,accepted,f344,2011,2011-08-07
s415,p415,What Is Citizen Science? – A Scientometric Meta-Analysis,"Context The concept of citizen science (CS) is currently referred to by many actors inside and outside science and research. Several descriptions of this purportedly new approach of science are often heard in connection with large datasets and the possibilities of mobilizing crowds outside science to assists with observations and classifications. However, other accounts refer to CS as a way of democratizing science, aiding concerned communities in creating data to influence policy and as a way of promoting political decision processes involving environment and health. Objective In this study we analyse two datasets (N = 1935, N = 633) retrieved from the Web of Science (WoS) with the aim of giving a scientometric description of what the concept of CS entails. We account for its development over time, and what strands of research that has adopted CS and give an assessment of what scientific output has been achieved in CS-related projects. To attain this, scientometric methods have been combined with qualitative approaches to render more precise search terms. Results Results indicate that there are three main focal points of CS. The largest is composed of research on biology, conservation and ecology, and utilizes CS mainly as a methodology of collecting and classifying data. A second strand of research has emerged through geographic information research, where citizens participate in the collection of geographic data. Thirdly, there is a line of research relating to the social sciences and epidemiology, which studies and facilitates public participation in relation to environmental issues and health. In terms of scientific output, the largest body of articles are to be found in biology and conservation research. In absolute numbers, the amount of publications generated by CS is low (N = 1935), but over the past decade a new and very productive line of CS based on digital platforms has emerged for the collection and classification of data.",j108,PLoS ONE,jv108,accepted,f345,2006,2006-03-31
s416,p416,Big data and the future of ecology,"The need for sound ecological science has escalated alongside the rise of the information age and “big data” across all sectors of society. Big data generally refer to massive volumes of data not readily handled by the usual data tools and practices and present unprecedented opportunities for advancing science and inform- ing resource management through data-intensive approaches. The era of big data need not be propelled only by “big science” – the term used to describe large-scale efforts that have had mixed success in the individual-driven culture of ecology. Collectively, ecologists already have big data to bolster the scientific effort – a large volume of distributed, high-value information – but many simply fail to contribute. We encourage ecologists to join the larger scientific community in global initiatives to address major scientific and societal problems by bringing their distributed data to the table and harnessing its collective power. The scientists who contribute such information will be at the forefront of socially relevant science – but will they be ecologists?",c100,ACM SIGMOD Conference,cp100,accepted,f346,2010,2010-07-06
s417,p417,Linear Mixed Models for Longitudinal Data,Abstract content goes here ...,c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f347,2019,2019-09-30
s418,p418,ImmPort: disseminating data to the public for the future of immunology,Abstract content goes here ...,j128,Immunologic research,jv128,accepted,f348,2002,2002-07-23
s419,p419,Social Science in the Era of Big Data,"Digital technologies keep track of everything we do and say while we are online, and we spend online an increasing portion of our time. Databases hidden behind web services and applications are constantly fed with information of our movements and communication patterns, and a significant dimension of our lives, quantified to unprecedented levels, gets stored in those vast online repositories. This article considers some of the implications of this torrent of data for social science research, and for the types of questions we can ask of the world we inhabit. The goal of the article is twofold: to explain why, in spite of all the data, theory still matters to build credible stories of what the data reveal; and to show how this allows social scientists to revisit old questions at the intersection of new technologies and disciplinary approaches. The article also considers how Big Data research can transform policy making, with a focus on how it can help us improve communication and governance in policy-relevant domains.",c72,Intelligent Systems in Molecular Biology,cp72,accepted,f349,2003,2003-06-29
s420,p420,Where are human subjects in Big Data research? The emerging ethics divide,"There are growing discontinuities between the research practices of data science and established tools of research ethics regulation. Some of the core commitments of existing research ethics regulations, such as the distinction between research and practice, cannot be cleanly exported from biomedical research to data science research. Such discontinuities have led some data science practitioners and researchers to move toward rejecting ethics regulations outright. These shifts occur at the same time as a proposal for major revisions to the Common Rule—the primary regulation governing human-subjects research in the USA—is under consideration for the first time in decades. We contextualize these revisions in long-running complaints about regulation of social science research and argue data science should be understood as continuous with social sciences in this regard. The proposed regulations are more flexible and scalable to the methods of non-biomedical research, yet problematically largely exclude data science methods from human-subjects regulation, particularly uses of public datasets. The ethical frameworks for Big Data research are highly contested and in flux, and the potential harms of data science research are unpredictable. We examine several contentious cases of research harms in data science, including the 2014 Facebook emotional contagion study and the 2016 use of geographical data techniques to identify the pseudonymous artist Banksy. To address disputes about application of human-subjects research ethics in data science, critical data studies should offer a historically nuanced theory of “data subjectivity” responsive to the epistemic methods, harms and benefits of data science and commerce.",c12,International Conference on Statistical and Scientific Database Management,cp12,accepted,f350,2002,2002-10-06
s421,p421,Lessons from lady beetles: accuracy of monitoring data from US and UK citizen-science programs,"Citizen scientists have the potential to play a crucial role in the study of rapidly changing lady beetle (Coccinellidae) populations. We used data derived from three coccinellid-focused citizen-science programs to examine the costs and benefits of data collection from direct citizen-science (data used without verification) and verified citizen-science (observations verified by trained experts) programs. Data collated through direct citizen science overestimated species richness and diversity values in comparison to verified data, thereby influencing interpretation. The use of citizen scientists to collect data also influenced research costs; our analysis shows that verified citizen science was more cost effective than traditional science (in terms of data gathered per dollar). The ability to collect a greater number of samples through direct citizen science may compensate for reduced accuracy, depending on the type of data collected and the type(s) and extent of errors committed by volunteers.",c6,Americas Conference on Information Systems,cp6,accepted,f351,2007,2007-09-27
s422,p422,The View from Above: Applications of Satellite Data in Economics,"The past decade or so has seen a dramatic change in the way that economists can learn by watching our planet from above. A revolution has taken place in remote sensing and allied fields such as computer science, engineering, and geography. Petabytes of satellite imagery have become publicly accessible at increasing resolution, many algorithms for extracting meaningful social science information from these images are now routine, and modern cloud-based processing power allows these algorithms to be run at global scale. This paper seeks to introduce economists to the science of remotely sensed data, and to give a flavor of how this new source of data has been used by economists so far and what might be done in the future.",c90,Computer Vision and Pattern Recognition,cp90,accepted,f352,2008,2008-08-31
s423,p423,A Criteria-based Assessment of the Coverage of Scopus and Web of Science,"Abstract Purpose The purpose of this study is to assess the coverage of the scientific literature in Scopus and Web of Science from the perspective of research evaluation. Design/methodology/approach The academic communities of Norway have agreed on certain criteria for what should be included as original research publications in research evaluation and funding contexts. These criteria have been applied since 2004 in a comprehensive bibliographic database called the Norwegian Science Index (NSI). The relative coverages of Scopus and Web of Science are compared with regard to publication type, field of research and language. Findings Our results show that Scopus covers 72 percent of the total Norwegian scientific and scholarly publication output in 2015 and 2016, while the corresponding figure for Web of Science Core Collection is 69 percent. The coverages are most comprehensive in medicine and health (89 and 87 percent) and in the natural sciences and technology (85 and 84 percent). The social sciences (48 percent in Scopus and 40 percent in Web of Science Core Collection) and particularly the humanities (27 and 23 percent) are much less covered in the two international data sources. Research limitation Comparing with data from only one country is a limitation of the study, but the criteria used to define a country’s scientific output as well as the identification of patterns of field-dependent partial representations in Scopus and Web of Science should be recognizable and useful also for other countries. Originality/value The novelty of this study is the criteria-based approach to studying coverage problems in the two data sources.",j58,Journal of Data and Information Science,jv58,accepted,f353,2012,2012-06-10
s428,p428,Taxonomic bias in biodiversity data and societal preferences,Abstract content goes here ...,j130,Scientific Reports,jv130,accepted,f354,2006,2006-10-03
s429,p429,The Pan-STARRS1 Database and Data Products,"This paper describes the organization of the database and the catalog data products from the Pan-STARRS1 3π Steradian Survey. The catalog data products are available in the form of an SQL-based relational database from MAST, the Mikulski Archive for Space Telescopes at STScI. The database is described in detail, including the construction of the database, the provenance of the data, the schema, and how the database tables are related. Examples of queries for a range of science goals are included.",j131,Astrophysical Journal Supplement Series,jv131,accepted,f355,2019,2019-03-13
s430,p430,JENDL-4.0: A New Library for Nuclear Science and Engineering,"The fourth version of the Japanese Evaluated Nuclear Data Library has been produced in cooperation with the Japanese Nuclear Data Committee. In the new library, much emphasis is placed on the improvements of fission product and minor actinoid data. Two nuclear model codes were developed in order to evaluate the cross sections of fission products and minor actinoids. Coupled-channel optical model parameters, which can be applied to wide mass and energy regions, were obtained for nuclear model calculations. Thermal cross sections of actinoids were carefully examined by considering experimental data or by the systematics of neighboring nuclei. Most of the fission cross sections were derived from experimental data. A simultaneous evaluation was performed for the fission cross sections of important uranium and plutonium isotopes above 10 keV. New evaluations were performed for the thirty fissionproduct nuclides that had not been contained in the previous library JENDL-3.3. The data for light elements and structural materials were partly reevaluated. Moreover, covariances were estimated mainly for actinoids. The new library was released as JENDL-4.0, and the data can be retrieved from the Web site of the JAEA Nuclear Data Center.",c62,International Conference on Software Reuse,cp62,accepted,f356,2006,2006-05-03
s431,p431,Citizen Science: A Developing Tool for Expanding Science Knowledge and Scientific Literacy,"Citizen science enlists the public in collecting large quantities of data across an array of habitats and locations over long spans of time. Citizen science projects have been remarkably successful in advancing scientific knowledge, and contributions from citizen scientists now provide a vast quantity of data about species occurrence and distribution around the world. Most citizen science projects also strive to help participants learn about the organisms they are observing and to experience the process by which scientific investigations are conducted. Developing and implementing public data-collection projects that yield both scientific and educational outcomes requires significant effort. This article describes the model for building and operating citizen science projects that has evolved at the Cornell Lab of Ornithology over the past two decades. We hope that our model will inform the fields of biodiversity monitoring, biological research, and science education while providing a window into the culture of citizen science.",c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f357,2022,2022-05-05
s432,p432,Citizen Science as an Ecological Research Tool: Challenges and Benefits,"Citizen science, the involvement of volunteers in research, has increased the scale of ecological field studies with continent-wide, centralized monitoring efforts and, more rarely, tapping of volunteers to conduct large, coordinated, field experiments. The unique benefit for the field of ecology lies in understanding processes occurring at broad geographic scales and on private lands, which are impossible to sample extensively with traditional field research models. Citizen science produces large, longitudinal data sets, whose potential for error and bias is poorly understood. Because it does not usually aim to uncover mechanisms underlying ecological patterns, citizen science is best viewed as complementary to more localized, hypothesis-driven research. In the process of addressing the impacts of current, global “experiments” altering habitat and climate, large-scale citizen science has led to new, quantitative approaches to emerging questions about the distribution and abundance of organisms across spa...",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f358,2011,2011-01-21
s434,p434,Economics in the age of big data,"Background Economic science has evolved over several decades toward greater emphasis on empirical work. The data revolution of the past decade is likely to have a further and profound effect on economic research. Increasingly, economists make use of newly available large-scale administrative data or private sector data that often are obtained through collaborations with private firms, giving rise to new opportunities and challenges. The rising use of non–publicly available data in economic research. Here we show the percentage of papers published in the American Economic Review (AER) that obtained an exemption from the AER’s data availability policy, as a share of all papers published by the AER that relied on any form of data (excluding simulations and laboratory experiments). Notes and comments, as well as AER Papers and Proceedings issues, are not included in the analysis. We obtained a record of exemptions directly from the AER administrative staff and coded each exemption manually to reflect public sector versus private data. Our check of nonexempt papers suggests that the AER records may possibly understate the percentage of papers that actually obtained exemptions. The asterisk indicates that data run from when the AER started collecting these data (December 2005 issue) to the September 2014 issue. To make full use of the data, we define year 2006 to cover October 2005 through September 2006, year 2007 to cover October 2006 through September 2007, and so on. Advances These new data are affecting economic research along several dimensions. Many fields have shifted from a reliance on relatively small-sample government surveys to administrative data with universal or near-universal population coverage. This shift is transformative, as it allows researchers to rigorously examine variation in wages, health, productivity, education, and other measures across different subpopulations; construct consistent long-run statistical indices; generate new quasi-experimental research designs; and track diverse outcomes from natural and controlled experiments. Perhaps even more notable is the expansion of private sector data on economic activity. These data, sometimes available from public sources but other times obtained through data-sharing agreements with private firms, can help to create more granular and real-time measurement of aggregate economic statistics. The data also offer researchers a look inside the “black box” of firms and markets by providing meaningful statistics on economic behavior such as search and information gathering, communication, decision-making, and microlevel transactions. Collaborations with data-oriented firms also create new opportunities to conduct and evaluate randomized experiments. Economic theory plays an important role in the analysis of large data sets with complex structure. It can be difficult to organize and study this type of data (or even to decide which variables to construct) without a simplifying conceptual framework, which is where economic models become useful. Better data also allow for sharper tests of existing models and tests of theories that had previously been difficult to assess. Outlook The advent of big data is already allowing for better measurement of economic effects and outcomes and is enabling novel research designs across a range of topics. Over time, these data are likely to affect the types of questions economists pose, by allowing for more focus on population variation and the analysis of a broader range of economic activities and interactions. We also expect economists to increasingly adopt the large-data statistical methods that have been developed in neighboring fields and that often may complement traditional econometric techniques. These data opportunities also raise some important challenges. Perhaps the primary one is developing methods for researchers to access and explore data in ways that respect privacy and confidentiality concerns. This is a major issue in working with both government administrative data and private sector firms. Other challenges include developing the appropriate data management and programming capabilities, as well as designing creative and scalable approaches to summarize, describe, and analyze large-scale and relatively unstructured data sets. These challenges notwithstanding, the next few decades are likely to be a very exciting time for economic research. The quality and quantity of data on economic activity are expanding rapidly. Empirical research increasingly relies on newly available large-scale administrative data or private sector data that often is obtained through collaboration with private firms. Here we highlight some challenges in accessing and using these new data. We also discuss how new data sets may change the statistical methods used by economists and the types of questions posed in empirical research.",j97,Science,jv97,accepted,f359,2012,2012-12-20
s435,p435,Computational Social Science,Abstract content goes here ...,j97,Science,jv97,accepted,f360,2012,2012-09-30
s436,p436,The reusable holdout: Preserving validity in adaptive data analysis,"Testing hypotheses privately Large data sets offer a vast scope for testing already-formulated ideas and exploring new ones. Unfortunately, researchers who attempt to do both on the same data set run the risk of making false discoveries, even when testing and exploration are carried out on distinct subsets of data. Based on ideas drawn from differential privacy, Dwork et al. now provide a theoretical solution. Ideas are tested against aggregate information, whereas individual data set components remain confidential. Preserving that privacy also preserves statistical inference validity. Science, this issue p. 636 A statistical approach allows large data sets to be reanalyzed to test new hypotheses. Misapplication of statistical data analysis is a common cause of spurious discoveries in scientific research. Existing approaches to ensuring the validity of inferences drawn from data assume a fixed procedure to be performed, selected before the data are examined. In common practice, however, data analysis is an intrinsically adaptive process, with new analyses generated on the basis of data exploration, as well as the results of previous analyses on the same data. We demonstrate a new approach for addressing the challenges of adaptivity based on insights from privacy-preserving data analysis. As an application, we show how to safely reuse a holdout data set many times to validate the results of adaptively chosen analyses.",j97,Science,jv97,accepted,f361,2012,2012-05-15
s437,p437,The iPlant Collaborative: Cyberinfrastructure for Enabling Data to Discovery for the Life Sciences,"The iPlant Collaborative provides life science research communities access to comprehensive, scalable, and cohesive computational infrastructure for data management; identity management; collaboration tools; and cloud, high-performance, high-throughput computing. iPlant provides training, learning material, and best practice resources to help all researchers make the best use of their data, expand their computational skill set, and effectively manage their data and computation when working as distributed teams. iPlant’s platform permits researchers to easily deposit and share their data and deploy new computational tools and analysis workflows, allowing the broader community to easily use and reuse those data and computational analyses.",j60,PLoS Biology,jv60,accepted,f362,2001,2001-06-06
s438,p438,The TESS science processing operations center,"The Transiting Exoplanet Survey Satellite (TESS) will conduct a search for Earth's closest cousins starting in early 2018 and is expected to discover ∼1,000 small planets with Rp < 4 R⊕ and measure the masses of at least 50 of these small worlds. The Science Processing Operations Center (SPOC) is being developed at NASA Ames Research Center based on the Kepler science pipeline and will generate calibrated pixels and light curves on the NASA Advanced Supercomputing Division's Pleiades supercomputer. The SPOC will also search for periodic transit events and generate validation products for the transit-like features in the light curves. All TESS SPOC data products will be archived to the Mikulski Archive for Space Telescopes (MAST).",c91,Workshop on Algorithms and Models for the Web-Graph,cp91,accepted,f363,2022,2022-09-25
s439,p439,The current state of citizen science as a tool for ecological research and public engagement,"Approaches to citizen science – an indispensable means of combining ecological research with environmental education and natural history observation – range from community-based monitoring to the use of the internet to “crowd-source” various scientific tasks, from data collection to discovery. With new tools and mechanisms for engaging learners, citizen science pushes the envelope of what ecologists can achieve, both in expanding the potential for spatial ecology research and in supplementing existing, but localized, research programs. The primary impacts of citizen science are seen in biological studies of global climate change, including analyses of phenology, landscape ecology, and macro-ecology, as well as in sub-disciplines focused on species (rare and invasive), disease, populations, communities, and ecosystems. Citizen science and the resulting ecological data can be viewed as a public good that is generated through increasingly collaborative tools and resources, while supporting public participation in science and Earth stewardship.",c1,Technical Symposium on Computer Science Education,cp1,accepted,f364,2002,2002-12-17
s440,p440,Fuzzy-Set Social Science,"In this innovative approach to the practice of social science, Charles Ragin explores the use of fuzzy sets to bridge the divide between quantitative and qualitative methods. Paradoxically, the fuzzy set is a powerful tool because it replaces an unwieldy, ""fuzzy"" instrument—the variable, which establishes only the positions of cases relative to each other, with a precise one—degree of membership in a well-defined set. Ragin argues that fuzzy sets allow a far richer dialogue between ideas and evidence in social research than previously possible. They let quantitative researchers abandon ""homogenizing assumptions"" about cases and causes, they extend diversity-oriented research strategies, and they provide a powerful connection between theory and data analysis. Most important, fuzzy sets can be carefully tailored to fit evolving theoretical concepts, sharpening quantitative tools with in-depth knowledge gained through qualitative, case-oriented inquiry. This book will revolutionize research methods not only in sociology, political science, and anthropology but in any field of inquiry dealing with complex patterns of causation.",c58,Australian Software Engineering Conference,cp58,accepted,f365,2021,2021-11-29
s441,p441,Online analysis enhances use of NASA Earth science data,"Giovanni, the Goddard Earth Sciences Data and Information Services Center (GES DISC) Interactive Online Visualization and Analysis Infrastructure, has provided researchers with advanced capabilities to perform data exploration and analysis with observational data from NASA Earth observation satellites. In the past 5–10 years, examining geophysical events and processes with remote-sensing data required a multistep process of data discovery, data acquisition, data management, and ultimately data analysis. Giovanni accelerates this process by enabling basic visualization and analysis directly on the World Wide Web. In the last two years, Giovanni has added new data acquisition functions and expanded analysis options to increase its usefulness to the Earth science research community.",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f366,2018,2018-03-05
s442,p442,The Deluge of Spurious Correlations in Big Data,Abstract content goes here ...,c1,Technical Symposium on Computer Science Education,cp1,accepted,f367,2002,2002-10-27
s444,p444,The KDD process for extracting useful knowledge from volumes of data,"AS WE MARCH INTO THE AGE of digital information, the problem of data overload looms ominously ahead. Our ability to analyze and understand massive datasets lags far behind our ability to gather and store the data. A new generation of computational techniques and tools is required to support the extraction of useful knowledge from the rapidly growing volumes of data. These techniques and tools are the subject of the emerging field of knowledge discovery in databases (KDD) and data mining. Large databases of digital information are ubiquitous. Data from the neighborhood store’s checkout register, your bank’s credit card authorization device, records in your doctor’s office, patterns in your telephone calls, and many more applications generate streams of digital records archived in huge databases, sometimes in so-called data warehouses. Current hardware and database technology allow efficient and inexpensive reliable data storage and access. However, whether the context is business, medicine, science, or government, the datasets themselves (in raw form) are of little direct value. What is of value is the knowledge that can be inferred from the data and put to use. For example, the marketing database of a consumer U s a m a F a y y a d ,",c9,Pacific Symposium on Biocomputing,cp9,accepted,f368,2009,2009-06-26
s446,p446,Critical Reviews in Food Science and Nutrition,"Background: Health researchers may struggle to choose suitable validated dietary assessment tools (DATs) for their target population. The aim of this review was to identify and collate information on validated UK DATs and validation studies for inclusion on a website to support researchers to choose appropriate DATs. Design: A systematic review of reviews of DATs was undertaken. DATs validated in UK populations were extracted from the studies identified. A searchable website was designed to display these data. Additionally, mean differences and limits of agreement between test and comparison methods were summarized by a method, weighting by sample size. Results: Over 900 validation results covering 5 life stages, 18 nutrients, 6 dietary assessment methods, and 9 validation method types were extracted from 63 validated DATs which were identified from 68 reviews. These were incorporated into www.nutritools.org. Limits of agreement were determined for about half of validations. Thirty four DATs were FFQs. Only 17 DATs were validated against biomarkers, and only 19 DATs were validated in infant/children/adolescents. Conclusions: The interactive www.nutritools.org website holds extensive validation data identified from this review and can be used to guide researchers to critically compare and choose a suitable DAT for their research question, leading to improvement of nutritional epidemiology research.",c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f369,2006,2006-02-02
s447,p447,The BIG Data Center: from deposition to integration to translation,"Biological data are generated at unprecedentedly exponential rates, posing considerable challenges in big data deposition, integration and translation. The BIG Data Center, established at Beijing Institute of Genomics (BIG), Chinese Academy of Sciences, provides a suite of database resources, including (i) Genome Sequence Archive, a data repository specialized for archiving raw sequence reads, (ii) Gene Expression Nebulas, a data portal of gene expression profiles based entirely on RNA-Seq data, (iii) Genome Variation Map, a comprehensive collection of genome variations for featured species, (iv) Genome Warehouse, a centralized resource housing genome-scale data with particular focus on economically important animals and plants, (v) Methylation Bank, an integrated database of whole-genome single-base resolution methylomes and (vi) Science Wikis, a central access point for biological wikis developed for community annotations. The BIG Data Center is dedicated to constructing and maintaining biological databases through big data integration and value-added curation, conducting basic research to translate big data into big knowledge and providing freely open access to a variety of data resources in support of worldwide research activities in both academia and industry. All of these resources are publicly available and can be found at http://bigd.big.ac.cn.",c35,EUROMICRO Conference on Software Engineering and Advanced Applications,cp35,accepted,f370,2005,2005-12-03
s448,p448,Handbook of Theoretical Computer Science,"""Of all the books I have covered in the Forum to date, this set is the most unique and possibly the most useful to the SIGACT community, in support both of teaching and research.... The books can be used by anyone wanting simply to gain an understanding of one of these areas, or by someone desiring to be in research in a topic, or by instructors wishing to find timely information on a subject they are teaching outside their major areas of expertise."" -- Rocky Ross, ""SIGACT News"" ""This is a reference which has a place in every computer science library."" -- Raymond Lauzzana, ""Languages of Design"" The Handbook of Theoretical Computer Science provides professionals and students with a comprehensive overview of the main results and developments in this rapidly evolving field. Volume A covers models of computation, complexity theory, data structures, and efficient computation in many recognized subdisciplines of theoretical computer science. Volume B takes up the theory of automata and rewriting systems, the foundations of modern programming languages, and logics for program specification and verification, and presents several studies on the theoretic modeling of advanced information processing. The two volumes contain thirty-seven chapters, with extensive chapter references and individual tables of contents for each chapter. There are 5,387 entry subject indexes that include notational symbols, and a list of contributors and affiliations in each volume.",c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f371,2018,2018-09-10
s449,p449,"Big Data, Little Data, No Data: Scholarship in the Networked World","""Big Data"" is on the covers of Science, Nature, the Economist, and Wired magazines, on the front pages of the Wall Street Journal and the New York Times. But despite the media hyperbole, as Christine Borgman points out in this examination of data and scholarly research, having the right data is usually better than having more data; little data can be just as valuable as big data. In many cases, there are no data -- because relevant data don't exist, cannot be found, or are not available. Moreover, data sharing is difficult, incentives to do so are minimal, and data practices vary widely across disciplines. Borgman, an often-cited authority on scholarly communication, argues that data have no value or meaning in isolation; they exist within a knowledge infrastructure -- an ecology of people, practices, technologies, institutions, material objects, and relationships. After laying out the premises of her investigation -- six ""provocations"" meant to inspire discussion about the uses of data in scholarship -- Borgman offers case studies of data practices in the sciences, the social sciences, and the humanities, and then considers the implications of her findings for scholarly practice and research policy. To manage and exploit data over the long term, Borgman argues, requires massive investment in knowledge infrastructures; at stake is the future of scholarship.",c3,Frontiers in Education Conference,cp3,accepted,f372,2016,2016-11-05
s450,p450,"Frascati manual 2015 : guidelines for collecting and reporting data in research and experimental development: the measurement of scientific, technological and innovation activities.","The Frascati Manual is firmly based on experience gained from collecting R&D 
statistics in both OECD and non-member countries. It is a result of the collective work 
of national experts in NESTI, the OECD Working Party of National Experts on Science 
and Technology Indicators. This group, with support from the OECD Secretariat, has 
worked over now more than 50 years as an effective community of practitioners to 
implement measurement approaches for the concepts of science, technology and 
innovation. This effort has resulted in a series of methodological manuals known as the 
“Frascati Family”, which in addition to this manual includes guidance documents on 
the measurement of innovation (the Oslo Manual), human resources devoted to science 
and technology, patents, and technological balance of payments, but most importantly, 
it has provided the basis for the main statistics and indicators on science and technology 
that are currently used.",c94,Vision,cp94,accepted,f373,2020,2020-01-04
s451,p451,Inquiry-based science instruction—what is it and does it matter? Results from a research synthesis years 1984 to 2002,"The goal of the Inquiry Synthesis Project was to synthesize findings from research conducted between 1984 and 2002 to address the research question, What is the impact of inquiry science instruction on K-12 student outcomes? The timeframe of 1984 to 2002 was selected to continue a line of synthesis work last completed in 1983 by Bredderman (Bredderman (1983) Review of Educational Research 53: 499-518) and Shymansky, Kyle, and Alport (Shymansky et al. (1983) Journal of Research in Science Teaching 20: 387-404), and to accommodate a practicable cut- off date given the research project timeline, which ran from 2001 to 2006. The research question for the project was addressed by developing a conceptual framework that clarifies and specifies what is meant by ''inquiry-based science instruction,'' and by using a mixed-methodology approach to analyze both numerical and text data describing the impact of instruction on K-12 student science conceptual learning. Various findings across 138 analyzed studies indicate a clear, positive trend favoring inquiry-based instructional practices, particularly instruction that emphasizes student active thinking and drawing conclusions from data. Teaching strategies that actively engage students in the learning process through scientific investigations are more likely to increase conceptual understanding than are strategies that rely on more passive techniques, which are often necessary in the current standardized-assessment laden educational environment.",c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f374,2006,2006-06-14
s452,p452,Politicization of Science in the Public Sphere,"This study explores time trends in public trust in science in the United States from 1974 to 2010. More precisely, I test Mooney’s (2005) claim that conservatives in the United States have become increasingly distrustful of science. Using data from the 1974 to 2010 General Social Survey, I examine group differences in trust in science and group-specific change in these attitudes over time. Results show that group differences in trust in science are largely stable over the period, except for respondents identifying as conservative. Conservatives began the period with the highest trust in science, relative to liberals and moderates, and ended the period with the lowest. The patterns for science are also unique when compared to public trust in other secular institutions. Results show enduring differences in trust in science by social class, ethnicity, gender, church attendance, and region. I explore the implications of these findings, specifically, the potential for political divisions to emerge over the cultural authority of science and the social role of experts in the formation of public policy.",c29,International Conference on Software Engineering,cp29,accepted,f375,2015,2015-07-15
s454,p454,Crisis informatics—New data for extraordinary times,"Focus on behaviors, not on fetishizing social media tools Crisis informatics is a multidisciplinary field combining computing and social science knowledge of disasters; its central tenet is that people use personal information and communication technology to respond to disaster in creative ways to cope with uncertainty. We study and develop computational support for collection and sociobehavioral analysis of online participation (i.e., tweets and Facebook posts) to address challenges in disaster warning, response, and recovery. Because such data are rarely tidy, we offer lessons—learned the hard way, as we have made every mistake described below—with respect to the opportunities and limitations of social media research on crisis events.",j97,Science,jv97,accepted,f376,2012,2012-06-10
s455,p455,The Analysis of Social Science Data with Missing Values,"Methods for handling missing data in social science data sets are reviewed. Limitations of common practical approaches, including complete-case analysis, available-case analysis and imputation, are illustrated on a simple missing-data problem with one complete and one incomplete variable. Two more principled approaches, namely maximum likelihood under a model for the data and missing-data mechanism and multiple imputation, are applied to the bivariate problem. General properties of these methods are outlined, and applications to more complex missing-data problems are discussed. The EM algorithm, a convenient method for computing maximum likelihood estimates in missing-data problems, is described and applied to two common models, the multivariate normal model for continuous data and the multinomial model for discrete data. Multiple imputation under explicit or implicit models is recommended as a method that retains the advantages of imputation and overcomes its limitations.",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f377,2015,2015-08-13
s456,p456,A Review of Microsoft Academic Services for Science of Science Studies,"Since the relaunch of Microsoft Academic Services (MAS) 4 years ago, scholarly communications have undergone dramatic changes: more ideas are being exchanged online, more authors are sharing their data, and more software tools used to make discoveries and reproduce the results are being distributed openly. The sheer amount of information available is overwhelming for individual humans to keep up and digest. In the meantime, artificial intelligence (AI) technologies have made great strides and the cost of computing has plummeted to the extent that it has become practical to employ intelligent agents to comprehensively collect and analyze scholarly communications. MAS is one such effort and this paper describes its recent progresses since the last disclosure. As there are plenty of independent studies affirming the effectiveness of MAS, this paper focuses on the use of three key AI technologies that underlies its prowess in capturing scholarly communications with adequate quality and broad coverage: (1) natural language understanding in extracting factoids from individual articles at the web scale, (2) knowledge assisted inference and reasoning in assembling the factoids into a knowledge graph, and (3) a reinforcement learning approach to assessing scholarly importance for entities participating in scholarly communications, called the saliency, that serves both as an analytic and a predictive metric in MAS. These elements enhance the capabilities of MAS in supporting the studies of science of science based on the GOTO principle, i.e., good and open data with transparent and objective methodologies. The current direction of development and how to access the regularly updated data and tools from MAS, including the knowledge graph, a REST API and a website, are also described.",c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f378,2015,2015-05-29
s457,p457,The Synthetic Data Vault,"The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault (SDV), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name SDV. When implementing the SDV, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-the-art multivariate modeling approach to model this data. The SDV iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the SDV to synthesize data by sampling from any part of the database. After building the SDV, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the SDV is a viable solution for synthetic data generation.",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f379,2017,2017-01-17
s458,p458,Data streams: algorithms and applications,"Data stream algorithms as an active research agenda emerged only over the past few years, even though the concept of making few passes over the data for performing computations has been around since the early days of Automata Theory. The data stream agenda now pervades many branches of Computer Science including databases, networking, knowledge discovery and data mining, and hardware systems. Industry is in synch too, with Data Stream Management Systems (DSMSs) and special hardware to deal with data speeds. Even beyond Computer Science, data stream concerns are emerging in physics, atmospheric science and statistics. Data Streams: Algorithms and Applications focuses on the algorithmic foundations of data streaming. In the data stream scenario, input arrives very rapidly and there is limited memory to store the input. Algorithms have to work with one or few passes over the data, space less than linear in the input size or time significantly less than the input size. In the past few years, a new theory has emerged for reasoning about algorithms that work within these constraints on space, time and number of passes. Some of the methods rely on metric embeddings, pseudo-random computations, sparse approximation theory and communication complexity. The applications for this scenario include IP network traffic analysis, mining text message streams and processing massive data sets in general. Data Streams: Algorithms and Applications surveys the emerging area of algorithms for processing data streams and associated applications. An extensive bibliography with over 200 entries points the reader to further resources for exploration.",c27,ACM-SIAM Symposium on Discrete Algorithms,cp27,accepted,f380,2022,2022-03-08
s459,p459,"Science friction: Data, metadata, and collaboration","When scientists from two or more disciplines work together on related problems, they often face what we call ‘science friction’. As science becomes more data-driven, collaborative, and interdisciplinary, demand increases for interoperability among data, tools, and services. Metadata – usually viewed simply as ‘data about data’, describing objects such as books, journal articles, or datasets – serve key roles in interoperability. Yet we find that metadata may be a source of friction between scientific collaborators, impeding data sharing. We propose an alternative view of metadata, focusing on its role in an ephemeral process of scientific communication, rather than as an enduring outcome or product. We report examples of highly useful, yet ad hoc, incomplete, loosely structured, and mutable, descriptions of data found in our ethnographic studies of several large projects in the environmental sciences. Based on this evidence, we argue that while metadata products can be powerful resources, usually they must be supplemented with metadata processes. Metadata-as-process suggests the very large role of the ad hoc, the incomplete, and the unfinished in everyday scientific work.",j133,Social Studies of Science,jv133,accepted,f381,2005,2005-12-24
s461,p461,Assessing citizen science data quality: an invasive species case study,"An increase in the number of citizen science programs has prompted an examination of their ability to provide data of sufficient quality. We tested the ability of volunteers relative to professionals in identifying invasive plant species, mapping their distributions, and estimating their abundance within plots. We generally found that volunteers perform almost as well as professionals in some areas, but that we should be cautious about data quality in both groups. We analyzed predictors of volunteer success (age, education, experience, science literacy, attitudes) in training‐related skills, but these proved to be poor predictors of performance and could not be used as effective eligibility criteria. However, volunteer success with species identification increased with their self‐identified comfort level. Based on our case study results, we offer lessons learned and their application to other programs and provide recommendations for future research in this area.",c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f382,2014,2014-07-16
s462,p462,Google's PageRank and beyond - the science of search engine rankings,"Why doesn't your home page appear on the first page of search results, even when you query your own name? How do other web pages always appear at the top? What creates these powerful rankings? And how? The first book ever about the science of web page rankings, Google's PageRank and Beyond supplies the answers to these and other questions and more. The book serves two very different audiences: the curious science reader and the technical computational reader. The chapters build in mathematical sophistication, so that the first five are accessible to the general academic reader. While other chapters are much more mathematical in nature, each one contains something for both audiences. For example, the authors include entertaining asides such as how search engines make money and how the Great Firewall of China influences research. The book includes an extensive background chapter designed to help readers learn more about the mathematics of search engines, and it contains several MATLAB codes and links to sample web data sets. The philosophy throughout is to encourage readers to experiment with the ideas and algorithms in the text. Any business seriously interested in improving its rankings in the major search engines can benefit from the clear examples, sample code, and list of resources provided. Many illustrative examples and entertaining asides MATLAB code Accessible and informal style Complete and self-contained section for mathematics review",c23,International Conference on Open and Big Data,cp23,accepted,f383,2012,2012-08-20
s463,p463,Data infrastructure literacy,"A recent report from the UN makes the case for “global data literacy” in order to realise the opportunities afforded by the “data revolution”. Here and in many other contexts, data literacy is characterised in terms of a combination of numerical, statistical and technical capacities. In this article, we argue for an expansion of the concept to include not just competencies in reading and working with datasets but also the ability to account for, intervene around and participate in the wider socio-technical infrastructures through which data is created, stored and analysed – which we call “data infrastructure literacy”. We illustrate this notion with examples of “inventive data practice” from previous and ongoing research on open data, online platforms, data journalism and data activism. Drawing on these perspectives, we argue that data literacy initiatives might cultivate sensibilities not only for data science but also for data sociology, data politics as well as wider public engagement with digital data infrastructures. The proposed notion of data infrastructure literacy is intended to make space for collective inquiry, experimentation, imagination and intervention around data in educational programmes and beyond, including how data infrastructures can be challenged, contested, reshaped and repurposed to align with interests and publics other than those originally intended.",j38,Big Data & Society,jv38,accepted,f384,2019,2019-07-22
s464,p464,Global multi-resolution terrain elevation data 2010 (GMTED2010),"For more information on the USGS—the Federal source for science about the Earth, its natural and living resources, natural hazards, and the environment, visit http://www.usgs.gov or call 1–888–ASK–USGS. For an overview of USGS information products, including maps, imagery, and publications, Any use of trade, product, or firm names is for descriptive purposes only and does not imply endorsement by the U.S. Government. Although this report is in the public domain, permission must be secured from the individual copyright owners to reproduce any copyrighted materials contained within this report. 10. Diagram showing the GMTED2010 layer extents (minimum and maximum latitude and longitude) are a result of the coordinate system inherited from the 1-arc-second SRTM",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f385,2002,2002-09-17
s465,p465,Big Data and Internet of Things: A Roadmap for Smart Environments,Abstract content goes here ...,c75,International Conference on Machine Learning,cp75,accepted,f386,2005,2005-05-15
s466,p466,Understanding the Science Experiences of Successful Women of Color: Science Identity as an Analytic Lens.,"In this study, we develop a model of science identity to make sense of the science experiences of 15 successful women of color over the course of their undergraduate and graduate studies in science and into science-related careers. In our view, science identity accounts both for how women make meaning of science experiences and how society structures possible meanings. Primary data included ethnographic interviews during students' undergraduate careers, follow-up interviews 6 years later, and ongoing member-checking. Our results highlight the importance of recognition by others for women in the three science identity trajectories: research scientist; altruistic scientist; and disrupted scientist. The women with research scientist identities were passionate about science and recognized themselves and were recognized by science faculty as science people. The women with altruistic scientist identities regarded science as a vehicle for altruism and created innovative meanings of ''science,'' ''recognition by others,'' and ''woman of color in science.'' The women with disrupted scientist identities sought, but did not often receive, recognition by meaningful scientific others. Although they were ultimately successful, their trajectories were more difficult because, in part, their bids for recognition were disrupted by the interaction with gendered, ethnic, and racial factors. This study clarifies theoretical conceptions of science identity, promotes a rethinking of recruitment and retention efforts, and illuminates various ways women of color experience, make meaning of, and negotiate the culture of science. 2007 Wiley Periodicals, Inc. J Res Sci Teach 44: 1187-1218, 2007.",c95,IEEE International Conference on Computer Vision,cp95,accepted,f387,2017,2017-12-07
s468,p468,Data Scientist,"The last installment of this column dealt with the specter of IT-caused unemployment. Here, the author considers a new IT-created employment opportunity--the data scientist. He looks at data, information, and knowledge and current IT job classifications to provide context, describes how big data has inspired the field of data science, and defines what data science is and what data scientists do.",c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f388,2019,2019-11-12
s469,p469,The V–Dem Measurement Model: Latent Variable Analysis for Cross-National and Cross-Temporal Expert-Coded Data,"This material is based upon work supported by the National Science Foundation (SES-1423944, PI: Daniel Pemstein), Riksbankens Jubileumsfond (Grant M13-0559:1, PI: Staffan I. Lindberg), the Swedish Research Council (2013.0166, PI: Staffan I. Lindberg and Jan Teorell), the Knut and Alice Wallenberg Foundation (PI: Staffan I. Lindberg), and the University of Gothenburg (E 2013/43); as well as internal grants from the Vice-Chancellor’s office, the Dean of the College of Social Sciences, and the Department of Political Science at University of Gothenburg. Marquardt acknowledges research support from the Russian Academic Excellence Project ‘5-100.’ We performed simulations and other computational tasks using resources provided by the Notre Dame Center for Research Computing (CRC) through the High Performance Computing section and the Swedish National Infrastructure for Computing (SNIC) at the National Supercomputer Centre in Sweden (SNIC 2016/1-382, SNIC 2017/1-406 and 2017/1-68). We specifically acknowledge the assistance of In-Saeng Suh at CRC and Johan Raber and Peter Mu nger at SNIC in facilitating our use of their respective systems.",j40,Social Science Research Network,jv40,accepted,f389,2017,2017-04-11
s470,p470,Data validation in citizen science: a case study from Project FeederWatch,"To become more widely accepted as a valuable research tool, citizen-science projects must find ways to ensure that data gathered by large numbers of people with varying levels of expertise are of consistently high quality. Here, we describe a data validation protocol developed for Project FeederWatch, a continent-wide bird monitoring program, that is designed to increase researchers' and participants' confidence in the data being collected.",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f390,2015,2015-10-24
s471,p471,The ethics of smart cities and urban science,"Software-enabled technologies and urban big data have become essential to the functioning of cities. Consequently, urban operational governance and city services are becoming highly responsive to a form of data-driven urbanism that is the key mode of production for smart cities. At the heart of data-driven urbanism is a computational understanding of city systems that reduces urban life to logic and calculative rules and procedures, which is underpinned by an instrumental rationality and realist epistemology. This rationality and epistemology are informed by and sustains urban science and urban informatics, which seek to make cities more knowable and controllable. This paper examines the forms, practices and ethics of smart cities and urban science, paying particular attention to: instrumental rationality and realist epistemology; privacy, datafication, dataveillance and geosurveillance; and data uses, such as social sorting and anticipatory governance. It argues that smart city initiatives and urban science need to be re-cast in three ways: a re-orientation in how cities are conceived; a reconfiguring of the underlying epistemology to openly recognize the contingent and relational nature of urban systems, processes and science; and the adoption of ethical principles designed to realize benefits of smart cities and urban science while reducing pernicious effects. This article is part of the themed issue ‘The ethical impact of data science’.",c65,Formal Concept Analysis,cp65,accepted,f391,2008,2008-05-16
s472,p472,The conundrum of sharing research data,"We must all accept that science is data and that data are science, and thus provide for, and justify the need for the support of, much-improved data curation. (Hanson, Sugden, & Alberts) 
 
Researchers are producing an unprecedented deluge of data by using new methods and instrumentation. Others may wish to mine these data for new discoveries and innovations. However, research data are not readily available as sharing is common in only a few fields such as astronomy and genomics. Data sharing practices in other fields vary widely. Moreover, research data take many forms, are handled in many ways, using many approaches, and often are difficult to interpret once removed from their initial context. Data sharing is thus a conundrum. Four rationales for sharing data are examined, drawing examples from the sciences, social sciences, and humanities: (1) to reproduce or to verify research, (2) to make results of publicly funded research available to the public, (3) to enable others to ask new questions of extant data, and (4) to advance the state of research and innovation. These rationales differ by the arguments for sharing, by beneficiaries, and by the motivations and incentives of the many stakeholders involved. The challenges are to understand which data might be shared, by whom, with whom, under what conditions, why, and to what effects. Answers will inform data policy and practice. © 2012 Wiley Periodicals, Inc.",c24,Decision Support Systems,cp24,accepted,f392,2013,2013-07-15
s473,p473,A Qualitative Framework for Collecting and Analyzing Data in Focus Group Research,"Despite the abundance of published material on conducting focus groups, scant specific information exists on how to analyze focus group data in social science research. Thus, the authors provide a new qualitative framework for collecting and analyzing focus group data. First, they identify types of data that can be collected during focus groups. Second, they identify the qualitative data analysis techniques best suited for analyzing these data. Third, they introduce what they term as a micro-interlocutor analysis, wherein meticulous information about which participant responds to each question, the order in which each participant responds, response characteristics, the nonverbal communication used, and the like is collected, analyzed, and interpreted. They conceptualize how conversation analysis offers great potential for analyzing focus group data. They believe that their framework goes far beyond analyzing only the verbal communication of focus group participants, thereby increasing the rigor of focus group analyses in social science research.",c100,ACM SIGMOD Conference,cp100,accepted,f393,2010,2010-10-29
s474,p474,Automated data reduction workflows for astronomy,"Data from complex modern astronomical instruments often consist of a large number of different science and calibration files, and their reduction requires a variety of software tools. The execution chain of the tools represents a complex workflow that needs to be tuned and supervised, often by individual researchers that are not necessarily experts for any specific instrument. The efficiency of data reduction can be improved by using automatic workflows to organise data and execute the sequence of data reduction steps. To realize such efficiency gains, we designed a system that allows intuitive representation, execution and modification of the data reduction workflow, and has facilities for inspection and interaction with the data. The European Southern Observatory (ESO) has developed Reflex, an environment to automate data reduction workflows. Reflex is implemented as a package of customized components for the Kepler workflow engine. Kepler provides the graphical user interface to create an executable flowchart-like representation of the data reduction process. Key features of Reflex are a rule-based data organiser, infrastructure to re-use results, thorough book-keeping, data progeny tracking, interactive user interfaces, and a novel concept to exploit information created during data organisation for the workflow execution. Reflex includes novel concepts to increase the efficiency of astronomical data processing. While Reflex is a specific implementation of astronomical scientific workflows within the Kepler workflow engine, the overall design choices and methods can also be applied to other environments for running automated science workflows.",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f394,2008,2008-04-20
s475,p475,Advancing Science Through Collaborative Data Sharing and Synthesis,"The demand for researchers to share their data has increased dramatically in recent years. There is a need to replicate and confirm scientific findings to bolster confidence in many research areas. Data sharing also serves the critical function of allowing synthesis of findings across trials. As innovative statistical methods have helped resolve barriers to synthesis analyses, data sharing and synthesis can help answer research questions that cannot be answered by individual trials alone. However, the sharing of data among researchers remains challenging and infrequent. This article aims to (a) increase support for data sharing and synthesis collaborations among researchers to advance scientific knowledge and (b) provide a model for establishing these collaborations using the example of the ongoing National Institute of Mental Health’s Collaborative Data Synthesis on Adolescent Depression Trials. This study brings together datasets from existing prevention and treatment trials in adolescent depression, as well as researchers and stakeholders, to answer questions about “for whom interventions work” and “by what pathways interventions have their effects.” This is critical to improving interventions, including increasing knowledge about intervention efficacy among minority populations, or what we call “scientific equity.” The collaborative model described is relevant to fields with research questions that can only be addressed by synthesizing individual-level data.",j96,Perspectives on Psychological Science,jv96,accepted,f395,2014,2014-10-07
s478,p478,An open science resource for establishing reliability and reproducibility in functional connectomics,Abstract content goes here ...,j14,Scientific Data,jv14,accepted,f396,2020,2020-11-25
s483,p483,Mars Reconnaissance Orbiter's High Resolution Imaging Science Experiment (HiRISE),"[1] The HiRISE camera features a 0.5 m diameter primary mirror, 12 m effective focal length, and a focal plane system that can acquire images containing up to 28 Gb (gigabits) of data in as little as 6 seconds. HiRISE will provide detailed images (0.25 to 1.3 m/pixel) covering ∼1% of the Martian surface during the 2-year Primary Science Phase (PSP) beginning November 2006. Most images will include color data covering 20% of the potential field of view. A top priority is to acquire ∼1000 stereo pairs and apply precision geometric corrections to enable topographic measurements to better than 25 cm vertical precision. We expect to return more than 12 Tb of HiRISE data during the 2-year PSP, and use pixel binning, conversion from 14 to 8 bit values, and a lossless compression system to increase coverage. HiRISE images are acquired via 14 CCD detectors, each with 2 output channels, and with multiple choices for pixel binning and number of Time Delay and Integration lines. HiRISE will support Mars exploration by locating and characterizing past, present, and future landing sites, unsuccessful landing sites, and past and potentially future rover traverses. We will investigate cratering, volcanism, tectonism, hydrology, sedimentary processes, stratigraphy, aeolian processes, mass wasting, landscape evolution, seasonal processes, climate change, spectrophotometry, glacial and periglacial processes, polar geology, and regolith properties. An Internet Web site (HiWeb) will enable anyone in the world to suggest HiRISE targets on Mars and to easily locate, view, and download HiRISE data products.",c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f397,2022,2022-03-09
s484,p484,A correlated topic model of Science,"Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139--177]. We derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We apply the CTM to the articles from Science published from 1990--1999, a data set that comprises 57M words. The CTM gives a better fit of the data than LDA, and we demonstrate its use as an exploratory tool of large document collections.",c92,Advances in Soft Computing,cp92,accepted,f398,2009,2009-10-31
s486,p486,Big Scholarly Data: A Survey,"With the rapid growth of digital publishing, harvesting, managing, and analyzing scholarly information have become increasingly challenging. The term Big Scholarly Data is coined for the rapidly growing scholarly data, which contains information including millions of authors, papers, citations, figures, tables, as well as scholarly networks and digital libraries. Nowadays, various scholarly data can be easily accessed and powerful data analysis technologies are being developed, which enable us to look into science itself with a new perspective. In this paper, we examine the background and state of the art of big scholarly data. We first introduce the background of scholarly data management and relevant technologies. Second, we review data analysis methods, such as statistical analysis, social network analysis, and content analysis for dealing with big scholarly data. Finally, we look into representative research issues in this area, including scientific impact evaluation, academic recommendation, and expert finding. For each issue, the background, main challenges, and latest research are covered. These discussions aim to provide a comprehensive review of this emerging area. This survey paper concludes with a discussion of open issues and promising future directions.",j138,IEEE Transactions on Big Data,jv138,accepted,f399,2013,2013-05-13
s487,p487,Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,"It is our great pleasure to welcome you to the 2016 ACM Conference on Knowledge Discovery and Data Mining -- KDD'16. We hope that the content and the professional network at KDD'16 will help you succeed professionally by enabling you to: identify technology trends early; make new/creative contributions; increase your productivity by using newer/better tools, processes or ways of organizing teams; identify new job opportunities; and hire new team members. 
 
We are living in an exciting time for our profession. On the one hand, we are witnessing the industrialization of data science, and the emergence of the industrial assembly line processes characterized by the division of labor, integrated processes/pipelines of work, standards, automation, and repeatability. Data science practitioners are organizing themselves in more sophisticated ways, embedding themselves in larger teams in many industry verticals, improving their productivity substantially, and achieving a much larger scale of social impact. On the other hand we are also witnessing astonishing progress from research in algorithms and systems -- for example the field of deep neural networks has revolutionized speech recognition, NLP, computer vision, image recognition, etc. By facilitating interaction between practitioners at large companies & startups on the one hand, and the algorithm development researchers including leading academics on the other, KDD'16 fosters technological and entrepreneurial innovation in the area of data science. 
 
This year's conference continues its tradition of being the premier forum for presentation of results in the field of data mining, both in the form of cutting edge research, and in the form of insights from the development and deployment of real world applications. Further, the conference continues with its tradition of a strong tutorial and workshop program on leading edge issues of data mining. The mission of this conference has broadened in recent years even as we placed a significant amount of focus on both the research and applied aspects of data mining. As an example of this broadened focus, this year we have introduced a strong hands-on tutorial program nduring the conference in which participants will learn how to use practical tools for data mining. KDD'16 also gives researchers and practitioners a unique opportunity to form professional networks, and to share their perspectives with others interested in the various aspects of data mining. For example, we have introduced office hours for budding entrepreneurs from our community to meet leading Venture Capitalists investing in this area. We hope that KDD 2016 conference will serve as a meeting ground for researchers, practitioners, funding agencies, and investors to help create new algorithms and commercial products. 
 
The call for papers attracted a significant number of submissions from countries all over the world. In particular, the research track attracted 784 submissions and the applied data science track attracted 331 submissions. Papers were accepted either as full papers or as posters. The overall acceptance rate either as full papers or posters was less than 20%. For full papers in the research track, the acceptance rate was lower than 10%. This is consistent with the fact that the KDD Conference is a premier conference in data mining and the acceptance rates historically tend to be low. It is noteworthy that the applied data science track received a larger number of submissions compared to previous years. We view this as an encouraging sign that research in data mining is increasingly becoming relevant to industrial applications. All papers were reviewed by at least three program committee members and then discussed by the PC members in a discussion moderated by a meta-reviewer. Borderline papers were thoroughly reviewed by the program chairs before final decisions were made.",c16,Knowledge Discovery and Data Mining,cp16,accepted,f400,2003,2003-05-31
s488,p488,Addressing big data issues in Scientific Data Infrastructure,"Big Data are becoming a new technology focus both in science and in industry. This paper discusses the challenges that are imposed by Big Data on the modern and future Scientific Data Infrastructure (SDI). The paper discusses a nature and definition of Big Data that include such features as Volume, Velocity, Variety, Value and Veracity. The paper refers to different scientific communities to define requirements on data management, access control and security. The paper introduces the Scientific Data Lifecycle Management (SDLM) model that includes all the major stages and reflects specifics in data management in modern e-Science. The paper proposes the SDI generic architecture model that provides a basis for building interoperable data or project centric SDI using modern technologies and best practices. The paper explains how the proposed models SDLM and SDI can be naturally implemented using modern cloud based infrastructure services provisioning model and suggests the major infrastructure components for Big Data.",c28,International Conference on Collaboration Technologies and Systems,cp28,accepted,f401,2009,2009-09-19
s489,p489,The National Institutes of Health's Big Data to Knowledge (BD2K) initiative: capitalizing on biomedical big data,"Biomedical research has and will continue to generate large amounts of data (termed ‘big data’) in many formats and at all levels. Consequently, there is an increasing need to better understand and mine the data to further knowledge and foster new discovery. The National Institutes of Health (NIH) has initiated a Big Data to Knowledge (BD2K) initiative to maximize the use of biomedical big data. BD2K seeks to better define how to extract value from the data, both for the individual investigator and the overall research community, create the analytic tools needed to enhance utility of the data, provide the next generation of trained personnel, and develop data science concepts and tools that can be made available to all stakeholders.",c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f402,2011,2011-03-27
s490,p490,"Statistics : methods and applications : a comprehensive reference for science, industry, and data mining",Elementary concepts in statistics -- Basic statistics and tables -- ANOVA/MANOVA -- Association rules -- Boosting trees -- Canonical analysis -- CHAID analysis -- Classification and regression trees (CART) -- Classification trees -- Cluster analysis -- Correspondence analysis -- Data mining techniques -- Discriminant function analysis -- Distribution fitting -- Experimental design (Industrial DOE) -- Factor analysis and principal components -- General discrimination analysis (GDA) -- General linear models (GLM) -- General regression models (GRM) -- Generalized additive models (GAM) -- Generalized linear/nonlinear models (GLZ) -- Log linear analysis of frequency tables -- Machine learning -- Multivariate adaptive regression splines (MARSplines) -- Multidimensional scaling (MDS) -- Multiple linear regression -- Neural networks -- Nonlinear estimation -- Nonparametric statistics -- Partial least squares (PLS) -- Power analysis -- Process analysis -- Quality control charts -- Reliabilty/item analysis -- Structural equation modeling -- Survival/failure time analysis -- Text mining -- Time series/forecasting -- Variance components and mixed model ANOVA/ANCOVA.,c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f403,2004,2004-06-16
s491,p491,Data Products,Abstract content goes here ...,c69,International Conference on Parallel Processing,cp69,accepted,f404,2010,2010-10-01
s492,p492,Response to Comment on “Estimating the reproducibility of psychological science”,"Gilbert et al. conclude that evidence from the Open Science Collaboration’s Reproducibility Project: Psychology indicates high reproducibility, given the study methodology. Their very optimistic assessment is limited by statistical misconceptions and by causal inferences from selectively interpreted, correlational data. Using the Reproducibility Project: Psychology data, both optimistic and pessimistic conclusions about reproducibility are possible, and neither are yet warranted.",j97,Science,jv97,accepted,f405,2012,2012-10-11
s493,p493,Big Data Methods,"Advances in data science, such as data mining, data visualization, and machine learning, are extremely well-suited to address numerous questions in the organizational sciences given the explosion of available data. Despite these opportunities, few scholars in our field have discussed the specific ways in which the lens of our science should be brought to bear on the topic of big data and big data's reciprocal impact on our science. The purpose of this paper is to provide an overview of the big data phenomenon and its potential for impacting organizational science in both positive and negative ways. We identifying the biggest opportunities afforded by big data along with the biggest obstacles, and we discuss specifically how we think our methods will be most impacted by the data analytics movement. We also provide a list of resources to help interested readers incorporate big data methods into their existing research. Our hope is that we stimulate interest in big data, motivate future research using big data sources, and encourage the application of associated data science techniques more broadly in the organizational sciences.",c60,IEEE International Conference on Software Engineering and Formal Methods,cp60,accepted,f406,2011,2011-05-01
s495,p495,SciMAT: A new science mapping analysis software tool,"This article presents a new open-source software tool, SciMAT, which performs science mapping analysis within a longitudinal framework. It provides different modules that help the analyst to carry out all the steps of the science mapping workflow. In addition, SciMAT presents three key features that are remarkable in respect to other science mapping software tools: (a) a powerful preprocessing module to clean the raw bibliographical data, (b) the use of bibliometric measures to study the impact of each studied element, and (c) a wizard to configure the analysis. © 2012 Wiley Periodicals, Inc.",c23,International Conference on Open and Big Data,cp23,accepted,f407,2012,2012-03-31
s496,p496,Optical storage arrays: a perspective for future big data storage,Abstract content goes here ...,j139,Light: Science & Applications,jv139,accepted,f408,2004,2004-05-15
s497,p497,Enhancing the quality of argumentation in school science,"The research reported in this paper focussed on the design of learning environments that support the teaching and learning of argumentation in a scientific context. The research took place over two years between 1999 and 2001 in junior high schools in the greater London area. The research was conducted in two phases. In the first developmental phase, working with a group of 12 science teachers, the main emphasis was to develop sets of materials and strategies to support argumentation in the classroom and to assess teachers‘ development with teaching argumentation. Data were collected by videoing and audio recording the teachers attempts to implement these lessons at the beginning and end of the year. During this phase, analytical tools for evaluating the quality of argumentation were developed based on Toulmin‘s argument pattern. Analysis of the data shows that there was significant development in the majority of teachers use of argumentation across the year. Results indicate that the pattern of use of argumentation is teacher specific, as is the nature of the change. In the second phase of the project, teachers taught the experimental groups a minimum of nine lessons which involved socioscientific or scientific argumentation. In addition, these teachers taught similar lessons to a control group at the beginning and end of the year. Here the emphasis lay on assessing the progression in student capabilities with argumentation. Hence data were collected from several lessons of two groups of students engaging in argumentation. Using a framework for evaluating the nature of the discourse and its quality, the findings show that there was an improvement in the quality of students‘ argumentation. In addition, the research offers methodological developments for work in this field.",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f409,2002,2002-09-27
s498,p498,Causal inference from observational data.,"Randomized controlled trials have long been considered the 'gold standard' for causal inference in clinical research. In the absence of randomized experiments, identification of reliable intervention points to improve oral health is often perceived as a challenge. But other fields of science, such as social science, have always been challenged by ethical constraints to conducting randomized controlled trials. Methods have been established to make causal inference using observational data, and these methods are becoming increasingly relevant in clinical medicine, health policy and public health research. This study provides an overview of state-of-the-art methods specifically designed for causal inference in observational data, including difference-in-differences (DiD) analyses, instrumental variables (IV), regression discontinuity designs (RDD) and fixed-effects panel data analysis. The described methods may be particularly useful in dental research, not least because of the increasing availability of routinely collected administrative data and electronic health records ('big data').",j140,Community Dentistry and Oral Epidemiology,jv140,accepted,f410,2008,2008-11-01
s499,p499,"A Vast Machine: Computer Models, Climate Data, and the Politics of Global Warming","Global warming skeptics often fall back on the argument that the scientific case for global warming is all model predictions, nothing but simulation; they warn us that we need to wait for real data, ""sound science."" In A Vast Machine Paul Edwards has news for these skeptics: without models, there are no data. Today, no collection of signals or observationseven from satellites, which can ""see"" the whole planet with a single instrumentbecomes global in time and space without passing through a series of data models. Everything we know about the world's climate we know through models. Edwards offers an engaging and innovative history of how scientists learned to understand the atmosphereto measure it, trace its past, and model its future. Edwards argues that all our knowledge about climate change comes from three kinds of computer models: simulation models of weather and climate; reanalysis models, which recreate climate history from historical weather data; and data models, used to combine and adjust measurements from many different sources. Meteorology creates knowledge through an infrastructure (weather stations and other data platforms) that covers the whole world, making global data. This infrastructure generates information so vast in quantity and so diverse in quality and form that it can be understood only by computer analysismaking data global. Edwards describes the science behind the scientific consensus on climate change, arguing that over the years data and models have converged to create a stable, reliable, and trustworthy basis for establishing the reality of global warming.",c9,Pacific Symposium on Biocomputing,cp9,accepted,f411,2009,2009-03-10
s501,p501,Guidelines for conducting and reporting case study research in software engineering,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f412,2004,2004-02-14
s502,p502,Software Engineering Economics,"This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f413,2014,2014-03-17
s504,p504,Software Engineering: A Practitioner's Approach,Abstract content goes here ...,c11,Hawaii International Conference on System Sciences,cp11,accepted,f414,2006,2006-09-23
s505,p505,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f415,2017,2017-11-24
s506,p506,Guidelines for snowballing in systematic literature studies and a replication in software engineering,"Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably.
 Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review.
 Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches.
 Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review.
 Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.",c31,International Conference on Evaluation & Assessment in Software Engineering,cp31,accepted,f416,2022,2022-08-24
s508,p508,Sampling in software engineering research: a critical review and guidelines,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f417,2004,2004-03-28
s509,p509,Object-oriented software engineering - a use case driven approach,Part 1. Introduction 1. System development as an industrial process Introduction A useful analogy System development characteristics Summary 2. The system life cycle Introduction System development as a process of change System development and reuse System development and methodology Objectory Summary 3. What is object-orientation? Introduction Object Class andinstance Polymorphism Inheritance Summary 4. Object-oriented system development Introduction Function/data methods Object-oriented analysis Object-oriented construction Object-oriented testing Summary 5. Object-oriented programming Introduction Objects Classes and instances Inheritance Polymorphism An example Summary Part II. Concepts 6. Architecture Introduction System development is model building Model architecture Requirements model Analysis model The design model The implementation model Test model Summary 7. Analysis Introduction The requirements model The analysis model Summary 8. Construction Introduction The design model Block design Working with construction Summary 9. Real-time specialization Introduction Classification of real-time systems Fundamental issues Analysis Construction Testing and verification Summary 10. Database Specialization Introduction Relational DBMS Object DBMS Discussion Summary 11. Components Introduction What is a component? Use of components Component management Summary 12. Testing Introduction On testing Unit testing Integration testing System testing The testing process Summary Part III. Applications 13. Case study: warehouse management system Introduction to the examples ACME Warehouse Management Inc. The requirements model The analysis model Construction 14. Case study: telecom Introduction Telecommunication switching systems The requirements model The analysis model The design model The implementation model 15. Managing object-oriented software engineering Introduction Project selection and preparation Project development organization Project organization and management Project staffing Software quality assurance Software metrics Summary 16. Other object-oriented methods Introduction A summary of object-oriented methods Object-Oriented Analysis (OOAD/Coad-Yourdon) Object-Oriented Design (OOD/Booch) Hierarchical Object-Oriented Design (HOOD) Object Modeling Technique (OMT) Responsibility-Driven Design Summary Appendix A On the development of Objectory Introduction Objectory as an activity From idea to reality References Index,c32,International Conference on Software Technology: Methods and Tools,cp32,accepted,f418,2010,2010-09-13
s511,p511,Guidelines for conducting systematic mapping studies in software engineering: An update,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f419,2010,2010-03-01
s512,p512,Experimentation in software engineering: an introduction,Abstract content goes here ...,c28,International Conference on Collaboration Technologies and Systems,cp28,accepted,f420,2009,2009-12-22
s513,p513,Systematic Mapping Studies in Software Engineering,"BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. 
 
OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. 
 
METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. 
 
RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. 
 
CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).",c31,International Conference on Evaluation & Assessment in Software Engineering,cp31,accepted,f421,2022,2022-12-17
s514,p514,Systematic literature reviews in software engineering - A systematic literature review,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f422,2010,2010-11-30
s517,p517,Software engineering in start-up companies: An analysis of 88 experience reports,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f423,2004,2004-11-15
s519,p519,Empirical software engineering experts on the use of students and professionals in experiments,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f424,2004,2004-07-17
s520,p520,Smart contracts vulnerabilities: a call for blockchain software engineering?,"Smart Contracts have gained tremendous popularity in the past few years, to the point that billions of US Dollars are currently exchanged every day through such technology. However, since the release of the Frontier network of Ethereum in 2015, there have been many cases in which the execution of Smart Contracts managing Ether coins has led to problems or conflicts. Compared to traditional Software Engineering, a discipline of Smart Contract and Blockchain programming, with standardized best practices that can help solve the mentioned problems and conflicts, is not yet sufficiently developed. Furthermore, Smart Contracts rely on a non-standard software life-cycle, according to which, for instance, delivered applications can hardly be updated or bugs resolved by releasing a new version of the software. In this paper we advocate the need for a discipline of Blockchain Software Engineering, addressing the issues posed by smart contract programming and other applications running on blockchains.We analyse a case of study where a bug discovered in a Smart Contract library, and perhaps ""unsafe"" programming, allowed an attack on Parity, a wallet application, causing the freezing of about 500K Ethers (about 150M USD, in November 2017). In this study we analyze the source code of Parity and the library, and discuss how recognised best practices could mitigate, if adopted and adapted, such detrimental software misbehavior. We also reflect on the specificity of Smart Contract software development, which makes some of the existing approaches insufficient, and call for the definition of a specific Blockchain Software Engineering.",c92,Advances in Soft Computing,cp92,accepted,f425,2009,2009-05-16
s521,p521,No Silver Bullet Essence and Accidents of Software Engineering,"But, as we look to the horizon of a decade hence, we see no silver bullet. There is no single development, in either technology or in management technique, that by itself promises even one order-of-magnitude improvement in productivity, in reliability, in simplicity. In this article, I shall try to show why, by examining both the nature of the software problem and the properties of the bullets proposed.",j79,Computer,jv79,accepted,f426,2014,2014-12-28
s524,p524,A Benchmark Study on Sentiment Analysis for Software Engineering Research,"A recent research trend has emerged to identify developers' emotions, by applying sentiment analysis to the content of communication traces left in collaborative development environments. Trying to overcome the limitations posed by using off-the-shelf sentiment analysis tools, researchers recently started to develop their own tools for the software engineering domain. In this paper, we report a benchmark study to assess the performance and reliability of three sentiment analysis tools specifically customized for software engineering. Furthermore, we offer a reflection on the open challenges, as they emerge from a qualitative analysis of misclassified texts.",c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f427,2012,2012-05-02
s525,p525,Word Embeddings for the Software Engineering Domain,"The software development process produces vast amounts of textual data expressed in natural language. Outcomes from the natural language processing community have been adapted in software engineering research for leveraging this rich textual information; these include methods and readily available tools, often furnished with pretrained models. State of the art pretrained models however, capture general, common sense knowledge, with limited value when it comes to handling data specific to a specialized domain. There is currently a lack of domain-specific pretrained models that would further enhance the processing of natural language artefacts related to software engineering. To this end, we release a word2vec model trained over 15GB of textual data from Stack Overflow posts. We illustrate how the model disambiguates polysemous words by interpreting them within their software engineering context. In addition, we present examples of fine-grained semantics captured by the model, that imply transferability of these results to diverse, targeted information retrieval tasks in software engineering and motivate for further reuse of the model.",c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f428,2012,2012-05-27
s526,p526,Introduction to Software Engineering,"INTRODUCTION The Need for Software Engineering Are Software Teams Really Necessary? Software Engineering Software Lifecycles Different Views of Software Engineering Activities Software Engineering as an Emerging Discipline Some Techniques of Software Engineering Standards Commonly Used for Software Development Processes The Year 2000 Problem and Similar Problems Organization of the Book PROJECT MANAGEMENT Sub-Teams Needed in Software Engineering Projects The Nature of Project Teams Project Management Software Project Estimation Project Scheduling Project Measurement Project Management Tools The Role of Networks in Project Management Groupware An Example: Project Management for a Year 2000 Conversion Project REQUIREMENTS Some Problems with Requirements Determination Requirements Elicitation Requirements Traceability Software Architectures and Requirements Reengineering System Requirements Assessment of Feasibility of System Requirements Usability Requirements Specifying Requirements Using State Diagrams and Decision Tables Specifying Requirements Using Petri Nets Ethical Issues Some Metrics for Requirements The Requirements Review The Major Project - Problem Statement The Major Project - Requirements Elicitation The Major Software Project - Requirements Analysis SOFTWARE DESIGN Introduction Software Design Patterns Introduction to Software Design Representations Procedurally-Oriented Design Representations Software Architectures Software Design Principles for Procedurally-Oriented Programs What is an Object? Object-Oriented Design Representations Software Design Principles for Object-Oriented Programs Class Design Issues An Example of Class Development - The String Class User Interfaces Software Interfaces Some Metrics for Design Design Reviews A Manager's Viewpoint of Design Architecture of the Major Software Engineering Project Preliminary Design of the Major Software Project Subsystem Design of the Major Software Project Detailed Design for the Major Software Project CODING The Choice of Programming Language Coding Styles Coding Standards Coding, Design, Requirements, and Change Some Coding Metrics Coding Reviews and Inspections Configuration Management A Management Perspective on Coding Coding of the Major Software Project TESTING AND INTEGRATION Types of Software Testing Black-Box Module Testing White-Box Module Testing Reducing the Number of Test Cases by Effective Test Strategies Testing Objects for Encapsulation and Completeness Testing Objects with Inheritance General Testing Issues for Object-Oriented Software Test Plans Software Integration Managing Change in the Integration Process Performance and Stress Testing Quality Assurance Software Reliability A Manager's Viewpoint on Testing and Integration Testing the Major Software Project Integrating the Major Software Project DELIVERY, INSTALLATION, AND DOCUMENTATION Delivery Installation Internal Documentation External Documentation Design Rationales Installation, User, Training, and Operations Manuals On-Line Documentation Reading Levels A Manager's View of Delivery, Installation, and Documentation Delivery, Installation, and Documentation of the Major Software Project MAINTENANCE Introduction Corrective Software Maintenance Adaptive Software Maintenance Preventative Software Maintenance and the Year 2000 Problem How to Read Requirements, Designs, and Source Code A Manager's Perspective on Software Maintenance Maintenance of the Major Software Project RESEARCH ISSUES IN SOFTWARE ENGINEERING Some Important Research Problems in Software Engineering How to Read the Software Engineering Research Literature APPENDIX: COMMAND-LINE ARGUMENTS REFERENCES",c74,IEEE International Conference on Tools with Artificial Intelligence,cp74,accepted,f429,2003,2003-07-16
s528,p528,A Survey of App Store Analysis for Software Engineering,"App Store Analysis studies information about applications obtained from app stores. App stores provide a wealth of information derived from users that would not exist had the applications been distributed via previous software deployment methods. App Store Analysis combines this non-technical information with technical information to learn trends and behaviours within these forms of software repositories. Findings from App Store Analysis have a direct and actionable impact on the software teams that develop software for app stores, and have led to techniques for requirements engineering, release planning, software design, security and testing. This survey describes and compares the areas of research that have been explored thus far, drawing out common aspects, trends and directions future research should take to address open problems and challenges.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f430,2014,2014-08-05
s529,p529,Guidelines for including the grey literature and conducting multivocal literature reviews in software engineering,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f431,2010,2010-07-12
s534,p534,Fundamentals of Software Engineering,Abstract content goes here ...,j75,Lecture Notes in Computer Science,jv75,accepted,f432,2015,2015-06-24
s535,p535,Robust Statistical Methods for Empirical Software Engineering,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f433,2004,2004-05-30
s537,p537,On negative results when using sentiment analysis tools for software engineering research,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f434,2004,2004-02-09
s538,p538,Leveraging Automated Sentiment Analysis in Software Engineering,"Automated sentiment analysis in software engineering textual artifacts has long been suffering from inaccuracies in those few tools available for the purpose. We conduct an in-depth qualitative study to identify the difficulties responsible for such low accuracy. Majority of the exposed difficulties are then carefully addressed in developing SentiStrength-SE, a tool for improved sentiment analysis especially designed for application in the software engineering domain. Using a benchmark dataset consisting of 5,600 manually annotated JIRA issue comments, we carry out both quantitative and qualitative evaluations of our tool. SentiStrength-SE achieves 73.85% precision and 85% recall, which are significantly higher than a state-of-the-art sentiment analysis tool we compare with.",c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f435,2012,2012-12-06
s539,p539,From Word Embeddings to Document Similarities for Improved Information Retrieval in Software Engineering,"The application of information retrieval techniques to search tasks in software engineering is made difficult by the lexical gap between search queries, usually expressed in natural language (e.g. English), and retrieved documents, usually expressed in code (e.g. programming languages). This is often the case in bug and feature location, community question answering, or more generally the communication between technical personnel and non-technical stake holders in a software project. In this paper, we propose bridging the lexical gap by projecting natural language statements and code snippets as meaning vectors in a shared representation space. In the proposed architecture, word embeddings are rst trained on API documents, tutorials, and reference documents, and then aggregated in order to estimate semantic similarities between documents. Empirical evaluations show that the learned vector space embeddings lead to improvements in a previously explored bug localization task and a newly de ned task of linking API documents to computer programming questions.",c29,International Conference on Software Engineering,cp29,accepted,f436,2015,2015-07-19
s540,p540,Software Engineering Education: Converging with the Startup Industry,"Startups are agents of change that bring in innovations and find solutions to problems at various scales. An all-rounded engineering team is a key driver for the ability to execute the entrepreneurial ambition, from building a minimum viable product to later stages of product vision. Software engineering education provides students with the knowledge to transition to mature companies with defined structure in place successfully. However, the fluidity, risk, time-sensitivity, and uncertainty of startups demand a dynamic and agile set of skills to rapidly identify, conceptualize and deliver features as per market needs. This requires the adoption of latest development trends in software processes, engineering and DevOps practices with automation to iterate fast with low governance and the ability to take on multiple roles. This paper presents a study of the dynamics and engineering at startups and compares it with the current curriculum of software engineering.",c36,Conference on Software Engineering Education and Training,cp36,accepted,f437,2015,2015-01-28
s542,p542,Experimentation in Software Engineering,Abstract content goes here ...,c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f438,2011,2011-08-06
s544,p544,Model-Based Software Engineering to Tame the IoT Jungle,"The Internet of Things (IoT) is a challenging combination of distribution and heterogeneity. A number of software engineering solutions address those challenges in isolation, but few solutions tackle them in combination, which poses a set of concrete challenges. The ThingML (Internet of Things Modeling Language) approach attempts to address those challenges. This model-driven, generative approach, which was inspired by UML, integrates concepts targeted at the IoT. Over the past six years, it has been continuously evolved and applied to cases in different domains, including a commercial e-health solution.",j147,IEEE Software,jv147,accepted,f439,2021,2021-01-11
s545,p545,Key Abstractions for IoT-Oriented Software Engineering,"Despite the progress in Internet of Things (IoT) research, a general software engineering approach for systematic development of IoT systems and applications is still missing. A synthesis of the state of the art in the area can help frame the key abstractions related to such development. Such a framework could be the basis for guidelines for IoT-oriented software engineering.",j147,IEEE Software,jv147,accepted,f440,2021,2021-07-06
s547,p547,Belief & Evidence in Empirical Software Engineering,"Empirical software engineering has produced a steady stream of evidence-based results concerning the factors that affect important outcomes such as cost, quality, and interval. However, programmers often also have strongly-held a priori opinions about these issues. These opinions are important, since developers are highlytrained professionals whose beliefs would doubtless affect their practice. As in evidence-based medicine, disseminating empirical findings to developers is a key step in ensuring that the findings impact practice. In this paper, we describe a case study, on the prior beliefs of developers at Microsoft, and the relationship of these beliefs to actual empirical data on the projects in which these developers work. Our findings are that a) programmers do indeed have very strong beliefs on certain topics b) their beliefs are primarily formed based on personal experience, rather than on findings in empirical research and c) beliefs can vary with each project, but do not necessarily correspond with actual evidence in that project. Our findings suggest that more effort should be taken to disseminate empirical findings to developers and that more in-depth study the interplay of belief and evidence in software practice is needed.",c29,International Conference on Software Engineering,cp29,accepted,f441,2015,2015-04-01
s548,p548,A Map of Threats to Validity of Systematic Literature Reviews in Software Engineering,"Context: The assessment of Threats to Validity (TTVs) is critical to secure the quality of empirical studies in Software Engineering (SE). In the recent decade, Systematic Literature Review (SLR) was becoming an increasingly important empirical research method in SE. One of the mechanisms of insuring the level of scientific value in the findings of an SLR is to rigorously assess its validity. Hence, it is necessary to realize the status quo and issues of TTVs of SLRs in SE. Objective: This study aims to investigate thestate-of-the-practice of TTVs of the SLRs published in SE, and further support SE researchers to improve the assessment and strategies against TTVs in order to increase the quality of SLRs in SE. Method: We conducted a tertiary study by reviewing the SLRs in SE that report the assessment of TTVs. Results: We identified 316 SLRs published from 2004 to the first half of 2015, in which TTVs are discussed. The issues associated to TTVs were also summarized and categorized. Conclusion: The common TTVs related to SLR research, such as internal validity and reliability, were thoroughly discussed in most SLRs. The threats to construct validity and external validity drew less attention. Moreover, there are few strategies and tactics being reported to cope with the various TTVs.",c37,Asia-Pacific Software Engineering Conference,cp37,accepted,f442,2008,2008-01-30
s549,p549,An Empirical Study of Practitioners' Perspectives on Green Software Engineering,"The energy consumption of software is an increasing concern as the use of mobile applications, embedded systems, and data center-based services expands. While research in green software engineering is correspondingly increasing, little is known about the current practices and perspectives of software engineers in the field. This paper describes the first empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report findings from a quantitative,targeted survey of 464 practitioners from ABB, Google, IBM, and Microsoft, which was motivated by and supported with qualitative data from 18 in-depth interviews with Microsoft employees. The major findings and implications from the collected data contextualize existing green software engineering research and suggest directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications.",c29,International Conference on Software Engineering,cp29,accepted,f443,2015,2015-05-06
s550,p550,On the pragmatic design of literature studies in software engineering: an experience-based guideline,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f444,2004,2004-10-28
s551,p551,A framework for gamification in software engineering,Abstract content goes here ...,j145,Journal of Systems and Software,jv145,accepted,f445,2010,2010-03-10
s552,p552,"Crowdsourcing in Software Engineering: Models, Motivations, and Challenges","Almost surreptitiously, crowdsourcing has entered software engineering practice. In-house development, contracting, and outsourcing still dominate, but many development projects use crowdsourcing-for example, to squash bugs, test software, or gather alternative UI designs. Although the overall impact has been mundane so far, crowdsourcing could lead to fundamental, disruptive changes in how software is developed. Various crowdsourcing models have been applied to software development. Such changes offer exciting opportunities, but several challenges must be met for crowdsourcing software development to reach its potential.",j147,IEEE Software,jv147,accepted,f446,2021,2021-10-03
s553,p553,Evidence-Based Software Engineering and Systematic Reviews,"In the decade since the idea of adapting the evidence-based paradigm for software engineering was first proposed, it has become a major tool of empirical software engineering. Evidence-Based Software Engineering and Systematic Reviews provides a clear introduction to the use of an evidence-based model for software engineering research and practice. The book explains the roles of primary studies (experiments, surveys, case studies) as elements of an over-arching evidence model, rather than as disjointed elements in the empirical spectrum. Supplying readers with a clear understanding of empirical software engineering best practices, it provides up-to-date guidance on how to conduct secondary studies in software engineeringreplacing the existing 2004 and 2007 technical reports. The book is divided into three parts. The first part discusses the nature of evidence and the evidence-based practices centered on a systematic review, both in general and as applying to software engineering. The second part examines the different elements that provide inputs to a systematic review (usually considered as forming a secondary study), especially the main forms of primary empirical study currently used in software engineering. The final part provides practical guidance on how to conduct systematic reviews (the guidelines), drawing together accumulated experiences to guide researchers and students in planning and conducting their own studies. The book includes an extensive glossary and an appendix that provides a catalogue of reviews that may be useful for practice and teaching.",c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f447,2006,2006-09-16
s554,p554,"Speed, Data, and Ecosystems: The Future of Software Engineering","An evaluation of recent industrial and societal trends revealed three key factors driving software engineering's future: speed, data, and ecosystems. These factors' implications have led to guidelines for companies to evolve their software engineering practices. This article is part of a special issue on the Future of Software Engineering.",j147,IEEE Software,jv147,accepted,f448,2021,2021-02-02
s556,p556,The Role of Ethnographic Studies in Empirical Software Engineering,"Ethnography is a qualitative research method used to study people and cultures. It is largely adopted in disciplines outside software engineering, including different areas of computer science. Ethnography can provide an in-depth understanding of the socio-technological realities surrounding everyday software development practice, i.e., it can help to uncover not only what practitioners do, but also why they do it. Despite its potential, ethnography has not been widely adopted by empirical software engineering researchers, and receives little attention in the related literature. The main goal of this paper is to explain how empirical software engineering researchers would benefit from adopting ethnography. This is achieved by explicating four roles that ethnography can play in furthering the goals of empirical software engineering: to strengthen investigations into the social and human aspects of software engineering; to inform the design of software engineering tools; to improve method and process development; and to inform research programmes. This article introduces ethnography, explains its origin, context, strengths and weaknesses, and presents a set of dimensions that position ethnography as a useful and usable approach to empirical software engineering research. Throughout the paper, relevant examples of ethnographic studies of software practice are used to illustrate the points being made.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f449,2014,2014-01-04
s557,p557,On the use of many quality attributes for software refactoring: a many-objective search-based software engineering approach,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f450,2004,2004-01-19
s558,p558,Crossover Designs in Software Engineering Experiments: Benefits and Perils,"In experiments with crossover design subjects apply more than one treatment. Crossover designs are widespread in software engineering experimentation: they require fewer subjects and control the variability among subjects. However, some researchers disapprove of crossover designs. The main criticisms are: the carryover threat and its troublesome analysis. Carryover is the persistence of the effect of one treatment when another treatment is applied later. It may invalidate the results of an experiment. Additionally, crossover designs are often not properly designed and/or analysed, limiting the validity of the results. In this paper, we aim to make SE researchers aware of the perils of crossover experiments and provide risk avoidance good practices. We study how another discipline (medicine) runs crossover experiments. We review the SE literature and discuss which good practices tend not to be adhered to, giving advice on how they should be applied in SE experiments. We illustrate the concepts discussed analysing a crossover experiment that we have run. We conclude that crossover experiments can yield valid results, provided they are properly designed and analysed, and that, if correctly addressed, carryover is no worse than other validity threats.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f451,2014,2014-09-01
s559,p559,Systems and Software Engineering,Abstract content goes here ...,c7,European Conference on Modelling and Simulation,cp7,accepted,f452,2015,2015-04-02
s560,p560,A discipline for software engineering,Abstract content goes here ...,c97,Interspeech,cp97,accepted,f453,2004,2004-05-19
s562,p562,Student Experiences Using GitHub in Software Engineering Courses: A Case Study,"GitHub has been embraced by the software development community as an important social platform for managing software projects and to support collaborative development. More recently, educators have begun to adopt it for hosting course content and student assignments. From our previous research, we found that educators leverage GitHub’s collaboration and transparency features to create, reuse and remix course materials, and to encourage student contributions and monitor student activity on assignments and projects. However, our previous research did not consider the student perspective. In this paper, we present a case study where GitHub is used as a learning platform for two software engineering courses. We gathered student perspectives on how the use of GitHub in their courses might benefit them and to identify the challenges they may face. The findings from our case study indicate that software engineering students do benefit from GitHub’s transparent and open workflow. However, students were concerned that since GitHub is not inherently an educational tool, it lacks key features important for education and poses learning and privacy concerns. Our findings provide recommendations for designers on how tools such as GitHub can be used to improve software engineering education, and also point to recommendations for instructors on how to use it more effectively in their courses.",c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f454,2022,2022-09-02
s563,p563,Software-Engineering the Internet of Things,New wiring transformed ENIAC into a versatile stored-program computer. Rewiring Internet of Things infrastructures into a general-purpose computing fabric can similarly change how modern computation interfaces with our environment.,j147,IEEE Software,jv147,accepted,f455,2021,2021-09-24
s564,p564,Software Engineering,SE 3162 Professional Responsibility in Computer Science and Software Engineering (1 semester credit hour) Professional and ethical responsibilities of computer scientists and software engineers as influenced by growth in computer use and networks. Costs and benefits of computer technology. Risks and liabilities of safety-critical systems. Social implications of the Internet. Interaction between human values and technical decisions involving computing. Intellectual Property. Global impact of computing. Prerequisites or Corequisites: CS 3345 and CS 3354 and ECS 3361. (Same as CS 3162) (1-0) S,c40,IEEE International Conference on Software Maintenance and Evolution,cp40,accepted,f456,2013,2013-08-26
s565,p565,Software Engineering Component Based Software Engineering,"Component-based software engineering (CBSE) (also known as component-based development (CBD)) is a branch of software engineering that emphasizes the separation of concerns in respect of the wide-ranging functionality available throughout a given software system. It is a reuse-based approach to defining, implementing and composing loosely coupled independent components into systems. This practice aims to bring about an equally wide-ranging degree of benefits in both the short-term and the long-term for the software itself and for organizations that sponsor such software. This approach promises to alleviate the software crisis at great extents. The objective of this paper is to gain attention towards this new component based software development paradigm and to highlight the benefits of the approach for making it a successful software development approach to the concerned community",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f457,2018,2018-07-25
s566,p566,Software-Specific Named Entity Recognition in Software Engineering Social Content,"Software engineering social content, such as Q&A discussions on Stack Overflow, has become a wealth of information on software engineering. This textual content is centered around software-specific entities, and their usage patterns, issues-solutions, and alternatives. However, existing approaches to analyzing software engineering texts treat software-specific entities in the same way as other content, and thus cannot support the recent advance of entity-centric applications, such as direct answers and knowledge graph. The first step towards enabling these entity-centric applications for software engineering is to recognize and classify software-specific entities, which is referred to as Named Entity Recognition (NER) in the literature. Existing NER methods are designed for recognizing person, location and organization in formal and social texts, which are not applicable to NER in software engineering. Existing information extraction methods for software engineering are limited to API identification and linking of a particular programming language. In this paper, we formulate the research problem of NER in software engineering. We identify the challenges in designing a software-specific NER system and propose a machine learning based approach applied on software engineering social content. Our NER system, called S-NER, is general for software engineering in that it can recognize a broad category of software entities for a wide range of popular programming languages, platform, and library. We conduct systematic experiments to evaluate our machine learning based S-NER against a well-designed, and to study the effectiveness of widely-adopted NER techniques and features in the face of the unique characteristics of software engineering social content.",c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",cp38,accepted,f458,2015,2015-01-04
s567,p567,Game development software engineering process life cycle: a systematic review,Abstract content goes here ...,j148,Journal of Software Engineering Research and Development,jv148,accepted,f459,2004,2004-05-15
s568,p568,Experimentation in Software Engineering,Abstract content goes here ...,c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f460,2005,2005-09-29
s569,p569,Modeling in Event-B - System and Software Engineering,"A practical text suitable for an introductory or advanced course in formal methods, this book presents a mathematical approach to modelling and designing systems using an extension of the B formal method: Event-B. Based on the idea of refinement, the author's systematic approach allows the user to construct models gradually and to facilitate a systematic reasoning method by means of proofs. Readers will learn how to build models of programs and, more generally, discrete systems, but this is all done with practice in mind. The numerous examples provided arise from various sources of computer system developments, including sequential programs, concurrent programs and electronic circuits. The book also contains a large number of exercises and projects ranging in difficulty. Each of the examples included in the book has been proved using the Rodin Platform tool set, which is available free for download at www.event-b.org.",c35,EUROMICRO Conference on Software Engineering and Advanced Applications,cp35,accepted,f461,2005,2005-01-27
s570,p570,Guide to the Software Engineering Body of Knowledge (SWEBOK(R)): Version 3.0,"In the Guide to the Software Engineering Body of Knowledge (SWEBOK Guide), the IEEE Computer Society establishes a baseline for the body of knowledge for the field of software engineering, and the work supports the Societys responsibility to promote the advancement of both theory and practice in this field. It should be noted that the Guide does not purport to define the body of knowledge but rather to serve as a compendium and guide to the knowledge that has been developing and evolving over the past four decades. Now in Version 3.0, the Guides 15 knowledge areas summarize generally accepted topics and list references for detailed information. The editors for Version 3.0 of the SWEBOK Guide are Pierre Bourque (cole de technologie suprieure (TS), Universit du Qubec) and Richard E. (Dick) Fairley (Software and Systems Engineering Associates (S2EA)).",c87,European Conference on Computer Vision,cp87,accepted,f462,2014,2014-05-25
s571,p571,Global Software Engineering: Evolution and Trends,"Professional software products and IT systems and services today are developed mostly by globally distributed teams, projects, and companies. Successfully orchestrating Global Software Engineering (GSE) has become the major success factor both for organizations and practitioners. Yet, more than a half of all distributed projects does not achieve the intended objectives and is canceled. This paper summarizes experiences from academia and industry in a way to facilitate knowledge and technology transfer. It is based on an evaluation of 10 years of research, and industry collaboration and experience reported at the IEEE International Conference on Software Engineering (ICGSE) series. The outcomes of our analysis show GSE as a field highly attached to industry and, thus, a considerable share of ICGSE papers address the transfer of Software Engineering concepts and solutions to the global stage. We found collaboration and teams, processes and organization, sourcing and supplier management, and success factors to be the topics gaining the most interest of researchers and practitioners. Beyond the analysis of the past conferences, we also look at current trends in GSE to motivate further research and industrial collaboration.",c39,International Conference on Global Software Engineering,cp39,accepted,f463,2020,2020-02-02
s572,p572,A Hitchhiker's guide to statistical tests for assessing randomized algorithms in software engineering,"Randomized algorithms are widely used to address many types of software engineering problems, especially in the area of software verification and validation with a strong emphasis on test automation. However, randomized algorithms are affected by chance and so require the use of appropriate statistical tests to be properly analysed in a sound manner. This paper features a systematic review regarding recent publications in 2009 and 2010 showing that, overall, empirical analyses involving randomized algorithms in software engineering tend to not properly account for the random nature of these algorithms. Many of the novel techniques presented clearly appear promising, but the lack of soundness in their empirical evaluations casts unfortunate doubts on their actual usefulness. In software engineering, although there are guidelines on how to carry out empirical analyses involving human subjects, those guidelines are not directly and fully applicable to randomized algorithms. Furthermore, many of the textbooks on statistical analysis are written from the viewpoints of social and natural sciences, which present different challenges from randomized algorithms. To address the questionable overall quality of the empirical analyses reported in the systematic review, this paper provides guidelines on how to carry out and properly analyse randomized algorithms applied to solve software engineering tasks, with a particular focus on software testing, which is by far the most frequent application area of randomized algorithms within software engineering. Copyright © 2012 John Wiley & Sons, Ltd.",j149,"Software testing, verification & reliability",jv149,accepted,f464,2012,2012-11-20
s574,p574,On Non-Functional Requirements in Software Engineering,Abstract content goes here ...,c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f465,2015,2015-04-25
s576,p576,Are Students Representatives of Professionals in Software Engineering Experiments?,"Background: Most of the experiments in software engineering (SE) employ students as subjects. This raises concerns about the realism of the results acquired through students and adaptability of the results to software industry. Aim: We compare students and professionals to understand how well students represent professionals as experimental subjects in SE research. Method: The comparison was made in the context of two test-driven development experiments conducted with students in an academic setting and with professionals in a software organization. We measured the code quality of several tasks implemented by both subject groups and checked whether students and professionals perform similarly in terms of code quality metrics. Results: Except for minor differences, neither of the subject groups is better than the other. Professionals produce larger, yet less complex, methods when they use their traditional development approach, whereas both subject groups perform similarly when they apply a new approach for the first time. Conclusion: Given a carefully scoped experiment on a development approach that is new to both students and professionals, similar performances are observed. Further investigation is necessary to analyze the effects of subject demographics and level of experience on the results of SE experiments.",c58,Australian Software Engineering Conference,cp58,accepted,f466,2021,2021-02-04
s577,p577,A practical guide to controlled experiments of software engineering tools with human participants,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f467,2004,2004-12-11
s578,p578,Docker [Software engineering],"In episode 217 of Software Engineering Radio, host Charles Anderson talks with James Turnbull, a software developer and security specialist who's vice president of services at Docker. Lightweight Docker containers are rapidly becoming a tool for deploying microservice-based architectures.",c78,Neural Information Processing Systems,cp78,accepted,f468,2012,2012-05-18
s579,p579,Towards a decision-making structure for selecting a research design in empirical software engineering,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f469,2004,2004-05-05
s580,p580,The “Physics” of Notations: Toward a Scientific Basis for Constructing Visual Notations in Software Engineering,"Visual notations form an integral part of the language of software engineering (SE). Yet historically, SE researchers and notation designers have ignored or undervalued issues of visual representation. In evaluating and comparing notations, details of visual syntax are rarely discussed. In designing notations, the majority of effort is spent on semantics, with graphical conventions largely an afterthought. Typically, no design rationale, scientific or otherwise, is provided for visual representation choices. While SE has developed mature methods for evaluating and designing semantics, it lacks equivalent methods for visual syntax. This paper defines a set of principles for designing cognitively effective visual notations: ones that are optimized for human communication and problem solving. Together these form a design theory, called the Physics of Notations as it focuses on the physical (perceptual) properties of notations rather than their logical (semantic) properties. The principles were synthesized from theory and empirical evidence from a wide range of fields and rest on an explicit theory of how visual notations communicate. They can be used to evaluate, compare, and improve existing visual notations as well as to construct new ones. The paper identifies serious design flaws in some of the leading SE notations, together with practical suggestions for improving them. It also showcases some examples of visual notation design excellence from SE and other fields.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f470,2014,2014-10-02
s581,p581,Gamification in software engineering - A systematic mapping,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f471,2010,2010-07-25
s583,p583,Case Study Research in Software Engineering - Guidelines and Examples,"Based on their own experiences of in-depth case studies of software projects in international corporations, in this bookthe authors present detailed practical guidelines on the preparation, conduct, design and reporting of case studies of software engineering. This is the first software engineering specific book on thecase study research method.",c85,International Conference on Graph Transformation,cp85,accepted,f472,2007,2007-02-23
s584,p584,A theory of distances in software engineering,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f473,2010,2010-09-23
s585,p585,Views on Internal and External Validity in Empirical Software Engineering,"Empirical methods have grown common in software engineering, but there is no consensus on how to apply them properly. Is practical relevance key? Do internally valid studies have any value? Should we replicate more to address the tradeoff between internal and external validity? We asked the community how empirical research should take place in software engineering, with a focus on the tradeoff between internal and external validity and replication, complemented with a literature review about the status of empirical research in software engineering. We found that the opinions differ considerably, and that there is no consensus in the community when to focus on internal or external validity and how to conduct and review replications.",c114,IEEE International Conference on Robotics and Automation,cp114,accepted,f474,2011,2011-11-28
s586,p586,A systematic review of systematic review process research in software engineering,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f475,2010,2010-05-19
s587,p587,Software Engineering: A Practitioners Approach,"Software engineering is the art of war. So if you don't know how to wage a war, then the weapons are useless. Software engineering has become very important because of the impact of large, expensive software systems and the role of software in safety-critical applications. This book supports a process to refound software engineering based on a solid theory, proven principles and best practices and fills a long-standing need in the software development communities to make the essential aspects of software development available in one comprehensive work. Written in an easy-to-understand tutorial format, SOFTWARE ENGINEERING: A Practitioners Approach provides professionals, researchers, and students at all levels with a clear coverage of: Analyzing, designing, programming and testing software projects. Set of objectives to which a prospective should be targeting to achieve. Two types of review questions-short answer type and descriptive type. List of key terms referring to abstract concepts, which may be used for better and crisp communication. Solution manual in electronic form available for qualified teachers on demand. Instructor's manual including power point slides, brief notes on teaching and list of projects with descriptions on demand. List of key references for the concepts in the chapter. Useful websites appended to each chapter for quick reference",c65,Formal Concept Analysis,cp65,accepted,f476,2008,2008-07-25
s588,p588,Agent-Oriented Software Engineering,Abstract content goes here ...,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f477,2019,2019-12-11
s589,p589,The (R) Evolution of social media in software engineering,"Software developers rely on media to communicate, learn, collaborate, and coordinate with others. Recently, social media has dramatically changed the landscape of software engineering, challenging some old assumptions about how developers learn and work with one another. We see the rise of the social programmer who actively participates in online communities and openly contributes to the creation of a large body of crowdsourced socio-technical content. In this paper, we examine the past, present, and future roles of social media in software engineering. We provide a review of research that examines the use of different media channels in software engineering from 1968 to the present day. We also provide preliminary results from a large survey with developers that actively use social media to understand how they communicate and collaborate, and to gain insights into the challenges they face. We find that while this particular population values social media, traditional channels, such as face-to-face communication, are still considered crucial. We synthesize findings from our historical review and survey to propose a roadmap for future research on this topic. Finally, we discuss implications for research methods as we argue that social media is poised to bring about a paradigm shift in software engineering research.",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f478,2018,2018-11-02
s590,p590,Search based software engineering for software product line engineering: a survey and directions for future work,"This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.",c41,Software Product Lines Conference,cp41,accepted,f479,2002,2002-09-08
s591,p591,Software Engineering Meets Control Theory,"The software engineering community has proposed numerous approaches for making software self-adaptive. These approaches take inspiration from machine learning and control theory, constructing software that monitors and modifies its own behavior to meet goals. Control theory, in particular, has received considerable attention as it represents a general methodology for creating adaptive systems. Control-theoretical software implementations, however, tend to be ad hoc. While such solutions often work in practice, it is difficult to understand and reason about the desired properties and behavior of the resulting adaptive software and its controller. This paper discusses a control design process for software systems which enables automatic analysis and synthesis of a controller that is guaranteed to have the desired properties and behavior. The paper documents the process and illustrates its use in an example that walks through all necessary steps for self-adaptive controller synthesis.",c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,cp99,accepted,f480,2015,2015-12-25
s592,p592,Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering,"Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f481,2014,2014-04-12
s593,p593,An empirically based terminology and taxonomy for global software engineering,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f482,2004,2004-07-04
s594,p594,Happy software developers solve problems better: psychological measurements in empirical software engineering,"For more than thirty years, it has been claimed that a way to improve software developers’ productivity and software quality is to focus on people and to provide incentives to make developers satisfied and happy. This claim has rarely been verified in software engineering research, which faces an additional challenge in comparison to more traditional engineering fields: software development is an intellectual activity and is dominated by often-neglected human factors (called human aspects in software engineering research). Among the many skills required for software development, developers must possess high analytical problem-solving skills and creativity for the software construction process. According to psychology research, affective states—emotions and moods—deeply influence the cognitive processing abilities and performance of workers, including creativity and analytical problem solving. Nonetheless, little research has investigated the correlation between the affective states, creativity, and analytical problem-solving performance of programmers. This article echoes the call to employ psychological measurements in software engineering research. We report a study with 42 participants to investigate the relationship between the affective states, creativity, and analytical problem-solving skills of software developers. The results offer support for the claim that happy developers are indeed better problem solvers in terms of their analytical abilities. The following contributions are made by this study: (1) providing a better understanding of the impact of affective states on the creativity and analytical problem-solving capacities of developers, (2) introducing and validating psychological measurements, theories, and concepts of affective states, creativity, and analytical-problem-solving skills in empirical software engineering, and (3) raising the need for studying the human factors of software engineering by employing a multidisciplinary viewpoint.",j150,PeerJ,jv150,accepted,f483,2017,2017-03-15
s595,p595,"Software Product Line Engineering - Foundations, Principles, and Techniques","Software product line engineering has proven to be the methodology for developing a diversity of software products and software intensive systems at lower costs, in shorter time, and with higher quality. In this book, Pohl and his co-authors present a framework for software product line engineering which they have developed based on their academic as well as industrial experience gained in projects over the last eight years. They do not only detail the technical aspect of the development, but also an integrated view of the business, organisation and process aspects are given. In addition, they explicitly point out the key differences of software product line engineering compared to traditional single software system development, as the need for two distinct development processes for domain and application engineering respectively, or the need to define and manage variability.",c8,The Compass,cp8,accepted,f484,2016,2016-06-10
s596,p596,Software engineering at the speed of light: how developers stay current using twitter,"The microblogging service Twitter has over 500 million users posting over 500 million tweets daily. Research has established that software developers use Twitter in their work, but this has not yet been examined in detail. Twitter is an important medium in some software engineering circles—understanding its use could lead to improved support, and learning more about the reasons for non-adoption could inform the design of improved tools. In a qualitative study, we surveyed 271 and interviewed 27 developers active on GitHub. We find that Twitter helps them keep up with the fast-paced development landscape. They use it to stay aware of industry changes, for learning, and for building relationships. We discover the challenges they experience and extract their coping strategies. Some developers do not want to or cannot embrace Twitter for their work—we show their reasons and alternative channels. We validate our findings in a follow-up survey with more than 1,200 respondents.",c29,International Conference on Software Engineering,cp29,accepted,f485,2015,2015-06-12
s597,p597,Behavioral software engineering: A definition and systematic literature review,Abstract content goes here ...,j145,Journal of Systems and Software,jv145,accepted,f486,2010,2010-10-31
s598,p598,Replication of empirical studies in software engineering research: a systematic mapping study,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f487,2004,2004-09-18
s601,p601,Systematic mapping study on software engineering for sustainability (SE4S),"Background/Context: The objective of achieving higher sustainability in our lifestyles by information and communication technology has lead to a plethora of research activities in related fields. Consequently, Software Engineering for Sustainability (SE4S) has developed as an active area of research. Objective/Aim: Though SE4S gained much attention over the past few years and has resulted in a number of contributions, there is only one rigorous survey of the field. We follow up on this systematic mapping study from 2012 with a more in-depth overview of the status of research, as most work has been conducted in the last 4 years. Method: The applied method is a systematic mapping study through which we investigate which contributions were made, which knowledge areas are most explored, and which research type facets have been used, to distill a common understanding of the state-of-the-art in SE4S. Results: We contribute an overview of current research topics and trends, and their distribution according to the research type facet and the application domains. Furthermore, we aggregate the topics into clusters and list proposed and used methods, frameworks, and tools. Conclusion: The research map shows that impact currently is limited to few knowledge areas and there is need for a future roadmap to fill the gaps.",c31,International Conference on Evaluation & Assessment in Software Engineering,cp31,accepted,f488,2022,2022-04-13
s602,p602,Fundamentals of Software Engineering,Abstract content goes here ...,j75,Lecture Notes in Computer Science,jv75,accepted,f489,2015,2015-01-29
s603,p603,Green in Software Engineering,Abstract content goes here ...,j112,Cambridge International Law Journal,jv112,accepted,f490,2012,2012-09-02
s604,p604,Challenges for Software Engineering in Automation,"This paper gives an 
introduction to the essential challenges of software engineering and 
requirements that software has to fulfill in the domain of automation. Besides, 
the functional characteristics, specific constraints and circumstances are 
considered for deriving requirements concerning usability, the technical 
process, the automation functions, used platform and the well-established 
models, which are described in detail. On the other hand, challenges result 
from the circumstances at different points in the single phases of the life 
cycle of the automated system. The requirements for life-cycle-management, 
tools and the changeability during runtime are described in detail.",c109,International Conference on Mobile Data Management,cp109,accepted,f491,2014,2014-07-11
s605,p605,Search Based Software Engineering,Abstract content goes here ...,j75,Lecture Notes in Computer Science,jv75,accepted,f492,2015,2015-08-12
s607,p607,Continuous software engineering and beyond: trends and challenges,"Throughout its short history, software development has been characterized by harmful disconnects between important activities e.g., planning, development and implementation. The problem is further exacerbated by the episodic and infrequent performance of activities such as planning, testing, integration and releases. Several emerging phenomena reflect attempts to address these problems. For example, the Enterprise Agile concept has emerged as a recognition that the benefits of agile software development will be sub- optimal if not complemented by an agile approach in related organizational function such as finance and HR. Continuous integration is a practice which has emerged to eliminate discontinuities between development and deployment. In a similar vein, the recent emphasis on DevOps recognizes that the integration between software development and its operational deployment needs to be a continuous one. We argue a similar continuity is required between business strategy and development, BizDev being the term we coin for this. These disconnects are even more problematic given the need for reliability and resilience in the complex and data-intensive systems being developed today. Drawing on the lean concept of flow, we identify a number of continuous activities which are important for software development in today’s context. These activities include continuous planning, continuous integration, continuous deployment, continuous delivery, continuous verification, continuous testing, continuous compliance,continuous security, continuous use, continuous trust, continuous run-time monitoring, continuous improvement (both process and product), all underpinned by continuous innovation. We use the umbrella term, ``Continuous *'' (continuous star) to identify this family of continuous activities.",c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f493,2016,2016-11-03
s608,p608,Continuous Software Engineering,Abstract content goes here ...,j112,Cambridge International Law Journal,jv112,accepted,f494,2012,2012-06-24
s609,p609,Introduction to Green in Software Engineering,Abstract content goes here ...,c21,Grid Computing Environments,cp21,accepted,f495,2005,2005-02-20
s610,p610,Lessons from applying the systematic literature review process within the software engineering domain,Abstract content goes here ...,j145,Journal of Systems and Software,jv145,accepted,f496,2010,2010-02-07
s611,p611,A Systematic Literature Review on Fault Prediction Performance in Software Engineering,"Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f497,2014,2014-08-13
s612,p612,Continuous Software Engineering: An Introduction,Abstract content goes here ...,c40,IEEE International Conference on Software Maintenance and Evolution,cp40,accepted,f498,2013,2013-03-29
s613,p613,How to effectively use topic models for software engineering tasks? An approach based on Genetic Algorithms,"Information Retrieval (IR) methods, and in particular topic models, have recently been used to support essential software engineering (SE) tasks, by enabling software textual retrieval and analysis. In all these approaches, topic models have been used on software artifacts in a similar manner as they were used on natural language documents (e.g., using the same settings and parameters) because the underlying assumption was that source code and natural language documents are similar. However, applying topic models on software data using the same settings as for natural language text did not always produce the expected results. Recent research investigated this assumption and showed that source code is much more repetitive and predictable as compared to the natural language text. Our paper builds on this new fundamental finding and proposes a novel solution to adapt, configure and effectively use a topic modeling technique, namely Latent Dirichlet Allocation (LDA), to achieve better (acceptable) performance across various SE tasks. Our paper introduces a novel solution called LDA-GA, which uses Genetic Algorithms (GA) to determine a near-optimal configuration for LDA in the context of three different SE tasks: (1) traceability link recovery, (2) feature location, and (3) software artifact labeling. The results of our empirical studies demonstrate that LDA-GA is able to identify robust LDA configurations, which lead to a higher accuracy on all the datasets for these SE tasks as compared to previously published results, heuristics, and the results of a combinatorial search.",c29,International Conference on Software Engineering,cp29,accepted,f499,2015,2015-07-18
s614,p614,Parameter tuning or default values? An empirical investigation in search-based software engineering,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f500,2004,2004-06-25
s615,p615,Design patterns: elements of reuseable object-oriented software,"The book is an introduction to the idea of design patterns in software engineering, and a catalog of twenty-three common patterns. The nice thing is, most experienced OOP designers will find out they've known about patterns all along. It's just that they've never considered them as such, or tried to centralize the idea behind a given pattern so that it will be easily reusable.",c61,Jahrestagung der Gesellschaft für Informatik,cp61,accepted,f501,2017,2017-12-10
s616,p616,Software Engineering in Industrial Automation: State-of-the-Art Review,"This paper presents one perspective on recent developments related to software engineering in the industrial automation sector that spans from manufacturing factory automation to process control systems and energy automation systems. The survey's methodology is based on the classic SWEBOK reference document that comprehensively defines the taxonomy of software engineering domain. This is mixed with classic automation artefacts, such as the set of the most influential international standards and dominating industrial practices. The survey focuses mainly on research publications which are believed to be representative of advanced industrial practices as well.",j151,IEEE Transactions on Industrial Informatics,jv151,accepted,f502,2020,2020-02-23
s617,p617,The mythical man-month: Essays on software engineering,"Like Bahbage, he lobbied for mathematical reform, stumped for the centrality of science in cultural advancement, argued that government support was crucial, and proved a stubborn and crotchety opponent when crossed. And, as Colin Burke reminds us in this fine and fresh new look at Bush, Bush envisioned machines relevant to the history of computing that never lived up to their promise. I doubt that Burke would agree with my description of Bush as a latter-day Babbage; nevertheless, this detailed study makes the comparison almost inevitable. Burke helps us appreciate how Bush's fascination with the mechanization of calculation and comparison caused his inventive work to swirl around problems relevant to the emergence of the modern computer. Moreover, Burke suggests that two of Bush's less familiar engines-one, the Rapid Selector, a bibliographic machine and a close cousin of the Memex of faddish fame; and the other, the Comparator, a cryptanalytic device-provide the stuff to fill in the holes in the history of the computer [p. ix). It is never very clear just what these holes are; this reader, at least, was not convinced that the careers of these two machines were anything but eddies along the shore of the main currents of computer evolution. They were decisive failures, as Burke admits, rooted in a stubborn commitment to intractdbk and ultimately unfashion-able if not outdated technologies. The strengths of this book indeed lie elsewhere. These exotic devices are of interest in themselves and deserve their biographer's attention. Burke details the labors of Bush and friends to use microfilm, electronics, and photoelectricity to mechanize the library-hereby resolving a putative information overload (it turns out that there wasn't one)-and help the U.S. Navy's cryptographers break enemy codes during World War 11. Burke is best, however, when discussing not machines themselves but when individuals and bureaucracies are at loggerheads. Ego, ambition, and organizational and technological vision were at stake. On the military side, and against much intcrnal resistance , Bush allies such as Stanford C. Hooper and Joseph Wenger dreamed of building the next generation of rapid analytic machines and, in doing so, dreamed of upgrading the scientific navy by forging alliances with "" college professors "" like Bush; on the civilian side, Bush and his "" boys "" worked to maneuver the navy into a project that promised much in the way of personal and institutional prestige, income for research, and opportunities for graduate …",j152,IEEE Annals of the History of Computing,jv152,accepted,f503,2016,2016-03-24
s618,p618,"Search-based software engineering: Trends, techniques and applications","In the past five years there has been a dramatic increase in work on Search-Based Software Engineering (SBSE), an approach to Software Engineering (SE) in which Search-Based Optimization (SBO) algorithms are used to address problems in SE. SBSE has been applied to problems throughout the SE lifecycle, from requirements and project planning to maintenance and reengineering. The approach is attractive because it offers a suite of adaptive automated and semiautomated solutions in situations typified by large complex problem spaces with multiple competing and conflicting objectives.
 This article1 provides a review and classification of literature on SBSE. The work identifies research trends and relationships between the techniques applied and the applications to which they have been applied and highlights gaps in the literature and avenues for further research.",c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f504,2004,2004-12-12
s619,p619,On the value of user preferences in search-based software engineering: A case study in software product lines,"Software design is a process of trading off competing objectives. If the user objective space is rich, then we should use optimizers that can fully exploit that richness. For example, this study configures software product lines (expressed as feature maps) using various search-based software engineering methods. As we increase the number of optimization objectives, we find that methods in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm). IBEA works best since it makes most use of user preference knowledge. Hence it does better on the standard measures (hypervolume and spread) but it also generates far more products with 0% violations of domain constraints. Our conclusion is that we need to change our methods for search-based software engineering, particularly when studying complex decision spaces.",c29,International Conference on Software Engineering,cp29,accepted,f505,2015,2015-05-15
s621,p621,Automatic query reformulations for text retrieval in software engineering,"There are more than twenty distinct software engineering tasks addressed with text retrieval (TR) techniques, such as, traceability link recovery, feature location, refactoring, reuse, etc. A common issue with all TR applications is that the results of the retrieval depend largely on the quality of the query. When a query performs poorly, it has to be reformulated and this is a difficult task for someone who had trouble writing a good query in the first place. We propose a recommender (called Refoqus) based on machine learning, which is trained with a sample of queries and relevant results. Then, for a given query, it automatically recommends a reformulation strategy that should improve its performance, based on the properties of the query. We evaluated Refoqus empirically against four baseline approaches that are used in natural language document retrieval. The data used for the evaluation corresponds to changes from five open source systems in Java and C++ and it is used in the context of TR-based concept location in source code. Refoqus outperformed the baselines and its recommendations lead to query performance improvement or preservation in 84% of the cases (in average).",c29,International Conference on Software Engineering,cp29,accepted,f506,2015,2015-09-21
s624,p624,"Worldviews, Research Methods, and their Relationship to Validity in Empirical Software Engineering Research","Background - Validity threats should be considered and consistently reported to judge the value of an empirical software engineering research study. The relevance of specific threats for a particular research study depends on the worldview or philosophical worldview of the researchers of the study. Problem/Gap - In software engineering, different categorizations exist, which leads to inconsistent reporting and consideration of threats. Contribution - In this paper, we relate different worldviews to software engineering research methods, identify generic categories for validity threats, and provide a categorization of validity threats with respect to their relevance for different world views. Thereafter, we provide a checklist aiding researchers in identifying relevant threats. Method - Different threat categorizations and threats have been identified in literature, and are reflected on in relation to software engineering research. Results - Software engineering is dominated by the pragmatist worldviews, and therefore use multiple methods in research. Maxwell's categorization of validity threats has been chosen as very suitable for reporting validity threats in software engineering research. Conclusion - We recommend to follow a checklist approach, and reporting first the philosophical worldview of the researcher when doing the research, the research methods and all threats relevant, including open, reduced, and mitigated threats.",c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f507,2019,2019-10-02
s625,p625,Towards a definition of sustainability in and for software engineering,"Sustainability is not supported by traditional software engineering methods. This lack of support leads to inefficient efforts to address sustainability or complete omission of this important concept. Defining and developing adequate support requires a commonly accepted definition of what sustainability means in and for software engineering.
 We contribute a description of the aspects of sustainability in software engineering.",c43,ACM Symposium on Applied Computing,cp43,accepted,f508,2001,2001-10-03
s626,p626,A Green Model for Sustainable Software Engineering,"Information Communication Technology (ICT) has a strong impact on sustainable development due its rising demands for energy and resources needed when building hardware and software products. Most of the efforts spent on Green ICT/IT have been dedicated to addressing the effects of hardware on the environment but little have been considering the effects of building software products as well. Efficient software will indirectly consume less energy by using up less hardware equipment to run. Our contributions in this paper are devoted to building a two level green software model that covers the sustainable life cycle of a software product and the software tools promoting green and environmentally sustainable software. In the first level we propose a new green software engineering process that is a hybrid process between sequential, iterative, and agile development processes to produce an environmentally sustainable one. Each stage of the software process is then further studied to produce a green and sustainable stage. We propose either green guidelines or green processes for each software stage in the engineering process. We add to the software life cycle the requirements stage and the testing stage. We also include in the first level a complete list of metrics to measure the greenness of each stage in terms of the first order effects of ICT on the environment for a green software engineering process. No effort has been placed before in designing a green software engineering process. The second level explains how software itself can be used as a tool to aid in green computing by monitoring resources in an energy efficient manner. Finally, we show and explain relationships that can be found between the two levels in our proposed model to make the software engineering process and product green and sustainable.",c57,IEEE International Conference on Engineering of Complex Computer Systems,cp57,accepted,f509,2003,2003-05-14
s631,p631,What is social debt in software engineering?,"“Social debt” in software engineering informally refers to unforeseen project cost connected to a “suboptimal” development community. The causes of suboptimal development communities can be many, ranging from global distance to organisational barriers to wrong or uninformed socio-technical decisions (i.e., decisions that influence both social and technical aspects of software development). Much like technical debt, social debt impacts heavily on software development success. We argue that, to ensure quality software engineering, practitioners should be provided with mechanisms to detect and manage the social debt connected to their development communities. This paper defines and elaborates on social debt, pointing out relevant research paths. We illustrate social debt by comparison with technical debt and discuss common real-life scenarios that exhibit “sub-optimal” development communities.",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f510,2005,2005-09-08
s632,p632,Systematic literature reviews in software engineering,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f511,2010,2010-12-27
s634,p634,Agent-based software engineering,"The technology of intelligent agents and multi-agent systems is expected to alter radically the way in which complex, distributed, open systems are conceptualised and implemented. The paper considers the problem of building a multi-agent system as a software engineering enterprise. Three issues are focused on: how agents might be specified; how these specifications might be refined or otherwise transformed into efficient implementations: and how implemented agents and multi-agent systems might subsequently be verified, to show that they are correct with respect to their specifications. These issues are discussed with reference to a number of case studies. The paper concludes by setting out some issues and open problems for future research.",c56,European Conference on Software Process Improvement,cp56,accepted,f512,2016,2016-04-27
s635,p635,A software engineering perspective on environmental modeling framework design: The Object Modeling System,Abstract content goes here ...,j47,Environmental Modelling & Software,jv47,accepted,f513,2002,2002-05-03
s636,p636,Standard Glossary of Software Engineering Terminology,"IEEE Std 610.12-1990, IEEE Standard Glossary of Software Engineering Terminology, identifies terms currently in use in the field of Software Engineering. Standard definitions for those terms are established.",c30,IEEE Aerospace Conference,cp30,accepted,f514,2006,2006-11-29
s637,p637,On the reliability of mapping studies in software engineering,Abstract content goes here ...,j145,Journal of Systems and Software,jv145,accepted,f515,2010,2010-08-06
s638,p638,"Educational software engineering: Where software engineering, education, and gaming meet","We define and advocate the subfield of educational software engineering (i.e., software engineering for education), which develops software engineering technologies (e.g., software testing and analysis, software analytics) for general educational tasks, going beyond educational tasks for software engineering. In this subfield, gaming technologies often play an important role together with software engineering technologies. We expect that researchers in educational software engineering would be among key players in the education domain and in the coming age of Massive Open Online Courses (MOOCs). Educational software engineering can and will contribute significant solutions to address various critical challenges in education especially MOOCs such as automatic grading, intelligent tutoring, problem generation, and plagiarism detection. In this position paper, we define educational software engineering and illustrate Pex for Fun (in short as Pex4Fun), one of our recent examples on leveraging software engineering and gaming technologies to address educational tasks on teaching and learning programming and software engineering skills.",c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f516,2021,2021-03-17
s639,p639,Preliminary Guidelines for Empirical Research in Software Engineering,"Empirical software engineering research needs research guidelines to improve the research and reporting processes. We propose a preliminary set of research guidelines aimed at stimulating discussion among software researchers. They are based on a review of research guidelines developed for medical researchers and on our own experience in doing and reviewing software engineering research. The guidelines are intended to assist researchers, reviewers, and meta-analysts in designing, conducting, and evaluating empirical studies. Editorial boards of software engineering journals may wish to use our recommendations as a basis for developing guidelines for reviewers and for framing policies for dealing with the design, data collection, and analysis and reporting of empirical studies.",c59,British Computer Society Conference on Human-Computer Interaction,cp59,accepted,f517,2019,2019-04-18
s640,p640,Software Product Line Engineering Foundations Principles And Techniques,"Thank you for reading software product line engineering foundations principles and techniques. Maybe you have knowledge that, people have look numerous times for their favorite books like this software product line engineering foundations principles and techniques, but end up in malicious downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they are facing with some infectious bugs inside their desktop computer.",c87,European Conference on Computer Vision,cp87,accepted,f518,2014,2014-08-27
s643,p643,Selecting Empirical Methods for Software Engineering Research,Abstract content goes here ...,c112,Very Large Data Bases Conference,cp112,accepted,f519,2018,2018-01-30
s644,p644,An open graph visualization system and its applications to software engineering,"We describe a package of practical tools and libraries for manipulating graphs and their drawings. Our design, which is aimed at facilitating the combination of the package components with other tools, includes stream and event interfaces for graph operations, high‐quality static and dynamic layout algorithms, and the ability to handle sizeable graphs. We conclude with a description of the applications of this package to a variety of software engineering tools. Copyright © 2000 John Wiley & Sons, Ltd.",j154,"Software, Practice & Experience",jv154,accepted,f520,2008,2008-07-26
s645,p645,Software design,"From the Publisher: 
 
Based on a curriculum module originally written for the Software Engineering Institute at Carnegie Mellon University, this text provides students with an introduction to the role of design in software engineering. The book surveys a wide range of design methods and evaluates their strengths and weaknesses in various applications. The author adopts a neutral approach, concentrating on the role of design in software development creating a more effective tutorial text for students. 
Features 
Provides a balanced introduction to software design, reviewing the leading design methods, both formal and informal, from a neutral viewpoint. 
Describes and evaluates a wide range of different design methods, including JSP, SSA/SD, JSD, object-oriented and object- based design 
Focuses on design principles and strategies, which can be directly applied in practice.",c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f521,2020,2020-02-29
s646,p646,Global software engineering and agile practices: a systematic review,"Agile practices have received attention from industry as an alternative to plan‐driven software development approaches. Agile encourages, for example, small self‐organized collocated teams, whereas global software engineering (GSE) implies distribution across cultural, temporal, and geographical boundaries. Hence, combining them is a challenge. A systematic review was conducted to capture the status of combining agility with GSE. The results were limited to peer‐reviewed conference papers or journal articles, published between 1999 and 2009. The synthesis was made through classifying the papers into different categories (e.g. publication year, contribution type, research method). At the end, 81 papers were judged as primary for further analysis. The distribution of papers over the years indicated that GSE and Agile in combination has received more attention in the last 5 years. However, the majority of the existing research is industrial experience reports in which Agile practices were modified with respect to the context and situational requirements. The emergent need in this research area is suggested to be developing a framework that considers various factors from different perspectives when incorporating Agile in GSE. Practitioners may use it as a decision‐making basis in early phases of software development. Copyright © 2011 John Wiley & Sons, Ltd.",c19,ACM Conference on Economics and Computation,cp19,accepted,f522,2002,2002-09-26
s649,p649,How to design gamification? A method for engineering gamified software,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f523,2010,2010-03-08
s651,p651,Dynamic adaptive Search Based Software Engineering,"Search Based Software Engineering (SBSE) has proved to be a very effective way of optimising software engineering problems. Nevertheless, its full potential as a means of dynamic adaptivity remains under explored. This paper sets out the agenda for Dynamic Adaptive SBSE, in which the optimisation is embedded into deployed software to create self-optimising adaptive systems. Dynamic Adaptive SBSE will move the research agenda forward to encompass both software development processes and the software products they produce, addressing the long-standing, and as yet largely unsolved, grand challenge of self-adaptive systems.",c3,Frontiers in Education Conference,cp3,accepted,f524,2016,2016-06-11
s652,p652,Where's the Theory for Software Engineering?,"Darwin's theory of natural selection, Maxwell's equations, the theory of demand and supply; almost all established academic disciplines place great emphasis on what their core theory is. This is not, however, the case in software engineering. What is the reason behind the software engineering community's apparent indifference to a concept that is so important to so many others?",j147,IEEE Software,jv147,accepted,f525,2021,2021-08-05
s653,p653,On the dataset shift problem in software engineering prediction models,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f526,2004,2004-07-11
s654,p654,The role of Artificial Intelligence in Software Engineering,"There has been a recent surge in interest in the application of Artificial Intelligence (AI) techniques to Software Engineering (SE) problems. The work is typified by recent advances in Search Based Software Engineering, but also by long established work in Probabilistic reasoning and machine learning for Software Engineering. This paper explores some of the relationships between these strands of closely related work, arguing that they have much in common and sets out some future challenges in the area of AI for SE.",c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f527,2005,2005-03-23
s655,p655,Clustering Methodologies for Software Engineering,"The size and complexity of industrial strength software systems are constantly increasing. This means that the task of managing a large software project is becoming even more challenging, especially in light of high turnover of experienced personnel. Software clustering approaches can help with the task of understanding large, complex software systems by automatically decomposing them into smaller, easier-to-manage subsystems. The main objective of this paper is to identify important research directions in the area of software clustering that require further attention in order to develop more effective and efficient clustering methodologies for software engineering. To that end, we first present the state of the art in software clustering research. We discuss the clustering methods that have received the most attention from the research community and outline their strengths and weaknesses. Our paper describes each phase of a clustering algorithm separately. We also present the most important approaches for evaluating the effectiveness of software clustering.",j155,Advances in Software Engineering,jv155,accepted,f528,2018,2018-06-12
s656,p656,Engineering Trustworthy Self-Adaptive Software with Dynamic Assurance Cases,"Building on concepts drawn from control theory, self-adaptive software handles environmental and internal uncertainties by dynamically adjusting its architecture and parameters in response to events such as workload changes and component failures. Self-adaptive software is increasingly expected to meet strict functional and non-functional requirements in applications from areas as diverse as manufacturing, healthcare and finance. To address this need, we introduce a methodology for the systematic ENgineering of TRUstworthy Self-adaptive sofTware (ENTRUST). ENTRUST uses a combination of (1) design-time and runtime modelling and verification, and (2) industry-adopted assurance processes to develop trustworthy self-adaptive software and assurance cases arguing the suitability of the software for its intended application. To evaluate the effectiveness of our methodology, we present a tool-supported instance of ENTRUST and its use to develop proof-of-concept self-adaptive software for embedded and service-based systems from the oceanic monitoring and e-finance domains, respectively. The experimental results show that ENTRUST can be used to engineer self-adaptive software systems in different application domains and to generate dynamic assurance cases for these systems.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f529,2014,2014-01-28
s658,p658,Embracing the Engineering Side of Software Engineering,"The author provides, based on 20 years of research and industrial experience, his assessment of software engineering research. He then builds on such analysis to provide recommendations on how we need to change as a research community to increase our impact, gain credibility, and ultimately ensure the success and recognition of our young discipline. The gist of the author's message is that we need to become a true engineering discipline.",j147,IEEE Software,jv147,accepted,f530,2021,2021-02-02
s659,p659,Software Ecosystems: Trends and Impacts on Software Engineering,"Economic and social issues are pointed out as Software Engineering (SE) challenges for the next years, since the field needs to treat issues beyond the technical side. These challenges require analyzing the field of SE from another perspective. In this sense, the study of software ecosystems (SECOs) is an emerging discipline that investigates the relationships among companies in the software industry. Companies work cooperatively and competitively in order to achieve their strategic objectives. They must engage in a new perspective, now also including third parties motivations and movements in the ecosystem, besides their own business viewpoint. Inspired on properties of natural and business ecosystems, SECO covers technical and business aspects of software development as well as partnership among companies. In this paper, we undertake a review on SECOs status as an emerging research topic in SE community. We map what is currently known about SECOs and also analyze them in a three-dimensional perspective in SE, i.e., technical, business and social. We observed that SECOs research is concentrated in eight main areas in which the most relevant ones are open source software, ecosystem modeling, and business issues. This paper also contributes to summarize the body of knowledge and presents a research agenda in SECOs.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f531,2007,2007-11-21
s660,p660,Software Engineering Game: Software Engineering Game,"The goal of this paper is to explore and evaluate the utility of different strategies used in educational games. These strategies include creating immersion and promote learning through visual gratification, feedback, scoring, reasoning, and cognitively demanding environments. A game prototype, based on the tactics and strategies outlined in the literature study, has been implemented and is subject for testing on students. The tests aims to determine how well different strategies would do in an actual implementation of a game.",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f532,2015,2015-10-04
s661,p661,Requirements Engineering - From System Goals to UML Models to Software Specifications,"Essential comprehensive coverage of the fundamentals of requirements engineering Requirements engineering (RE) deals with the variety of prerequisites that must be met by a software system within an organization in order for that system to produce stellar results. With that explanation in mind, this must-have book presents a disciplined approach to the engineering of high-quality requirements. Serving as a helpful introduction to the fundamental concepts and principles of requirements engineering, this guide offers a comprehensive review of the aim, scope, and role of requirements engineering as well as best practices and flaws to avoid. Shares state-of-the-art techniques for domain analysis, requirements elicitation, risk analysis, conflict management, and more Features in-depth treatment of system modeling in the specific context of engineering requirements Presents various forms of reasoning about models for requirements quality assurance Discusses the transitions from requirements to software specifications to software architecture In addition, case studies are included that complement the many examples provided in the book in order to show you how the described method and techniques are applied in practical situations.",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f533,2005,2005-03-20
s662,p662,Systematic literature reviews in software engineering - A tertiary study,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f534,2010,2010-01-28
s663,p663,A Process Framework for Global Software Engineering Teams,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f535,2010,2010-08-11
s664,p664,On software engineering repositories and their open problems,"In the last decade, a large number of software repositories have been created for different purposes. In this paper we present a survey of the publicly available repositories and classify the most common ones as well as discussing the problems faced by researchers when applying machine learning or statistical techniques to them.",c31,International Conference on Evaluation & Assessment in Software Engineering,cp31,accepted,f536,2022,2022-02-01
s665,p665,Recommended Steps for Thematic Synthesis in Software Engineering,"Thematic analysis is an approach that is often used for identifying, analyzing, and reporting patterns (themes) within data in primary qualitative research. 'Thematic synthesis' draws on the principles of thematic analysis and identifies the recurring themes or issues from multiple studies, interprets and explains these themes, and draws conclusions in systematic reviews. This paper conceptualizes the thematic synthesis approach in software engineering as a scientific inquiry involving five steps that parallel those of primary research. The process and outcome associated with each step are described and illustrated with examples from systematic reviews in software engineering.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f537,2010,2010-05-20
s667,p667,The state of the art in end-user software engineering,"Most programs today are written not by professional software developers, but by people with expertise in other domains working towards goals for which they need computational support. For example, a teacher might write a grading spreadsheet to save time grading, or an interaction designer might use an interface builder to test some user interface design ideas. Although these end-user programmers may not have the same goals as professional developers, they do face many of the same software engineering challenges, including understanding their requirements, as well as making decisions about design, reuse, integration, testing, and debugging. This article summarizes and classifies research on these activities, defining the area of End-User Software Engineering (EUSE) and related terminology. The article then discusses empirical research about end-user software engineering activities and the technologies designed to support them. The article also addresses several crosscutting issues in the design of EUSE tools, including the roles of risk, reward, and domain complexity, and self-efficacy in the design of EUSE tools and the potential of educating users about software engineering principles.",j49,ACM Computing Surveys,jv49,accepted,f538,2012,2012-03-17
s669,p669,Software engineering metrics and models,Abstract content goes here ...,c1,Technical Symposium on Computer Science Education,cp1,accepted,f539,2002,2002-08-08
s670,p670,Traffic engineering in software defined networks,"Software Defined Networking is a new networking paradigm that separates the network control plane from the packet forwarding plane and provides applications with an abstracted centralized view of the distributed network state. A logically centralized controller that has a global network view is responsible for all the control decisions and it communicates with the network-wide distributed forwarding elements via standardized interfaces. Google recently announced [5] that it is using a Software Defined Network (SDN) to interconnect its data centers due to the ease, efficiency and flexibility in performing traffic engineering functions. It expects the SDN architecture to result in better network capacity utilization and improved delay and loss performance. The contribution of this paper is on the effective use of SDNs for traffic engineering especially when SDNs are incrementally introduced into an existing network. In particular, we show how to leverage the centralized controller to get significant improvements in network utilization as well as to reduce packet losses and delays. We show that these improvements are possible even in cases where there is only a partial deployment of SDN capability in a network. We formulate the SDN controller's optimization problem for traffic engineering with partial deployment and develop fast Fully Polynomial Time Approximation Schemes (FPTAS) for solving these problems. We show, by both analysis and ns-2 simulations, the performance gains that are achievable using these algorithms even with an incrementally deployed SDN.",c19,ACM Conference on Economics and Computation,cp19,accepted,f540,2002,2002-05-22
s671,p671,B4: experience with a globally-deployed software defined wan,"We present the design, implementation, and evaluation of B4, a private WAN connecting Google's data centers across the planet. B4 has a number of unique characteristics: i) massive bandwidth requirements deployed to a modest number of sites, ii) elastic traffic demand that seeks to maximize average bandwidth, and iii) full control over the edge servers and network, which enables rate limiting and demand measurement at the edge. These characteristics led to a Software Defined Networking architecture using OpenFlow to control relatively simple switches built from merchant silicon. B4's centralized traffic engineering service drives links to near 100% utilization, while splitting application flows among multiple paths to balance capacity against application priority/demands. We describe experience with three years of B4 production deployment, lessons learned, and areas for future work.",c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f541,2018,2018-11-01
s672,p672,Software product lines - practices and patterns,"Foreword. Preface. Acknowledgements. Dedication. Reader's Guide. I. SOFTWARE PRODUCT LINE FUNDAMENTALS. 1. Basic Ideas and Terms. What Is a Software Product Line? What Software Product Lines Are Not. Fortuitous Small-Grained Reuse. Single-System Development with Reuse. Just Component-Based Development. Just a Reconfigurable Architecture. Releases and Versions of Single Products. Just a Set of Technical Standards. A Note on Terminology. For Further Reading. Discussion Questions. 2. Benefits. Organizational Benefits. Individual Benefits. Benefits versus Costs. For Further Reading. Discussion Questions. 3. The Three Essential Activities. What Are the Essential Activities? Core Asset Development. Product Development. Management. All Three Together. For Further Reading. Discussion Questions. II. SOFTWARE PRODUCT LINE PRACTICE AREAS. Describing the Practice Areas. Starting versus Running a Product Line. Organizing the Practice Areas. 4. Software Engineering Practice Areas. Architecture Definition. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Architecture Evaluation. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Component Development. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. COTS Utilization. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Mining Existing Assets. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Requirements Engineering. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Software System Integration. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Testing. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Understanding Relevant Domains. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. 5. Technical Management Practice Areas. Configuration Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Data Collection, Metrics, and Tracking. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Make/Buy/Mine/Commission Analysis. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Process Definition. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Scoping. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Technical Planning. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Technical Risk Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Tool Support. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. 6. Organizational Management Practice Areas. Building a Business Case. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Customer Interface Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Developing an Acquisition Strategy. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Funding. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Launching and Institutionalizing. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Market Analysis. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Operations. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Organizational Planning. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Organizational Risk Management. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Structuring the Organization. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. Discussion Questions. Technology Forecasting. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. Training. Aspects Peculiar to Product Lines. Application to Core Asset Development. Application to Product Development. Specific Practices. Practice Risks. For Further Reading. Discussion Questions. III. PUTTING THE PRACTICE AREAS INTO ACTION. 7. Software Product Line Practice Patterns. The Value of Patterns. Software Product Line Practice Pattern Descriptions. The Curriculum Pattern. The Essentials Coverage Pattern. Each Asset Pattern. What to Build Pattern. Product Parts Pattern. Assembly Line Pattern. Monitor Pattern. Product Builder Pattern. Cold Start Pattern. In Motion Pattern. Process Pattern. Factory Pattern. Other Patterns. Practice Area Coverage. Discussion Questions. 8. Product Line Technical Probe. What Is the Product Line Technical Probe? Probe Interview Questions. Probe Participants. Probe Process. Using the Probe Results. Conducting a Mini Self-Probe. Discussion Questions. 9. Cummins Engine Company: Embracing the Future. Prologue. Company History. A Product Line of Engine Software. Getting off the Ground. An Organization Structured for Cooperation. Running the Product Line. Results. Lessons Learned. Epilogue. Practice Area Compendium. For Further Reading. Discussion Questions. 10. Control Channel Toolkit: A Software Product Line that Controls Satellites. Contextual Background. Organizational Profiles. Project History. Control Channels. Launching CCT. Developing a Business Case for CCT. Developing the Acquisition Strategy and Funding CCT. Structuring the CCT Organization. Organizational and Technical Planning. Operations. Engineering the CCT Core Assets. Domain Analysis. Architecture. Component Engineering. Testing: Application and Test Engineering. Sustainment Engineering: Product Line Evolution. Documentation. Managing the CCT Effort. Early Benefits from CCT. First CCT Product. Benefits beyond CCT Products. Lessons and Issues. Tool Support Is Inadequate. Domain Analysis Documentation Is Important. An Early Architecture Focus Is Best. Product Builders Need More Support. CCT Users Need Reuse Metrics. It Pays to Be Flexible, and Cross-Unit Teams Work. A Real Product Is a Benefit. Summary. For Further Reading. Discussion Questions. 11. Successful Software product Line Development in Small Organization. Introduction. The Early Years. The MERGER Software Product Line. Market Maker Software Product Line Practices. Architecture Definition. Component Development. Structuring (and Staffing) the Organization. Testing. Data Collection and Metrics. Launching and Institutionalizing the Product Line. Understanding the Market. Technology Forecasting. A Few Observations. Effects of Company Culture. Cost Issues. The Customer Paradox. Tool Support. Lessons Learned. Drawbacks. Conclusions: Software Product Lines in Small Organizations. For Further Reading. Discussion Questions. 12. Conclusions: Practices, Patterns and Payoffs. The Practices. The Patterns. The Success Factors. The Payoff. Finale. Glossary. Bibliography. Index.",c100,ACM SIGMOD Conference,cp100,accepted,f542,2010,2010-01-18
s673,p673,Identifying relevant studies in software engineering,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f543,2010,2010-05-16
s674,p674,On Parameter Tuning in Search Based Software Engineering,Abstract content goes here ...,c49,International Symposium on Search Based Software Engineering,cp49,accepted,f544,2012,2012-02-15
s675,p675,Human-Centered Software Engineering - Integrating Usability in the Software Development Lifecycle,Abstract content goes here ...,c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f545,2006,2006-10-16
s676,p676,Software Product Line Engineering,Abstract content goes here ...,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,cp79,accepted,f546,2020,2020-04-13
s677,p677,Motivation in software engineering: A systematic review update,"Background/Aim - Given the relevance and importance that the understanding of motivation has gained in the field of software engineering, this work was carried out in order to update the results of a literature review carried out in 2006 on motivation in software engineering. Method - Based on guidelines for this specific type of study, we replicated the original study protocol. Results - The combination of manual and automatic searches retrieved 6,534 papers, of which 53 relevant papers were selected for data extraction and analysis. Conclusions - Studies address motivation using several viewpoints and approaches and, even though the number of researches increased in this area, the overall understanding of what actually motivates software engineers does not seem to have significantly advanced in the last five years.",c31,International Conference on Evaluation & Assessment in Software Engineering,cp31,accepted,f547,2022,2022-11-05
s678,p678,A Mapping Study on Requirements Engineering in Agile Software Development,"Agile software development (ASD) methods have gained popularity in the industry and been the subject of an increasing amount of academic research. Although requirements engineering (RE) in ASD has been studied, the overall understanding of RE in ASD as a phenomenon is still weak. We conducted a mapping study of RE in ASD to review the scientific literature. 28 articles on the topic were identified and analyzed. The results indicate that the definition of agile RE is vague. The proposed benefits from agile RE included lower process overheads, a better requirements understanding, a reduced tendency to over allocate development resources, responsiveness to change, rapid delivery of value, and improved customer relationships. The problematic areas of agile RE were the use of customer representatives, the user story requirements format, the prioritization of requirements, growing technical debt, tacit requirements knowledge, and imprecise effort estimation. We also report proposed solutions to the identified problems.",c35,EUROMICRO Conference on Software Engineering and Advanced Applications,cp35,accepted,f548,2005,2005-04-02
s681,p681,The role of non-exact replications in software engineering experiments,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f549,2004,2004-05-22
s683,p683,Toward sustainable software engineering: NIER track,"Current software engineering practices have significant effects on the environment. Examples include e-waste from computers made obsolete due to software upgrades, and changes in the power demands of new versions of software. Sustainable software engineering aims to create reliable, long-lasting software that meets the needs of users while reducing environmental impacts. We conducted three related research efforts to explore this area. First, we investigated the extent to which users thought about the environmental impact of their software usage. Second, we created a tool called GreenTracker, which measures the energy consumption of software in order to raise awareness about the environmental impact of software usage. Finally, we explored the indirect environmental effects of software in order to understand how software affects sustainability beyond its own power consumption. The relationship between environmental sustainability and software engineering is complex; understanding both direct and indirect effects is critical to helping humans live more sustainably.",c29,International Conference on Software Engineering,cp29,accepted,f550,2015,2015-11-20
s684,p684,Research synthesis in software engineering: A tertiary study,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f551,2010,2010-06-11
s685,p685,Software engineering issues for mobile application development,"This paper provides an overview of important software engineering research issues related to the development of applications that run on mobile devices. Among the topics are development processes, tools, user interface design, application portability, quality, and security.",c81,IEEE Annual Symposium on Foundations of Computer Science,cp81,accepted,f552,2004,2004-12-27
s686,p686,Six years of systematic literature reviews in software engineering: An updated tertiary study,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f553,2010,2010-09-14
s687,p687,Requirements Engineering: Foundation for Software Quality,Abstract content goes here ...,j75,Lecture Notes in Computer Science,jv75,accepted,f554,2015,2015-07-08
s688,p688,Bridging metamodels and ontologies in software engineering,Abstract content goes here ...,j145,Journal of Systems and Software,jv145,accepted,f555,2010,2010-01-22
s689,p689,Software engineering / Ian Sommerville.,Abstract content goes here ...,c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f556,2011,2011-10-31
s690,p690,A Methodology for Collecting Valid Software Engineering Data,"An effective data collection method for evaluating software development methodologies and for studying the software development process is described. The method uses goal-directed data collection to evaluate methodologies with respect to the claims made for them. Such claims are used as a basis for defining the goals of the data collection, establishing a list of questions of interest to be answered by data analysis, defining a set of data categorization schemes, and designing a data collection form. The data to be collected are based on the changes made to the software during development, and are obtained when the changes are made. To ensure accuracy of the data, validation is performed concurrently with software development and data collection. Validation is based on interviews with those people supplying the data. Results from using the methodology show that data validation is a necessary part of change data collection. Without it, as much as 50 percent of the data may be erroneous. Feasibility of the data collection methodology was demonstrated by applying it to five different projects in two different environments. The application showed that the methodology was both feasible and useful.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f557,2014,2014-09-27
s691,p691,Qualitative research in software engineering,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f558,2004,2004-08-11
s692,p692,Component-Based Software Engineering,Abstract content goes here ...,j75,Lecture Notes in Computer Science,jv75,accepted,f559,2015,2015-01-23
s693,p693,Software Metrics : A Rigorous and Practical Approach,"From the Publisher: 
The Second Edition of Software Metrics provides an up-to-date, coherent, and rigorous framework for controlling, managing, and predicting software development processes. With an emphasis on real-world applications, Fenton and Pfleeger apply basic ideas in measurement theory to quantify software development resources, processes, and products. The book offers an accessible and comprehensive introduction to software metrics, now an essential component of software engineering for both classroom and industry. Software Metrics features extensive case studies from Hewlett Packard, IBM, the U.S. Department of Defense, Motorola, and others, in addition to worked examples and exercises. The Second Edition includes up-to-date material on process maturity and measurement, goal-question-metric, planning a metrics program, measurement in practice, experimentation, empirical studies, ISO9216, and metric tools.",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f560,2017,2017-01-07
s694,p694,Recommendation Systems for Software Engineering,"Software development can be challenging because of the large information spaces that developers must navigate. Without assistance, developers can become bogged down and spend a disproportionate amount of their time seeking information at the expense of other value-producing tasks. Recommendation systems for software engineering (RSSEs) are software tools that can assist developers with a wide range of activities, from reusing code to writing effective bug reports. The authors provide an overview of recommendation systems for software engineering: what they are, what they can do for developers, and what they might do in the future.",j147,IEEE Software,jv147,accepted,f561,2021,2021-12-25
s698,p698,An examination of software engineering work practices,"This paper presents work practice data of the daily activities of software engineers. Four separate studies are presented; one looking longitudinally at an individual SE; two looking at a software engineering group; and one looking at company-wide tool usage statistics. We also discuss the advantages in considering work practices in designing tools for software engineers, and include some requirements for a tool we have developed as a result of our studies.",c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f562,2011,2011-05-14
s699,p699,"Search Based Software Engineering: Techniques, Taxonomy, Tutorial",Abstract content goes here ...,c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f563,2022,2022-09-16
s701,p701,Non-Functional Requirements in Software Engineering,Abstract content goes here ...,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f564,2001,2001-06-09
s702,p702,An examination of software engineering work practices,"This paper presents work practice data of the daily activities of software engineers. Four separate studies are presented; one looking longitudinally at an individual SE; two looking at a software engineering group; and one looking at company-wide tool usage statistics. We also discuss the advantages in considering work practices in designing tools for software engineers, and include some requirements for a tool we have developed as a result of our studies.",c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f565,2011,2011-05-21
s703,p703,"Search Based Software Engineering: Techniques, Taxonomy, Tutorial",Abstract content goes here ...,c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f566,2022,2022-03-08
s704,p704,"Software Engineering: A Practitioner's Approach, 7Th Edition","As recognized, adventure as skillfully as experience about lesson, amusement, as well as deal can be gotten by just checking out a ebook software engineering a practitioner39s approach 7th edition then it is not directly done, you could take even more around this life, concerning the world. We allow you this proper as capably as simple artifice to acquire those all. We find the money for software engineering a practitioner39s approach 7th edition and numerous books collections from fictions to scientific research in any way. in the midst of them is this software engineering a practitioner39s approach 7th edition that can be your partner. Page Url",c77,Networks,cp77,accepted,f567,2019,2019-05-18
s705,p705,Ontologies for Software Engineering and Software Technology,Abstract content goes here ...,c95,IEEE International Conference on Computer Vision,cp95,accepted,f568,2017,2017-02-08
s709,p709,Collaboration Tools for Global Software Engineering,"Software engineering involves people collaborating to develop better software. Collaboration is challenging, especially across time zones and without face-to-face meetings. We therefore use collaboration tools all along the product life cycle to let us work together, stay together, and achieve results together. This article summarizes experiences and trends chosen from recent IEEE International Conference on Global Software Engineering (IGSCE) conferences.",j147,IEEE Software,jv147,accepted,f569,2021,2021-09-24
s710,p710,Managing Software Engineering Knowledge,Abstract content goes here ...,c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f570,2019,2019-12-04
s711,p711,A Comparison Between Five Models Of Software Engineering,"This research deals with a vital and important issue in computer world. It is concerned with the software management processes that examine the area of software development through the development models, which are known as software development life cycle. It represents five of the development models namely, waterfall, Iteration, V-shaped, spiral and Extreme programming. These models have advantages and disadvantages as well. Therefore, the main objective of this research is to represent different models of software development and make a comparison between them to show the features and defects of each model.",c12,International Conference on Statistical and Scientific Database Management,cp12,accepted,f571,2002,2002-06-16
s712,p712,Assessing traceability of software engineering artifacts,Abstract content goes here ...,j157,Requirements Engineering,jv157,accepted,f572,2001,2001-08-14
s714,p714,The Guide to the Software Engineering Body of Knowledge,"Reporting on the SWEBOK project, the authors-who represent the project's editorial team-discuss the three-phase plan to characterize a body of knowledge, a vital step toward developing software engineering as a profession.",j147,IEEE Software,jv147,accepted,f573,2021,2021-05-13
s716,p716,Validity concerns in software engineering research,"Empirical studies that use software repository artifacts have become popular in the last decade due to the ready availability of open source project archives. In this paper, we survey empirical studies in the last three years of ICSE and FSE proceedings, and categorize these studies in terms of open source projects vs. proprietary source projects and the diversity of subject programs used in these studies. Our survey has shown that almost half (49%) of recent empirical studies used solely open source projects. Existing studies either draw general conclusions from these results or explicitly disclaim any conclusions that can extend beyond specific subject software.
 We conclude that researchers in empirical software engineering must consider the external validity concerns that arise from using only several well-known open source software projects, and that discussion of data source selection is an important discussion topic in software engineering research. Furthermore, we propose a community research infrastructure for software repository benchmarks and sharing the empirical analysis results, in order to address external validity concerns and to raise the bar for empirical software engineering research that analyzes software artifacts.",c69,International Conference on Parallel Processing,cp69,accepted,f574,2010,2010-06-11
s717,p717,How Reliable Are Systematic Reviews in Empirical Software Engineering?,"BACKGROUND-The systematic review is becoming a more commonly employed research instrument in empirical software engineering. Before undue reliance is placed on the outcomes of such reviews it would seem useful to consider the robustness of the approach in this particular research context. OBJECTIVE-The aim of this study is to assess the reliability of systematic reviews as a research instrument. In particular, we wish to investigate the consistency of process and the stability of outcomes. METHOD-We compare the results of two independent reviews undertaken with a common research question. RESULTS-The two reviews find similar answers to the research question, although the means of arriving at those answers vary. CONCLUSIONS-In addressing a well-bounded research question, groups of researchers with similar domain experience can arrive at the same review outcomes, even though they may do so in different ways. This provides evidence that, in this context at least, the systematic review is a robust research method.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f575,2014,2014-11-28
s718,p718,Software intelligence: the future of mining software engineering data,"Mining software engineering data has emerged as a successful research direction over the past decade. In this position paper, we advocate Software Intelligence (SI) as the future of mining software engineering data, within modern software engineering research, practice, and education. We coin the name SI as an inspiration from the Business Intelligence (BI) field, which offers concepts and techniques to improve business decision making by using fact-based support systems. Similarly, SI offers software practitioners (not just developers) up-to-date and pertinent information to support their daily decision-making processes. SI should support decision-making processes throughout the lifetime of a software system not just during its development phase.
 The vision of SI has yet to become a reality that would enable software engineering research to have a strong impact on modern software practice. Nevertheless, recent advances in the Mining Software Repositories (MSR) field show great promise and provide strong support for realizing SI in the near future. This position paper summarizes the state of practice and research of SI, and lays out future research directions for mining software engineering data to enable SI.",c100,ACM SIGMOD Conference,cp100,accepted,f576,2010,2010-06-09
s719,p719,Applying empirical software engineering to software architecture: challenges and lessons learned,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f577,2004,2004-12-02
s720,p720,Global Software Engineering: A Software Process Approach,Abstract content goes here ...,c85,International Conference on Graph Transformation,cp85,accepted,f578,2007,2007-01-24
s721,p721,Collaborative Software Engineering: Challenges and Prospects,Abstract content goes here ...,c21,Grid Computing Environments,cp21,accepted,f579,2005,2005-03-20
s722,p722,"Softwares Product Lines, Global Development and Ecosystems: Collaboration in Software Engineering",Abstract content goes here ...,c31,International Conference on Evaluation & Assessment in Software Engineering,cp31,accepted,f580,2022,2022-05-04
s723,p723,Guide to the Software Engineering Body of Knowledge,data types Sorting and searching parallel and distributed algorithms 3. [AR] Computer Architecture,c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,cp86,accepted,f581,2012,2012-04-29
s724,p724,Agent-Oriented Software Engineering: The State of the Art,Abstract content goes here ...,c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f582,2015,2015-01-26
s725,p725,"Object-Oriented Software Engineering Using UML, Patterns, and Java","This widely used book teaches practical object-oriented software engineering with the key real world tools UML, design patterns and Java. This step-by-step approach allows the reader to address complex and changing problems with practical and state-of-the-art solutions. This book uses examples from real systems and examines the interaction between such techniques as UML, Java-based technologies, design patterns, rationale, configuration management, and quality control. It also discusses project management related issues and their impacts. A valuable book for development engineers, software engineers, consulting engineers, software architects, product managers, project leaders, and knowledge managers.",c49,International Symposium on Search Based Software Engineering,cp49,accepted,f583,2012,2012-01-08
s726,p726,Curating GitHub for engineered software projects,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f584,2004,2004-02-11
s727,p727,Component-Based Software Engineering: Putting the Pieces Together,Abstract content goes here ...,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f585,2019,2019-06-26
s728,p728,Software engineering in an uncertain world,"In this paper, we argue that the reality of today's software systems requires us to consider uncertainty as a first-class concern in the design, implementation, and deployment of those systems. We further argue that this induces a paradigm shift, and a number of research challenges that must be addressed.",c9,Pacific Symposium on Biocomputing,cp9,accepted,f586,2009,2009-02-10
s729,p729,Evidence-based software engineering,"Our objective is to describe how software engineering might benefit from an evidence-based approach and to identify the potential difficulties associated with the approach. We compared the organisation and technical infrastructure supporting evidence-based medicine (EBM) with the situation in software engineering. We considered the impact that factors peculiar to software engineering (i.e. the skill factor and the lifecycle factor) would have on our ability to practice evidence-based software engineering (EBSE). EBSE promises a number of benefits by encouraging integration of research results with a view to supporting the needs of many different stakeholder groups. However, we do not currently have the infrastructure needed for widespread adoption of EBSE. The skill factor means software engineering experiments are vulnerable to subject and experimenter bias. The lifecycle factor means it is difficult to determine how technologies will behave once deployed. Software engineering would benefit from adopting what it can of the evidence approach provided that it deals with the specific problems that arise from the nature of software engineering.",c94,Vision,cp94,accepted,f587,2020,2020-05-09
s730,p730,Social media for software engineering,"Social media has changed the way that people collaborate and share information. In this paper, we highlight its impact for enabling new ways for software teams to form and work together. Individuals will self-organize within and across organizational boundaries. Grassroots software development communities will emerge centered around new technologies, common processes and attractive target markets. Companies consisting of lone individuals will able to leverage social media to conceive of, design, develop, and deploy successful and profitable product lines. A challenge for researchers who are interested in studying, influencing, and supporting this shift in software teaming is to make sure that their research methods protect the privacy and reputation of their stakeholders.",c97,Interspeech,cp97,accepted,f588,2004,2004-09-27
s733,p733,Handbook of software reliability engineering,Technical foundations introduction software reliability and system reliability the operational profile software reliability modelling survey model evaluation and recalibration techniques practices and experiences best current practice of SRE software reliability measurement experience measurement-based analysis of software reliability software fault and failure classification techniques trend analysis in validation and maintenance software reliability and field data analysis software reliability process assessment emerging techniques software reliability prediction metrics software reliability and testing fault-tolerant SRE software reliability using fault trees software reliability process simulation neural networks and software reliability. Appendices: software reliability tools software failure data set repository.,c69,International Conference on Parallel Processing,cp69,accepted,f589,2010,2010-12-27
s735,p735,Guide to Advanced Empirical Software Engineering,Abstract content goes here ...,c15,International Conference on Conceptual Structures,cp15,accepted,f590,2011,2011-06-24
s736,p736,Software Engineering for Self-Adaptive Systems [outcome of a Dagstuhl Seminar],Abstract content goes here ...,c26,PS,cp26,accepted,f591,2010,2010-11-07
s737,p737,The mythical man-month - essays on software engineering (2. ed.),"1. The Tar Pit. 2. The Mythical Man-Month. 3. The Surgical Team. 4. Aristocracy, Democracy, and System Design. 5. The Second-System Effect. 6. Passing the Word. 7. Why Did the Tower of Babel Fail? 8. Calling the Shot. 9. Ten Pounds in a Five-Pound Sack. 10. The Documentary Hypothesis. 11. Plan to Throw One Away. 12. Sharp Tools. 13. The Whole and the Parts. 14. Hatching a Castrophe. 15. The Other Face. 16. No Silver Bullet -- Essence and Accident. 17. ""No Silver Bullet"" ReFired. 18. Propositions of The Mythical Man-Month: True or False? 19. The Mythical Man-Month After 20 Years. Epilogue. Notes and references. Index. 0201835959T04062001",c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f592,2006,2006-10-07
s738,p738,Data Mining for Software Engineering,"To improve software productivity and quality, software engineers are increasingly applying data mining algorithms to various software engineering tasks. However, mining SE data poses several challenges. The authors present various algorithms to effectively mine sequences, graphs, and text from such data.",j79,Computer,jv79,accepted,f593,2014,2014-09-10
s739,p739,A software engineering approach to ontology building,Abstract content goes here ...,j159,Information Systems,jv159,accepted,f594,2006,2006-10-19
s740,p740,"Model-driven software development - technology, engineering, management","Part I: Introduction. 1. Introduction. 2. MDSD - Basic Ideas and Terminology. 3. Case Study: A Typical Web Application. 4. Concept Formation. 5. Classification. Part II: Domain Architectures. 6. Metamodeling. 7. MDSD-Capable Target Architectures. 8. Building Domain Architectures. 9. Code Generation Techniques. 10. Model Transformation Techniques. 11. MDSD Tools: Roles, Architecture, Selection Criteria, and Pointers. 12. The MDA Standard. Part III: Processes and Engineering. 13. MDSD Process Building Blocks and Best Practices. 14. Testing. 15. Versioning. 16. Case Study: Embedded Component Infrastructures. 17. Case Study: An Enterprise System. Part IV: Management. 18. Decision Support. 1.9 Organizational Aspects. 20. Adoption Strategies for MDSD. References. Index.",c109,International Conference on Mobile Data Management,cp109,accepted,f595,2014,2014-06-08
s741,p741,Software Engineering Challenges in Game Development,"In Software Engineering (SE), video game development is unique yet similar to other software endeavors. It is unique in that it combines the work of teams covering multiple disciplines (art, music, acting, programming, etc.), and that engaging game play is sought after through the use of prototypes and iterations. With that, game development is faced with challenges that can be addressed using traditional SE practices. The industry needs to adopt sound SE practices for their distinct needs such as managing multimedia assets and finding the “fun” in game play. The industry must take on the challenges by evolving SE methods to meet their needs. This work investigates these challenges and highlights engineering practices to mitigate these challenges.",c88,Symposium on the Theory of Computing,cp88,accepted,f596,2014,2014-09-13
s742,p742,Software Engineering Best Practices,"Proven techniques for software development success 
 
In this practical guide, software-quality guru Capers Jones reveals best practices for ensuring software development success by illustrating the engineering methods used by the most successful large software projects at leading companies such as IBM, Microsoft, Sony, and EDS. 
 
Software Engineering Best Practices covers estimating and planning; requirements analysis; change control; quality control; progress and cost tracking; and maintenance and support after delivery. Agile development, extreme programming, joint application design (JAD), six-sigma for software, and other methods are discussed. 
 
Table of contents 
Chapter 1. Introduction and Definitions of Software Best Practices;Chapter 2. Overview of 50 Software Best Practices;Chapter 3. A Preview of Software Development and Maintenance in 2049;Chapter 4. How Software Personnel Learn New Skills;Chapter 5. Software Team Organization and Specialization;Chapter 6. Project Management and Software Engineering;Chapter 7. Requirements, Business Analysis, Architecture, Enterprise Architecture, and Design;Chapter 8. Programming and Code Development;Chapter 9. Software Quality: The Key to Successful Software Engineering;Index",c61,Jahrestagung der Gesellschaft für Informatik,cp61,accepted,f597,2017,2017-06-04
s743,p743,Systematic literature reviews in software engineering: Preliminary results from interviews with researchers,"Systematic Literature Reviews (SLRs) have been gaining significant attention from software engineering researchers since 2004. Several researchers have reported their experiences of and lessons learned from applying systematic reviews to different subject matters in software engineering. However, there has been no attempt at independently exploring experiences and perceptions of the practitioners of systematic reviews in order to gain an in-depth understanding of various aspects of systemic reviews as a new research methodology in software engineering. We assert that there is a need of evidence-based body of knowledge about the application of systematic reviews in software engineering. To address this need, we have started an empirical research program that aims to contribute to the growing body of knowledge about systematic reviews in software engineering. This paper reports the design, logistics, and results of the first phase empirical study carried out in this program. The results provide interesting insights into different aspects of systematic reviews based on the analysis of the data gathered from 17 interviewees with varying levels of knowledge of and experiences in systematic reviews. The findings from this study are expected to contribute to the existing knowledge about using systematic reviews and help further improve the state-of-the-practice of this research methodology in software engineering.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f598,2010,2010-10-13
s745,p745,Development of a Software Engineering Ontology for Multisite Software Development,"This paper aims to present an ontology model of software engineering to represent its knowledge. The fundamental knowledge relating to software engineering is well described in the textbook entitled Software Engineering by Sommerville that is now in its eighth edition (2004) and the white paper, Software Engineering Body of Knowledge (SWEBOK), by the IEEE (203) upon which software engineering ontology is based. This paper gives an analysis of what software engineering ontology is, what it consists of, and what it is used for in the form of usage example scenarios. The usage scenarios presented in this paper highlight the characteristics of the software engineering ontology. The software engineering ontology assists in defining information for the exchange of semantic project information and is used as a communication framework. Its users are software engineers sharing domain knowledge as well as instance knowledge of software engineering.",j1,IEEE Transactions on Knowledge and Data Engineering,jv1,accepted,f599,2020,2020-06-26
s746,p746,Action research use in software engineering: An initial survey,"This paper presents a literature survey of action research (AR) studies published in nine major Software Engineering (SE) journals and three conference proceedings in the period 1993 to June 2009. A strict selection based on distinguishing SE from Information Systems research has identified 16 papers. Although they represent a very small fraction of the studies being conducted in SE, such papers concern with different SE contexts allowing to get information about the increasing tendency in the AR use in software engineering. However, as shown by the initial results, SE researchers should invest more on rigor when defining, applying and reporting AR studies inSE.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f600,2010,2010-08-06
s747,p747,What Do We Know about Knowledge Management? Practical Implications for Software Engineering,"There have been many claims about knowledge management's benefits in software engineering, such as decreased time and cost for development, increased quality, and better decision-making abilities. Although we can find some success stories illustrating these claims, particularly on aspects related to the systems and engineering schools, more research is necessary to explore the intersection between each school and the software engineering field. Researchers should continue to emphasize the need for a broad focus across multiple KM schools to suceed in improving KM's practical application in software engineering.",j147,IEEE Software,jv147,accepted,f601,2021,2021-08-05
s748,p748,Ontology Classification for Semantic-Web-Based Software Engineering,"The semantic Web is the second generation of the Web, which helps sharing and reusing data across application, enterprise, and community boundaries. Ontology defines a set of representational primitives with which a domain of knowledge is modeled. The main purpose of the semantic Web and ontology is to integrate heterogeneous data and enable interoperability among disparate systems. Ontology has been used to model software engineering knowledge by denoting the artifacts that are designed or produced during the engineering process. The semantic Web allows publishing reusable software engineering knowledge resources and providing services for searching and querying. This paper classifies the ontologies developed for software engineering, reviews the current efforts on applying the semantic Web techniques on different software engineering aspects, and presents the benefits of their applications. We also foresee the possible future research directions.",j69,IEEE Transactions on Services Computing,jv69,accepted,f602,2002,2002-07-15
s749,p749,Context in industrial software engineering research,"In order to draw valid conclusions when aggregating evidence it is important to describe the context in which industrial studies were conducted. This paper structures the context for empirical industrial studies and provides a checklist. The aim is to aid researchers in making informed decisions concerning which parts of the context to include in the descriptions. Furthermore, descriptions of industrial studies were surveyed.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f603,2010,2010-06-30
s750,p750,Software Engineering for Spreadsheets,The idiosyncratic structure of spreadsheets allows the adaptation of proven software engineering principles to an end-user domain and thus makes software engineering accessible to many users.,j147,IEEE Software,jv147,accepted,f604,2021,2021-09-15
s751,p751,Global Software Engineering: The Future of Socio-technical Coordination,"Globally-distributed projects are rapidly becoming the norm for large software systems, even as it becomes clear that global distribution of a project seriously impairs critical coordination mechanisms. In this paper, I describe a desired future for global development and the problems that stand in the way of achieving that vision. I review research and lay out research challenges in four critical areas: software architecture, eliciting and communicating requirements, environments and tools, and orchestrating global development. I conclude by noting the need for a systematic understanding of what drives the need to coordinate and effective mechanisms for bringing it about.",c76,International Conference on Artificial Neural Networks,cp76,accepted,f605,2013,2013-05-18
s752,p752,"On ""Software engineering""","Software engineers work on multidisciplinary teams to identify and develop software solutions and to maintain software intensive systems of all sizes. The focus of this program is on the rigorous engineering practices necessary to build, maintain, and protect modern software intensive systems. Consistent with this focus, the software engineering baccalaureate program consists of a rigorous curriculum of science, math, computer science, and software engineering courses.",c77,Networks,cp77,accepted,f606,2019,2019-03-12
s753,p753,The Current State and Future of Search Based Software Engineering,"This paper describes work on the application of optimization techniques in software engineering. These optimization techniques come from the operations research and metaheuristic computation research communities. The paper briefly reviews widely used optimization techniques and the key ingredients required for their successful application to software engineering, providing an overview of existing results in eight software engineering application domains. The paper also describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of open problems, challenges and areas for future work.",c64,Experimental Software Engineering Network,cp64,accepted,f607,2014,2014-03-25
s754,p754,Trends in Embedded Software Engineering,"Software's importance in the development of embedded systems has been growing rapidly over the last 20 years. Because of current embedded systems' complexity, they require sophisticated engineering methods for systematically developing high-quality software. Embedded software development differs from IT system development in several ways. For example, IT systems developers can use standard hardware and software platforms and don't face the resource requirements that embedded systems developers must take into account. To meet embedded software's extrafunctional requirements, embedded systems development is shifting from programming to model-driven development. Another important trend is the emphasis on the quality assurance of safety-related systems.",j147,IEEE Software,jv147,accepted,f608,2021,2021-08-09
s755,p755,Motivation in Software Engineering: A systematic literature review,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f609,2010,2010-05-07
s756,p756,Models of motivation in software engineering,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f610,2010,2010-01-21
s757,p757,Experience and Knowledge Management in Software Engineering,Abstract content goes here ...,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f611,2005,2005-04-13
s758,p758,ERP Customization as Software Engineering: Knowledge Sharing and Cooperation,"Enterprise resource planning (ERP) vendors provide multiple configuration possibilities ranging from module selection to master data provision to steer access rights for different users. These configuration possibilities cover anticipated variability. If the customer requires adaptation beyond what's anticipated, the source code of the product must be adapted. Customizations in this article's context are source code based adaptations of software products. The size and complexity of customizations range from simple report generation to developing independent add-ons that support specific businesses, for example, solutions for flight carriers. The size and lead time of such projects can compare to a full-size software development project. Enterprise resource planning (ERP) systems must be configured and customized to fit a specific company. The authors discuss cooperation with regard to ERP systems customization.",j147,IEEE Software,jv147,accepted,f612,2021,2021-04-22
s759,p759,Context and Adaptivity in Pervasive Computing Environments: Links with Software Engineering and Ontological Engineering,"In this article we present a review of selected literature of context-aware pervasive computing while integrating theory and practice from various disciplines in order to construct a theoretical grounding and a technical follow-up path for our future research. This paper is not meant to provide an extensive review of the literature, but rather to integrate and extend fundamental and promising theoretical and technical aspects found in the literature. Our purpose is to use the constructed theory and practice in order to enable anywhere and anytime adaptive e-learning environments. We particularly elaborate on context, adaptivity, context-aware systems, ontologies and software development issues. Furthermore, we represent our view point for context-aware pervasive application development particularly based on higher abstraction where ontologies and semantic web activities, also web itself, are of crucial.",j160,Journal of Software,jv160,accepted,f613,2022,2022-07-07
s760,p760,Using the inverted classroom to teach software engineering,"An inverted classroom is a teaching environment that mixes the use of technology with hands-on activities. In an inverted classroom, typical in-class lecture time is replaced with laboratory and in-class activities. Outside class time, lectures are delivered over some other medium such as video on-demand. In a three credit hour course for instance, contact hours are spent having students actively engaged in learning activities. Outside of class, students are focused on viewing 3-6 hours of lectures per week. Additional time outside of class is spent completing learning activities. In this paper we present the inverted classroom model in the context of a software engineering curriculum. The paper motivates the use of the inverted classroom and suggests how different courses from the Software Engineering 2004 Model Curriculum Volume can incorporate the use of the inverted classroom. In addition, we present the results of a pilot course that utilized the inverted classroom model at Miami University and describe courses that are currently in process of piloting its use.",c59,British Computer Society Conference on Human-Computer Interaction,cp59,accepted,f614,2019,2019-06-25
s761,p761,Sentiment Polarity Detection for Software Development,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f615,2004,2004-01-06
s762,p762,Progress on approaches to software defect prediction,"Software defect prediction is one of the most popular research topics in software engineering. It aims to predict defect-prone software modules before defects are discovered, therefore it can be used to better prioritise software quality assurance effort. In recent years, especially for recent 3 years, many new defect prediction studies have been proposed. The goal of this study is to comprehensively review, analyse and discuss the state-of-the-art of defect prediction. The authors survey almost 70 representative defect prediction papers in recent years (January 2014-April 2017), most of which are published in the prominent software engineering journals and top conferences. The selected defect prediction papers are summarised to four aspects: machine learning-based prediction algorithms, manipulating the data, effort-aware prediction and empirical studies. The research community is still facing a number of challenges for building methods and many research opportunities exist. The identified challenges can give some practical guidelines for both software engineering researchers and practitioners in future software defect prediction.",j161,IET Software,jv161,accepted,f616,2013,2013-04-17
s763,p763,\{PROMISE\} Repository of empirical software engineering data,Abstract content goes here ...,c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f617,2008,2008-05-25
s764,p764,Usability engineering,Abstract content goes here ...,c65,Formal Concept Analysis,cp65,accepted,f618,2008,2008-03-04
s765,p765,Managing the software process,Foreword. Preface. I. SOFTWARE PROCESS MATURITY. A Software Maturity Framework. The Principles of Software Process Change. Software Process Assessment. The Initial Process. II. THE REPEATABLE PROCESS. Managing Software Organizations. The Project Plan. Software Configuration Management-Part 1: Software Quality Assurance. III. THE DEFINED PROCESS. Software Standards. Software Inspections. Software Testing. Software Configuration Management (Continued). Defining the Software Process. The Software Engineering Process Group IV. THE MANAGED PROCESS. Data Gathering and Analysis. Managing Software Quality. V. THE OPTIMIZING PROCESS. Defect Prevention. Automating The Software Process. Contracting for Software. Conclusion. Appendices. Index. 0201180952T04062001,c111,International Society for Music Information Retrieval Conference,cp111,accepted,f619,2001,2001-08-02
s766,p766,Software agents,"The software world is one of great richness and diversity. Many thousands of software products are available to users today, providing a wide variety of information and services in a wide variety of domains. While most of these programs provide their users with significant value when used in isolation, there is increasing demand for programs that can interoperate – to exchange information and services with other programs and thereby solve problems that cannot be solved alone. Part of what makes interoperation difficult is heterogeneity. Programs are written by different people, at different times, in different languages; and, as a result, they often provide different interfaces. The difficulties created by heterogeneity are exacerbated by dynamics in the software environment. Programs are frequently rewritten; new programs are added; old programs removed. Agent-based software engineering was invented to facilitate the creation of software able to interoperate in such settings. In this approach to software development, application programs are written as software agents, i.e. software “components” that communicate with their peers by exchanging messages in an expressive agent communication language. Agents can be as simple as subroutines; but typically they are larger entities with some sort of persistent control (e.g. distinct control threads within a single address space, distinct processes on a single machine, or separate processes on different machines). The salient feature of the language used by agents is its expressiveness. It allows for the exchange of data and logical information, individual commands and scripts (i.e. programs). Using this language, agents can communicate complex information and goals, directly or indirectly “programming” each other in useful ways. Agent-based software engineering is often compared to object-oriented programming. Like an “object”, an agent provides a message-based interface independent of its internal data structures and algorithms. The primary difference between the two approaches lies in the language of the interface. In general object-oriented programming, the meaning of a message can vary from one object to another. In agent-based software engineering, agents use a common language with an agent-independent semantics. The concept of agent-based software engineering raises a number of important questions.",c109,International Conference on Mobile Data Management,cp109,accepted,f620,2014,2014-08-20
s767,p767,Property-Based Software Engineering Measurement,"Little theory exists in the field of software system measurement. Concepts such as complexity, coupling, cohesion or even size are very often subject to interpretation and appear to have inconsistent definitions in the literature. As a consequence, there is little guidance provided to the analyst attempting to define proper measures for specific problems. Many controversies in the literature are simply misunderstandings and stem from the fact that some people talk about different measurement concepts under the same label (complexity is the most common case). There is a need to define unambiguously the most important measurement concepts used in the measurement of software products. One way of doing so is to define precisely what mathematical properties characterize these concepts, regardless of the specific software artifacts to which these concepts are applied. Such a mathematical framework could generate a consensus in the software engineering community and provide a means for better communication among researchers, better guidelines for analysts, and better evaluation methods for commercial static analyzers for practitioners. We propose a mathematical framework which is generic, because it is not specific to any particular software artifact, and rigorous, because it is based on precise mathematical concepts. We use this framework to propose definitions of several important measurement concepts (size, length, complexity, cohesion, coupling). It does not intend to be complete or fully objective; other frameworks could have been proposed and different choices could have been made. However, we believe that the formalisms and properties we introduce are convenient and intuitive. This framework contributes constructively to a firmer theoretical ground of software measurement.",c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f621,2014,2014-05-04
s768,p768,Reporting Experiments in Software Engineering,Abstract content goes here ...,c75,International Conference on Machine Learning,cp75,accepted,f622,2005,2005-02-10
s769,p769,Using Mapping Studies in Software Engineering,"Background: A mapping study provides a systematic and objective procedure for identifying the nature and extent of the empirical study data that is available to answer a particular research question. Such studies can also form a useful preliminary step for PhD study. Aim: We set out to assess how effective such studies have been when used for software engineering topics, and to identify the specific challenges that they present. Method: We have conducted an informal review of a number of mapping studies in software engineering, describing their main characteristics and the forms of analysis employed. Results: We examine the experiences and outcomes from six mapping studies, of which four are published. From these we note a recurring theme about the problems of classification and a preponderance of ‘gaps’ in the set of empirical studies. Conclusions: We identify our challenges as improving classification guidelines, encouraging better reporting of primary studies, and argue for identifying some ’empirical grand challenges’ for software engineering as a focus for the community.",c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f623,2014,2014-04-03
s770,p770,The role of replications in Empirical Software Engineering,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f624,2004,2004-01-29
s772,p772,Strength of evidence in systematic reviews in software engineering,"Systematic reviews are only as good as the evidence they are based on. It is important, therefore, that users of systematic reviews know how much confidence they can place in the conclusions and recommendations arising from such reviews. In this paper we present an overview of some of the most influential systems for assessing the quality of individual primary studies and for grading the overall strength of a body of evidence. We also present an example of the use of such systems based on a systematic review of empirical studies of agile software development. Our findings suggest that the systems used in other disciplines for grading the strength of evidence for and reporting of systematic reviews, especially those that take account of qualitative and observational studies are of particular relevance for software engineering.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f625,2010,2010-06-04
s773,p773,The Rise and Evolution of Agile Software Development,"Agile software development has dominated the second half of the past 50 years of software engineering. Retrospectives, one of the most common agile practices, enables reflection on past performance, discussion of current progress, and charting forth directions for future improvement. Because of agile’s burgeoning popularity as the software development model of choice and a significant research subdomain of software engineering, it demands a retrospective of its own. This article provides a historical overview of agile’s main focus areas and a holistic synthesis of its trends, their evolution over the past two decades, agile’s current status, and, forecast from these, agile’s likely future. This article is part of a theme issue on software engineering’s 50th anniversary.",j147,IEEE Software,jv147,accepted,f626,2021,2021-06-11
s774,p774,A survey of controlled experiments in software engineering,"The classical method for identifying cause-effect relationships is to conduct controlled experiments. This paper reports upon the present state of how controlled experiments in software engineering are conducted and the extent to which relevant information is reported. Among the 5,453 scientific articles published in 12 leading software engineering journals and conferences in the decade from 1993 to 2002, 103 articles (1.9 percent) reported controlled experiments in which individuals or teams performed one or more software engineering tasks. This survey quantitatively characterizes the topics of the experiments and their subjects (number of subjects, students versus professionals, recruitment, and rewards for participation), tasks (type of task, duration, and type and size of application) and environments (location, development tools). Furthermore, the survey reports on how internal and external validity is addressed and the extent to which experiments are replicated. The gathered data reflects the relevance of software engineering experiments to industrial practice and the scientific maturity of software engineering research.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f627,2014,2014-01-16
s775,p775,Software engineering: theory and practice,Abstract content goes here ...,c100,ACM SIGMOD Conference,cp100,accepted,f628,2010,2010-11-25
s776,p776,A Software Engineering Lifecycle Standard for Very Small Enterprises,Abstract content goes here ...,c56,European Conference on Software Process Improvement,cp56,accepted,f629,2016,2016-09-16
s777,p777,08031 -- Software Engineering for Self-Adaptive Systems: A Research Road Map,"Software's ability to adapt at run-time to changing user needs, system intrusions or faults, changing operational environment, and resource variability has been proposed as a means to cope with the complexity of today's software-intensive systems. Such self-adaptive systems can configure and reconfigure themselves, augment their functionality, continually optimize themselves, protect themselves, and recover themselves, while keeping most of their complexity hidden from the user and administrator. In this paper, we present research road map for software engineering of self-adaptive systems focusing on four views, which we identify as essential: requirements, modelling, engineering, and assurances.",c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f630,2019,2019-07-13
s778,p778,A Brief History of Software Engineering,"This personal perspective on the art of programming begins with a look at the state of programming from about 1960, and it follows programming's development through the present day. The article examines key contributions to the field of software engineering and identifies major obstacles, which persist even today.",j152,IEEE Annals of the History of Computing,jv152,accepted,f631,2016,2016-05-18
s779,p779,Problem Oriented Software Engineering: Solving the Package Router Control Problem,"Problem orientation is gaining interest as a way of approaching the development of software intensive systems, and yet, a significant example that explores its use is missing from the literature. In this paper, we present the basic elements of Problem Oriented Software Engineering (POSE), which aims at bringing both nonformal and formal aspects of software development together in a single framework. We provide an example of a detailed and systematic POSE development of a software problem: that of designing the controller for a package router. The problem is drawn from the literature, but the analysis presented here is new. The aim of the example is twofold: to illustrate the main aspects of POSE and how it supports software engineering design and to demonstrate how a nontrivial problem can be dealt with by the approach.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f632,2014,2014-01-02
s781,p781,Towards individualized software engineering: empirical studies should collect psychometrics,"Even though software is developed by humans, research in software engineering primarily focuses on the technologies, methods and processes they use while disregarding the importance of the humans themselves. In this paper we argue that most studies in software engineering should give much more weight to human factors. In particular empirical software engineering studies involving human developers should always consider collecting psychometric data on the humans involved. We focus on personality as one important psychometric factor and present initial results from an empirical study investigating correlations between personality and attitudes to software engineering processes and tools. We discuss what are currently hindering a more wide-spread use of psychometrics and how overcoming these hurdles could lead to a more individualized software engineering.",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f633,2005,2005-03-21
s782,p782,A survey of social software engineering,"Software engineering is a complex socio-technical activity, due to the need for discussing and sharing knowledge among team members. This has raised the need for effective ways of sharing ideas, knowledge, and artifacts among groups and their members. The social aspect of software engineering process also demands computer support to facilitate the development by means of collaborative tools, applications and environments. In this paper, we present a survey of relevant works from psychology, mathematics and computer science studies. The combination of these fields provides the required infrastructure for engineering social and collaborative applications as well as the software engineering process. We also discuss possible solutions for the encountered shortcomings, and how they can improve software development.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f634,2011,2011-01-06
s783,p783,Metamodelling for software engineering,"Interestingly, metamodelling for software engineering that you really wait for now is coming. It's significant to wait for the representative and beneficial books to read. Every book that is provided in better way and utterance will be expected by many peoples. Even you are a good reader or not, feeling to read this book will always appear when you find it. But, when you feel hard to find it as yours, what to do? Borrow to your friends and don't know when to give back it to her or him.",c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,cp79,accepted,f635,2020,2020-01-22
s784,p784,An Environment to Support Large Scale Experimentation in Software Engineering,"Experimental studies have been used as a mechanism to acquire knowledge through a scientific approach based on measurement of phenomena in different areas. However it is hard to run such studies when they require models (simulation), produce amount of information, and explore science in scale. In this case, a computerized infrastructure is necessary and constitutes a complex system to be built. In this paper we discuss an experimentation environment that has being built to support large scale experimentation and scientific knowledge management in software engineering.",c57,IEEE International Conference on Engineering of Complex Computer Systems,cp57,accepted,f636,2003,2003-04-05
s785,p785,Missing Data in Software Engineering,Abstract content goes here ...,c106,Chinese Conference on Biometric Recognition,cp106,accepted,f637,2016,2016-11-17
s786,p786,Agile Software Engineering,Abstract content goes here ...,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f638,2019,2019-08-10
s787,p787,Evaluating guidelines for reporting empirical software engineering studies,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f639,2004,2004-03-08
s788,p788,Rationale-Based Software Engineering,Abstract content goes here ...,c43,ACM Symposium on Applied Computing,cp43,accepted,f640,2001,2001-03-22
s790,p790,Knowledge management in software engineering,"Software organizations' main assets are not plants, buildings, or expensive machines. A software organization's main asset is its intellectual capital, as it is in sectors such asconsulting, law, investment banking, and advertising. The major problem with intellectual capital is that it has legs and walks home every day. At the same rate experience walks out the door, inexperience walks in the door. Whether or not many software organizations admit it, they face the challenge ofsustaining the level of competence needed to win contracts and fulfill undertakings.",j147,IEEE Software,jv147,accepted,f641,2021,2021-05-16
s791,p791,Ontology-Based Software Engineering- Software Engineering 2.0,"This paper describes the use of ontologies in different aspects of software engineering. This use of ontologies varies from support for software developers at multiple sites to the use of an ontology to provide semantics in different categories of software, particularly on the Web. The world's first and only software engineering ontology and a project management ontology in conjunction with a domain ontology are used to provide support for software development that is taking place at multiple sites. Ontologies are used to provide semantics to deal with heterogeneity in the representation of multiple information sources, enable the selection and composition of web services and grid resources, provide the shared knowledge base for multiagent systems, provide semantics and structure for trust and reputation systems and privacy based systems and codification of shared knowledge within different domains in business, science, manufacturing, engineering and utilities. They, therefore, bring a new paradigm to software engineering through the use of semantics as a central mechanism which will revolutionize the way software is developed and consumed in the future leading to the development of software as a service bringing about the dawn of software engineering 2.0.",c58,Australian Software Engineering Conference,cp58,accepted,f642,2021,2021-09-21
s793,p793,The Future of Empirical Methods in Software Engineering Research,"We present the vision that for all fields of software engineering (SE), empirical research methods should enable the development of scientific knowledge about how useful different SE technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new SE technology and is a major input to important SE decisions in industry. Major challenges to the pursuit of this vision are: more SE research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research.",c44,International Workshop on Green and Sustainable Software,cp44,accepted,f643,2008,2008-05-18
s794,p794,The Emerging Role of Data Scientists on Software Development Teams,"Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.",c29,International Conference on Software Engineering,cp29,accepted,f644,2015,2015-08-24
s796,p796,The role of replications in empirical software engineering—a word of warning,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f645,2004,2004-05-16
s797,p797,The Focus Group Method as an Empirical Tool in Software Engineering,Abstract content goes here ...,c80,International Conference on Learning Representations,cp80,accepted,f646,2005,2005-06-14
s800,p800,Thermodynamics : An Engineering Approach,"Basic concepts of thermodynamics properties of pure substances the first law of thermodynamics - closed systems, control volumes the second law of thermodynamics entropy - a measure of disorder energy - a measure of work potential gas power cycles vapour and combined power cycles refrigeration cycles thermodynamics property gas mixtures gas vapour mixtures and air conditioning chemical reactions chemical and phase equilibrium thermodynamics of high-speed fluid flow property tables and charts (SI units, English units) about the software.",c17,International Conference on Enterprise Information Systems,cp17,accepted,f647,2008,2008-03-31
s801,p801,Software Engineering Foundations: A Software Science Perspective,"To deal with the difficulties inherent in large-scale software development, the foundations of software engineering are yet to be explored. This comprehensive text is the first book to cover the theoretical and empirical foundations of software engineering. It provides a framework of software engineering methodologies and covers a wide range of foundations such as philosophy, informatics, and engineering economics. Self-contained and requiring only basic programming experience, this book is filled with in-depth comments, annotated references, real-world problems and heuristic questions. Software Engineering Foundations is an important book for software engineers and students alike.",c105,Biometrics and Identity Management,cp105,accepted,f648,2006,2006-03-14
s802,p802,A Systematic Review of Theory Use in Software Engineering Experiments,"Empirically based theories are generally perceived as foundational to science. However, in many disciplines, the nature, role and even the necessity of theories remain matters for debate, particularly in young or practical disciplines such as software engineering. This article reports a systematic review of the explicit use of theory in a comprehensive set of 103 articles reporting experiments, from of a total of 5,453 articles published in major software engineering journals and conferences in the decade 1993-2002. Of the 103 articles, 24 use a total of 40 theories in various ways to explain the cause-effect relationship(s) under investigation. The majority of these use theory in the experimental design to justify research questions and hypotheses, some use theory to provide post hoc explanations of their results, and a few test or modify theory. A third of the theories are proposed by authors of the reviewed articles. The interdisciplinary nature of the theories used is greater than that of research in software engineering in general. We found that theory use and awareness of theoretical issues are present, but that theory-driven research is, as yet, not a major issue in empirical software engineering. Several articles comment explicitly on the lack of relevant theory. We call for an increased awareness of the potential benefits of involving theory, when feasible. To support software engineering researchers who wish to use theory, we show which of the reviewed articles on which topics use which theories for what purposes, as well as details of the theories' characteristics",j142,IEEE Transactions on Software Engineering,jv142,accepted,f649,2014,2014-12-01
s803,p803,Feature-Oriented Software Product Lines: Concepts and Implementation,"While standardization has empowered the software industry to substantially scale software development and to provide affordable software to a broad market, it often does not address smaller market segments, nor the needs and wishes of individual customers. Software product lines reconcile mass production and standardization with mass customization in software engineering. Ideally, based on a set of reusable parts, a software manufacturer can generate a software product based on the requirements of its customer. The concept of features is central to achieving this level of automation, because features bridge the gap between the requirements the customer has and the functionality a product provides. Thus features are a central concept in all phases of product-line development. The authors take a developers viewpoint, focus on the development, maintenance, and implementation of product-line variability, and especially concentrate on automated product derivation based on a users feature selection. The book consists of three parts. Part I provides a general introduction to feature-oriented software product lines, describing the product-line approach and introducing the product-line development process with its two elements of domain and application engineering. The pivotal part II covers a wide variety of implementation techniques including design patterns, frameworks, components, feature-oriented programming, and aspect-oriented programming, as well as tool-based approaches including preprocessors, build systems, version-control systems, and virtual separation of concerns. Finally, part III is devoted to advanced topics related to feature-oriented product lines like refactoring, feature interaction, and analysis tools specific to product lines. In addition, an appendix lists various helpful tools for software product-line development, along with a description of how they relate to the topics covered in this book. To tie the book together, the authors use two running examples that are well documented in the product-line literature: data management for embedded systems, and variations of graph data structures. They start every chapter by explicitly stating the respective learning goals and finish it with a set of exercises; additional teaching material is also available online. All these features make the book ideally suited for teaching both for academic classes and for professionals interested in self-study.",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f650,2008,2008-08-05
s806,p806,Comprehensive Evaluation of an Educational Software Engineering Simulation Environment,"Software engineering educational approaches are often evaluated only anecdotally, or in informal pilot studies. We describe a more comprehensive approach to evaluating a software engineering educational technique (SimSE, a graphical, interactive, customizable, game-based software engineering simulation environment). Our method for evaluating SimSE went above and beyond anecdotal experience and approached evaluation from a number of different angles through a family of studies designed to assess SimSE's effectiveness and guide its development. In this paper, we demonstrate the insights and lessons that can be gained when using such a multi-angled evaluation approach. Our hope is that, from this paper, educators will: (1) learn ideas about how to more comprehensively evaluate their own approaches, and (2) be provided with evidence about the educational effectiveness of SimSE.",c36,Conference on Software Engineering Education and Training,cp36,accepted,f651,2015,2015-11-08
s807,p807,A Software Chasm: Software Engineering and Scientific Computing,"Some time ago, a chasm opened between the scientific-computing community and the software engineering community. Originally, computing meant scientific computing. Today, science and engineering applications are at the heart of software systems such as environmental monitoring systems, rocket guidance systems, safety studies for nuclear stations, and fuel injection systems. Failures of such health-, mission-, or safety-related systems have served as examples to promote the use of software engineering best practices. Yet, the bulk of the software engineering community's research is on anything but scientific-application software. This chasm has many possible causes. In this article, we look at the impact of one particular contributor in industry.",j147,IEEE Software,jv147,accepted,f652,2021,2021-10-16
s808,p808,Checklists for Software Engineering Case Study Research,"Case study is an important research methodology for software engineering. We have identified the need for checklists supporting researchers and reviewers in conducting and reviewing case studies. We derived checklists for researchers and reviewers respectively, using systematic qualitative procedures. Based on nine sources on case studies, checklists are derived and validated, and hereby presented for further use and improvement.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f653,2010,2010-06-16
s809,p809,Software Design and Architecture The once and future focus of software engineering,"The design of software has been a focus of software engineering research since the field's beginning. This paper explores key aspects of this research focus and shows why design will remain a principal focus. The intrinsic elements of software design, both process and product, are discussed: concept formation, use of experience, and means for representation, reasoning, and directing the design activity. Design is presented as being an activity engaged by a wide range of stakeholders, acting throughout most of a system's lifecycle, making a set of key choices which constitute the application's architecture. Directions for design research are outlined, including: (a) drawing lessons, inspiration, and techniques from design fields outside of computer science, (b) emphasizing the design of application ""character"" (functionality and style) as well as the application's structure, and (c) expanding the notion of software to encompass the design of additional kinds of intangible complex artifacts.",c37,Asia-Pacific Software Engineering Conference,cp37,accepted,f654,2008,2008-10-20
s811,p811,Mining software engineering data,"Software engineering data (such as code bases, execution traces, historical code changes, mailing lists, and bug databases) contains a wealth of information about a project's status, progress, and evolution. Using well-established data mining techniques, practitioners and researchers have started exploring the potential of this valuable data in order to better manage their projects and to produce higher quality software systems that are delivered on time and within budget. This tutorial presents the latest research in mining software engineering data, discusses challenges associated with mining software engineering data, highlights success stories of mining software engineering data, and outlines future research directions. Attendees will acquire the knowledge and skills needed to integrate the mining of software engineering data in their own research or practice. This tutorial builds on several successful offerings at ICSE since 2007.",c16,Knowledge Discovery and Data Mining,cp16,accepted,f655,2003,2003-01-30
s812,p812,Agile human-centered software engineering,"We seek to close the gap between software engineering (SE) and human-computer interaction (HCI) by indicating interdisciplinary interfaces throughout the different phases of SE and HCI lifecycles. As agile representatives of SE, Extreme Programming (XP) and Agile Modeling (AM) contribute helpful principles and practices for a common engineering approach. We present a cross-discipline user interface design lifecycle that integrates SE and HCI under the umbrella of agile development. Melting IT budgets, pressure of time and the demand to build better software in less time must be supported by traveling as light as possible. We did, therefore, choose not just to mediate both disciplines. Following our surveys, a rather radical approach best fits the demands of engineering organizations.",c59,British Computer Society Conference on Human-Computer Interaction,cp59,accepted,f656,2019,2019-03-15
s814,p814,What Every Engineer Should Know about Software Engineering,"THE PROFESSION OF SOFTWARE ENGINEERING Introduction Software Engineering as an Engineering Profession Standards and Certifications Misconceptions about Software Engineering Further Reading SOFTWARE PROPERTIES, PROCESSES, AND STANDARDS Introduction Characteristics of Software Software Processes and Methodologies Software Standards Further Reading SOFTWARE REQUIREMENTS SPECIFICATION Introduction Requirements Engineering Concepts Requirements Specifications Requirements Elicitation Requirements Modeling Requirements Documentation Recommendations on Requirements Further Reading DESIGNING SOFTWARE Introduction Software Design Concepts Software Design Modeling Pattern-Based Design Design Documentation Further Reading BUILDING SOFTWARE Introduction Programming Languages Software Construction Tools Becoming a Better Code Developer Further Reading SOFTWARE QUALITY ASSURANCE Introduction Quality Models and Standards Software Testing Metrics Fault Tolerance Maintenance and Reusability Further Reading MANAGING SOFTWARE PROJECTS AND SOFTWARE ENGINEERS Introduction Software Engineers Are People Too Project Management Basics Tracking and Reporting Progress Software Cost Estimation Project Cost Justification Risk Management Further Reading THE FUTURE OF SOFTWARE ENGINEERING Introduction Open Source Outsourcing and Offshoring Global Software Development Further Reading APPENDIX A: SOFTWARE REQUIREMENTS FOR A WASTEWATER PUMPING STATION WET WELL CONTROL SYSTEM (REV. 01.01.00) Introduction Overall Description Specific Requirements References APPENDIX B: SOFTWARE DESIGN FOR A WASTEWATER PUMPING STATION WET WELL CONTROL SYSTEM (REV. 01.01.00) Introduction Overall Description Design Decomposition References APPENDIX C: OBJECT MODELS FOR A WASTEWATER PUMPING STATION WET WELL CONTROL SYSTEM INDEX",c9,Pacific Symposium on Biocomputing,cp9,accepted,f657,2009,2009-05-26
s815,p815,Problem Oriented Software Engineering: A design-theoretic framework for software engineering,"A key challenge for software engineering is to learn how to reconcile the formal world of the machine and its software with the non-formal real world. In this paper, we discuss elements of problem oriented software engineering (POSE), an approach that brings both non- formal and formal aspects of software development together in a single theoretical framework for software engineering design. POSE presents development as the representation and step-wise transformation of software problems. It allows for the identification and clarification of system requirements, the understanding and structuring of the problem world, the structuring and specification of a hardware/software machine that can ensure satisfaction of the requirements in the problem world, and the construction of adequacy arguments, convincing both to developers and to customers, users and other interested parties, that the system will provide what is needed. Examples are used throughout the paper to illustrate how formal and non-formal descriptions are reconciled under POSE.",c60,IEEE International Conference on Software Engineering and Formal Methods,cp60,accepted,f658,2011,2011-07-31
s816,p816,Applications of Ontologies in Software Engineering,"The emerging field of semantic web technologies promises new stimulus for Software Engineering research. However, since the underlying concepts of the semantic web have a long tradition in the knowledge engineering field, it is sometimes hard for software engineers to overlook the variety of ontology-enabled approaches to Software Engineering. In this paper we therefore present some examples of ontology applications throughout the Software Engineering lifecycle. We discuss the advantages of ontologies in each case and provide a framework for classifying the usage of ontologies in Software Engineering.",c39,International Conference on Global Software Engineering,cp39,accepted,f659,2020,2020-12-09
s817,p817,"Software engineering, 8th Edition",Abstract content goes here ...,c41,Software Product Lines Conference,cp41,accepted,f660,2002,2002-12-19
s818,p818,Data Quality: Some Comments on the NASA Software Defect Datasets,"Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f661,2014,2014-09-16
s819,p819,Challenges in automotive software engineering,"The amount of software in cars grows exponentially. Driving forces of this development are cheaper and more powerful hardware and the demand for innovations by new functions. The rapid increase of software and software based functionality brings various challenges (see [21], [23], [25], [26]) for the automotive industries, for their organization, key competencies, processes, methods, tools, models, product structures, division of work, logistics, maintenance, and long term strategies. From a software engineering perspective, the automotive industry is an ideal and fascinating application domain for advanced techniques. Although the automotive industry may adopt general results and solutions from the software engineering body of knowledge gained in other domains, the specific constraints and domain specific requirements in the automotive industry ask for individual solutions and bring various challenges for automotive software engineering. In cars we find literally all interesting problems and challenging issues of software and systems engineering.",c29,International Conference on Software Engineering,cp29,accepted,f662,2015,2015-09-02
s820,p820,Rationale Management in Software Engineering,Abstract content goes here ...,c9,Pacific Symposium on Biocomputing,cp9,accepted,f663,2009,2009-10-28
s822,p822,Changing the paradigm of software engineering,"Software evolution, iterative, and agile development represent a fundamental departure from the previous waterfall-based paradigm of software engineering.",c17,International Conference on Enterprise Information Systems,cp17,accepted,f664,2008,2008-07-04
s823,p823,Software engineering for adaptive and self-managing systems,"The objective of this workshop is to consolidate the interest in the software engineering community on autonomic, self-managing, self-healing, self-optimizing, self-configuring, and self-adaptive systems. The workshop will provide a forum for researchers to share new results, raise awareness of new adaptive concerns, and promote collaboration among the community. This workshop will be the first of several to assess progress and identify challenges in this important area.",c29,International Conference on Software Engineering,cp29,accepted,f665,2015,2015-02-17
s824,p824,Some future trends and implications for systems and software engineering processes,"In response to the increasing criticality of software within systems and the increasing demands being put onto 21st century systems, systems and software engineering processes will evolve significantly over the next two decades. This paper identifies eight relatively surprise‐free trends—the increasing interaction of software engineering and systems engineering; increased emphasis on users and end value; increased emphasis on systems and software dependability; increasingly rapid change; increasing global connectivity and need for systems to interoperate; increasingly complex systems of systems; increasing needs for COTS, reuse, and legacy systems and software integration; and computational plenty. It also identifies two “wild card” trends: increasing software autonomy and combinations of biology and computing. It then discusses the likely influences of these trends on systems and software engineering processes between now and 2025, and presents an emerging scalable spiral process model for coping with the resulting challenges and opportunities of developing 21st century software‐intensive systems and systems of systems. © 2006 Wiley Periodicals, Inc. Syst Eng 9: 1–19, 2006",j162,Systems Engineering,jv162,accepted,f666,2008,2008-08-20
s825,p825,Value-Based Software Engineering: Overview and Agenda,Abstract content goes here ...,c113,International Conference on Image Analysis and Processing,cp113,accepted,f667,2002,2002-02-11
s826,p826,"Software Engineering, 8. Auflage",Abstract content goes here ...,c61,Jahrestagung der Gesellschaft für Informatik,cp61,accepted,f668,2017,2017-07-13
s827,p827,SE2004: Recommendations for Undergraduate Software Engineering Curricula,"Universities throughout the world have established undergraduate programs in software engineering, which complement existing programs in computer science and computer engineering. To provide guidance in designing an effective curriculum, the IEEE Computer Society and the ACM have developed the Software Engineering 2004 (SE2004) set of recommendations. The SE2004 document guides universities and colleges regarding the knowledge they should teach in undergraduate software engineering programs. It also provides sample courses and curriculum patterns. SE2004 begins with an overview of software engineering, explaining how it is both a computing and an engineering discipline. It then outlines the principles that drove the document's development and describes expected student outcomes. Next, SE2004 details the knowledge that universities and colleges should teach, known as SEEK (software engineering education knowledge), in a software engineering program. These recommendations are followed by general pedagogical guidelines, sample courses, and sample curriculum patterns",j147,IEEE Software,jv147,accepted,f669,2021,2021-09-19
s828,p828,The unspoken revolution in software engineering,In this article the author describes the outsourcing aspects of software engineering. The author finds outsourcing so fascinating partly because it serves as a magnifier and revelator of just about everything in software engineering. The development of offshoring also raises a new challenge for those of us entrusted with educating future software professionals in the industrialized world.,j79,Computer,jv79,accepted,f670,2014,2014-10-18
s829,p829,The Role of Controlled Experiments in Software Engineering Research,Abstract content goes here ...,c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,cp99,accepted,f671,2015,2015-04-07
s830,p830,Using Ontologies in Software Engineering and Technology,Abstract content goes here ...,c58,Australian Software Engineering Conference,cp58,accepted,f672,2021,2021-05-24
s831,p831,Essentials of software engineering,"Updated with new case studies and content, the fully revised Third Edition of Essentials of Software Engineering offers a comprehensive, accessible, and concise introduction to core topics and methodologies of software development. Designed for undergraduate students in introductory courses, the text covers all essential topics emphasized by the IEEE Computer Society-sponsored Software Engineering Body of Knowledge (SWEBOK). In-depth coverage of key issues, combined with a strong focus on software quality, makes Essentials of Software Engineering, Third Edition the perfect text for students entering the fast-growing and lucrative field of software development. The text includes thorough overviews of programming concepts, system analysis and design, principles of software engineering, development and support processes, methodologies, and product management. The revised and updated Third Edition includes all-new sections on SCRUM and HTML-Script-SQL Design Examples, as well as expanded discussions of User-Interface Design, Flow of Interactions, Cognitive Models, and other UI Design issues. Covering all phases of the software production lifecycle and emphasizing quality throughout, Essentials of Software Engineering is a superb resource for students of software engineering. Key Features: Revised and fully updated throughout, with all-new sections on SCRUM and HTML-Script-SQL Design Examples, as well as expanded discussions of other central topics Provides coverage of all essential topics emphasized by SWEBOK Covers essential topics required for students to complete individual and team projects in an affordable and accessible paperback format. Contains an all-new Appendix with examples of Essential Software Development Plan (SDP), Essential Software Requirements Specifications (SRS), Essential Software Design, and Essential Test Plan",c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f673,2018,2018-09-17
s833,p833,Status of Empirical Research in Software Engineering,Abstract content goes here ...,c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f674,2002,2002-09-27
s834,p834,Software engineering - theory and practice (3. ed.),"Keep your method to be right here and read this page completed. You could delight in searching guide software engineering theory and practice%0D that you really describe obtain. Right here, getting the soft data of guide software engineering theory and practice%0D can be done easily by downloading and install in the web link web page that we supply below. Of course, the software engineering theory and practice%0D will be your own quicker. It's no should await the book software engineering theory and practice%0D to receive some days later after purchasing. It's no have to go outside under the heats at mid day to visit guide store.",c71,IEEE International Conference on Information Reuse and Integration,cp71,accepted,f675,2022,2022-02-20
s835,p835,Reflections on software engineering education,"The ""engineering"" focus in software engineering education leaves instructors vulnerable to several traps. It also misleads students as to SE's essential human and social dimensions. Here, the author discusses how this limited conception of SE contributes to five assumptions that can trap SE educators: (i) an SE course needs an industrial project. (ii) SE is like other branches of engineering. (iii) Planning in SE is poorly done relative to other fields. (iv) The user interface is part of low-level design. (v) SWEBOK represents the state of the practice",j147,IEEE Software,jv147,accepted,f676,2021,2021-12-11
s836,p836,Integrating Security and Software Engineering: Advances and Future Visions,A Sample of Contents: Integrating Security and Software Engineering A Methodology to Develop Secure Systems Using Patterns Extending Security in Agile Software Development Methods Access Control Specification in UML.,c8,The Compass,cp8,accepted,f677,2016,2016-08-15
s837,p837,Systematic Review in Software Engineering,Abstract content goes here ...,c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f678,2018,2018-07-18
s838,p838,Evidence-Based Software Engineering for Practitioners,"Software managers and practitioners often must make decisions about what technologies to employ on their projects. They might be aware of problems with their current development practices (for example, production bottlenecks or numerous defect reports from customers) and want to resolve them. Or, they might have read about a new technology and want to take advantage of its promised benefits. However, practitioners can have difficulty making informed decisions about whether to adopt a new technology because there's little objective evidence to confirm its suitability, limits, qualities, costs, and inherent risks. This can lead to poor decisions about technology adoption. Software engineers might make incorrect decisions about adopting new techniques it they don't consider scientific evidence about the techniques' efficacy. They should consider using procedures similar to ones developed for evidence-based medicine. Software companies are often under pressure to adopt immature technologies because of market and management pressures. We suggest that practitioners consider evidence-based software engineering as a mechanism to support and improve their technology adoption decisions.",j147,IEEE Software,jv147,accepted,f679,2021,2021-05-20
s839,p839,Software Engineering (7th Edition),Abstract content goes here ...,c62,International Conference on Software Reuse,cp62,accepted,f680,2006,2006-02-22
s840,p840,Proceedings 25th International Conference on Software Engineering,The following topics are dealt with: software components; software testing; formal methods; software design; program analysis; software architecture; software engineering education; software fault correction.,c30,IEEE Aerospace Conference,cp30,accepted,f681,2006,2006-10-30
s843,p843,Software Engineering with Reusable Components,Abstract content goes here ...,c69,International Conference on Parallel Processing,cp69,accepted,f682,2010,2010-06-29
s844,p844,A survey on the use of topic models when mining software repositories,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f683,2004,2004-10-30
s845,p845,A systematic review of statistical power in software engineering experiments,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f684,2010,2010-03-08
s846,p846,Obfuscator-LLVM -- Software Protection for the Masses,"Software security with respect to reverse-engineering is a challenging discipline that has been researched for several years and which is still active. At the same time, this field is inherently practical, and thus of industrial relevance: indeed, protecting a piece of software against tampering, malicious modifications or reverse-engineering is a very difficult task. In this paper, we present and discuss a software obfuscation prototype tool based on the LLVM compilation suite. Our tool is built as different passes, where some of them have been open-sourced and are freely available, that work on the LLVM Intermediate Representation (IR) code. This approach brings several advantages, including the fact that it is language-agnostic and mostly independent of the target architecture. Our current prototype supports basic instruction substitutions, insertion of bogus control-flow constructs mixed with opaque predicates, control-flow flattening, procedures merging as well as a code tamper-proofing algorithm embedding code and data checksums directly in the control-flow flattening mechanism.",c7,European Conference on Modelling and Simulation,cp7,accepted,f685,2015,2015-08-25
s847,p847,Software Bots,"Although the development and widespread adoption of software bots has occurred in just a few years, bots have taken on many diverse tasks and roles. This article discusses current bot technology and presents a practical case study on how to use bots in software engineering.",j147,IEEE Software,jv147,accepted,f686,2021,2021-08-22
s849,p849,Software engineering for security: a roadmap,"Is there such a thing anymore as a software system that doesn’t need to be secure? Almost every softwarecontrolled system faces threats from potential adversaries, from Internet-aware client applications running on PCs, to complex telecommunications and power systems accessible over the Internet, to commodity software with copy protection mechanisms. Software engineers must be cognizant of these threats and engineer systems with credible defenses, while still delivering value to customers. In this paper, we present our perspectives on the research issues that arise in the interactions between software engineering and security.",c29,International Conference on Software Engineering,cp29,accepted,f687,2015,2015-06-05
s850,p850,Encyclopedia of Software Engineering,"From the Publisher: 
Encompasses the field of software development process--from design to transpiration to testing and everything in between. Includes all functional disciplines, software tools and languages associated with software engineering of large and/or complex projects. Organized alphabetically--every major area contains an overview article that defines the topic. Each sub-discipline has a specific article covering history, current practice, practical data and projections about future practice.",c81,IEEE Annual Symposium on Foundations of Computer Science,cp81,accepted,f688,2004,2004-11-22
s851,p851,"Software Engineering 3 - Domains, Requirements, and Software Design",Abstract content goes here ...,c114,IEEE International Conference on Robotics and Automation,cp114,accepted,f689,2011,2011-05-09
s852,p852,Sustainability Design and Software: The Karlskrona Manifesto,"Sustainability has emerged as a broad concern for society. Many engineering disciplines have been grappling with challenges in how we sustain technical, social and ecological systems. In the software engineering community, for example, maintainability has been a concern for a long time. But too often, these issues are treated in isolation from one another. Misperceptions among practitioners and research communities persist, rooted in a lack of coherent understanding of sustainability, and how it relates to software systems research and practice. This article presents a cross-disciplinary initiative to create a common ground and a point of reference for the global community of research and practice in software and sustainability, to be used for effectively communicating key issues, goals, values and principles of sustainability design for software-intensive systems.The centrepiece of this effort is the Karlskrona Manifesto for Sustainability Design, a vehicle for a much needed conversation about sustainability within and beyond the software community, and an articulation of the fundamental principles underpinning design choices that affect sustainability. We describe the motivation for developing this manifesto, including some considerations of the genre of the manifesto as well as the dynamics of its creation. We illustrate the collaborative reflective writing process and present the current edition of the manifesto itself. We assess immediate implications and applications of the articulated principles, compare these to current practice, and suggest future steps.",c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,cp20,accepted,f690,2014,2014-10-10
s853,p853,The \{PROMISE\} Repository of Software Engineering Databases.,Abstract content goes here ...,c30,IEEE Aerospace Conference,cp30,accepted,f691,2006,2006-03-05
s854,p854,Variability in Software Systems—A Systematic Literature Review,"Context: Variability (i.e., the ability of software systems or artifacts to be adjusted for different contexts) became a key property of many systems. Objective: We analyze existing research on variability in software systems. We investigate variability handling in major software engineering phases (e.g., requirements engineering, architecting). Method: We performed a systematic literature review. A manual search covered 13 premium software engineering journals and 18 premium conferences, resulting in 15,430 papers searched and 196 papers considered for analysis. To improve reliability and to increase reproducibility, we complemented the manual search with a targeted automated search. Results: Software quality attributes have not received much attention in the context of variability. Variability is studied in all software engineering phases, but testing is underrepresented. Data to motivate the applicability of current approaches are often insufficient; research designs are vaguely described. Conclusions: Based on our findings we propose dimensions of variability in software engineering. This empirically grounded classification provides a step towards a unifying, integrated perspective of variability in software systems, spanning across disparate or loosely coupled research themes in the software engineering community. Finally, we provide recommendations to bridge the gap between research and practice and point to opportunities for future research.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f692,2014,2014-06-27
s855,p855,Concepts and Guidelines of Feature Modeling for Product Line Software Engineering,Abstract content goes here ...,c62,International Conference on Software Reuse,cp62,accepted,f693,2006,2006-10-29
s857,p857,Software-engineering research revisited,"The author discusses three major changes that he suggests are occurring as a result of the software engineering industry adopting the industry-as-laboratory approach, in which researchers identify problems through close involvement with industrial projects and create and evaluate solutions in an almost indivisible research activity. This approach emphasizes what people actually do or can do in practice, rather than what is possible in principle. The three changes are a greater reliance on empirical definition of problems, an emphasis on real case studies, and a greater emphasis on contextual issues.<<ETX>>",j147,IEEE Software,jv147,accepted,f694,2021,2021-02-09
s859,p859,Object-oriented and classical software engineering,"From the Publisher: 
Classical and Object-Oriented Software Engineering is designed for an introductory software engineering course. This book provides an excellent introduction to software engineering fundamentals,covering both traditional and object-oriented techniques. 
Schach's unique organization and style makes it excellent for use in a classroom setting. It presents the underlying software engineering theory in Part I and follows it up with the more practical life-cycle material in Part II. Many software engineering books are more like reference books,which do not provide the appropriate fundamentals before inundating students with implementation details. 
In this edition,more practical material has been added to help students understand how to use what they are learning. This has been done through the use of ""How To"" boxes and greater implementation detail in the case study. Additionally,the new edition contains the references to the most current literature and includes an overview of extreme programmming. 
The website in this edition will be more extensive. It will include Solutions,PowerPoints that incorporate lecture notes,newly developed self-quiz questions,and source code for the term project and case study.",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f695,2018,2018-08-16
s860,p860,Software Engineering Metrics: What Do They Measure and How Do We Know?,"Construct validity is about the question, how we know that we're measuring the attribute that we think we're measuring? This is discussed in formal, theoretical ways in the computing literature (in terms of the representational theory of measurement) but rarely in simpler ways that foster application by practitioners. Construct validity starts with a thorough analysis of the construct, the attribute we are attempting to measure. In the IEEE Standard 1061, direct measures need not be validated. ""Direct"" measurement of an attribute involves a metric that depends only on the value of the attribute, but few or no software engineering attributes or tasks are so simple that measures of them can be direct. Thus, all metrics should be validated. The paper continues with a framework for evaluating proposed metrics, and applies it to two uses of bug counts. Bug counts capture only a small part of the meaning of the attributes they are being used to measure. Multidimensional analyses of attributes appear promising as a means of capturing the quality of the attribute in question. Analysis fragments run throughout the paper, illustrating the breakdown of an attribute or task of interest into sub-attributes for grouped study.",c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,cp86,accepted,f696,2012,2012-07-29
s861,p861,A software engineering framework for context-aware pervasive computing,"There is growing interest in the use of context-awareness as a technique for developing pervasive computing applications that are flexible, adaptable, and capable of acting autonomously on behalf of users. However, context-awareness introduces various software engineering challenges, as well as privacy and usability concerns. In this paper, we present a conceptual framework and software infrastructure that together address known software engineering challenges, and enable further practical exploration of social and usability issues by facilitating the prototyping and fine-tuning of context-aware applications.",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f697,2015,2015-03-10
s862,p862,Empirical Research Methods in Software Engineering,Abstract content goes here ...,c64,Experimental Software Engineering Network,cp64,accepted,f698,2014,2014-10-12
s863,p863,Facts and fallacies of software engineering,"There's a problem with those facts—and, as you might imagine, those fallacies. Many of these fundamentally important facts are learned by a software engineer, but over the short lifespan of the software field, all too many of them have been forgotten. While reading Facts and Fallacies of Software Engineering, you may experience moments of ""Oh, yes, I had forgotten that,"" alongside some ""Is that really true?"" thoughts.",c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f699,2011,2011-01-29
s864,p864,Categories for software engineering,Abstract content goes here ...,c94,Vision,cp94,accepted,f700,2020,2020-01-28
s865,p865,A Survey of Formal Concept Analysis Support for Software Engineering Activities,Abstract content goes here ...,c65,Formal Concept Analysis,cp65,accepted,f701,2008,2008-06-03
s866,p866,"The role of experimentation in software engineering: past, current, and future",Software engineering needs to follow the model of other physical sciences and develop an experimental paradigm for the field. This paper proposes the approach towards developing an experimental component of such a paradigm. The approach is based upon a quality improvement paradigm that addresses the role of experimentation and process improvement in the content of industrial development. The paper outlines a classification scheme for characterizing such experiments.,c50,International Conference on Automated Software Engineering,cp50,accepted,f702,2008,2008-08-04
s867,p867,Bayesian Analysis of Empirical Software Engineering Cost Models,"Many parametric software estimation models have evolved in the last two decades (L.H. Putnam and W. Myers, 1992; C. Jones, 1997; R.M. Park et al., 1992). Almost all of these parametric models have been empirically calibrated to actual data from completed software projects. The most commonly used technique for empirical calibration has been the popular classical multiple regression approach. As discussed in the paper, the multiple regression approach imposes a few assumptions frequently violated by software engineering datasets. The paper illustrates the problems faced by the multiple regression approach during the calibration of one of the popular software engineering cost models, COCOMO II. It describes the use of a pragmatic 10 percent weighted average approach that was used for the first publicly available calibrated version (S. Chulani et al., 1998). It then moves on to show how a more sophisticated Bayesian approach can be used to alleviate some of the problems faced by multiple regression. It compares and contrasts the two empirical approaches, and concludes that the Bayesian approach was better and more robust than the multiple regression approach.",c111,International Society for Music Information Retrieval Conference,cp111,accepted,f703,2001,2001-10-24
s869,p869,Methodologies and software engineering for agent systems : the agent-oriented software engineering handbook,Concepts and Abstractions of Agent-Oriented Software Engineering.- Agent-Based Abstractions for Software Development.- On the Use of Agents as Components of Software Systems.- A Survey on Agent-Oriented Oriented Software Engineering Research.- Methodologies for Agent-Based Systems Development.- The Gaia Methodology.- The Tropos Methodology.- The MaSE Methodology.- A Comparative Evaluation of Agent-Oriented Methodologies.- Special-Purpose Methodologies.- The ADELFE Methodology.- The Message Methodology.- The SADDE Methodology.- The Prometheus Methodology.- Tools and Infrastructures for Agent-Oriented Software Engineering.- The AUML Approach.- FIPA-Compliant Agent Infrastructures.- Coordination Infrastructures in the Engineering of Multiagent Systems.- Non Traditional Approaches to Agent-Oriented Software Engineering.- Engineering Amorphous Computing Systems.- Making Self-Organising Adaptive Multiagent Systems Work.- Engineering Swarming Systems.- Online Engineering and Open Computational Systems.- Emerging Trends and Perspectives.- Agents for Ubiquitous Computing.- Agents and the Grid.- Roadmap of Agent-Oriented Software Engineering.,c105,Biometrics and Identity Management,cp105,accepted,f704,2006,2006-02-23
s870,p870,End-user software engineering,"End-user programming has become the most common form of programming in use today [2], but there has been little investigation into the dependability of the programs end users create. This is problematic because the dependability of these programs can be very important; in some cases, errors in end-user programs, such as formula errors in spreadsheets, have cost millions of dollars. (For example, see www.theregister.co.uk/content/67/31298.html or panko.cba.hawaii.edu/ssr/Mypapers/whatknow.htm.) We have been investigating ways to address this problem by developing a software engineering paradigm viable for end-user programming, an approach we call end-user software engineering.",j37,Communications of the ACM,jv37,accepted,f705,2011,2011-09-24
s871,p871,"Models in software engineering – an introduction
",Abstract content goes here ...,j165,Journal of Software and Systems Modeling,jv165,accepted,f706,2021,2021-04-27
s872,p872,Teaching software engineering through game design,"Many projects currently used in Software Engineering curricula lack both the ""fun factor"" needed to engage students, as well as the practical realism of engineering projects that include other computer science disciplines such as Software Engineering, Networks, or Human Computer Interaction. This paper reports on our endeavor to enhance interest and retention in an existing Software Engineering curriculum through the use of computer game-based projects. Specifically, a set of game-centric, project-based modules have been developed that enable students to: (1) actively participate in the different phases of the software lifecycle taking a single project from requirement elicitation to testing and maintenance; (2) expose students to real issues in project and team management over the course of a 2-semester project; and at the same time (3) introduce students to the different aspects of computer game design. Preliminary results suggest the merits of our approach, showing improved class participation and performance.",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f707,2002,2002-03-09
s875,p875,Software engineering with Ada,"From the Publisher: 
Grady Booch, a renowned authority in software development, and Doug Bryan combined their Ada programming and software engineering expertise for the new edition of this best-selling book. Their up-to-date introduction to Ada programming provides a foundation for using the language with software engineering and object-oriented design. Programmers will find Software Engineering with Ada, Third Edition to be a complete reference for creating large-scale Ada systems and understanding the software engineering aspects of these systems. Features of the third edition include techniques for combining object-oriented design principles and software engineering to maximize the potential of Ada; extensive examples of small-sized code that will benefit new Ada programmers; six chapters devoted to design; five new large-scale programming exercises that build upon the software engineering principles developed in the design chapters; design projects on topics such as environment monitoring, database systems, and generic tree packages; an introduction to up-to-date object-oriented design methodology; and a new appendix on the Ada 9X program.",c19,ACM Conference on Economics and Computation,cp19,accepted,f708,2002,2002-04-15
s876,p876,Software engineering concepts,"Software engineering concepts , Software engineering concepts , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",c57,IEEE International Conference on Engineering of Complex Computer Systems,cp57,accepted,f709,2003,2003-10-11
s877,p877,Writing good software engineering research papers,"Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to XSE 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.",c39,International Conference on Global Software Engineering,cp39,accepted,f710,2020,2020-10-01
s878,p878,Using benchmarking to advance research: a challenge to software engineering,"Benchmarks have been used in computer science to compare the performance of computer systems, information retrieval algorithms, databases, and many other technologies. The creation and widespread use of a benchmark within a research area is frequently accompanied by rapid technical progress and community building. These observations have led us to formulate a theory of benchmarking within scientific disciplines. Based on this theory, we challenge software engineering research to become more scientific and cohesive by working as a community to define benchmarks. In support of this challenge, we present a case study of the reverse engineering community, where we have successfully used benchmarks to advance the state of research.",c57,IEEE International Conference on Engineering of Complex Computer Systems,cp57,accepted,f711,2003,2003-02-01
s880,p880,An experimental card game for teaching software engineering,"The typical software engineering course consists of lectures in which concepts and theories are conveyed, along with a small ""toy"" software engineering project which attempts to give students the opportunity to put this knowledge into practice. Although both of these components are essential, neither one provides students with adequate practical knowledge regarding the process of software engineering. Namely, lectures allow only passive learning, and projects are so constrained by the time and scope requirements of the academic environment that they cannot be large enough to exhibit many of the phenomena occurring in realworld software engineering processes. To address this problem, we have developed Problems and Programmers, an educational card game that simulates the software engineering process and is designed to teach those process issues that are not sufficiently highlighted by lectures and projects. We describe how the game is designed, the mechanics of its game play, and the results of an experiment we conducted involving students playing the game.",c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f712,2022,2022-10-06
s881,p881,Software engineering and middleware: a roadmap,"The construction of a large class of distributed systems can be simplified by leveraging middleware, which is layered between network operating systems and application components. Middleware resolves heterogeneity, and facilitates communication and coordination of distributed components. Existing middleware products enable software engineers to build systems that are distributed across a local-area network. State-of-the-art middleware research aims to push this boundary towards Internet-scale distribution, adaptive and reconfigurable middleware and middleware for dependable and wireless systems. The challenge for software engineering research is to devise notations, techniques, methods and tools for distributed system construction that systematically build and exploit the capabilities that middleware deliver. 1 I N T R O D U C T I O N Various commercial trends have lead to an increasing demand for distributed systems. Firstly, the number of mergers between companies is continuing to increase. The different divisions of a newly merged company have to deliver unified services to their customers and this usually demands an integration of their IT systems. The time available for delivery of such an integration is often so short that building a new system is not an option and therefore existing system components have to be integrated into a distributed system that appears as an integrating computing facility. Secondly, the time available for providing new services are decreasing. Often this can only be achieved if components are procured off-the-shelf and then integrated into a system rather than built from scratch. Components to be integrated may have incompatible requirements for their hardware and operating system platforms; they have to be deployed on different hosts, forcing the resulting system to be distributed. Finally, the Internet provides new opportunities to offer products and services to a vast number of potential customers. In this setting, it is difficult to estimate the scalability requirements. Permission to make digital or hard copies of all or part of this work lbr personal or classroom use is granted without fee provided that copies are not made or distributed tbr profit or commercial advantage and that copies bear this notice and the lull citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a tee. Future of Sofware Engineering Limerick Ireland Copyright ACM 2000 1-58113-253-0/00/6...$5.00 An e-commerce site that was designed to cope with a given number of transactions per day may suddenly find itself exposed to demand that is by orders of magnitude larger. The required scalability cannot usually be achieved by centralized or client-server architectures but demands a distributed system. Distributed systems can integrate legacy components, thus preserving investment, they can decrease the time to market, they can be scalable and tolerant against failures. The caveat, however, is that the construction of a truly distributed systems is considerably more difficult than building a centralized or client/server system. This is because there are multiple points of failure in a distributed system, system components need to communicate with each other through a network, which complicates communication and opens the door for security attacks. Middleware has been devised in order to conceal these difficulties from application engineers as much as possible; As they solve a real problem and simplify distributed system construction, middleware products are rapidly being adopted in industry [6]. In order to build distributed systems that meet the requirements, software engineers have to know what middleware is available, which one is best suited to the problems at hand, and how middleware can be used in the architecture, design and implementation of distributed systems. The principal contribution of this paper is an assessment of both, the state-of-the-practice that current middleware products offer and the state-of-the-art in middleware research. Software engineers increasingly use middleware to build distributed systems. Any research into distributed software engineering that ignores this trend will only have limited impact. We, therefore, analyze the influence that the increasing use of middleware should have on the software engineering research agenda. We argue that requirements engineering techniques are needed that focus on non-functional requirements, as these influence the selection and use of middleware. We identify that software architecture research should produce methods that guide engineers towards selecting the right middleware and employing it so that it meets a set of non-functional requirements. We then highlight that the use of middleware is not transparent for system design and that design methods are needed that address this issue.",c29,International Conference on Software Engineering,cp29,accepted,f713,2015,2015-09-06
s882,p882,"Object Oriented Software Engineering, Conquering Complex and Changing Systems","From the Publisher: 
This book is based on object-oriented techniques applied to software engineering. Employing the latest technologies such as UML, Patterns, and Java, Bernd Bruegge and Allen H. Dutoit offer a cohesive, class-tested presentation of object-oriented software engineering in a step-by-step format based on ten years of teaching and real-world software engineering experience. This text teaches practical experience in developing complex software appropriate for software engineering project courses, as well as industry R & D practitioners. The reader benefits from timely exposure to state-of-the-art tools and methods. 
 
Unlike other texts based on the teaching premise of multiple classes or developing multiple systems, this book focuses on techniques and applications in a reasonably complex environment, such as multi-team development projects including 20 to 60 participants. The book is based on concrete examples from real applications such as accident management, emissions modeling, facility management, and centralized traffic control. 
 
Provides an integrated communication infrastructure for distributed development 
Shows the state of the art in Software Engineering: UML, Java, Design Patterns, Distributed Development, and Multiproject Management 
Illustrates how the reader learns to develop in a distributed team with hands-on experience on real system development problems 
Offers a CD-ROM containing the materials used in courses taught by the authors-problem statements, requirement analysis documents, system design documents, test manuals, prototypes, and all the artifacts produced during the development of a facility management system 
Presents Companion Website (www.prenhall.com/bruegge) with supplemental material such as problem statements, requirement analysis documents, system design documents, test manuals, and solutions to exercises",c98,North American Chapter of the Association for Computational Linguistics,cp98,accepted,f714,2009,2009-06-11
s883,p883,Conducting realistic experiments in software engineering,"An important goal of most empirical software engineering research is the transfer of research results to industrial applications. Two important obstacles for this transfer are the lack of control of variables of case studies, i.e., the lack of explanatory power, and the lack of realism of controlled experiments. While it may be difficult to increase the explanatory power of case studies, there is a large potential for increasing the realism of controlled software engineering experiments. To convince industry about the validity and applicability of the experimental results, the tasks, subjects and the environments of the experiments should be as realistic as practically possible. Such experiments are, however, more expensive than experiments involving students, small tasks and pen-and-paper environments. Consequently, a change towards more realistic experiments requires a change in the amount of resources spent on software engineering experiments. This paper argues that software engineering researchers should apply for resources enabling expensive and realistic software engineering experiments similar to how other researchers apply for resources for expensive software and hardware that are necessary for their research. The paper describes experiences from recent experiments that varied in size from involving one software professional for 5 days to 130 software professionals, from 9 consultancy companies, for one day each.",c75,International Conference on Machine Learning,cp75,accepted,f715,2005,2005-02-03
s884,p884,The use of program dependence graphs in software engineering,"This paper describes a language-independent program representation-the program dependence graph-and discusses how program dependence graphs, together with operations such as program slicing, can provide the basis for powerful programmmg tools that address important software-engineering problems, such as understanding what an existing program does and how it works, understanding the differences between several versions of a program, and creating new programs by combining pieces of old pro- grams. The paper primarily surveys work in this area that has been carried out at the University of Wisconsin during the past five years.",c29,International Conference on Software Engineering,cp29,accepted,f716,2015,2015-10-01
s885,p885,Component-based software engineering - new challenges in software development,"The primary role of component-based software engineering is to address the development of systems as an assembly of parts (components), the development of parts as reusable entities, and the maintenance and upgrading of systems by customising and replacing such parts. This requires established methodologies and tool support covering the entire component and system lifecycle including technological, organisational, marketing, legal, and other aspects. The traditional disciplines from software engineering need new methodologies to support component-based development.",c6,Americas Conference on Information Systems,cp6,accepted,f717,2007,2007-04-18
s886,p886,Process Models in Software Engineering,"Software systems come and go through a series of passages that account for their inception, initial development, productive operation, upkeep, and retirement from one generation to another. This article categorizes and examines a number of methods for describing or modeling how software systems are developed. It begins with background and definitions of traditional software life-cycle models that dominate most textbook discussions and current software development practices. This is followed by a more comprehensive review of the alternative models of software evolution that are of current use as the basis for organizing software engineering projects and technologies. 
 
 
Keywords: 
 
software process model; 
definition; 
software life-cycle models; 
development models; 
production process models",c50,International Conference on Automated Software Engineering,cp50,accepted,f718,2008,2008-04-18
s887,p887,"Software engineering education in the era of outsourcing, distributed development, and open source software: challenges and opportunities",Abstract content goes here ...,c85,International Conference on Graph Transformation,cp85,accepted,f719,2007,2007-08-06
s888,p888,Tool Integration in Software Engineering Environments,"This article presents doctoral research on tool int egration within software engineering environments. Tool int egration concerns the techniques used to form coalitions of to ls that provide an environment supporting some, or all, act ivities within the software engineering process. Some inte res ing phenomena have been observed, such as the ad hoc na ture of tool integration in one particular software enginee ring company. This observation is at variance to the com m n perception of widespread integration suggested by t ool vendors and some previous academic literature. Ini tial results suggest that integration must be implemented for bu siness reasons, not for its own sake.",c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f720,2001,2001-08-25
s889,p889,Ontologies in the Software Engineering Process,"The term ontology has become popular in several fields of Informatics like Artificial Intelligence, Agent systems, Database or Web Technology. Deviating from its original philosophical meaning, in the context of Computer Sciences the term ontology stands for a formal explicit specification of a shared conceptualization. Software Engineering (SE) is a field where conceptualisation plays a major role, e.g. in the early phases of software development, in the definition, use and re-use of software components and as a basis for their integration. Thus, ontologies are likely to invade the SE field as well soon. In this contribution, conceptual modeling as practiced in SE and Information Systems projects is contrasted with the ontology approach. The corresponding life cycle models for their development are compared. Finally, some perspectives of an Ontology-based Software Engineering (OBSE) approach are outlined.",c67,Enterprise Application Integration,cp67,accepted,f721,2002,2002-01-27
s891,p891,Thinking on the Development of Software Engineering Technology,"The paper gives some thinking according to the following four aspects: 1) from the law of things development, revealing the development history of software engineering technology; 2) from the point of software natural characteristic, analyzing the construction of every abstraction layer of virtual machine; 3) from the point of software development, proposing the research content of software engineering discipline, and research the pattern of industrialized software production; 4) based on the appearance of Internet technology, exploring the development trend of software technology.",c8,The Compass,cp8,accepted,f722,2016,2016-12-23
s892,p892,Models in software engineering - an introduction,Abstract content goes here ...,c102,International Conference on Biometrics,cp102,accepted,f723,2022,2022-08-26
s893,p893,Software engineering (6th ed.),Abstract content goes here ...,c84,The Web Conference,cp84,accepted,f724,2006,2006-03-27
s894,p894,Conducting on-line surveys in software engineering,"One purpose of empirical software engineering is to enable an understanding of factors that influence software development. Surveys are an appropriate empirical strategy to gather data from a large population (e.g., about methods, tools, developers, companies) and to achieve an understanding of that population. Although surveys are quite often performed, for example, in social sciences and marketing research, they are underrepresented in empirical software engineering research, which most often uses controlled experiments and case studies. Consequently, also the methodological support how to perform such studies in software engineering is rather low. However, with the increasing pervasion of the Internet it is possible to perform surveys easily and cost-effectively over Internet pages (i.e., on-line), while at the same time the interest in performing surveys is growing. The purpose of this paper is twofold. First we want to arise the awareness of on-line surveys and discuss methods how to perform these in the context of software engineering. Second, we report our experience in performing on-line surveys in the form of lessons learned and guidelines.",c91,Workshop on Algorithms and Models for the Web-Graph,cp91,accepted,f725,2022,2022-04-08
s895,p895,A Ranking of Software Engineering Measures Based on Expert Opinion,"This research proposes a framework based on expert opinion elicitation, developed to select the software engineering measures which are the best software reliability indicators. The current research is based on the top 30 measures identified in an earlier study conducted by Lawrence Livermore National Laboratory. A set of ranking criteria and their levels were identified. The score of each measure for each ranking criterion was elicited through expert opinion and then aggregated into a single score using multiattribute utility theory. The basic aggregation scheme selected was a linear additive scheme. A comprehensive sensitivity analysis was carried out. The sensitivity analysis included: variation of the ranking criteria levels, variation of the weights, variation of the aggregation schemes. The top-ranked measures were identified. Use of these measures in each software development phase can lead to a more reliable quantitative prediction of software reliability.",c11,Hawaii International Conference on System Sciences,cp11,accepted,f726,2006,2006-02-10
s896,p896,Research in software engineering: an analysis of the literature,Abstract content goes here ...,j144,Information and Software Technology,jv144,accepted,f727,2010,2010-03-25
s897,p897,Experimentation in software engineering,"A framework is presented for analyzing most of the experimental work performed in software engineering over the past several years. The framework of experimentation consists of four categories corresponding to phases of the experimentation process: definition, planning, operation, and interpretation. A variety of experiments are described within the framework and their contribution to the software engineering discipline is discussed. Some recommendations for the application of the experimental process in software engineering are included.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f728,2014,2014-11-21
s898,p898,Principles of software engineering management,"From time to time books emerge which become part of APL's folklore; the obvious example being ""APL-An Interactive Approach"" by Gilman and Roses. Some are more recent and more tangentical in their relevance; exam-pies are Myers ""Refiable Software Through Composite Design"" and Tufte's ""Visualization of Quantitative Data"". I believe that this most recent book of Mr. Gilb's may be destined to become part of this select group. A long-standing belief in the APL world is, that we can deliver ""productivity""-recent explorations of topics like Function Point Analysis are (belatedly) showing good quality evidence to back this belief. At heart I feel that Mr. Gilb is saying that even though we're moving in the fight direction we're still a very long way from the goal we ought to be shooting for-but that we can achieve the ultimate objective if we take a wider view of the world. Point one-the term 'software engineering' is being hijacked and is in imminent danger of becoming the vogue term for a much smaller range of topics than it purports to cover. Point two-APL applications have long tended to solve broader and more strategic problems than the more established languages. For all of his 442 pages, Mr. Gilb's messages axe brief, relevant and, I believe, achievable. They also embody common sense-that rarest of commodities-it is hard to believe that so many people can have been neglecting the principles for so long. I'd like to summarise his message in three ways. [1] Measure what you're being asked to do in terms meaningful to the business, user or application. And measure what you achieve. [2] Deliver your solution in many incremental stages-at all times delivering the highest value at least cost in the shortest time (remember, you just quantified all of this). [3] Early detection and solution of problems costs orders of magnitude less to fix than late detection and remedy. Think about it-its not novel is it? And doesn't the seemed message have a familiar ring7 What Mr. Glib is attempting to do is to bring rigour into the process. I think that if we were to adopt these methods in a similarly serious-minded vein we might easily discover a lot about ourselves and just possibly find a new way of getting our message into wider environments. I offer a challenge to the APL COmmunity in general and the APL90 organisers in particular. Lets find at least …",c3,Frontiers in Education Conference,cp3,accepted,f729,2016,2016-01-21
s899,p899,What makes good research in software engineering?,Abstract content goes here ...,j166,International Journal on Software Tools for Technology Transfer (STTT),jv166,accepted,f730,2016,2016-04-27
s900,p900,Principles of software engineering management,"From time to time books emerge which become part of APL's folklore; the obvious example being ""APL-An Interactive Approach"" by Gilman and Roses. Some are more recent and more tangentical in their relevance; exam-pies are Myers ""Refiable Software Through Composite Design"" and Tufte's ""Visualization of Quantitative Data"". I believe that this most recent book of Mr. Gilb's may be destined to become part of this select group. A long-standing belief in the APL world is, that we can deliver ""productivity""-recent explorations of topics like Function Point Analysis are (belatedly) showing good quality evidence to back this belief. At heart I feel that Mr. Gilb is saying that even though we're moving in the fight direction we're still a very long way from the goal we ought to be shooting for-but that we can achieve the ultimate objective if we take a wider view of the world. Point one-the term 'software engineering' is being hijacked and is in imminent danger of becoming the vogue term for a much smaller range of topics than it purports to cover. Point two-APL applications have long tended to solve broader and more strategic problems than the more established languages. For all of his 442 pages, Mr. Gilb's messages axe brief, relevant and, I believe, achievable. They also embody common sense-that rarest of commodities-it is hard to believe that so many people can have been neglecting the principles for so long. I'd like to summarise his message in three ways. [1] Measure what you're being asked to do in terms meaningful to the business, user or application. And measure what you achieve. [2] Deliver your solution in many incremental stages-at all times delivering the highest value at least cost in the shortest time (remember, you just quantified all of this). [3] Early detection and solution of problems costs orders of magnitude less to fix than late detection and remedy. Think about it-its not novel is it? And doesn't the seemed message have a familiar ring7 What Mr. Glib is attempting to do is to bring rigour into the process. I think that if we were to adopt these methods in a similarly serious-minded vein we might easily discover a lot about ourselves and just possibly find a new way of getting our message into wider environments. I offer a challenge to the APL COmmunity in general and the APL90 organisers in particular. Lets find at least …",c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f731,2011,2011-09-28
s901,p901,Experimentation in software engineering,"A framework is presented for analyzing most of the experimental work performed in software engineering over the past several years. The framework of experimentation consists of four categories corresponding to phases of the experimentation process: definition, planning, operation, and interpretation. A variety of experiments are described within the framework and their contribution to the software engineering discipline is discussed. Some recommendations for the application of the experimental process in software engineering are included.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f732,2014,2014-06-21
s903,p903,Empirical Data Modeling in Software Engineering Using Radical Basis Functions,"Many empirical studies in software engineering involve relationships between various process and product characteristics derived via linear regression analysis. We propose an alternative modeling approach using radial basis functions (RBFs) which provide a flexible way to generalize linear regression function. Further, RBF models possess strong mathematical properties of universal and best approximation. We present an objective modeling methodology for determining model parameters using our recent SG algorithm, followed by a model selection procedure based on generalization ability. Finally, we describe a detailed RBF modeling study for software effort estimation using a well-known NASA dataset.",c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f733,2014,2014-11-28
s904,p904,Software engineering for large-scale multi-agent systems - SELMAS'05,Abstract content goes here ...,c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f734,2020,2020-07-17
s905,p905,Knowledge-Sharing Issues in Experimental Software Engineering,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f735,2004,2004-09-03
s906,p906,On the application of measurement theory in software engineering,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f736,2004,2004-03-04
s910,p910,Software Engineering,"This paper provides a definition of the term ""software engineering"" and a survey of the current state of the art and likely future trends in the field. The survey covers the technology available in the various phases of the software life cycle—requirements engineering, design, coding, test, and maintenance—and in the overall area of software management and integrated technology-management approaches. It is oriented primarily toward discussing the domain of applicability of techniques (where and when they work), rather than how they work in detail. To cover the latter, an extensive set of 104 references is provided.",j167,IEEE transactions on computers,jv167,accepted,f737,2004,2004-04-05
s911,p911,Value-based software engineering: reinventing,"The Value-Based Software Engineering (VBSE) agenda described in the preceding article has the objectives of integrating value considerations into current and emerging software engineering principles and practices, and of developing an overall framework in which they compatibly reinforce each other. In this paper, we provide a case study illustrating some of the key VBSE practices, and focusing on a particular anomaly in the monitoring and control area: the ""Earned Value Management System."" This is a most useful technique for monitoring and controlling the cost, schedule, and progress of a complex project. But it has absolutely nothing to say about the stakeholder value of the system being developed. The paper introduces an example order-processing software project, and shows how the use of Benefits Realization Analysis, stake-holder value proposition elicitation and reconciliation, and business case analysis provides a framework for stakeholder-earned-value monitoring and control.",c50,International Conference on Automated Software Engineering,cp50,accepted,f738,2008,2008-01-02
s913,p913,Automotive software engineering,"Information technology has become the driving force of innovation in many areas of technology and also in cars. Embedded software controls the functions of cars, supports and assists the driver and realizes systems for information and entertainment. Software in automobiles is today one of the great challenges for software engineering. On modem cars we find all issues of software systems in a nutshell. It is a challenge for software and systems engineering.",c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,cp86,accepted,f739,2012,2012-08-07
s914,p914,Software Engineering Measurement,THE GOALS OF SOFTWARE ENGINEERING MEASUREMENT Software Engineering Measurement The Rationale for Effective Measurement Measurement across the Life Cycle Model Reasonable and Attainable Goals for Software Measurement Summary THE CONDUCT OF SCIENTIFIC INVESTIGATIONS The Principals of Scientific Investigation Measurement Measurement Issues Measurement Standards Principles of Experimentation MEASURING SOFTWARE DEVELOPMENT Measurement Domains Modeling: Mapping among Measurement Domains The Process of Software Measurement Summary VALIDATION OF SOFTWARE MEASURES Understanding What Is Being Measured Criterion-Oriented Validity Content Validity Construct Validity Empirical Validity Reliability STATIC SOFTWARE MEASUREMENT Introduction Primitive Measures of Source Code Measures of Software Quality Summary DERIVED SOFTWARE MEASURES Introduction Software Science Metrics Sources of Variation The Principal Components of Measurement Principal Components Analysis as a Validation Tool Discovering New Sources of Variation Domain Metrics A Unitary Measure of Software Complexity Summary MODELING WITH METRICS Introduction Simple Linear Regression Non-Linear Models Problems Associated with Multicollinearity Regression as a Metric Validation Tool Canonical Correlation MEASURING SOFTWARE EVOLUTION Introduction Measuring Evolving Software Measuring Changes to Modules across Builds Summary SOFTWARE SPECIFICATION AND DESIGN Introduction Software Operational Requirements Specification Software Functional Requirements Specification Software Module Requirements Specification A Formal Description of Program Operation Configuration Control for the Requirements Measuring Software Design Alternatives Maintainability DYNAMIC SOFTWARE MEASUREMENT Introduction A Stochastic Description of Program Operation The Profiles of Software Dynamics Estimates for Profiles Code Instrumentation Instrumenting for the Profiles Partial Complexity A Measure of Cohesion Entropy Testability Revisited THE MEASUREMENT OF SOFTWARE TESTING ACTIVITY Introduction Static and Dynamic Measurement A Metaphor for Test Activity Measurement Based Testing Fractional Measures Introduction to Statistical Testing SOFTWARE AVAILABILITY Introduction Software Reliability Availability Security Maintainability IMPLEMENTING A SOFTWARE MEASUREMENT PLAN The Software Measurement Process Building a Measurement Process Measurement Process Improvement Institutionalizing Measurement Process Improvement A Network Based Measurement System IMPLEMENTING A SOFTWARE RESEARCH PLAN What Is Software Research? Implementing a Research Plan Defining Software Research Objectives Budgeting for Software Research Research Pays APPENDIXES REVIEW OF MATHEMATICAL FUNDAMENTALS Matrix Algebra Some Notions of Probability Discrete Probability Distributions Continuous Probability Distributions Statistics Tests of Hypotheses Introduction to Modeling A STANDARD FOR THE MEASUREMENT OF C PROGRAMMING LANGUAGE ATTRIBUTES Introduction Compiler Directives Style and Statement Metrics Lexical Metrics Control Flowgraph Metrics Coupling Metrics Definitions Tokens,c49,International Symposium on Search Based Software Engineering,cp49,accepted,f740,2012,2012-01-23
s916,p916,Formulating software engineering as a search problem,"Metaheuristic techniques such as genetic algorithms, simulated annealing and tabu search have found wide application in most areas of engineering. These techniques have also been applied in business, financial and economic modelling. Metaheuristics have been applied to three areas of software engineering: test data generation, module clustering and cost/effort prediction, yet there remain many software engineering problems which have yet to be tackled using metaheuristics. It is surprising that metaheuristics have not been more widely applied to software engineering; many problems in software engineering are characterised by precisely the features which make metaheuristics search applicable. In the paper it is argued that the features which make metaheuristics applicable for engineering and business applications outside software engineering also suggest that there is great potential for the exploitation of metaheuristics within software engineering. The paper briefly reviews the principal metaheuristic search techniques and surveys existing work on the application of metaheuristics to the three software engineering areas of test data generation, module clustering and cost/effort prediction. It also shows how metaheuristic search techniques can be applied to three additional areas of software engineering: maintenance/evolution system integration and requirements scheduling. The software engineering problem areas considered thus span the range of the software development process, from initial planning, cost estimation and requirements analysis through to integration, maintenance and evolution of legacy systems. The aim is to justify the claim that many problems in software engineering can be reformulated as search problems, to which metaheuristic techniques can be applied. The goal of the paper is to stimulate greater interest in metaheuristic search as a tool of optimisation of software engineering problems and to encourage the investigation and exploitation of these technologies in finding near optimal solutions to the complex constraint-based scenarios which arise so frequently in software engineering.",c21,Grid Computing Environments,cp21,accepted,f741,2005,2005-07-24
s918,p918,End-user software engineering with assertions in the spreadsheet paradigm,"There has been little research on end-user program development beyond the activity of programming. Devising ways to address additional activities related to end-user program development may be critical, however, because research shows that a large proportion of the programs written by end users contain faults. Toward this end, we have been working on ways to provide formal ""software engineering"" methodologies to end-user programmers. This paper describes an approach we have developed for supporting assertions in end-user software, focusing on the spreadsheet paradigm. We also report the results of a controlled experiment, with 59 end-user subjects, to investigate the usefulness of this approach. Our results show that the end users were able to use the assertions to reason about their spreadsheets, and that doing so was tied to both greater correctness and greater efficiency.",c56,European Conference on Software Process Improvement,cp56,accepted,f742,2016,2016-03-25
s919,p919,Writing good software engineering research papers: minitutorial,"Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to ICSE 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.",c29,International Conference on Software Engineering,cp29,accepted,f743,2015,2015-11-28
s920,p920,Problems and Programmers: an educational software engineering card game,"Problems and Programmers is an educational card game that we have developed to help teach software engineering. It is based on the observation that students, in a typical software engineering course, gain little practical experience in issues regarding the software process. The underlying problem is time: any course faces the practical constraint of only being able to involve students in at most a few small software development projects. Problems and Programmers overcomes this limitation by providing a simulation of the software process. In playing the game, students become aware of not only general lessons, such as the fact that they must continuously make tradeoffs among multiple potential next steps, but also specific issues such as the fact that inspections improve the quality of code but delay its delivery time. We describe game play of Problems and Programmers, discuss its underlying design, and report on the results of a small experiment in which twenty-eight students played the game.",c18,Conference on Innovative Data Systems Research,cp18,accepted,f744,2012,2012-01-31
s921,p921,Software engineering: a practitioner's approach (2nd ed.),Abstract content goes here ...,c113,International Conference on Image Analysis and Processing,cp113,accepted,f745,2002,2002-05-06
s922,p922,Software engineering education: a roadmap,"Software’s increasingly critical role in systems of widespread significance presents new challenges for the education of software engineers. Not only is our dependence on software increasing, but the character of software production is itself changing ‐ and with it the demands on the software developers. Four challenges for educators of software developers help identify aspirations for software engineering education.",c29,International Conference on Software Engineering,cp29,accepted,f746,2015,2015-03-22
s923,p923,A software engineering experiment in software component generation,"The paper presents results of a software engineering experiment in which a new technology for constructing program generators from domain-specific specification languages has been compared with a reuse technology that employs sets of reusable Ada program templates. Both technologies were applied to a common problem domain, constructing message translation and validation modules for military command, control, communications and information systems (C/sup 3/I). The experiment employed four subjects to conduct trials of use of the two technologies on a common set of test examples. The experiment was conducted with personnel supplied and supervised by an independent contractor. Test cases consisted of message specifications taken from Air Force C/sup 3/I systems. The main results are that greater productivity was achieved and fewer error were introduced when subjects used the program generator than when they used Ada templates to implement software modules from sets of specifications. The differences in the average performance of the subjects are statistically significant at confidence levels exceeding 99 percent.",c88,Symposium on the Theory of Computing,cp88,accepted,f747,2014,2014-02-05
s924,p924,Software Engineering Processes: Principles and Applications,Fundamentals of the Software Engineering Process Introduction A Unified Framework of the Software Engineering Process Process Algebra Process-Based Software Engineering Software Engineering Process System Modeling The CMM Model The ISO 9001 Model The BOOTSTRAP Model The ISO/IEC 15504 (SPICE) Model The Software Engineering Process Reference Model: SEPRM Software Engineering Process System Analysis Benchmarking the SEPRM Processes Comparative Analysis of Current Process Models Transformation of Capability Levels Between Current Process Models Software Engineering Process Establishment Software Process Establishment Methodologies An Extension of ISO/IEC TR 15504 Model Software Engineering Process Assessment Software Process Assessment Methodologies Software Process Assessment Supporting Tools Software Engineering Process Improvement Software Process Improvement Methodologies Case Studies in Software Process Improvement Review And Perspectives Bibliography Appendices Index,c40,IEEE International Conference on Software Maintenance and Evolution,cp40,accepted,f748,2013,2013-02-03
s925,p925,Search Based Software Engineering,Abstract content goes here ...,j75,Lecture Notes in Computer Science,jv75,accepted,f749,2015,2015-03-29
s926,p926,Software engineering risk management,"Welcome to Software Engineering Risk Management (SERIM). As a professional associated with the development of software, you are well aware that the software development process can truly be a jungle, filled with hazards that lie in wait to sabotage your projects. These hazards (risks) are numerous and often complex. The purpose of this application is to help you find a safer path through this jungle by assessing risk factors, analyzing risks from several different perspectives, and developing focused action plans to manage risks before they sabotage your projects. I have used the mathematics of probability to design the formulas to help you assess and manage risks in the complex software development environment (Complete information on the SERIM ModelOs equations is included in this application.)",c56,European Conference on Software Process Improvement,cp56,accepted,f750,2016,2016-11-24
s927,p927,Measurement and experimentation in software engineering,"The contributions of measurement and experimentation to the state of the art in software engineering are reviewed. The role of measurement in developing theoretical models is discussed, and concerns for reliability and validity are stressed. Current approaches to measuring software characteristics are presented as examples. In particular, software complexity metrics related to control flow, module interconnectedness, and Halstead's Software Science are discussed. The use of experimental methods in evaluating cause-effect relationships is also discussed. Example programs of experimental research which investigated conditional statements and control flow are reviewed. The conclusion argues that many advances in software engineering will be related to improvements in the measurement and experimental evaluation of software techniques and practices.",j168,Proceedings of the IEEE,jv168,accepted,f751,2019,2019-10-09
s928,p928,"Component-based software engineering: technologies, development frameworks, and quality assurance schemes","Component-based software development approach is based on the idea to develop software systems by selecting appropriate off-the-shelf components and then to assemble them with a well-defined software architecture. Because the new software development paradigm is very different from the traditional approach, quality assurance (QA) for component-based software development is a new topic in the software engineering community. In this paper, we survey current component-based software technologies, describe their advantages and disadvantages, and discuss the features they inherit. We also address QA issues for component-based software. As a major contribution, we propose a QA model for component-based software which covers component requirement analysis, component development, component certification, component customization, and system architecture design, integration, testing and maintenance.",c76,International Conference on Artificial Neural Networks,cp76,accepted,f752,2013,2013-08-05
s929,p929,A method for assessing the software engineering capability of contractors,"This document provides guidelines and procedures for assessing the ability of potential DoD contractors to develop software in accordance with modem software engineering methods. It includes spl-:ific questions and a method for evaluating the results. ,I General Introduction The purpose of this document is to facilitate objective and consistent assessments of the ability of potential DoD contractors to dovelop software in accordance with modem software engineering methods. Such assessments would be conducted either In the pre-solicitation qualification process, in the formal source selection process, or both. While this doc~ument Is Intended to guide the assessment of a contractor's overall software engineering capability, it can also be valuable in the assessment ."", a specific project team's software engineering capability. Alternatively, this document can be used as an aid to software development organizations in conducting an internal as issment cf their own softvare engineering capability. The document is designed to help An assessment team define the highest priorldy steps for the Improvement of an organization's capability. Because an understanding of proper software engineering practice is only now developing, standard, well-accepted measures do not yet exist. The assessment questions listed in the body of this .)•cum,'nt are phrased so that an affirmative answer indicates that an organization has a desirable characteristic. Some of the questions pertain to advanced concepts of software engineering that may not yet be sufficiently refined or disseminated to be incorporated in a contractor's standard practice; therefore, not all assessment questions need be answered affirmatively for an organization to be considered to have a modern software engineering capability. The capability of a contractor to perform software engineering has been divided into three areas: 1. organization and resource management 2. software engine.)rlng process and its management 3. tools and technology. The qualities that the questions assess are different for each of these areas acnd are described in the introductions to the questions for each area. 093087 SEt Assessment Methodology A full assessment of software engineering capability' Includes some evaluation of the experience level of the software development personnel. Addendum A contains suggested questions for use In this evaluation.",c14,International Conference on Exploring Services Science,cp14,accepted,f753,2016,2016-01-05
s930,p930,Toward a Discipline of Software Engineering,"Despite rapid changes in computing and software development, some fundamental ideas have remained constant. This article describes eight such concepts that together constitute a viable foundation for a software engineering discipline: abstraction, analysis and design methods and notations, user interface prototyping, modularity and architecture, software life cycle and process, reuse, metrics, and automated support.",j147,IEEE Software,jv147,accepted,f754,2021,2021-03-21
s931,p931,Software Engineering: An Engineering Approach,"From the Publisher: 
A clear-cut, practical approach to software development! Emphasizing both the design and analysis of the technology, Peters and Pedrycz have written a comprehensive and complete text on a quantitative approach to software engineering. As you read the text, youll learn the software design practices that are standard practice in the industry today. Practical approaches to specifying, designing and testing software as well as the foundations of Software Engineering are also presented. Key Features 
 
*Thorough coverage is provided on the quantitative aspects of software Engineering including software measures, software quality, software costs and software reliability. 
*A complete case study allows students to trace the application of methods and practices in each chapter. 
*Examples found throughout the text are in C++ and Java. 
*A wide range of elementary and intermediate problems as well as more advanced research problems are available at the end of each chapter. 
*Students are given the opportunity to expand their horizons through frequent references to related web pages.",c61,Jahrestagung der Gesellschaft für Informatik,cp61,accepted,f755,2017,2017-06-04
s932,p932,Evaluating Software Engineering Technologies,"Many new software development practices, tools, and techniques have been introduced in recent years. Few, however, have been empirically evaluated. The objectives of this study were to measure technology use in a production environment, develop a statistical model for evaluating the effectiveness of technologies, and evaluate the effects of some specific technologies on productivity and reliability. A carefully matched sample of 22 projects from the Software Engineering Laboratory database was studied using an analysis-of-covariance procedure. Limited use of the technologies considered in the analysis produced approximately a 30 percent increase in software reliability. These technologies did not demonstrate any direct effect on development productivity.",j142,IEEE Transactions on Software Engineering,jv142,accepted,f756,2014,2014-04-08
s933,p933,Using a behavioral theory of program comprehension in software engineering,"A theory is presented of how a programmer goes about understanding a program. The theory is based on a representation of knowledge about programs as a succession of knowledge domains which bridge between the problem domain and the executing program. A hypothesis and verify process is used by programmers to reconstruct these domains when they seek to understand a program.
 The theory is useful in several ways in software engineering: It makes accurate predictions about the effectiveness of documentation; it can be used to systematically evaluate and critique other claims about documentation, and it may even be a useful guideline to a programmer in actually constructing documentation.",c29,International Conference on Software Engineering,cp29,accepted,f757,2015,2015-03-02
s935,p935,"Software Engineering: Design, Reliability, and Management","Software engineering: design, reliability, and management , Software engineering: design, reliability, and management , مرکز فناوری اطلاعات و اطلاع رسانی کشاورزی",c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f758,2004,2004-11-21
s936,p936,Advances in Software Engineering and Knowledge Engineering,"The papers collected in this book were invited by the editors as tutorial courses or keynote speeches for the Fourth International Conference on Software Engineering and Knowledge Engineering. The book offers wide coverage of the main topics involved with the specifications, prototyping, development and maintenance of software systems and knowledge-based systems. The main issues in the area of software engineering and knowledge engineering are addressed and for each analyzed topic the corresponding research state is reported.",c87,European Conference on Computer Vision,cp87,accepted,f759,2014,2014-11-07
s937,p937,The software engineering laboratory - an operational software experience factory,"For 15 years, the Software Engineering Laboratory (SEL) has been carrying out studies and experiments for the purpose of understand- ing, assessing, and improving software and software processes within a production software development environment at the National Aeronautics and Space Administration/Goddard Space Flight Center (NASA/GSFC). The SEL comprises three major organizations: NASA/GSFC, Flight Dynamics Division University of Maryland, Department of Computer Science Computer Sciences Corporation, Flight Dynamics Technology Group - These organizations have jointly carried out several hundred software studies, producing hundreds of reports, papers, and documents, all of which de scribe some aspect of the software engineering technology that has been analyzed in the flight dynamics environment at NASA. The studies range from small, controlled experiments (such as analyzing the effectiveness of code readingversus that of functional testing) tolarge, multiple- project studies (such as assessing the impacts of Ada on a production environment). The organization's driving goal is to improve the software process continually, so that sustained improvement may be observed in the resulting products. This paper discusses the SEL as a functioning example of an operational software experience factory and summarizes the characteristics of and major lessons learned from 15 years of SEL operations.",c29,International Conference on Software Engineering,cp29,accepted,f760,2015,2015-09-27
s938,p938,Software engineering tools and environments: a roadmap,"Tools and environments to aid developers in producing software have existed, in one form or another, since the early days of computer programming. They are becoming increasingly crucial as the demand for software increases, time-to-market decreases, and diversity and complexity grow beyond anything imagined a few decades ago. In this paper, we briefly review some of the history of tools and environments in software engineering, and then discuss some key challenges that we believe the field faces over the next decade.",c29,International Conference on Software Engineering,cp29,accepted,f761,2015,2015-02-17
s939,p939,A Mature Profession of Software Engineering.,"Abstract : A model is presented that allows the characterization of the maturity of a profession in terms of eight infrastructure components: initial professional education, accreditation, skills development, certification, licensing, professional development, a code of ethics, and a professional society. Several mature professions are examined to provide examples of the nature of these components. The current states of the components of software engineering are described, and predictions are made for the evolution of those components as the profession matures.",c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f762,2011,2011-11-12
s940,p940,Software engineering with ada,"Now, we come to offer you the right catalogues of book to open. software engineering with ada is one of the literary work in this world in suitable to be reading material. That's not only this book gives reference, but also it will show you the amazing benefits of reading a book. Developing your countless minds is needed; moreover you are kind of people with great curiosity. So, the book is very appropriate for you.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f763,2010,2010-01-31
s941,p941,Cleanroom software engineering: technology and process,"Cleanroom software engineering is a process for developing and certifying high-reliability software. Combining theory-based engineering technologies in project management, incremental development, software specification and design, correctness verification, and statistical quality certification, the Cleanroom process answers today's call for more reliable software and provides methods for more cost-effective software development.Cleanroom originated with Harlan D. Mills, an IBM Fellow and a visionary in software engineering. Written by colleagues of Mills and some of the most experienced developers and practitioners of Cleanroom, Cleanroom Software Engineering provides a roadmap for software management, development, and testing as disciplined engineering practices. This book serves both as an introduction for those new to Cleanroom and as a reference guide for the growing practitioner community. Readers will discover a proven way to raise both quality and productivity in their software-intensive products, while reducing costs.Highlights Explains basic Cleanroom theory Introduces the sequence-based specification method Elaborates the full management, development, and certification process in a Cleanroom Reference Model (CRM) Shows how the Cleanroom process dovetails with the SEI's Capability Maturity Model for Software (CMM) Includes a large case study to illustrate how Cleanroom methods scale up to large projects.",c60,IEEE International Conference on Software Engineering and Formal Methods,cp60,accepted,f764,2011,2011-01-22
s942,p942,Software Pioneers: Contributions to Software Engineering,Abstract content goes here ...,c67,Enterprise Application Integration,cp67,accepted,f765,2002,2002-08-03
s943,p943,Experimental design and analysis in software engineering,Abstract content goes here ...,j169,Annals of Software Engineering,jv169,accepted,f766,2013,2013-10-26
s945,p945,Understanding the Philosophical Underpinnings of Software Engineering Research in Information Systems,Abstract content goes here ...,c58,Australian Software Engineering Conference,cp58,accepted,f767,2021,2021-10-14
s946,p946,Toward computer-supported concurrent software engineering,"An experimental software engineering environment called the flexible environment for collaborative software engineering (Flecse), which supports concurrent software engineering, is discussed. Flecse features tools designed to surmount collaboration problems that software engineers are increasingly encountering. The implementation of five important themes of concurrent software engineering in Flecse tools, concepts, life cycles, integration, and sharing, is examined.<<ETX>>",j79,Computer,jv79,accepted,f768,2014,2014-02-28
s947,p947,Hints for Reviewing Empirical Work in Software Engineering,Abstract content goes here ...,j141,Empirical Software Engineering,jv141,accepted,f769,2004,2004-08-31
s948,p948,Software Engineering Project Management,"From the Publisher: 
The 2nd edition of Thayer's popular, bestselling book presents a top-down practical view of managing a successful software engineering project. The book builds on a framework for project managements activities based on the planning, organizing, staffing, directing, and controlling model. Thayer provides information designed to help readers understand and successfully perform the unique role of a project manager. 400 pp. Pub: 8/97.",c58,Australian Software Engineering Conference,cp58,accepted,f770,2021,2021-12-17
s949,p949,"Process-Centered Software Engineering Environments, A Brief History and Future Challenges",Abstract content goes here ...,j169,Annals of Software Engineering,jv169,accepted,f771,2013,2013-03-20
s950,p950,Software engineering code of ethics,"T he Board of Governors of the IEEE Computer Society established a steering committee in May 1993 for evaluating, planning, and coordinating actions related to establishing software engineering as a profession. In that same year the ACM Council endorsed the establishment of a Commission on Software Engineering. By January 1994, both societies formed a joint steering committee “to establish the appropriate set(s) of standards for professional practice of software engineering upon which industrial decisions, professional certification, and educational curricula can be based.” To accomplish these tasks they made the following recommendations: ACM and the IEEE Computer Society join forces to create a code of professional practices within our industry. Now, we ask for your comments.",c94,Vision,cp94,accepted,f772,2020,2020-04-19
s951,p951,Predicate Logic for Software Engineering,"The interpretations of logical expressions found in most introductory textbooks are not suitable for use in software engineering applications because they do not deal with partial functions. More advanced papers and texts deal with partial functions in a variety of complex ways. This paper proposes a very simple change to the classic interpretation of predicate expressions, one that defines their value for all values of all variables, yet is almost identical to the standard definitions. It then illustrates the application of this interpretation in software documentation. >",c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f773,2001,2001-06-02
s953,p953,Computational intelligence in software engineering,"The paper provides a unified view of computational intelligence in the context of software engineering. Technologies such as fuzzy sets, neural and evolutionary computing useful in software development are considered. The links between software engineering and computational intelligence are identified. An illustration is given in terms of a fuzzy software quality model.",c67,Enterprise Application Integration,cp67,accepted,f774,2002,2002-02-03
s955,p955,Perspectives in Software Engineering,"Software engineering refers to the process of creating software systems. It applies loosely to techniques which reduce high software cost and complexity while increasing reliability and mochfiability. This paper outlines the procedures used in the development of computer software, emphasizing large-scale software development, and pmpomtmg areas where problems exist and solutions have been proposed Solutions from both the management and the programmer points of vtew are then given for many of these problem areas.",c11,Hawaii International Conference on System Sciences,cp11,accepted,f775,2006,2006-07-27
s956,p956,Outline of a Paradigm Change in Software Engineering,Abstract content goes here ...,c56,European Conference on Software Process Improvement,cp56,accepted,f776,2016,2016-03-07
s958,p958,Ethical Issues in Empirical Studies of Software Engineering,"The popularity of empirical methods in software engineering research is on the rise. Surveys, experiments, metrics, case studies, and field studies are examples of empirical methods used to investigate both software engineering processes and products. The increased application of empirical methods has also brought about an increase in discussions about adapting these methods to the peculiarities of software engineering. In contrast, the ethical issues raised by empirical methods have received little, if any, attention in the software engineering literature. This article is intended to introduce the ethical issues raised by empirical research to the software engineering research community and to stimulate discussion of how best to deal with these ethical issues. Through a review of the ethical codes of several fields that commonly employ humans and artifacts as research subjects, we have identified major ethical issues relevant to empirical studies of software engineering. These issues are illustrated with real empirical studies of software engineering.",c17,International Conference on Enterprise Information Systems,cp17,accepted,f777,2008,2008-01-04
s960,p960,Agent orientation in software engineering,"Agent-Oriented Software Engineering (AOSE) is rapidly emerging in response to urgent needs in both software engineering and agent-based computing. While these two disciplines coexisted without remarkable interaction until some years ago, today there is rich and fruitful interaction among them and various approaches are available that bring together techniques, concepts and ideas from both sides. This article offers a guide to the broad body of literature on AOSE. The guide, which is intended to be of value to both researchers and practitioners, is structured according to key issues and key topics that arise when dealing with AOSE: methods and frameworks for requirements engineering, analysis, design, and implementation; languages for programming, communication and coordination and ontology specification; and development tools and platforms.",j170,Knowledge engineering review (Print),jv170,accepted,f778,2019,2019-04-29
s961,p961,Simulation in software engineering training,"Simulation is frequently used for training in many application areas like aviation and economics, but not in software engineering. We present the SESAM project which focuses on software engineering education using simulation. In the SESAM project a simulator was developed. Using this simulator, a student can take the role of a software project manager. The simulated software project can be finished within a couple of hours because it is simulated in ""quick-motion"" mode. The background and goals of the SESAM project are presented. A new simulation model, the so called QA model, is introduced. The model behavior is demonstrated by investigating and comparing different strategies for software development. The results of experiments based on the QA model are reported. Finally, conclusions are drawn from the experiments and future work is outlined.",c78,Neural Information Processing Systems,cp78,accepted,f779,2012,2012-07-13
s962,p962,A survey of Agent-Oriented Software Engineering,"Agent-Oriented Software Engineering is the one of the most recent contributions to the field of Software Engineering. It has several benefits compared to existing development approaches, in particular the ability to let agents represent high-level abstractions of active entities in a software system. This paper gives an overview of recent research and industrial applications of both general high-level methodologies and on more specific design methodologies for industry-strength software engineering.",c25,International Conference on Contemporary Computing,cp25,accepted,f780,2014,2014-06-21
s963,p963,"Software engineering, the software process and their support","Computers are being applied more and more widely, penetrating ever deeper into the very fabric of society. Mankind is becoming increasingly dependent on the availability of software and its continuing validity. To achieve this consistently and reliably, in an operational domain that is forever changing, requires disciplined execution of the software development and evolution process and its effective management. That is the goal of advanced software engineering [1]. This paper summarises basic concepts of software engineering and of the software development process. This leads to a principle of uncertainty, analysis of its implications for the software development process, an overview of computer-assisted software engineering (CASE) and brief comments on the societal relevance of these topics. For researchers in the field and practitioners familiar with individual concepts, issues and specific solutions, the paper provides a unifying framework, a basis for conceptual advance. Those without a significant practical software engineering background and experienced graduate students will extend general familiarity with fresh insights, new concepts and additional detail. Undergraduate and graduate students without significant experience may treat the paper as an introductory text.",j171,Software Engineering Journal,jv171,accepted,f781,2007,2007-11-23
s964,p964,Taming Agents and Objects in Software Engineering,Abstract content goes here ...,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f782,2016,2016-10-03
s965,p965,Assessing process-centered software engineering environments,"Process-centered software engineering environments (PSEEs) are the most recent generation of environments supporting software development activities. They exploit an representation of the process (called the process model that specifies how to carry out software development activities, the roles and tasks of software developers, and how to use and control software development tools. A process model is therefore a vehicle to better understand and communicate the process. If it is expressed in a formal notation, it can be used to support a variety of activities such as process analysis, process simulation, and process enactment. PSEEs provide automatic support for these activities. They exploit languages based on different paradigms, such as Petri nets and rule-based systems. They include facilities to edit and analyze process models. By enacting the process model, a PSEE provides a variety of services, such as assistance for software developers, automation of routine tasks, invocation and control of software development tools, and enforcement of mandatory rules and practices. Several PSEEs have been developed, both as research projects and as commercial products. The initial deployment and exploitation of this technology have made it possible to produce a significant amount of experiences, comments, evaluations, and feedback. We still lack, however, consistent and comprehensive assessment methods that can be used to collect and organize this information. This article aims at contributing to the definition of such methods, by providing a systematic comparison grid and by accomplishing an initial evaluation of the state of the art in the field. This evaluation takes into account the systems that have been developed by the authors in the past five years, as well as the main characteristics of other well-known environments",c60,IEEE International Conference on Software Engineering and Formal Methods,cp60,accepted,f783,2011,2011-07-02
s966,p966,Software Engineering Process Group Guide,"Abstract : Improving the process of software systems development and maintenance is the most reliable way to improve product quality. This document offers guidance on how to establish a software engineering process group (SEPG) and related software engineering process improvement functions. The process group works with line organizations to improve process quality by helping to assess current status, plan and implement improvements, and transfer technology to facilitate improvement in practice.",c112,Very Large Data Bases Conference,cp112,accepted,f784,2018,2018-04-27
s968,p968,Component Metadata for Software Engineering Tasks,Abstract content goes here ...,c22,International Conference on Data Technologies and Applications,cp22,accepted,f785,2020,2020-03-11
s969,p969,Reuse and Productivity in Integrated Computer-Aided Software Engineering: An Empirical Study,"Growing competition in the investment banking industry has given rise to increasing demand for high functionality software applications that can be developed in a short period of time. Yet delivering such applications creates a bottleneck in software development activities. This dilemma can be addressed when firms shift to development methods that emphasize software reusability. This article examines the productivity implications of object and repository-based integrated computer-aided software engineering (ICASE) software development in the context of a major investment bank's information systems strategy. The strategy emphasizes software reusability. Our empirical results, based on data from 20 projects that delivered software for the bank's New Trades Processing Architecture (NTPA), indicate an order of magnitude gain in software development productivity and the importance of reuse as a driver in realizing this result. In addition, results are presented on the extent of the learning that occurred over a two-year period after ICASE was introduced, and on the influence of the link between application characteristics and the ICASE tool set in achieving development performance. This work demonstrates the viability of the firm's IS strategy and offers new ideas for code reuse and software development productivity measurement that can be applied in development environments that emphasize reuse.",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f786,2008,2008-11-01
s970,p970,Representing Software Engineering Models: The TAME Goal Oriented Approach,"A methodology and a knowledge representation and reasoning framework for top-down goal-oriented characterization, modeling, and execution of software engineering activities is presented. A prototype system (ES-TAME) which demonstrates the underlying knowledge representation and reasoning principles is described. ES-TAME provides an object-oriented metamodel concept that provides support for tailorable and reusable software engineering models (SEMs). It provides the basic mechanisms, functions, and attributes for all the other models. It is based on interobject relationships, dynamic viewpoints, and selective inheritance in addition to traditional object-oriented mechanisms. Descriptive SEMs include representations for basic software engineering activities. They are controlled and made operational by active GQM (goal-question-metric paradigm) models which are built by a systematic mechanism for defining and evaluating project and corporate goals and using measurement to provide feedback in real-time. >",c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",cp38,accepted,f787,2015,2015-03-15
s972,p972,A Knowledge-Based Environment for Modeling and Simulating Software Engineering Processes,"The design and representation schemes used in constructing a prototype computational environment for modeling and simulating multiagent software engineering processes are described. This environment is called the articulator. An overview of the articulator's architecture identifying five principal components is provided. Three of the components, the knowledge metamodel, the software process behavior simulator, and a knowledge base querying mechanism, are detailed and examples are included. The conclusion reiterates what is unique to this approach in applying knowledge engineering techniques to the problems of understanding the statics and dynamics of complex software engineering processes. >",j1,IEEE Transactions on Knowledge and Data Engineering,jv1,accepted,f788,2020,2020-06-22
s973,p973,"Reuse-based software engineering: techniques, organization, and controls",Reuse-Based Software Engineering offers an in-depth discussion of the fundamental issues and total coverage of the state-of-the-art. The inclusion of review questions and exercises makes it an excellent tutorial for both academics and professionals.,c62,International Conference on Software Reuse,cp62,accepted,f789,2006,2006-06-23
s974,p974,Towards a software engineering approach to Web site development,"The World Wide Web (WWW) has become ""the"" global infrastructure for delivering information and services. The demands and expectations of information providers and consumers are pushing WWW technology towards higher-level quality of presentation, including active contents and improved usability of the hypermedia distributed infrastructure. This technological evolution, however, is not supported by adequate Web design methodologies. Web site development is usually carried out without following a well-defined process and lacks suitable tool support. In addition, Web technologies are quite powerful but rather low-level and their semantics is often left largely unspecified. As a consequence, understanding the conceptual structure of a complex Web site and managing its evolution are complex and difficult tasks. The approach we advocate here is based on sound software engineering principles. The Web site development process goes through requirements analysis, design, and implementation in a high-level language. We define an object-oriented modeling framework, called WOOM, which provides constructs and abstractions for a high-level implementation of a Web site. An important feature of WOOM is that it clearly separates the data that are presented through the site from the context in which the user accesses such data. This feature not only enhances separation of concerns in the design stage, but also favors its subsequent evolution. The paper provides a view of the approach and of its current prototype implementation.",c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f790,2012,2012-09-22
s976,p976,Has twenty-five years of empirical software engineering made a difference?,"Our activities in software engineering typically fall into one of three categories, (1) to invent new phenomena, (2) to understand existing phenomena, and (3) to facilitate inspirational education. This paper explores the place of empirical software engineering in the first two of these activities. In this exploration evidence is drawn from the empirical literature in the areas of software inspections and software cost modelling and estimation. This research is then compared with the literature published in the Journal of Empirical Software Engineering. This evidence throws light on aspects of theory derivation, experimental methods and analysis, and also the challenges that we face as empirical software engineering evolves into the future.",c80,International Conference on Learning Representations,cp80,accepted,f791,2005,2005-11-15
s977,p977,A flexible transaction model for software engineering,"It is generally recognized that the classical transaction model, providing atomicity and serializability, is too strong for certain application areas since it unnecessarily restricts concurrency. The author is concerned with supporting cooperative work in multiuser design environments, particularly teams of programmers cooperating to develop and maintain software systems. An extended transaction model that meets the special requirements of software engineering projects is presented, possible implementation techniques are described, and a number of issues regarding the incorporation of such a model into multiuser software development environments are discussed.<<ETX>>",c107,British Machine Vision Conference,cp107,accepted,f792,2012,2012-07-12
s979,p979,Computer-Aided Software Engineering in a distributed workstation environment,"Computer-Aided Software Engineering environments are becoming essential for complex software projects, just as CAD systems have become essential for complex hardware projects. DSEE, the DOMAIN Software Engineering Environment, is a distributed, production quality, software development environment that runs on Apollo workstations. DSEE provides source code control, configuration management, release control, advice management, task management, and user-defined dependency tracking with automatic notification.
 DSEE incorporates some of the best ideas from existing systems. This paper describes DSEE, contrasts it other systems, and discusses some of the technical issues involved in the construction of a highly-reliable, safe, efficient, and distributed development environment.",c63,IEEE International Software Metrics Symposium,cp63,accepted,f793,2008,2008-07-12
s980,p980,Research synthesis in software engineering: a case for meta-analysis,"The use of meta-analytic techniques to summarize empirical software engineering research results is illustrated using a set of 5 published experiments from the literature. The intent of the analysis is to guide future work in this area through objective summarization of the literature to date. A focus on effect magnitude, in addition to statistical significance is championed, and the reader is provided with an illustration of simple methods for computing effect magnitudes.",c39,International Conference on Global Software Engineering,cp39,accepted,f794,2020,2020-01-06
s981,p981,Experimental Software Engineering Issues: Critical Assessment and Future Directions,Abstract content goes here ...,j75,Lecture Notes in Computer Science,jv75,accepted,f795,2015,2015-09-14
s982,p982,A critique of diffusion theory as a managerial framework for understanding adoption of software engineering innovations,"The authors provide a brief overview of classical diffusion theory and suggest the potential applicability of this theory to problems related to predicting the adoption of technological innovations, including those related to software engineering. They critically evaluate the theory, identifying elements that must be extended and modified before it can be applied to technology transition, in general, and software engineering, specifically. They offer suggestions on ways in which these limitations might be overcome.<<ETX>>",c21,Grid Computing Environments,cp21,accepted,f796,2005,2005-02-19
s983,p983,Object-oriented software engineering - practical software development using UML and Java,1. Software and Software Engineering. 2. Review of Object Orientation and Java. 3. Basing Software Development on Reusable Technology. 4. Developing Requirements. 5. Modelling with Classes. 6. Using Design Patterns. 7. Focusing on Users and Their Tasks. 8. Modelling Interactions and Behaviour. 9. Architecting and Designing Software. 10. Testing and Inspecting to Ensure High Quality. 11. Managing the Software Process. 12. Review. Appendix A: Summary of UML Notation used in this Book. Appendix B: Summary of the Documentation Formats Recommended in this Book. Appendix C: System Descriptions. Appendix D: Answers to Selected Exercises. Glossary. Index.,c105,Biometrics and Identity Management,cp105,accepted,f797,2006,2006-08-24
s984,p984,Distributed component technologies and their software engineering implications,"In this state-of-the-art report, we review advances in distributed component technologies, such as the Enterprise JavaBeans (EJB) specification and the CORBA component model (CCM). We assess the state of industrial practice in the use of distributed components. We show several architectural styles for whose implementation distributed components have been used successfully. We review the use of iterative and incremental development processes and the notion of a model-driven architecture. We then assess the state of the art in research into novel software engineering methods and tools for the modelling, reasoning and deployment of distributed components. The open problems identified during this review result in the formulation of a research agenda that will contribute to the systematic engineering of distributed systems based on component technologies.",c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f798,2012,2012-01-09
s986,p986,Extreme programming for software engineering education?,"The eXtreme Programming (XP) software development methodology, has received considerable attention in recent years. The adherents of XP anecdotally extol its benefits, particularly as a method that is highly responsive to changing customer's desires. While XP has acquired numerous vocal advocates, the interactions and dependencies between XP practices have not been adequately studied. Good software engineering practice requires expertise in a complex set of activities that involve the intellectual skills of planning, designing, evaluating, and revising. The authors explore the practices of XP in the context of software engineering education. To do so, one must examine the practices of XP as they influence the acquisition of software engineering skills. The practices of XP, in combination or isolation, may provide critical features to aid or hinder the development of increasingly capable practitioners. This paper evaluates the practices of XP in the context of acquiring these necessary software engineering skills.",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f799,2018,2018-10-01
s987,p987,Modeling Articulation Work in Software Engineering Processes,"Current software process modeling techniques do not generally support articulation work. Articulation work is the diagnosis, recovery and resumption of development activities that unexpectedly fail. It is an integral part of software process enactment since software processes can sometimes fail or breakdown. This paper presents a knowledge-based model of articulation work in software engineering processes. I t uses empirically-grounded heuristics t o address three problems in articulation work: diagnosing failed development activities, determining appropriate recovery, and resuming software processes. We first investigate the role and importance of articulation work with respect t o planned software development activities. We then outline a knowledge-based model of articulation work. The model has been implemented in a knowledgebased software process modeling environment called the Articulator. Combining the available software process modeling techniques and the model of articulation leads to a better foundation in process improvement and evolution.",c41,Software Product Lines Conference,cp41,accepted,f800,2002,2002-02-17
s988,p988,Agent-Oriented Software Engineering for Internet Applications,Abstract content goes here ...,c9,Pacific Symposium on Biocomputing,cp9,accepted,f801,2009,2009-02-13
s990,p990,Thinking objectively: software engineering in the small,"In 1968, the NATO Software Engineering Conference in Garmisch, Germany [6] initiated the concept of software engineering, identifying the problems with producing large, high-quality software applications. In 1975, De Remer [2] introduced the terms, “programming in the small” and “programming in the large” to differentiate the development characteristics of large-scale software development from detailed programming (for example, data structures and algorithms). The principal source of large-scale software at the time was development contracts issued by the U.S. Department of Defense. Since then, virtually all software engineering literature has concentrated explicitly and implicitly on the model of DoD contract software development. Since the late 1970s, the microcomputer revolution has dramatically increased the quantity of software produced, the average size of programs, and the number of companies involved in software development. Much more software is produced for internal use, commercial applications, and the mass-market than for deep-pocketed government and large industry. Using the number of units sold, mass-market software dwarfs the other forms of software sales. The growth of the software industry has produced many small companies that do not do contract software, but rather compete in other areas. This gives rise to at least four significant development issues that have not been adequately addressed in software engineering literature: company size, development mode, development size, and development speed. We discuss these issues and then discuss some of the shortcomings of current software engineering thinking for small companies.",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f802,2018,2018-07-17
s991,p991,in Software Engineering,"Software engineers work on multidisciplinary teams to identify and develop software solutions and to maintain software intensive systems of all sizes. The focus of this program is on the rigorous engineering practices necessary to build, maintain, and protect modern software intensive systems. Consistent with this focus, the software engineering baccalaureate program consists of a rigorous curriculum of science, math, computer science, and software engineering courses.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f803,2011,2011-12-13
s993,p993,Knowledge-based software engineering,"Knowledge-based software engineering emphasizes the fact that creating software is a knowledge-intensive activity, and proposes that making more knowledge available will facilitate the timely production of high-quality software. The author gives four reasons for software engineering being an interesting area for AI research. He also stipulates that KBSE researchers must answer several crucial questions: what part of the software process is targeted; what knowledge is applicable and how can it be represented, acquired and maintained; and how can one present the knowledge to developers to improve the quality and cost of software development?.<<ETX>>",c14,International Conference on Exploring Services Science,cp14,accepted,f804,2016,2016-07-08
s994,p994,Design rationale for software engineering: a survey,"The authors provide an introduction to design rationale and why it is important in software engineering. They look at the recent history of argumentation methods. They survey a number of the major systems developed for the support of design rationale, comparing their features and discussing their differences. They look at advantages and disadvantages of the various approaches to design rationale with special attention paid to how they can be used in the process software engineering. They conclude with a discussion of some open issues which are important for the inclusion of design rationale systems in the software engineering process.<<ETX>>",c56,European Conference on Software Process Improvement,cp56,accepted,f805,2016,2016-08-06
s995,p995,Software engineering: a roadmap,"This paper provides a roadmap for software engineering. It 
identifies the principal research challenges being faced by 
the discipline and brings together the threads derived from 
the key research specialisations within software 
engineering. The paper draws heavily on the roadmaps 
covering specific areas of software engineering research 
collected in this volume.",c29,International Conference on Software Engineering,cp29,accepted,f806,2015,2015-01-13
s996,p996,Ginger2: An Environment for Computer-Aided Empirical Software Engineering,"Empirical software engineering can be viewed as a series of actions to obtain knowledge and a better understanding about some aspects of software development, given a set of problem statements in the form of issues, questions or hypotheses. Experience has made us aware of the criticality of integrating the various types of data that are collected and analyzed as well as the criticality of integrating the various types of activities that take place, such as experiment design and the experiment itself. This has led us to develop a Computer-Aided Empirical Software Engineering (CAESE) framework to support the empirical software engineering lifecycle. The paper first presents the CAESE framework that consists of three elements: (1) a process model for the ""lifecycle"" of empirical software engineering studies, including needs analysis, experiment design, actual experimentation, and analyzing and packaging results; (2) a model that helps empirical software engineers decide how to look at the ""world"" to be studied in a coherent manner; (3) an architecture, based on which CAESE environments can be built, consisting of tool sets for each phase of the process model, a process management mechanism, and the two types of integration mechanism that are vital for handling multiple types of data: data integration and control integration. Next, the paper describes the Ginger2 environment as an instantiation of our framework. It concludes with reports on case studies using Ginger2, which dealt with a variety of empirical data types including mouse and keystrokes, eye traces, 3D movement, skin resistance level, and videotaped data.",c14,International Conference on Exploring Services Science,cp14,accepted,f807,2016,2016-10-22
s998,p998,DAMOKLES - A Database System for Software ENgineering Environments,Abstract content goes here ...,c97,Interspeech,cp97,accepted,f808,2004,2004-12-16
s999,p999,Software Engineering: An Object-Oriented Perspective,"From the Publisher: 
This book has been written to communicate the complexity of software engineering, a field that is on the rise. Braude has combined practical industrial experience with up-to-date academic experience to give the reader a feel for the complexity and important issues of real-world development. A longitudinal case study using IEEE standards is implemented throughout the book, along with many other examples, which enables the reader to understand the implications of quality factors, proper requirements documents, appropriate design, and appropriate project management techniques.",c105,Biometrics and Identity Management,cp105,accepted,f809,2006,2006-09-11
s1001,p1001,Bioinformatics enrichment tools: paths toward the comprehensive functional analysis of large gene lists,"Functional analysis of large gene lists, derived in most cases from emerging high-throughput genomic, proteomic and bioinformatics scanning approaches, is still a challenging and daunting task. The gene-annotation enrichment analysis is a promising high-throughput strategy that increases the likelihood for investigators to identify biological processes most pertinent to their study. Approximately 68 bioinformatics enrichment tools that are currently available in the community are collected in this survey. Tools are uniquely categorized into three major classes, according to their underlying enrichment algorithms. The comprehensive collections, unique tool classifications and associated questions/issues will provide a more comprehensive and up-to-date view regarding the advantages, pitfalls and recent trends in a simpler tool-class level rather than by a tool-by-tool approach. Thus, the survey will help tool designers/developers and experienced end users understand the underlying algorithms and pertinent details of particular tool categories/tools, enabling them to make the best choices for their particular research interests.",j102,Nucleic Acids Research,jv102,accepted,f810,2002,2002-04-24
s1002,p1002,Bioconductor: open software development for computational biology and bioinformatics,Abstract content goes here ...,j5,Genome Biology,jv5,accepted,f811,2020,2020-02-09
s1004,p1004,"Expasy, the Swiss Bioinformatics Resource Portal, as designed by its users","Abstract The SIB Swiss Institute of Bioinformatics (https://www.sib.swiss) creates, maintains and disseminates a portfolio of reliable and state-of-the-art bioinformatics services and resources for the storage, analysis and interpretation of biological data. Through Expasy (https://www.expasy.org), the Swiss Bioinformatics Resource Portal, the scientific community worldwide, freely accesses more than 160 SIB resources supporting a wide range of life science and biomedical research areas. In 2020, Expasy was redesigned through a user-centric approach, known as User-Centred Design (UCD), whose aim is to create user interfaces that are easy-to-use, efficient and targeting the intended community. This approach, widely used in other fields such as marketing, e-commerce, and design of mobile applications, is still scarcely explored in bioinformatics. In total, around 50 people were actively involved, including internal stakeholders and end-users. In addition to an optimised interface that meets users' needs and expectations, the new version of Expasy provides an up-to-date and accurate description of high-quality resources based on a standardised ontology, allowing to connect functionally-related resources.",c30,IEEE Aerospace Conference,cp30,accepted,f812,2006,2006-01-19
s1005,p1005,"Nanopore sequencing technology, bioinformatics and applications",Abstract content goes here ...,j0,Nature Biotechnology,jv0,accepted,f813,2006,2006-08-23
s1008,p1008,The PATRIC Bioinformatics Resource Center: expanding data and analysis capabilities,"The PathoSystems Resource Integration Center (PATRIC) is the bacterial Bioinformatics Resource Center funded by the National Institute of Allergy and Infectious Diseases (https://www.patricbrc.org). PATRIC supports bioinformatic analyses of all bacteria with a special emphasis on pathogens, offering a rich comparative analysis environment that provides users with access to over 250 000 uniformly annotated and publicly available genomes with curated metadata. PATRIC offers web-based visualization and comparative analysis tools, a private workspace in which users can analyze their own data in the context of the public collections, services that streamline complex bioinformatic workflows and command-line tools for bulk data analysis. Over the past several years, as genomic and other omics-related experiments have become more cost-effective and widespread, we have observed considerable growth in the usage of and demand for easy-to-use, publicly available bioinformatic tools and services. Here we report the recent updates to the PATRIC resource, including new web-based comparative analysis tools, eight new services and the release of a command-line interface to access, query and analyze data.",c28,International Conference on Collaboration Technologies and Systems,cp28,accepted,f814,2009,2009-06-24
s1009,p1009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Abstract content goes here ...,c69,International Conference on Parallel Processing,cp69,accepted,f815,2010,2010-09-30
s1011,p1011,Protein Sequence Analysis Using the MPI Bioinformatics Toolkit,"The MPI Bioinformatics Toolkit (https://toolkit.tuebingen.mpg.de) provides interactive access to a wide range of the best‐performing bioinformatics tools and databases, including the state‐of‐the‐art protein sequence comparison methods HHblits and HHpred. The Toolkit currently includes 35 external and in‐house tools, covering functionalities such as sequence similarity searching, prediction of sequence features, and sequence classification. Due to this breadth of functionality, the tight interconnection of its constituent tools, and its ease of use, the Toolkit has become an important resource for biomedical research and for teaching protein sequence analysis to students in the life sciences. In this article, we provide detailed information on utilizing the three most widely accessed tools within the Toolkit: HHpred for the detection of homologs, HHpred in conjunction with MODELLER for structure prediction and homology modeling, and CLANS for the visualization of relationships in large sequence datasets. © 2020 The Authors.",c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f816,2016,2016-04-24
s1012,p1012,Snakemake - a scalable bioinformatics workflow engine,Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.,c8,The Compass,cp8,accepted,f817,2016,2016-07-02
s1013,p1013,Augur: a bioinformatics toolkit for phylogenetic analyses of human pathogens,"Summary and statement of need The analysis of human pathogens requires a diverse collection of bioinformatics tools. These tools include standard genomic and phylogenetic software and custom software developed to handle the relatively numerous and short genomes of viruses and bacteria. Researchers increasingly depend on the outputs of these tools to infer transmission dynamics of human diseases and make actionable recommendations to public health officials (Black et al., 2020; Gardy et al., 2015). In order to enable real-time analyses of pathogen evolution, bioinformatics tools must scale rapidly with the number of samples and be flexible enough to adapt to a variety of questions and organisms. To meet these needs, we developed Augur, a bioinformatics toolkit designed for phylogenetic analyses of human pathogens.",j174,Journal of Open Source Software,jv174,accepted,f818,2003,2003-01-04
s1014,p1014,"Improvements to PATRIC, the all-bacterial Bioinformatics Database and Analysis Resource Center","The Pathosystems Resource Integration Center (PATRIC) is the bacterial Bioinformatics Resource Center (https://www.patricbrc.org). Recent changes to PATRIC include a redesign of the web interface and some new services that provide users with a platform that takes them from raw reads to an integrated analysis experience. The redesigned interface allows researchers direct access to tools and data, and the emphasis has changed to user-created genome-groups, with detailed summaries and views of the data that researchers have selected. Perhaps the biggest change has been the enhanced capability for researchers to analyze their private data and compare it to the available public data. Researchers can assemble their raw sequence reads and annotate the contigs using RASTtk. PATRIC also provides services for RNA-Seq, variation, model reconstruction and differential expression analysis, all delivered through an updated private workspace. Private data can be compared by ‘virtual integration’ to any of PATRIC's public data. The number of genomes available for comparison in PATRIC has expanded to over 80 000, with a special emphasis on genomes with antimicrobial resistance data. PATRIC uses this data to improve both subsystem annotation and k-mer classification, and tags new genomes as having signatures that indicate susceptibility or resistance to specific antibiotics.",c71,IEEE International Conference on Information Reuse and Integration,cp71,accepted,f819,2022,2022-01-06
s1015,p1015,Human Splicing Finder: an online bioinformatics tool to predict splicing signals,"Thousands of mutations are identified yearly. Although many directly affect protein expression, an increasing proportion of mutations is now believed to influence mRNA splicing. They mostly affect existing splice sites, but synonymous, non-synonymous or nonsense mutations can also create or disrupt splice sites or auxiliary cis-splicing sequences. To facilitate the analysis of the different mutations, we designed Human Splicing Finder (HSF), a tool to predict the effects of mutations on splicing signals or to identify splicing motifs in any human sequence. It contains all available matrices for auxiliary sequence prediction as well as new ones for binding sites of the 9G8 and Tra2-β Serine-Arginine proteins and the hnRNP A1 ribonucleoprotein. We also developed new Position Weight Matrices to assess the strength of 5′ and 3′ splice sites and branch points. We evaluated HSF efficiency using a set of 83 intronic and 35 exonic mutations known to result in splicing defects. We showed that the mutation effect was correctly predicted in almost all cases. HSF could thus represent a valuable resource for research, diagnostic and therapeutic (e.g. therapeutic exon skipping) purposes as well as for global studies, such as the GEN2PHEN European Project or the Human Variome Project.",j102,Nucleic Acids Research,jv102,accepted,f820,2002,2002-05-31
s1016,p1016,Trends in the development of miRNA bioinformatics tools,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that regulate gene expression via recognition of cognate sequences and interference of transcriptional, translational or epigenetic processes. Bioinformatics tools developed for miRNA study include those for miRNA prediction and discovery, structure, analysis and target prediction. We manually curated 95 review papers and ∼1000 miRNA bioinformatics tools published since 2003. We classified and ranked them based on citation number or PageRank score, and then performed network analysis and text mining (TM) to study the miRNA tools development trends. Five key trends were observed: (1) miRNA identification and target prediction have been hot spots in the past decade; (2) manual curation and TM are the main methods for collecting miRNA knowledge from literature; (3) most early tools are well maintained and widely used; (4) classic machine learning methods retain their utility; however, novel ones have begun to emerge; (5) disease-associated miRNA tools are emerging. Our analysis yields significant insight into the past development and future directions of miRNA tools.",c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f821,2020,2020-08-18
s1017,p1017,Deep learning in bioinformatics,"In the era of big data, transformation of biomedical big data into valuable knowledge has been one of the most important challenges in bioinformatics. Deep learning has advanced rapidly since the early 2000s and now demonstrates state-of-the-art performance in various fields. Accordingly, application of deep learning in bioinformatics to gain insight from data has been emphasized in both academia and industry. Here, we review deep learning in bioinformatics, presenting examples of current research. To provide a useful and comprehensive perspective, we categorize research both by the bioinformatics domain (i.e. omics, biomedical imaging, biomedical signal processing) and deep learning architecture (i.e. deep neural networks, convolutional neural networks, recurrent neural networks, emergent architectures) and present brief descriptions of each study. Additionally, we discuss theoretical and practical issues of deep learning in bioinformatics and suggest future research directions. We believe that this review will provide valuable insights and serve as a starting point for researchers to apply deep learning approaches in their bioinformatics studies.",c58,Australian Software Engineering Conference,cp58,accepted,f822,2021,2021-12-26
s1018,p1018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),Abstract content goes here ...,c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f823,2020,2020-03-04
s1019,p1019,Piercing the dark matter: bioinformatics of long-range sequencing and mapping,Abstract content goes here ...,j175,Nature reviews genetics,jv175,accepted,f824,2007,2007-02-21
s1020,p1020,Single-cell RNA sequencing technologies and bioinformatics pipelines,Abstract content goes here ...,j176,Experimental and Molecular Medicine,jv176,accepted,f825,2020,2020-05-09
s1021,p1021,BATMAN-TCM: a Bioinformatics Analysis Tool for Molecular mechANism of Traditional Chinese Medicine,Abstract content goes here ...,j130,Scientific Reports,jv130,accepted,f826,2006,2006-05-17
s1022,p1022,Unipro UGENE: a unified bioinformatics toolkit,"UNLABELLED
Unipro UGENE is a multiplatform open-source software with the main goal of assisting molecular biologists without much expertise in bioinformatics to manage, analyze and visualize their data. UGENE integrates widely used bioinformatics tools within a common user interface. The toolkit supports multiple biological data formats and allows the retrieval of data from remote data sources. It provides visualization modules for biological objects such as annotated genome sequences, Next Generation Sequencing (NGS) assembly data, multiple sequence alignments, phylogenetic trees and 3D structures. Most of the integrated algorithms are tuned for maximum performance by the usage of multithreading and special processor instructions. UGENE includes a visual environment for creating reusable workflows that can be launched on local resources or in a High Performance Computing (HPC) environment. UGENE is written in C++ using the Qt framework. The built-in plugin system and structured UGENE API make it possible to extend the toolkit with new functionality.


AVAILABILITY AND IMPLEMENTATION
UGENE binaries are freely available for MS Windows, Linux and Mac OS X at http://ugene.unipro.ru/download.html. UGENE code is licensed under the GPLv2; the information about the code licensing and copyright of integrated tools can be found in the LICENSE.3rd_party file provided with the source bundle.",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f827,2017,2017-08-06
s1023,p1023,Ensemble deep learning in bioinformatics,Abstract content goes here ...,j177,Nature Machine Intelligence,jv177,accepted,f828,2018,2018-06-21
s1024,p1024,The bioinformatics toolbox for circRNA discovery and analysis,"Abstract Circular RNAs (circRNAs) are a unique class of RNA molecule identified more than 40 years ago which are produced by a covalent linkage via back-splicing of linear RNA. Recent advances in sequencing technologies and bioinformatics tools have led directly to an ever-expanding field of types and biological functions of circRNAs. In parallel with technological developments, practical applications of circRNAs have arisen including their utilization as biomarkers of human disease. Currently, circRNA-associated bioinformatics tools can support projects including circRNA annotation, circRNA identification and network analysis of competing endogenous RNA (ceRNA). In this review, we collected about 100 circRNA-associated bioinformatics tools and summarized their current attributes and capabilities. We also performed network analysis and text mining on circRNA tool publications in order to reveal trends in their ongoing development.",c87,European Conference on Computer Vision,cp87,accepted,f829,2014,2014-03-19
s1025,p1025,Deep learning-based clustering approaches for bioinformatics,"Abstract Clustering is central to many data-driven bioinformatics research and serves a powerful computational method. In particular, clustering helps at analyzing unstructured and high-dimensional data in the form of sequences, expressions, texts and images. Further, clustering is used to gain insights into biological processes in the genomics level, e.g. clustering of gene expressions provides insights on the natural structure inherent in the data, understanding gene functions, cellular processes, subtypes of cells and understanding gene regulations. Subsequently, clustering approaches, including hierarchical, centroid-based, distribution-based, density-based and self-organizing maps, have long been studied and used in classical machine learning settings. In contrast, deep learning (DL)-based representation and feature learning for clustering have not been reviewed and employed extensively. Since the quality of clustering is not only dependent on the distribution of data points but also on the learned representation, deep neural networks can be effective means to transform mappings from a high-dimensional data space into a lower-dimensional feature space, leading to improved clustering results. In this paper, we review state-of-the-art DL-based approaches for cluster analysis that are based on representation learning, which we hope to be useful, particularly for bioinformatics research. Further, we explore in detail the training procedures of DL-based clustering algorithms, point out different clustering quality metrics and evaluate several DL-based approaches on three bioinformatics use cases, including bioimaging, cancer genomics and biomedical text mining. We believe this review and the evaluation results will provide valuable insights and serve a starting point for researchers wanting to apply DL-based unsupervised methods to solve emerging bioinformatics research problems.",c40,IEEE International Conference on Software Maintenance and Evolution,cp40,accepted,f830,2013,2013-11-03
s1027,p1027,The EMBL-EBI bioinformatics web and programmatic tools framework,"Since 2009 the EMBL-EBI Job Dispatcher framework has provided free access to a range of mainstream sequence analysis applications. These include sequence similarity search services (https://www.ebi.ac.uk/Tools/sss/) such as BLAST, FASTA and PSI-Search, multiple sequence alignment tools (https://www.ebi.ac.uk/Tools/msa/) such as Clustal Omega, MAFFT and T-Coffee, and other sequence analysis tools (https://www.ebi.ac.uk/Tools/pfa/) such as InterProScan. Through these services users can search mainstream sequence databases such as ENA, UniProt and Ensembl Genomes, utilising a uniform web interface or systematically through Web Services interfaces (https://www.ebi.ac.uk/Tools/webservices/) using common programming languages, and obtain enriched results with novel visualisations. Integration with EBI Search (https://www.ebi.ac.uk/ebisearch/) and the dbfetch retrieval service (https://www.ebi.ac.uk/Tools/dbfetch/) further expands the usefulness of the framework. New tools and updates such as NCBI BLAST+, InterProScan 5 and PfamScan, new categories such as RNA analysis tools (https://www.ebi.ac.uk/Tools/rna/), new databases such as ENA non-coding, WormBase ParaSite, Pfam and Rfam, and new workflow methods, together with the retirement of depreciated services, ensure that the framework remains relevant to today's biological community.",c25,International Conference on Contemporary Computing,cp25,accepted,f831,2014,2014-05-03
s1028,p1028,"Deep learning in bioinformatics: introduction, application, and perspective in big data era","Deep learning, which is especially formidable in handling big data, has achieved great success in various fields, including bioinformatics. With the advances of the big data era in biology, it is foreseeable that deep learning will become increasingly important in the field and will be incorporated in vast majorities of analysis pipelines. In this review, we provide both the exoteric introduction of deep learning, and concrete examples and implementations of its representative applications in bioinformatics. We start from the recent achievements of deep learning in the bioinformatics field, pointing out the problems which are suitable to use deep learning. After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures. After that, we provide eight examples, covering five bioinformatics research directions and all the four kinds of data type, with the implementation written in Tensorflow and Keras. Finally, we discuss the common issues, such as overfitting and interpretability, that users will encounter when adopting deep learning methods and provide corresponding suggestions. The implementations are freely available at https://github.com/lykaust15/Deep_learning_examples.",j153,bioRxiv,jv153,accepted,f832,2020,2020-08-05
s1029,p1029,Bioinformatics and Computational Biology Solutions Using R and Bioconductor,"the difficulty of assessing utilities, “a formal decision model can. . . synthesize current best evidence and clinical judgment. . . and via its utility component, link this evidence to clinical decisions. . . Decision makers are. . . reluctant to delegate [decisions]. . . to model-based formalisms [but are]. . . increasingly. . . relying on structured approaches to make more informed decisions” (p. 69). The “Decision Making” chapter continues with a presentation of the main mathematical details of maximizing SEU that includes as an example a costeffectiveness analysis of stroke interventions combining QUALYs and the Markov model developed in earlier sections. The chapter concludes with a presentation of hypothesis testing and a brief mention of sample size selection. The final chapter of the “Methods” section, “Simulation,” introduces Markov chain Monte Carlo and includes a section on Monte Carlo estimation of expected utility. The second section, “Case Studies,” has chapters titled “Meta-Analysis,” “Decision Trees,” and “Chronic Disease Modeling.” In the first of these, hierarchical Bayesian meta-analysis of exchangeable studies is developed in some detail, including WinBUGS-style directed graphs and code. Topics include combination of continuous and dichotomous endpoints using latent variables and sensitivity to prior specification. The chapter summary (p. 124) suggests that the resulting posterior distributions of effect magnitudes, “can. . . become components of a formal decision analysis [or]. . . of a comprehensive decision model.” However, no such examples are presented, and this chapter, while an excellent, practical introduction to Bayesian meta-analysis, is not well integrated with the main topic of the book. The “Decision Trees” chapter is organized around a case study of the decision between axillary lymph node dissection or not followed by the choice of none or one of three adjuvant therapies in the treatment of early-stage breast cancer. It is a good, clear presentation of backward induction and uses most of the machinery set up in the methods chapters: posterior distributions, predictive distributions, QUALYs, and SEU maximization. The final case study, “Chronic Disease Modeling,” deals with optimizing the frequency of radiological screening for breast cancer. The core models are a continuous-time four-state Markov model (healthy, preclinical, clinical, and dead) for which the inferential component is the estimation of age-dependent transition densities (through sojourn-time distributions), a sensitivity model that may depend on age and tumor size, a model of the number of auxiliary lymph nodes involved that depends on age and tumor size, and a survival model that depends on node involvement and a vector of other prognostic variables. The decision to be made is the screening schedule as a function of age, and in theory, the trade-off is between the cost of screening and the expected gain in QUALYs. Needless to say, this case study is only sketched, but the sketch is sufficiently detailed to give an idea of the modeling strategies and how the backward induction was implemented. I have one small quibble: regarding the statement on page 33: “to speak of a probability distribution for [the parameter] β we need to imagine a metapopulation, or a universe of possible populations, each with a different [value of the parameter],” I believe that this is not at all what Bayesians mean by probability. Rather, Bayesian inference treats probabilities as epistemic—referring to states of uncertainty based on incomplete information, not to alternate universes. To say, for example, that the speed of light, C, has a distribution is to say that I am uncertain about the exact value of C in this universe and that my uncertainty is described by a probability distribution. Speaking as a statistician, Modeling in Medical Decision Making: A Bayesian Approach would be good to use as one component in a graduate course on decision making following an introductory Bayesian course. It assumes an understanding of calculus, or at minimum, calculus notation. For established statisticians and biostatisticians, the book is a good way to get up to speed on Bayesian decision analysis in health care and could serve as an entry point into the large published literature alluded to at the beginning of this review.",c65,Formal Concept Analysis,cp65,accepted,f833,2008,2008-07-01
s1030,p1030,The Bio3D packages for structural bioinformatics,"Bio3D is a family of R packages for the analysis of biomolecular sequence, structure, and dynamics. Major functionality includes biomolecular database searching and retrieval, sequence and structure conservation analysis, ensemble normal mode analysis, protein structure and correlation network analysis, principal component, and related multivariate analysis methods. Here, we review recent package developments, including a new underlying segregation into separate packages for distinct analysis, and introduce a new method for structure analysis named ensemble difference distance matrix analysis (eDDM). The eDDM approach calculates and compares atomic distance matrices across large sets of homologous atomic structures to help identify the residue wise determinants underlying specific functional processes. An eDDM workflow is detailed along with an example application to a large protein family. As a new member of the Bio3D family, the Bio3D‐eddm package supports both experimental and theoretical simulation‐generated structures, is integrated with other methods for dissecting sequence‐structure–function relationships, and can be used in a highly automated and reproducible manner. Bio3D is distributed as an integrated set of platform independent open source R packages available from: http://thegrantlab.org/bio3d/.",j179,Protein Science,jv179,accepted,f834,2001,2001-05-24
s1031,p1031,ExPASy: SIB bioinformatics resource portal,"ExPASy (http://www.expasy.org) has worldwide reputation as one of the main bioinformatics resources for proteomics. It has now evolved, becoming an extensible and integrative portal accessing many scientific resources, databases and software tools in different areas of life sciences. Scientists can henceforth access seamlessly a wide range of resources in many different domains, such as proteomics, genomics, phylogeny/evolution, systems biology, population genetics, transcriptomics, etc. The individual resources (databases, web-based and downloadable software tools) are hosted in a ‘decentralized’ way by different groups of the SIB Swiss Institute of Bioinformatics and partner institutions. Specifically, a single web portal provides a common entry point to a wide range of resources developed and operated by different SIB groups and external institutions. The portal features a search function across ‘selected’ resources. Additionally, the availability and usage of resources are monitored. The portal is aimed for both expert users and people who are not familiar with a specific domain in life sciences. The new web interface provides, in particular, visual guidance for newcomers to ExPASy.",c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f835,2004,2004-01-14
s1032,p1032,Perseus: A Bioinformatics Platform for Integrative Analysis of Proteomics Data in Cancer Research.,Abstract content goes here ...,c37,Asia-Pacific Software Engineering Conference,cp37,accepted,f836,2008,2008-07-13
s1033,p1033,BMC Bioinformatics,"BMC Bioinformatics is part of the BMC series which publishes subject-specific journals focused on the needs of individual research communities across all areas of biology and medicine. We do not make editorial decisions on the basis of the interest of a study or its likely impact. Studies must be scientifically valid; for research articles this includes a scientifically sound research question, the use of suitable methods and analysis, and following community-agreed standards relevant to the research field.  Specific criteria for other article types can be found in the submission guidelines.  BMC series open, inclusive and trusted.",c16,Knowledge Discovery and Data Mining,cp16,accepted,f837,2003,2003-02-07
s1034,p1034,"PATRIC, the bacterial bioinformatics database and analysis resource","The Pathosystems Resource Integration Center (PATRIC) is the all-bacterial Bioinformatics Resource Center (BRC) (http://www.patricbrc.org). A joint effort by two of the original National Institute of Allergy and Infectious Diseases-funded BRCs, PATRIC provides researchers with an online resource that stores and integrates a variety of data types [e.g. genomics, transcriptomics, protein–protein interactions (PPIs), three-dimensional protein structures and sequence typing data] and associated metadata. Datatypes are summarized for individual genomes and across taxonomic levels. All genomes in PATRIC, currently more than 10 000, are consistently annotated using RAST, the Rapid Annotations using Subsystems Technology. Summaries of different data types are also provided for individual genes, where comparisons of different annotations are available, and also include available transcriptomic data. PATRIC provides a variety of ways for researchers to find data of interest and a private workspace where they can store both genomic and gene associations, and their own private data. Both private and public data can be analyzed together using a suite of tools to perform comparative genomic or transcriptomic analysis. PATRIC also includes integrated information related to disease and PPIs. All the data and integrated analysis and visualization tools are freely available. This manuscript describes updates to the PATRIC since its initial report in the 2007 NAR Database Issue.",c71,IEEE International Conference on Information Reuse and Integration,cp71,accepted,f838,2022,2022-06-30
s1035,p1035,Bioinformatics and Computational Tools for Next-Generation Sequencing Analysis in Clinical Genetics,"Clinical genetics has an important role in the healthcare system to provide a definitive diagnosis for many rare syndromes. It also can have an influence over genetics prevention, disease prognosis and assisting the selection of the best options of care/treatment for patients. Next-generation sequencing (NGS) has transformed clinical genetics making possible to analyze hundreds of genes at an unprecedented speed and at a lower price when comparing to conventional Sanger sequencing. Despite the growing literature concerning NGS in a clinical setting, this review aims to fill the gap that exists among (bio)informaticians, molecular geneticists and clinicians, by presenting a general overview of the NGS technology and workflow. First, we will review the current NGS platforms, focusing on the two main platforms Illumina and Ion Torrent, and discussing the major strong points and weaknesses intrinsic to each platform. Next, the NGS analytical bioinformatic pipelines are dissected, giving some emphasis to the algorithms commonly used to generate process data and to analyze sequence variants. Finally, the main challenges around NGS bioinformatics are placed in perspective for future developments. Even with the huge achievements made in NGS technology and bioinformatics, further improvements in bioinformatic algorithms are still required to deal with complex and genetically heterogeneous disorders.",j180,Journal of Clinical Medicine,jv180,accepted,f839,2003,2003-07-26
s1036,p1036,Recent Advances of Deep Learning in Bioinformatics and Computational Biology,"Extracting inherent valuable knowledge from omics big data remains as a daunting problem in bioinformatics and computational biology. Deep learning, as an emerging branch from machine learning, has exhibited unprecedented performance in quite a few applications from academia and industry. We highlight the difference and similarity in widely utilized models in deep learning studies, through discussing their basic structures, and reviewing diverse applications and disadvantages. We anticipate the work can serve as a meaningful perspective for further development of its theory, algorithm and application in bioinformatic and computational biology.",j68,Frontiers in Genetics,jv68,accepted,f840,2012,2012-02-15
s1037,p1037,VectorBase: an updated bioinformatics resource for invertebrate vectors and other organisms related with human diseases,"VectorBase is a National Institute of Allergy and Infectious Diseases supported Bioinformatics Resource Center (BRC) for invertebrate vectors of human pathogens. Now in its 11th year, VectorBase currently hosts the genomes of 35 organisms including a number of non-vectors for comparative analysis. Hosted data range from genome assemblies with annotated gene features, transcript and protein expression data to population genetics including variation and insecticide-resistance phenotypes. Here we describe improvements to our resource and the set of tools available for interrogating and accessing BRC data including the integration of Web Apollo to facilitate community annotation and providing Galaxy to support user-based workflows. VectorBase also actively supports our community through hands-on workshops and online tutorials. All information and data are freely available from our website at https://www.vectorbase.org/.",c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f841,2021,2021-09-28
s1039,p1039,A new bioinformatics analysis tools framework at EMBL–EBI,"The EMBL-EBI provides access to various mainstream sequence analysis applications. These include sequence similarity search services such as BLAST, FASTA, InterProScan and multiple sequence alignment tools such as ClustalW, T-Coffee and MUSCLE. Through the sequence similarity search services, the users can search mainstream sequence databases such as EMBL-Bank and UniProt, and more than 2000 completed genomes and proteomes. We present here a new framework aimed at both novice as well as expert users that exposes novel methods of obtaining annotations and visualizing sequence analysis results through one uniform and consistent interface. These services are available over the web and via Web Services interfaces for users who require systematic access or want to interface with customized pipe-lines and workflows using common programming languages. The framework features novel result visualizations and integration of domain and functional predictions for protein database searches. It is available at http://www.ebi.ac.uk/Tools/sss for sequence similarity searches and at http://www.ebi.ac.uk/Tools/msa for multiple sequence alignments.",c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f842,2006,2006-05-31
s1041,p1041,The MPI bioinformatics Toolkit as an integrative platform for advanced protein sequence and structure analysis,"The MPI Bioinformatics Toolkit (http://toolkit.tuebingen.mpg.de) is an open, interactive web service for comprehensive and collaborative protein bioinformatic analysis. It offers a wide array of interconnected, state-of-the-art bioinformatics tools to experts and non-experts alike, developed both externally (e.g. BLAST+, HMMER3, MUSCLE) and internally (e.g. HHpred, HHblits, PCOILS). While a beta version of the Toolkit was released 10 years ago, the current production-level release has been available since 2008 and has serviced more than 1.6 million external user queries. The usage of the Toolkit has continued to increase linearly over the years, reaching more than 400 000 queries in 2015. In fact, through the breadth of its tools and their tight interconnection, the Toolkit has become an excellent platform for experimental scientists as well as a useful resource for teaching bioinformatic inquiry to students in the life sciences. In this article, we report on the evolution of the Toolkit over the last ten years, focusing on the expansion of the tool repertoire (e.g. CS-BLAST, HHblits) and on infrastructural work needed to remain operative in a changing web environment.",c8,The Compass,cp8,accepted,f843,2016,2016-05-02
s1042,p1042,A Survey of Data Mining and Deep Learning in Bioinformatics,Abstract content goes here ...,j181,Journal of medical systems,jv181,accepted,f844,2013,2013-08-24
s1043,p1043,Bioinformatics and Computational Biology Solutions Using R and Bioconductor (Statistics for Biology and Health),Abstract content goes here ...,c24,Decision Support Systems,cp24,accepted,f845,2013,2013-08-02
s1044,p1044,Sequence clustering in bioinformatics: an empirical study,"Sequence clustering is a basic bioinformatics task that is attracting renewed attention with the development of metagenomics and microbiomics. The latest sequencing techniques have decreased costs and as a result, massive amounts of DNA/RNA sequences are being produced. The challenge is to cluster the sequence data using stable, quick and accurate methods. For microbiome sequencing data, 16S ribosomal RNA operational taxonomic units are typically used. However, there is often a gap between algorithm developers and bioinformatics users. Different software tools can produce diverse results and users can find them difficult to analyze. Understanding the different clustering mechanisms is crucial to understanding the results that they produce. In this review, we selected several popular clustering tools, briefly explained the key computing principles, analyzed their characters and compared them using two independent benchmark datasets. Our aim is to assist bioinformatics users in employing suitable clustering tools effectively to analyze big sequencing data. Related data, codes and software tools were accessible at the link http://lab.malab.cn/∼lg/clustering/.",c90,Computer Vision and Pattern Recognition,cp90,accepted,f846,2008,2008-09-14
s1046,p1046,Taverna: a tool for the composition and enactment of bioinformatics workflows,"MOTIVATION
In silico experiments in bioinformatics involve the co-ordinated use of computational tools and information repositories. A growing number of these resources are being made available with programmatic access in the form of Web services. Bioinformatics scientists will need to orchestrate these Web services in workflows as part of their analyses.


RESULTS
The Taverna project has developed a tool for the composition and enactment of bioinformatics workflows for the life sciences community. The tool includes a workbench application which provides a graphical user interface for the composition of workflows. These workflows are written in a new language called the simple conceptual unified flow language (Scufl), where by each step within a workflow represents one atomic task. Two examples are used to illustrate the ease by which in silico experiments can be represented as Scufl workflows using the workbench application.",c113,International Conference on Image Analysis and Processing,cp113,accepted,f847,2002,2002-08-02
s1047,p1047,Data-driven advice for applying machine learning to bioinformatics problems,"As the bioinformatics field grows, it must keep pace not only with new data but with new algorithms. Here we contribute a thorough analysis of 13 state-of-the-art, commonly used machine learning algorithms on a set of 165 publicly available classification problems in order to provide data-driven algorithm recommendations to current researchers. We present a number of statistical and visual comparisons of algorithm performance and quantify the effect of model selection and algorithm tuning for each algorithm and dataset. The analysis culminates in the recommendation of five algorithms with hyperparameters that maximize classifier performance across the tested problems, as well as general guidelines for applying machine learning to supervised classification problems.",c9,Pacific Symposium on Biocomputing,cp9,accepted,f848,2009,2009-09-06
s1050,p1050,Metabolomics technology and bioinformatics for precision medicine,"Precision medicine is rapidly emerging as a strategy to tailor medical treatment to a small group or even individual patients based on their genetics, environment and lifestyle. Precision medicine relies heavily on developments in systems biology and omics disciplines, including metabolomics. Combination of metabolomics with sophisticated bioinformatics analysis and mathematical modeling has an extreme power to provide a metabolic snapshot of the patient over the course of disease and treatment or classifying patients into subpopulations and subgroups requiring individual medical treatment. Although a powerful approach, metabolomics have certain limitations in technology and bioinformatics. We will review various aspects of metabolomics technology and bioinformatics, from data generation, bioinformatics analysis, data fusion and mathematical modeling to data management, in the context of precision medicine.",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f849,2005,2005-06-05
s1051,p1051,A brief history of bioinformatics,"It is easy for today's students and researchers to believe that modern bioinformatics emerged recently to assist next-generation sequencing data analysis. However, the very beginnings of bioinformatics occurred more than 50 years ago, when desktop computers were still a hypothesis and DNA could not yet be sequenced. The foundations of bioinformatics were laid in the early 1960s with the application of computational methods to protein sequence analysis (notably, de novo sequence assembly, biological sequence databases and substitution models). Later on, DNA analysis also emerged due to parallel advances in (i) molecular biology methods, which allowed easier manipulation of DNA, as well as its sequencing, and (ii) computer science, which saw the rise of increasingly miniaturized and more powerful computers, as well as novel software better suited to handle bioinformatics tasks. In the 1990s through the 2000s, major improvements in sequencing technology, along with reduced costs, gave rise to an exponential increase of data. The arrival of 'Big Data' has laid out new challenges in terms of data mining and management, calling for more expertise from computer science into the field. Coupled with an ever-increasing amount of bioinformatics tools, biological Big Data had (and continues to have) profound implications on the predictive power and reproducibility of bioinformatics results. To overcome this issue, universities are now fully integrating this discipline into the curriculum of biology students. Recent subdisciplines such as synthetic biology, systems biology and whole-cell modeling have emerged from the ever-increasing complementarity between computer science and biology.",c100,ACM SIGMOD Conference,cp100,accepted,f850,2010,2010-10-31
s1052,p1052,A novel features ranking metric with application to scalable visual and bioinformatics data classification,Abstract content goes here ...,j184,Neurocomputing,jv184,accepted,f851,2021,2021-05-17
s1053,p1053,An overview of topic modeling and its current applications in bioinformatics,Abstract content goes here ...,j185,SpringerPlus,jv185,accepted,f852,2014,2014-09-28
s1054,p1054,Protein Bioinformatics Databases and Resources.,Abstract content goes here ...,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",cp38,accepted,f853,2015,2015-05-23
s1055,p1055,A comparison of sequencing platforms and bioinformatics pipelines for compositional analysis of the gut microbiome,Abstract content goes here ...,j186,BMC Microbiology,jv186,accepted,f854,2011,2011-05-08
s1056,p1056,Manual for Using Homomorphic Encryption for Bioinformatics,"Biological data science is an emerging field facing multiple challenges for hosting, sharing, computing on, and interacting with large data sets. Privacy regulations and concerns about the risks of leaking sensitive personal health and genomic data add another layer of complexity to the problem. Recent advances in cryptography over the last five years have yielded a tool, homomorphic encryption, which can be used to encrypt data in such a way that storage can be outsourced to an untrusted cloud, and the data can be computed on in a meaningful way in encrypted form, without access to decryption keys. This paper introduces homomorphic encryption to the bioinformatics community, and presents an informal “manual” for using the Simple Encrypted Arithmetic Library (SEAL), which we have made publicly available for bioinformatic, genomic, and other research purposes.",j168,Proceedings of the IEEE,jv168,accepted,f855,2019,2019-08-06
s1057,p1057,A Review of Bioinformatics Tools for Bio-Prospecting from Metagenomic Sequence Data,"The microbiome can be defined as the community of microorganisms that live in a particular environment. Metagenomics is the practice of sequencing DNA from the genomes of all organisms present in a particular sample, and has become a common method for the study of microbiome population structure and function. Increasingly, researchers are finding novel genes encoded within metagenomes, many of which may be of interest to the biotechnology and pharmaceutical industries. However, such “bioprospecting” requires a suite of sophisticated bioinformatics tools to make sense of the data. This review summarizes the most commonly used bioinformatics tools for the assembly and annotation of metagenomic sequence data with the aim of discovering novel genes.",j68,Frontiers in Genetics,jv68,accepted,f856,2012,2012-06-30
s1058,p1058,A cloud-compatible bioinformatics pipeline for ultrarapid pathogen identification from next-generation sequencing of clinical samples,"Unbiased next-generation sequencing (NGS) approaches enable comprehensive pathogen detection in the clinical microbiology laboratory and have numerous applications for public health surveillance, outbreak investigation, and the diagnosis of infectious diseases. However, practical deployment of the technology is hindered by the bioinformatics challenge of analyzing results accurately and in a clinically relevant timeframe. Here we describe SURPI (“sequence-based ultrarapid pathogen identification”), a computational pipeline for pathogen identification from complex metagenomic NGS data generated from clinical samples, and demonstrate use of the pipeline in the analysis of 237 clinical samples comprising more than 1.1 billion sequences. Deployable on both cloud-based and standalone servers, SURPI leverages two state-of-the-art aligners for accelerated analyses, SNAP and RAPSearch, which are as accurate as existing bioinformatics tools but orders of magnitude faster in performance. In fast mode, SURPI detects viruses and bacteria by scanning data sets of 7–500 million reads in 11 min to 5 h, while in comprehensive mode, all known microorganisms are identified, followed by de novo assembly and protein homology searches for divergent viruses in 50 min to 16 h. SURPI has also directly contributed to real-time microbial diagnosis in acutely ill patients, underscoring its potential key role in the development of unbiased NGS-based clinical assays in infectious diseases that demand rapid turnaround times.",j187,Genome Research,jv187,accepted,f857,2006,2006-11-06
s1060,p1060,Encyclopedia of Bioinformatics and Computational Biology: ABC of Bioinformatics,Abstract content goes here ...,c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f858,2018,2018-09-24
s1061,p1061,Bioinformatics and peptidomics approaches to the discovery and analysis of food-derived bioactive peptides,Abstract content goes here ...,j188,Analytical and Bioanalytical Chemistry,jv188,accepted,f859,2002,2002-06-23
s1063,p1063,CLIMB (the Cloud Infrastructure for Microbial Bioinformatics): an online resource for the medical microbiology community,"The increasing availability and decreasing cost of high-throughput sequencing has transformed academic medical microbiology, delivering an explosion in available genomes while also driving advances in bioinformatics. However, many microbiologists are unable to exploit the resulting large genomics datasets because they do not have access to relevant computational resources and to an appropriate bioinformatics infrastructure. Here, we present the Cloud Infrastructure for Microbial Bioinformatics (CLIMB) facility, a shared computing infrastructure that has been designed from the ground up to provide an environment where microbiologists can share and reuse methods and data. DATA SUMMARY The paper describes a new, freely available public resource and therefore no data has been generated. The resource can be accessed at http://www.climb.ac.uk. Source code for software developed for the project can be found at http://github.com/MRC-CLIMB/ I/We confirm all supporting data, code and protocols have been provided within the article or through supplementary data files. IMPACT STATEMENT Technological advances mean that genome sequencing is now relatively simple, quick, and affordable. However, handling large genome datasets remains a significant challenge for many microbiologists, with substantial requirements for computational resources and expertise in data storage and analysis. This has led to fragmentary approaches to software development and data sharing that reduce the reproducibility of research and limits opportunities for bioinformatics training. Here, we describe a nationwide electronic infrastructure that has been designed to support the UK microbiology community, providing simple mechanisms for accessing large, shared, computational resources designed to meet the bioinformatic needs of microbiologists.",j153,bioRxiv,jv153,accepted,f860,2020,2020-04-28
s1064,p1064,An overview of bioinformatics tools for epitope prediction: Implications on vaccine development,Abstract content goes here ...,j189,Journal of Biomedical Informatics,jv189,accepted,f861,2012,2012-12-02
s1065,p1065,Snakemake - a scalable bioinformatics workflow engine,"SUMMARY
Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.


AVAILABILITY
http://snakemake.googlecode.com.


CONTACT
johannes.koester@uni-due.de.",c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",cp38,accepted,f862,2015,2015-01-07
s1066,p1066,Network Inference and Reconstruction in Bioinformatics,Abstract content goes here ...,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f863,2001,2001-01-15
s1067,p1067,Text Mining for Bioinformatics Using Biomedical Literature,Abstract content goes here ...,c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f864,2015,2015-11-09
s1068,p1068,A global perspective on evolving bioinformatics and data science training needs,"Abstract Bioinformatics is now intrinsic to life science research, but the past decade has witnessed a continuing deficiency in this essential expertise. Basic data stewardship is still taught relatively rarely in life science education programmes, creating a chasm between theory and practice, and fuelling demand for bioinformatics training across all educational levels and career roles. Concerned by this, surveys have been conducted in recent years to monitor bioinformatics and computational training needs worldwide. This article briefly reviews the principal findings of a number of these studies. We see that there is still a strong appetite for short courses to improve expertise and confidence in data analysis and interpretation; strikingly, however, the most urgent appeal is for bioinformatics to be woven into the fabric of life science degree programmes. Satisfying the relentless training needs of current and future generations of life scientists will require a concerted response from stakeholders across the globe, who need to deliver sustainable solutions capable of both transforming education curricula and cultivating a new cadre of trainer scientists.",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f865,2018,2018-05-15
s1069,p1069,The Road to Metagenomics: From Microbiology to DNA Sequencing Technologies and Bioinformatics,"The study of microorganisms that pervade each and every part of this planet has encountered many challenges through time such as the discovery of unknown organisms and the understanding of how they interact with their environment. The aim of this review is to take the reader along the timeline and major milestones that led us to modern metagenomics. This new and thriving area is likely to be an important contributor to solve different problems. The transition from classical microbiology to modern metagenomics studies has required the development of new branches of knowledge and specialization. Here, we will review how the availability of high-throughput sequencing technologies has transformed microbiology and bioinformatics and how to tackle the inherent computational challenges that arise from the DNA sequencing revolution. New computational methods are constantly developed to collect, process, and extract useful biological information from a variety of samples and complex datasets, but metagenomics needs the integration of several of these computational methods. Despite the level of specialization needed in bioinformatics, it is important that life-scientists have a good understanding of it for a correct experimental design, which allows them to reveal the information in a metagenome.",j68,Frontiers in Genetics,jv68,accepted,f866,2012,2012-12-14
s1070,p1070,Feature selection methods for big data bioinformatics: A survey from the search perspective.,Abstract content goes here ...,j190,Methods,jv190,accepted,f867,2011,2011-11-22
s1072,p1072,The development and application of bioinformatics core competencies to improve bioinformatics training and education,"Bioinformatics is recognized as part of the essential knowledge base of numerous career paths in biomedical research and healthcare. However, there is little agreement in the field over what that knowledge entails or how best to provide it. These disagreements are compounded by the wide range of populations in need of bioinformatics training, with divergent prior backgrounds and intended application areas. The Curriculum Task Force of the International Society of Computational Biology (ISCB) Education Committee has sought to provide a framework for training needs and curricula in terms of a set of bioinformatics core competencies that cut across many user personas and training programs. The initial competencies developed based on surveys of employers and training programs have since been refined through a multiyear process of community engagement. This report describes the current status of the competencies and presents a series of use cases illustrating how they are being applied in diverse training contexts. These use cases are intended to demonstrate how others can make use of the competencies and engage in the process of their continuing refinement and application. The report concludes with a consideration of remaining challenges and future plans.",c100,ACM SIGMOD Conference,cp100,accepted,f868,2010,2010-11-03
s1073,p1073,PipeCraft: Flexible open‐source toolkit for bioinformatics analysis of custom high‐throughput amplicon sequencing data,"High‐throughput sequencing methods have become a routine analysis tool in environmental sciences as well as in public and private sector. These methods provide vast amount of data, which need to be analysed in several steps. Although the bioinformatics may be applied using several public tools, many analytical pipelines allow too few options for the optimal analysis for more complicated or customized designs. Here, we introduce PipeCraft, a flexible and handy bioinformatics pipeline with a user‐friendly graphical interface that links several public tools for analysing amplicon sequencing data. Users are able to customize the pipeline by selecting the most suitable tools and options to process raw sequences from Illumina, Pacific Biosciences, Ion Torrent and Roche 454 sequencing platforms. We described the design and options of PipeCraft and evaluated its performance by analysing the data sets from three different sequencing platforms. We demonstrated that PipeCraft is able to process large data sets within 24 hr. The graphical user interface and the automated links between various bioinformatics tools enable easy customization of the workflow. All analytical steps and options are recorded in log files and are easily traceable.",j191,Molecular Ecology Resources,jv191,accepted,f869,2021,2021-12-12
s1075,p1075,Enabling the democratization of the genomics revolution with a fully integrated web-based bioinformatics platform,"Continued advancements in sequencing technologies have fueled the development of new sequencing applications and promise to flood current databases with raw data. A number of factors prevent the seamless and easy use of these data, including the breadth of project goals, the wide array of tools that individually perform fractions of any given analysis, the large number of associated software/hardware dependencies, and the detailed expertise required to perform these analyses. To address these issues, we have developed an intuitive web-based environment with a wide assortment of integrated and cutting-edge bioinformatics tools. These preconfigured workflows provide even novice next-generation sequencing users with the ability to perform many complex analyses with only a few mouse clicks, and, within the context of the same environment, to visualize and further interrogate their results. This bioinformatics platform is an initial attempt at Empowering the Development of Genomics Expertise (EDGE) in a wide range of applications.",j153,bioRxiv,jv153,accepted,f870,2020,2020-01-21
s1077,p1077,The Impact of Bioinformatics on Vaccine Design and Development,"Vaccines are the pharmaceutical products that offer the best cost‐benefit ratio in the pre‐ vention or treatment of diseases. In that a vaccine is a pharmaceutical product, vaccine development and production are costly and it takes years for this to be accomplished. Several approaches have been applied to reduce the times and costs of vaccine develop‐ ment, mainly focusing on the selection of appropriate antigens or antigenic structures, carriers, and adjuvants. One of these approaches is the incorporation of bioinformatics methods and analyses into vaccine development. This chapter provides an overview of the application of bioinformatics strategies in vaccine design and development, supply‐ ing some successful examples of vaccines in which bioinformatics has furnished a cutting edge in their development. Reverse vaccinology, immunoinformatics, and structural vac ‐ cinology are described and addressed in the design and development of specific vaccines against infectious diseases caused by bacteria, viruses, and parasites. These include some emerging or re‐emerging infectious diseases, as well as therapeutic vaccines to fight can‐ cer, allergies, and substance abuse, which have been facilitated and improved by using bioinformatics tools or which are under development based on bioinformatics strategies. antigenic B‐cell (IEDB) and CTL epitopes (NetCTL.1.2 server). They determined, by in silico studies, surface accessibility, surface flexibility, hydrophilicity, homology modeling (MODELLER ver. 9.12, CHARMM, WhatIF, PROCHECK, Verify 3D), and structure‐based epitope prediction for E protein, NS3, and NS5. They performed molecular docking of the ZIKV‐E protein with HLA‐A0201, of the ZIKV‐NS3 protein with HLA‐B2705, and of the ZIKV‐NS5 protein with HLA‐C0801 (PatchDock rigid‐body docking server, FireDock server). these",c107,British Machine Vision Conference,cp107,accepted,f871,2012,2012-06-11
s1078,p1078,Single-Cell Transcriptomics Bioinformatics and Computational Challenges,"The emerging single-cell RNA-Seq (scRNA-Seq) technology holds the promise to revolutionize our understanding of diseases and associated biological processes at an unprecedented resolution. It opens the door to reveal intercellular heterogeneity and has been employed to a variety of applications, ranging from characterizing cancer cells subpopulations to elucidating tumor resistance mechanisms. Parallel to improving experimental protocols to deal with technological issues, deriving new analytical methods to interpret the complexity in scRNA-Seq data is just as challenging. Here, we review current state-of-the-art bioinformatics tools and methods for scRNA-Seq analysis, as well as addressing some critical analytical challenges that the field faces.",j68,Frontiers in Genetics,jv68,accepted,f872,2012,2012-11-06
s1079,p1079,Systems Bioinformatics: increasing precision of computational diagnostics and therapeutics through network-based approaches,"Abstract Systems Bioinformatics is a relatively new approach, which lies in the intersection of systems biology and classical bioinformatics. It focuses on integrating information across different levels using a bottom-up approach as in systems biology with a data-driven top-down approach as in bioinformatics. The advent of omics technologies has provided the stepping-stone for the emergence of Systems Bioinformatics. These technologies provide a spectrum of information ranging from genomics, transcriptomics and proteomics to epigenomics, pharmacogenomics, metagenomics and metabolomics. Systems Bioinformatics is the framework in which systems approaches are applied to such data, setting the level of resolution as well as the boundary of the system of interest and studying the emerging properties of the system as a whole rather than the sum of the properties derived from the system’s individual components. A key approach in Systems Bioinformatics is the construction of multiple networks representing each level of the omics spectrum and their integration in a layered network that exchanges information within and between layers. Here, we provide evidence on how Systems Bioinformatics enhances computational therapeutics and diagnostics, hence paving the way to precision medicine. The aim of this review is to familiarize the reader with the emerging field of Systems Bioinformatics and to provide a comprehensive overview of its current state-of-the-art methods and technologies. Moreover, we provide examples of success stories and case studies that utilize such methods and tools to significantly advance research in the fields of systems biology and systems medicine.",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f873,2002,2002-08-28
s1080,p1080,Bioinformatics core competencies for undergraduate life sciences education,"Bioinformatics is becoming increasingly central to research in the life sciences. However, despite its importance, bioinformatics skills and knowledge are not well integrated in undergraduate biology education. This curricular gap prevents biology students from harnessing the full potential of their education, limiting their career opportunities and slowing genomic research innovation. To advance the integration of bioinformatics into life sciences education, a framework of core bioinformatics competencies is needed. To that end, we here report the results of a survey of life sciences faculty in the United States about teaching bioinformatics to undergraduate life scientists. Responses were received from 1,260 faculty representing institutions in all fifty states with a combined capacity to educate hundreds of thousands of students every year. Results indicate strong, widespread agreement that bioinformatics knowledge and skills are critical for undergraduate life scientists, as well as considerable agreement about which skills are necessary. Perceptions of the importance of some skills varied with the respondent’s degree of training, time since degree earned, and/or the Carnegie classification of the respondent’s institution. To assess which skills are currently being taught, we analyzed syllabi of courses with bioinformatics content submitted by survey respondents. Finally, we used the survey results, the analysis of syllabi, and our collective research and teaching expertise to develop a set of bioinformatics core competencies for undergraduate life sciences students. These core competencies are intended to serve as a guide for institutions as they work to integrate bioinformatics into their life sciences curricula. Significance Statement Bioinformatics, an interdisciplinary field that uses techniques from computer science and mathematics to store, manage, and analyze biological data, is becoming increasingly central to modern biology research. Given the widespread use of bioinformatics and its impacts on societal problem-solving (e.g., in healthcare, agriculture, and natural resources management), there is a growing need for the integration of bioinformatics competencies into undergraduate life sciences education. Here, we present a set of bioinformatics core competencies for undergraduate life scientists developed using the results of a large national survey and the expertise of our working group of bioinformaticians and educators. We also present results from the survey on the importance of bioinformatics skills and the current state of integration of bioinformatics into biology education.",j153,bioRxiv,jv153,accepted,f874,2020,2020-10-04
s1081,p1081,"Review of Current Methods, Applications, and Data Management for the Bioinformatics Analysis of Whole Exome Sequencing","The advent of next-generation sequencing technologies has greatly promoted advances in the study of human diseases at the genomic, transcriptomic, and epigenetic levels. Exome sequencing, where the coding region of the genome is captured and sequenced at a deep level, has proven to be a cost-effective method to detect disease-causing variants and discover gene targets. In this review, we outline the general framework of whole exome sequence data analysis. We focus on established bioinformatics tools and applications that support five analytical steps: raw data quality assessment, preprocessing, alignment, post-processing, and variant analysis (detection, annotation, and prioritization). We evaluate the performance of open-source alignment programs and variant calling tools using simulated and benchmark datasets, and highlight the challenges posed by the lack of concordance among variant detection tools. Based on these results, we recommend adopting multiple tools and resources to reduce false positives and increase the sensitivity of variant calling. In addition, we briefly discuss the current status and solutions for big data management, analysis, and summarization in the field of bioinformatics.",j192,Cancer Informatics,jv192,accepted,f875,2016,2016-02-22
s1082,p1082,Bioinformatics applications on Apache Spark,"Abstract With the rapid development of next-generation sequencing technology, ever-increasing quantities of genomic data pose a tremendous challenge to data processing. Therefore, there is an urgent need for highly scalable and powerful computational systems. Among the state-of–the-art parallel computing platforms, Apache Spark is a fast, general-purpose, in-memory, iterative computing framework for large-scale data processing that ensures high fault tolerance and high scalability by introducing the resilient distributed dataset abstraction. In terms of performance, Spark can be up to 100 times faster in terms of memory access and 10 times faster in terms of disk access than Hadoop. Moreover, it provides advanced application programming interfaces in Java, Scala, Python, and R. It also supports some advanced components, including Spark SQL for structured data processing, MLlib for machine learning, GraphX for computing graphs, and Spark Streaming for stream computing. We surveyed Spark-based applications used in next-generation sequencing and other biological domains, such as epigenetics, phylogeny, and drug discovery. The results of this survey are used to provide a comprehensive guideline allowing bioinformatics researchers to apply Spark in their own fields.",j193,GigaScience,jv193,accepted,f876,2006,2006-05-30
s1084,p1084,Designing a course model for distance-based online bioinformatics training in Africa: The H3ABioNet experience,"Africa is not unique in its need for basic bioinformatics training for individuals from a diverse range of academic backgrounds. However, particular logistical challenges in Africa, most notably access to bioinformatics expertise and internet stability, must be addressed in order to meet this need on the continent. H3ABioNet (www.h3abionet.org), the Pan African Bioinformatics Network for H3Africa, has therefore developed an innovative, free-of-charge “Introduction to Bioinformatics” course, taking these challenges into account as part of its educational efforts to provide on-site training and develop local expertise inside its network. A multiple-delivery–mode learning model was selected for this 3-month course in order to increase access to (mostly) African, expert bioinformatics trainers. The content of the course was developed to include a range of fundamental bioinformatics topics at the introductory level. For the first iteration of the course (2016), classrooms with a total of 364 enrolled participants were hosted at 20 institutions across 10 African countries. To ensure that classroom success did not depend on stable internet, trainers pre-recorded their lectures, and classrooms downloaded and watched these locally during biweekly contact sessions. The trainers were available via video conferencing to take questions during contact sessions, as well as via online “question and discussion” forums outside of contact session time. This learning model, developed for a resource-limited setting, could easily be adapted to other settings.",c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f877,2004,2004-05-24
s1085,p1085,Microarray bioinformatics in cancer- a review.,"Bioinformatics is one of the newest fields of biological research, and should be viewed broadly as the use of mathematical, statistical, and computational methods for the processing and analysis of biological data. Over the last decade, the rapid growth of information and technology in both ""genomics"" and ""omics"" eras has been overwhelming for the laboratory scientists to process experimental results. Traditional gene-by-gene approaches in research are insufficient to meet the growth and demand of biological research in understanding the true biology. The massive amounts of data generated by new technologies as genomic sequencing and microarray chips make the management of data and the integration of multiple platforms of high importance; this is then followed by data analysis and interpretation to achieve biological understanding and therapeutic progress. Global views of analyzing the magnitude of information are necessary and traditional approaches to lab work have steadily been changing towards a bioinformatics era. Research is moving from being restricted to a laboratory environment to working with computers in a ""virtual lab"" environment. The present review article shall put light on this emerging field and its applicability towards cancer research.",j195,Journal of B.U.ON. : official journal of the Balkan Union of Oncology,jv195,accepted,f878,2007,2007-07-22
s1086,p1086,Bioinformatics for precision oncology,"Abstract Molecular profiling of tumor biopsies plays an increasingly important role not only in cancer research, but also in the clinical management of cancer patients. Multi-omics approaches hold the promise of improving diagnostics, prognostics and personalized treatment. To deliver on this promise of precision oncology, appropriate bioinformatics methods for managing, integrating and analyzing large and complex data are necessary. Here, we discuss the specific requirements of bioinformatics methods and software that arise in the setting of clinical oncology, owing to a stricter regulatory environment and the need for rapid, highly reproducible and robust procedures. We describe the workflow of a molecular tumor board and the specific bioinformatics support that it requires, from the primary analysis of raw molecular profiling data to the automatic generation of a clinical report and its delivery to decision-making clinical oncologists. Such workflows have to various degrees been implemented in many clinical trials, as well as in molecular tumor boards at specialized cancer centers and university hospitals worldwide. We review these and more recent efforts to include other high-dimensional multi-omics patient profiles into the tumor board, as well as the state of clinical decision support software to translate molecular findings into treatment recommendations.",c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f879,2021,2021-08-12
s1087,p1087,The European Bioinformatics Institute in 2016: Data growth and integration,"New technologies are revolutionising biological research and its applications by making it easier and cheaper to generate ever-greater volumes and types of data. In response, the services and infrastructure of the European Bioinformatics Institute (EMBL-EBI, www.ebi.ac.uk) are continually expanding: total disk capacity increases significantly every year to keep pace with demand (75 petabytes as of December 2015), and interoperability between resources remains a strategic priority. Since 2014 we have launched two new resources: the European Variation Archive for genetic variation data and EMPIAR for two-dimensional electron microscopy data, as well as a Resource Description Framework platform. We also launched the Embassy Cloud service, which allows users to run large analyses in a virtual environment next to EMBL-EBI's vast public data resources.",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f880,2018,2018-05-07
s1088,p1088,Reproducible bioinformatics project: a community for reproducible bioinformatics analysis pipelines,Abstract content goes here ...,j153,bioRxiv,jv153,accepted,f881,2020,2020-11-12
s1089,p1089,Developing reproducible bioinformatics analysis workflows for heterogeneous computing environments to support African genomics,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f882,2007,2007-08-10
s1090,p1090,"Bioinformatics approaches, prospects and challenges of food bioactive peptide research",Abstract content goes here ...,c26,PS,cp26,accepted,f883,2010,2010-03-09
s1091,p1091,Metagenomics and Bioinformatics in Microbial Ecology: Current Status and Beyond,"Metagenomic approaches are now commonly used in microbial ecology to study microbial communities in more detail, including many strains that cannot be cultivated in the laboratory. Bioinformatic analyses make it possible to mine huge metagenomic datasets and discover general patterns that govern microbial ecosystems. However, the findings of typical metagenomic and bioinformatic analyses still do not completely describe the ecology and evolution of microbes in their environments. Most analyses still depend on straightforward sequence similarity searches against reference databases. We herein review the current state of metagenomics and bioinformatics in microbial ecology and discuss future directions for the field. New techniques will allow us to go beyond routine analyses and broaden our knowledge of microbial ecosystems. We need to enrich reference databases, promote platforms that enable meta- or comprehensive analyses of diverse metagenomic datasets, devise methods that utilize long-read sequence information, and develop more powerful bioinformatic methods to analyze data from diverse perspectives.",j196,Microbes and Environments,jv196,accepted,f884,2005,2005-10-04
s1092,p1092,Barriers to integration of bioinformatics into undergraduate life sciences education: A national study of US life sciences faculty uncover significant barriers to integrating bioinformatics into undergraduate instruction,"Bioinformatics, a discipline that combines aspects of biology, statistics, and computer science, is increasingly important for biological research. However, bioinformatics instruction is rarely integrated into life sciences curricula at the undergraduate level. To understand why, the Network for Integrating Bioinformatics into Life Sciences Education (NIBLSE, “nibbles”) recently undertook an extensive survey of life sciences faculty in the United States. The survey responses to open-ended questions about barriers to integration were subjected to keyword analysis. The barrier most frequently reported by the ~1,260 respondents was lack of faculty training. Faculty at associate’s-granting institutions report the least training in bioinformatics and the least integration of bioinformatics into their teaching. Faculty from underrepresented minority groups (URMs) in STEM reported training barriers at a higher rate than others, although the number of URM respondents was small. Interestingly, the cohort of faculty with the most recently awarded PhD degrees reported the most training but were teaching bioinformatics at a lower rate than faculty who earned their degrees in previous decades. Other barriers reported included lack of student interest in bioinformatics; lack of student preparation in mathematics, statistics, and computer science; already overly full curricula; and limited access to resources, including hardware, software, and vetted teaching materials. The results of the survey, the largest to date on bioinformatics education, will guide efforts to further integrate bioinformatics instruction into undergraduate life sciences education.",j153,bioRxiv,jv153,accepted,f885,2020,2020-12-12
s1093,p1093,Bioinformatics in translational drug discovery,"Bioinformatics approaches are becoming ever more essential in translational drug discovery both in academia and within the pharmaceutical industry. Computational exploitation of the increasing volumes of data generated during all phases of drug discovery is enabling key challenges of the process to be addressed. Here, we highlight some of the areas in which bioinformatics resources and methods are being developed to support the drug discovery pipeline. These include the creation of large data warehouses, bioinformatics algorithms to analyse ‘big data’ that identify novel drug targets and/or biomarkers, programs to assess the tractability of targets, and prediction of repositioning opportunities that use licensed drugs to treat additional indications.",j197,Bioscience Reports,jv197,accepted,f886,2010,2010-03-26
s1094,p1094,Proceedings of the 16th Annual UT-KBRIN Bioinformatics Summit 2016: bioinformatics,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f887,2007,2007-05-03
s1095,p1095,Overview of random forest methodology and practical guidance with emphasis on computational biology and bioinformatics,"The random forest (RF) algorithm by Leo Breiman has become a standard data analysis tool in bioinformatics. It has shown excellent performance in settings where the number of variables is much larger than the number of observations, can cope with complex interaction structures as well as highly correlated variables and return measures of variable importance. This paper synthesizes 10 years of RF development with emphasis on applications to bioinformatics and computational biology. Special attention is paid to practical aspects such as the selection of parameters, available RF implementations, and important pitfalls and biases of RF and its variable importance measures (VIMs). The paper surveys recent developments of the methodology relevant to bioinformatics as well as some representative examples of RF applications in this context and possible directions for future research. © 2012 Wiley Periodicals, Inc.",j198,Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery,jv198,accepted,f888,2007,2007-10-05
s1096,p1096,Bioinformatics for clinical next generation sequencing.,"BACKGROUND
Next generation sequencing (NGS)-based assays continue to redefine the field of genetic testing. Owing to the complexity of the data, bioinformatics has become a necessary component in any laboratory implementing a clinical NGS test.


CONTENT
The computational components of an NGS-based work flow can be conceptualized as primary, secondary, and tertiary analytics. Each of these components addresses a necessary step in the transformation of raw data into clinically actionable knowledge. Understanding the basic concepts of these analysis steps is important in assessing and addressing the informatics needs of a molecular diagnostics laboratory. Equally critical is a familiarity with the regulatory requirements addressing the bioinformatics analyses. These and other topics are covered in this review article.


SUMMARY
Bioinformatics has become an important component in clinical laboratories generating, analyzing, maintaining, and interpreting data from molecular genetics testing. Given the rapid adoption of NGS-based clinical testing, service providers must develop informatics work flows that adhere to the rigor of clinical laboratory standards, yet are flexible to changes as the chemistry and software for analyzing sequencing data mature.",j199,Clinical Chemistry,jv199,accepted,f889,2008,2008-12-01
s1097,p1097,"Knowledge Discovery and interactive Data Mining in Bioinformatics - State-of-the-Art, future challenges and research directions",Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f890,2007,2007-08-18
s1098,p1098,An internet-based bioinformatics toolkit for plant biosecurity diagnosis and surveillance of viruses and viroids,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f891,2007,2007-02-04
s1099,p1099,Bioinformatics For Geneticists,"Thank you for reading bioinformatics for geneticists. As you may know, people have search numerous times for their favorite novels like this bioinformatics for geneticists, but end up in harmful downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they are facing with some malicious virus inside their laptop. bioinformatics for geneticists is available in our book collection an online access to it is set as public so you can download it instantly. Our digital library saves in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the bioinformatics for geneticists is universally compatible with any devices to read.",c50,International Conference on Automated Software Engineering,cp50,accepted,f892,2008,2008-11-19
s1101,p1101,Applied bioinformatics for the identification of regulatory elements,Abstract content goes here ...,j175,Nature reviews genetics,jv175,accepted,f893,2007,2007-03-04
s1102,p1102,In the loop: promoter–enhancer interactions and bioinformatics,"Enhancer–promoter regulation is a fundamental mechanism underlying differential transcriptional regulation. Spatial chromatin organization brings remote enhancers in contact with target promoters in cis to regulate gene expression. There is considerable evidence for promoter–enhancer interactions (PEIs). In the recent years, genome-wide analyses have identified signatures and mapped novel enhancers; however, being able to precisely identify their target gene(s) requires massive biological and bioinformatics efforts. In this review, we give a short overview of the chromatin landscape and transcriptional regulation. We discuss some key concepts and problems related to chromatin interaction detection technologies, and emerging knowledge from genome-wide chromatin interaction data sets. Then, we critically review different types of bioinformatics analysis methods and tools related to representation and visualization of PEI data, raw data processing and PEI prediction. Lastly, we provide specific examples of how PEIs have been used to elucidate a functional role of non-coding single-nucleotide polymorphisms. The topic is at the forefront of epigenetic research, and by highlighting some future bioinformatics challenges in the field, this review provides a comprehensive background for future PEI studies.",c83,International Conference on Computer Graphics and Interactive Techniques,cp83,accepted,f894,2015,2015-12-10
s1103,p1103,Getting Started with Microbiome Analysis: Sample Acquisition to Bioinformatics,"Historically, in order to study microbes, it was necessary to grow them in the laboratory. It was clear though that many microbe communities were refractory to study because none of the members could be grown outside of their native habitat. The development of culture‐independent methods to study microbiota using high‐throughput sequencing of the 16S ribosomal RNA gene variable regions present in all prokaryotic organisms has provided new opportunities to investigate complex microbial communities. In this unit, the process for a microbiome analysis is described. Many of the components required for this process may already exist. A pipeline is described for acquisition of samples from different sites on the human body, isolation of microbial DNA, and DNA sequencing using the Illumina MiSeq sequencing platform. Finally, a new analytical workflow for basic bioinformatics data analysis, QWRAP, is described, which can be used by clinical and basic science investigators. Curr. Protoc. Hum. Genet. 82:18.8.1‐18.8.29. © 2014 by John Wiley & Sons, Inc.",j200,Current Protocols in Human Genetics,jv200,accepted,f895,2008,2008-03-17
s1104,p1104,Big Data Analytics in Bioinformatics: A Machine Learning Perspective,"Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. 
Usually big data tools perform computation in batch-mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. 
However, there lack standard big data architectures and tools for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f896,2008,2008-01-15
s1105,p1105,A selective review of robust variable selection with applications in bioinformatics,"A drastic amount of data have been and are being generated in bioinformatics studies. In the analysis of such data, the standard modeling approaches can be challenged by the heavy-tailed errors and outliers in response variables, the contamination in predictors (which may be caused by, for instance, technical problems in microarray gene expression studies), model mis-specification and others. Robust methods are needed to tackle these challenges. When there are a large number of predictors, variable selection can be as important as estimation. As a generic variable selection and regularization tool, penalization has been extensively adopted. In this article, we provide a selective review of robust penalized variable selection approaches especially designed for high-dimensional data from bioinformatics and biomedical studies. We discuss the robust loss functions, penalty functions and computational algorithms. The theoretical properties and implementation are also briefly examined. Application examples of the robust penalization approaches in representative bioinformatics and biomedical studies are also illustrated.",c112,Very Large Data Bases Conference,cp112,accepted,f897,2018,2018-12-07
s1106,p1106,Design and bioinformatics analysis of genome-wide CLIP experiments,"The past decades have witnessed a surge of discoveries revealing RNA regulation as a central player in cellular processes. RNAs are regulated by RNA-binding proteins (RBPs) at all post-transcriptional stages, including splicing, transportation, stabilization and translation. Defects in the functions of these RBPs underlie a broad spectrum of human pathologies. Systematic identification of RBP functional targets is among the key biomedical research questions and provides a new direction for drug discovery. The advent of cross-linking immunoprecipitation coupled with high-throughput sequencing (genome-wide CLIP) technology has recently enabled the investigation of genome-wide RBP–RNA binding at single base-pair resolution. This technology has evolved through the development of three distinct versions: HITS-CLIP, PAR-CLIP and iCLIP. Meanwhile, numerous bioinformatics pipelines for handling the genome-wide CLIP data have also been developed. In this review, we discuss the genome-wide CLIP technology and focus on bioinformatics analysis. Specifically, we compare the strengths and weaknesses, as well as the scopes, of various bioinformatics tools. To assist readers in choosing optimal procedures for their analysis, we also review experimental design and procedures that affect bioinformatics analyses.",j102,Nucleic Acids Research,jv102,accepted,f898,2002,2002-09-28
s1109,p1109,"Big data analytics in bioinformatics: architectures, techniques, tools and issues",Abstract content goes here ...,j201,Network Modeling Analysis in Health Informatics and Bioinformatics,jv201,accepted,f899,2013,2013-11-02
s1110,p1110,Big Data Bioinformatics,"Recent technological advances allow for high throughput profiling of biological systems in a cost‐efficient manner. The low cost of data generation is leading us to the “big data” era. The availability of big data provides unprecedented opportunities but also raises new challenges for data mining and analysis. In this review, we introduce key concepts in the analysis of big data, including both “machine learning” algorithms as well as “unsupervised” and “supervised” examples of each. We note packages for the R programming language that are available to perform machine learning analyses. In addition to programming based solutions, we review webservers that allow users with limited or no programming background to perform these analyses on large data compendia. J. Cell. Physiol. 229: 1896–1900, 2014. © 2014 Wiley Periodicals, Inc.",j202,Journal of Cellular Physiology,jv202,accepted,f900,2020,2020-11-03
s1111,p1111,Bioinformatics Curriculum Guidelines: Toward a Definition of Core Competencies,"Rapid advances in the life sciences and in related information technologies necessitate the ongoing refinement of bioinformatics educational programs in order to maintain their relevance. As the discipline of bioinformatics and computational biology expands and matures, it is important to characterize the elements that contribute to the success of professionals in this field. These individuals work in a wide variety of settings, including bioinformatics core facilities, biological and medical research laboratories, software development organizations, pharmaceutical and instrument development companies, and institutions that provide education, service, and training. In response to this need, the Curriculum Task Force of the International Society for Computational Biology (ISCB) Education Committee seeks to define curricular guidelines for those who train and educate bioinformaticians. The previous report of the task force summarized a survey that was conducted to gather input regarding the skill set needed by bioinformaticians [1]. The current article details a subsequent effort, wherein the task force broadened its perspectives by examining bioinformatics career opportunities, surveying directors of bioinformatics core facilities, and reviewing bioinformatics education programs. 
 
The bioinformatics literature provides valuable perspectives on bioinformatics education by defining skill sets needed by bioinformaticians, presenting approaches for providing informatics training to biologists, and discussing the roles of bioinformatics core facilities in training and education. 
 
The skill sets required for success in the field of bioinformatics are considered by several authors: Altman [2] defines five broad areas of competency and lists key technologies; Ranganathan [3] presents highlights from the Workshops on Education in Bioinformatics, discussing challenges and possible solutions; Yale's interdepartmental PhD program in computational biology and bioinformatics is described in [4], which lists the general areas of knowledge of bioinformatics; in a related article, a graduate of Yale's PhD program reflects on the skills needed by a bioinformatician [5]; Altman and Klein [6] describe the Stanford Biomedical Informatics (BMI) Training Program, presenting observed trends among BMI students; the American Medical Informatics Association defines competencies in the related field of biomedical informatics in [7]; and the approaches used in several German universities to implement bioinformatics education are described in [8]. 
 
Several approaches to providing bioinformatics training for biologists are described in the literature. Tan et al. [9] report on workshops conducted to identify a minimum skill set for biologists to be able to address the informatics challenges of the “-omics” era. They define a requisite skill set by analyzing responses to questions about the knowledge, skills, and abilities that biologists should possess. The authors in [10] present examples of strategies and methods for incorporating bioinformatics content into undergraduate life sciences curricula. Pevzner and Shamir [11] propose that undergraduate biology curricula should contain an additional course, “Algorithmic, Mathematical, and Statistical Concepts in Biology.” Wingren and Botstein [12] present a graduate course in quantitative biology that is based on original, pathbreaking papers in diverse areas of biology. Johnson and Friedman [13] evaluate the effectiveness of incorporating biological informatics into a clinical informatics program. The results reported are based on interviews of four students and informal assessments of bioinformatics faculty. 
 
The challenges and opportunities relevant to training and education in the context of bioinformatics core facilities are discussed by Lewitter et al. [14]. Relatedly, Lewitter and Rebhan [15] provide guidance regarding the role of a bioinformatics core facility in hiring biologists and in furthering their education in bioinformatics. Richter and Sexton [16] describe a need for highly trained bioinformaticians in core facilities and provide a list of requisite skills. Similarly, Kallioniemi et al. [17] highlight the roles of bioinformatics core units in education and training. 
 
This manuscript expands the body of knowledge pertaining to bioinformatics curriculum guidelines by presenting the results from a broad set of surveys (of core facility directors, of career opportunities, and of existing curricula). Although there is some overlap in the findings of the surveys, they are reported separately, in order to avoid masking the unique aspects of each of the perspectives and to demonstrate that the same themes arise, even when different perspectives are considered. The authors derive from their surveys an initial set of core competencies and relate the competencies to three different categories of professions that have a need for bioinformatics training.",c102,International Conference on Biometrics,cp102,accepted,f901,2022,2022-03-15
s1113,p1113,From protein structure to function with bioinformatics,Abstract content goes here ...,c1,Technical Symposium on Computer Science Education,cp1,accepted,f902,2002,2002-03-24
s1114,p1114,Modern bioinformatics meets traditional Chinese medicine,"MOTIVATION
Traditional Chinese medicine (TCM) is gaining increasing attention with the emergence of integrative medicine and personalized medicine, characterized by pattern differentiation on individual variance and treatments based on natural herbal synergism. Investigating the effectiveness and safety of the potential mechanisms of TCM and the combination principles of drug therapies will bridge the cultural gap with Western medicine and improve the development of integrative medicine. Dealing with rapidly growing amounts of biomedical data and their heterogeneous nature are two important tasks among modern biomedical communities. Bioinformatics, as an emerging interdisciplinary field of computer science and biology, has become a useful tool for easing the data deluge pressure by automating the computation processes with informatics methods. Using these methods to retrieve, store and analyze the biomedical data can effectively reveal the associated knowledge hidden in the data, and thus promote the discovery of integrated information. Recently, these techniques of bioinformatics have been used for facilitating the interactional effects of both Western medicine and TCM. The analysis of TCM data using computational technologies provides biological evidence for the basic understanding of TCM mechanisms, safety and efficacy of TCM treatments. At the same time, the carrier and targets associated with TCM remedies can inspire the rethinking of modern drug development. This review summarizes the significant achievements of applying bioinformatics techniques to many aspects of the research in TCM, such as analysis of TCM-related '-omics' data and techniques for analyzing biological processes and pharmaceutical mechanisms of TCM, which have shown certain potential of bringing new thoughts to both sides.",c23,International Conference on Open and Big Data,cp23,accepted,f903,2012,2012-11-21
s1116,p1116,Genomics Virtual Laboratory: A Practical Bioinformatics Workbench for the Cloud,"Background Analyzing high throughput genomics data is a complex and compute intensive task, generally requiring numerous software tools and large reference data sets, tied together in successive stages of data transformation and visualisation. A computational platform enabling best practice genomics analysis ideally meets a number of requirements, including: a wide range of analysis and visualisation tools, closely linked to large user and reference data sets; workflow platform(s) enabling accessible, reproducible, portable analyses, through a flexible set of interfaces; highly available, scalable computational resources; and flexibility and versatility in the use of these resources to meet demands and expertise of a variety of users. Access to an appropriate computational platform can be a significant barrier to researchers, as establishing such a platform requires a large upfront investment in hardware, experience, and expertise. Results We designed and implemented the Genomics Virtual Laboratory (GVL) as a middleware layer of machine images, cloud management tools, and online services that enable researchers to build arbitrarily sized compute clusters on demand, pre-populated with fully configured bioinformatics tools, reference datasets and workflow and visualisation options. The platform is flexible in that users can conduct analyses through web-based (Galaxy, RStudio, IPython Notebook) or command-line interfaces, and add/remove compute nodes and data resources as required. Best-practice tutorials and protocols provide a path from introductory training to practice. The GVL is available on the OpenStack-based Australian Research Cloud (http://nectar.org.au) and the Amazon Web Services cloud. The principles, implementation and build process are designed to be cloud-agnostic. Conclusions This paper provides a blueprint for the design and implementation of a cloud-based Genomics Virtual Laboratory. We discuss scope, design considerations and technical and logistical constraints, and explore the value added to the research community through the suite of services and resources provided by our implementation.",j108,PLoS ONE,jv108,accepted,f904,2006,2006-06-11
s1117,p1117,Survey of MapReduce frame operation in bioinformatics,"Bioinformatics is challenged by the fact that traditional analysis tools have difficulty in processing large-scale data from high-throughput sequencing. The open source Apache Hadoop project, which adopts the MapReduce framework and a distributed file system, has recently given bioinformatics researchers an opportunity to achieve scalable, efficient and reliable computing performance on Linux clusters and on cloud computing services. In this article, we present MapReduce frame-based applications that can be employed in the next-generation sequencing and other biological domains. In addition, we discuss the challenges faced by this field as well as the future works on parallel computing in bioinformatics.",c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f905,2019,2019-03-16
s1118,p1118,Survey of Natural Language Processing Techniques in Bioinformatics,"Informatics methods, such as text mining and natural language processing, are always involved in bioinformatics research. In this study, we discuss text mining and natural language processing methods in bioinformatics from two perspectives. First, we aim to search for knowledge on biology, retrieve references using text mining methods, and reconstruct databases. For example, protein-protein interactions and gene-disease relationship can be mined from PubMed. Then, we analyze the applications of text mining and natural language processing techniques in bioinformatics, including predicting protein structure and function, detecting noncoding RNA. Finally, numerous methods and applications, as well as their contributions to bioinformatics, are discussed for future use by text mining and natural language processing researchers.",c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f906,2012,2012-11-21
s1120,p1120,Bioinformatics Education—Perspectives and Challenges out of Africa,"The discipline of bioinformatics has developed rapidly since the complete sequencing of the first genomes in the 1990s. The development of many high-throughput techniques during the last decades has ensured that bioinformatics has grown into a discipline that overlaps with, and is required for, the modern practice of virtually every field in the life sciences. This has placed a scientific premium on the availability of skilled bioinformaticians, a qualification that is extremely scarce on the African continent. The reasons for this are numerous, although the absence of a skilled bioinformatician at academic institutions to initiate a training process and build sustained capacity seems to be a common African shortcoming. This dearth of bioinformatics expertise has had a knock-on effect on the establishment of many modern high-throughput projects at African institutes, including the comprehensive and systematic analysis of genomes from African populations, which are among the most genetically diverse anywhere on the planet. Recent funding initiatives from the National Institutes of Health and the Wellcome Trust are aimed at ameliorating this shortcoming. In this paper, we discuss the problems that have limited the establishment of the bioinformatics field in Africa, as well as propose specific actions that will help with the education and training of bioinformaticians on the continent. This is an absolute requirement in anticipation of a boom in high-throughput approaches to human health issues unique to data from African populations.",c114,IEEE International Conference on Robotics and Automation,cp114,accepted,f907,2011,2011-07-28
s1121,p1121,A Survey of Scholarly Literature Describing the Field of Bioinformatics Education and Bioinformatics Educational Research,"This article provides an overview of the state of research in bioinformatics education in the years 1998 through 2013. It identifies current curricular approaches for integrating bioinformatics education, concepts and skills being taught, pedagogical approaches and methods of delivery, and educational research and evaluation results.",j204,CBE - Life Sciences Education,jv204,accepted,f908,2019,2019-07-31
s1122,p1122,IEEE/ACM Transactions on Computational Biology and Bioinformatics,"This special issue of the IEEE/ACM Transactions on Computational Biology and Bioinformatic includes a selection of papers presented at the Seventh Brazilian Symposium on Bioinformatics (BSB 2012) held 15-17 August 2012 in Campo Grande (Mato Grosso do Sul), Brazil. BSB is an international symposium that covers all aspects of bioinformatics and computational biology. The symposium is organized by the special interest group in Computational Biology of the Brazilian Computer Society (SBC).",c84,The Web Conference,cp84,accepted,f909,2006,2006-04-18
s1123,p1123,Verification and validation of bioinformatics software without a gold standard: a case study of BWA and Bowtie,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f910,2007,2007-11-29
s1124,p1124,Random Forest for Bioinformatics,Abstract content goes here ...,c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f911,2006,2006-11-26
s1125,p1125,A widespread self-cleaving ribozyme class is revealed by bioinformatics,Abstract content goes here ...,j205,Nature Chemical Biology,jv205,accepted,f912,2020,2020-09-07
s1126,p1126,Challenges in RNA virus bioinformatics,"Abstract Motivation: Computer-assisted studies of structure, function and evolution of viruses remains a neglected area of research. The attention of bioinformaticians to this interesting and challenging field is far from commensurate with its medical and biotechnological importance. It is telling that out of >200 talks held at ISMB 2013, the largest international bioinformatics conference, only one presentation explicitly dealt with viruses. In contrast to many broad, established and well-organized bioinformatics communities (e.g. structural genomics, ontologies, next-generation sequencing, expression analysis), research groups focusing on viruses can probably be counted on the fingers of two hands. Results: The purpose of this review is to increase awareness among bioinformatics researchers about the pressing needs and unsolved problems of computational virology. We focus primarily on RNA viruses that pose problems to many standard bioinformatics analyses owing to their compact genome organization, fast mutation rate and low evolutionary conservation. We provide an overview of tools and algorithms for handling viral sequencing data, detecting functionally important RNA structures, classifying viral proteins into families and investigating the origin and evolution of viruses. Contact: manja@uni-jena.de Supplementary information: Supplementary data are available at Bioinformatics online. The references for this article can be found in the Supplementary Material.",c106,Chinese Conference on Biometric Recognition,cp106,accepted,f913,2016,2016-09-13
s1127,p1127,Buying in to bioinformatics: an introduction to commercial sequence analysis software,"Advancements in high-throughput nucleotide sequencing techniques have brought with them state-of-the-art bioinformatics programs and software packages. Given the importance of molecular sequence data in contemporary life science research, these software suites are becoming an essential component of many labs and classrooms, and as such are frequently designed for non-computer specialists and marketed as one-stop bioinformatics toolkits. Although beautifully designed and powerful, user-friendly bioinformatics packages can be expensive and, as more arrive on the market each year, it can be difficult for researchers, teachers and students to choose the right software for their needs, especially if they do not have a bioinformatics background. This review highlights some of the currently available and most popular commercial bioinformatics packages, discussing their prices, usability, features and suitability for teaching. Although several commercial bioinformatics programs are arguably overpriced and overhyped, many are well designed, sophisticated and, in my opinion, worth the investment. If you are just beginning your foray into molecular sequence analysis or an experienced genomicist, I encourage you to explore proprietary software bundles. They have the potential to streamline your research, increase your productivity, energize your classroom and, if anything, add a bit of zest to the often dry detached world of bioinformatics.",c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f914,2019,2019-09-11
s1128,p1128,MEIGO: an open-source software suite based on metaheuristics for global optimization in systems biology and bioinformatics,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f915,2007,2007-12-13
s1129,p1129,An explosion of bioinformatics careers,"Big data is everywhere, and its influence and practical omnipresence across multiple industries will just continue to grow. For life scientists with expertise and an interest in bioinformatics, computer science, statistics, and related skill sets, the job outlook couldn’t be rosier. Big pharma, biotech, and software companies are clamoring to hire professionals with experience in bioinformatics and the identification, compilation, analysis, and visualization of huge amounts of biological and health care information. With the rapid development of new tools to make sense of life science research and outcomes, spurred by innovative research in bioinformatics itself, scientists who are entranced by data can pursue more career options than ever before. Read the Feature (Full-Text HTML) Read the Feature (PDF)",c11,Hawaii International Conference on System Sciences,cp11,accepted,f916,2006,2006-04-01
s1131,p1131,Integration of bioinformatics to biodegradation,Abstract content goes here ...,j207,Biological Procedures Online,jv207,accepted,f917,2018,2018-05-10
s1132,p1132,Crowdsourcing for bioinformatics,"MOTIVATION
Bioinformatics is faced with a variety of problems that require human involvement. Tasks like genome annotation, image analysis, knowledge-base population and protein structure determination all benefit from human input. In some cases, people are needed in vast quantities, whereas in others, we need just a few with rare abilities. Crowdsourcing encompasses an emerging collection of approaches for harnessing such distributed human intelligence. Recently, the bioinformatics community has begun to apply crowdsourcing in a variety of contexts, yet few resources are available that describe how these human-powered systems work and how to use them effectively in scientific domains.


RESULTS
Here, we provide a framework for understanding and applying several different types of crowdsourcing. The framework considers two broad classes: systems for solving large-volume 'microtasks' and systems for solving high-difficulty 'megatasks'. Within these classes, we discuss system types, including volunteer labor, games with a purpose, microtask markets and open innovation contests. We illustrate each system type with successful examples in bioinformatics and conclude with a guide for matching problems to crowdsourcing solutions that highlights the positives and negatives of different approaches.",c65,Formal Concept Analysis,cp65,accepted,f918,2008,2008-02-07
s1134,p1134,A primer to frequent itemset mining for bioinformatics,"Over the past two decades, pattern mining techniques have become an integral part of many bioinformatics solutions. Frequent itemset mining is a popular group of pattern mining techniques designed to identify elements that frequently co-occur. An archetypical example is the identification of products that often end up together in the same shopping basket in supermarket transactions. A number of algorithms have been developed to address variations of this computationally non-trivial problem. Frequent itemset mining techniques are able to efficiently capture the characteristics of (complex) data and succinctly summarize it. Owing to these and other interesting properties, these techniques have proven their value in biological data analysis. Nevertheless, information about the bioinformatics applications of these techniques remains scattered. In this primer, we introduce frequent itemset mining and their derived association rules for life scientists. We give an overview of various algorithms, and illustrate how they can be used in several real-life bioinformatics application domains. We end with a discussion of the future potential and open challenges for frequent itemset mining in the life sciences.",c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f919,2022,2022-03-20
s1137,p1137,ViPR: an open bioinformatics database and analysis resource for virology research,"The Virus Pathogen Database and Analysis Resource (ViPR, www.ViPRbrc.org) is an integrated repository of data and analysis tools for multiple virus families, supported by the National Institute of Allergy and Infectious Diseases (NIAID) Bioinformatics Resource Centers (BRC) program. ViPR contains information for human pathogenic viruses belonging to the Arenaviridae, Bunyaviridae, Caliciviridae, Coronaviridae, Flaviviridae, Filoviridae, Hepeviridae, Herpesviridae, Paramyxoviridae, Picornaviridae, Poxviridae, Reoviridae, Rhabdoviridae and Togaviridae families, with plans to support additional virus families in the future. ViPR captures various types of information, including sequence records, gene and protein annotations, 3D protein structures, immune epitope locations, clinical and surveillance metadata and novel data derived from comparative genomics analysis. Analytical and visualization tools for metadata-driven statistical sequence analysis, multiple sequence alignment, phylogenetic tree construction, BLAST comparison and sequence variation determination are also provided. Data filtering and analysis workflows can be combined and the results saved in personal ‘Workbenches’ for future use. ViPR tools and data are available without charge as a service to the virology research community to help facilitate the development of diagnostics, prophylactics and therapeutics for priority pathogens and other viruses.",c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f920,2021,2021-01-27
s1138,p1138,Bioinformatics analysis of the epitope regions for norovirus capsid protein,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f921,2007,2007-05-18
s1139,p1139,Bioinformatics opportunities for identification and study of medicinal plants,"Plants have been used as a source of medicine since historic times and several commercially important drugs are of plant-based origin. The traditional approach towards discovery of plant-based drugs often times involves significant amount of time and expenditure. These labor-intensive approaches have struggled to keep pace with the rapid development of high-throughput technologies. In the era of high volume, high-throughput data generation across the biosciences, bioinformatics plays a crucial role. This has generally been the case in the context of drug designing and discovery. However, there has been limited attention to date to the potential application of bioinformatics approaches that can leverage plant-based knowledge. Here, we review bioinformatics studies that have contributed to medicinal plants research. In particular, we highlight areas in medicinal plant research where the application of bioinformatics methodologies may result in quicker and potentially cost-effective leads toward finding plant-based remedies.",c3,Frontiers in Education Conference,cp3,accepted,f922,2016,2016-10-20
s1140,p1140,Detecting the knowledge structure of bioinformatics by mining full-text collections,Abstract content goes here ...,j104,Scientometrics,jv104,accepted,f923,2010,2010-05-17
s1141,p1141,Bioinformatics: Sequence and Genome Analysis,Preface Chapter 1. Historical introduction and overview Chapter 2. Collecting and storing sequences in the laboratory Chapter 3. Alignment of pairs of sequences Chapter 4. Introduction to probability and statistical analysis of sequence alignments Chapter 5. Multiple sequence alignment Chapter 6. Sequence database searching for similar sequences Chapter 7. Phylogenetic prediction Chapter 8. Prediction of RNA secondary structure Chapter 9. Gene prediction and regulation Chapter 10. Protein classification and structure prediction Chapter 11. Genome analysis Chapter 12. Bioinformatics programming using Perl and Perl modules Chapter 13. Analysis of microarrays,c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f924,2019,2019-03-27
s1142,p1142,Pattern Recognition in Bioinformatics,Abstract content goes here ...,j75,Lecture Notes in Computer Science,jv75,accepted,f925,2015,2015-01-19
s1143,p1143,Computational and Bioinformatics Frameworks for Next-Generation Whole Exome and Genome Sequencing,"It has become increasingly apparent that one of the major hurdles in the genomic age will be the bioinformatics challenges of next-generation sequencing. We provide an overview of a general framework of bioinformatics analysis. For each of the three stages of (1) alignment, (2) variant calling, and (3) filtering and annotation, we describe the analysis required and survey the different software packages that are used. Furthermore, we discuss possible future developments as data sources grow and highlight opportunities for new bioinformatics tools to be developed.",c7,European Conference on Modelling and Simulation,cp7,accepted,f926,2015,2015-05-18
s1144,p1144,ALSoD: A user‐friendly online bioinformatics tool for amyotrophic lateral sclerosis genetics,"Amyotrophic lateral sclerosis (ALS) is the commonest adult onset motor neuron disease, with a peak age of onset in the seventh decade. With advances in genetic technology, there is an enormous increase in the volume of genetic data produced, and a corresponding need for storage, analysis, and interpretation, particularly as our understanding of the relationships between genotype and phenotype mature. Here, we present a system to enable this in the form of the ALS Online Database (ALSoD at http://alsod.iop.kcl.ac.uk), a freely available database that has been transformed from a single gene storage facility recording mutations in the SOD1 gene to a multigene ALS bioinformatics repository and analytical instrument combining genotype, phenotype, and geographical information with associated analysis tools. These include a comparison tool to evaluate genes side by side or jointly with user configurable features, a pathogenicity prediction tool using a combination of computational approaches to distinguish variants with nonfunctional characteristics from disease‐associated mutations with more dangerous consequences, and a credibility tool to enable ALS researchers to objectively assess the evidence for gene causation in ALS. Furthermore, integration of external tools, systems for feedback, annotation by users, and two‐way links to collaborators hosting complementary databases further enhance the functionality of ALSoD. Hum Mutat 33:1345–1351, 2012. © 2012 Wiley Periodicals, Inc.",j209,Human Mutation,jv209,accepted,f927,2005,2005-06-29
s1145,p1145,Role of bioinformatics and pharmacogenomics in drug discovery and development process,Abstract content goes here ...,j201,Network Modeling Analysis in Health Informatics and Bioinformatics,jv201,accepted,f928,2013,2013-09-15
s1146,p1146,Current Challenges in the Bioinformatics of Single Cell Genomics,"Single cell genomics is a rapidly growing field with many new techniques emerging in the past few years. However, few bioinformatics tools specific for single cell genomics analysis are available. Single cell DNA/RNA sequencing data usually have low genome coverage and high amplification bias, which makes bioinformatics analysis challenging. Many current bioinformatics tools developed for bulk cell sequencing do not work well with single cell sequencing data. Here, we summarize current challenges in the bioinformatics analysis of single cell genomic DNA sequencing and single cell transcriptomes. These challenges include calling copy number variations, identifying mutated genes in tumor samples, reconstructing cell lineages, recovering low abundant transcripts, and improving the accuracy of quantitative analysis of transcripts. Development in single cell genomics bioinformatics analysis will promote the application of this technology to basic biology and medical research.",j210,Frontiers in Oncology,jv210,accepted,f929,2001,2001-05-07
s1147,p1147,Milestones in graphical bioinformatics,"After reviewing the field of graphical bioinformatics, we have selected two dozen of the most significant publications that represent milestones of graphical bioinformatics. These publications can be viewed as forming the backbone of graphical bioinformatics, the branch of bioinformatics that initiates analysis of DNA, RNA, and proteins by considering various graphical representations of these sequences. Graphical bioinformatics, a division of bioinformatics that analyzes sequences of DNA, RNA, proteins, and proteomics maps by developing and using tools of discrete mathematics and graph theory in particular, has expanded since the year 2000, although pioneering contributions date back to Hamory (1983) and Jeffrey (1990). We chronologically follow the development of graphical bioinformatics, without assuming that readers are familiar with discrete mathematics or graph theory. Readers unfamiliar with graph theory may even have some advantage over those who have been only superficially exposed to graph theory, inview of wide misconceptions and misinformation about chemical graph theory among quantum chemists, physical chemists, and medicinal chemists in past decades. © 2013 Wiley Periodicals, Inc.",c10,Big Data,cp10,accepted,f930,2021,2021-10-02
s1148,p1148,Metscape 2 bioinformatics tool for the analysis and visualization of metabolomics and gene expression data,"MOTIVATION
Metabolomics is a rapidly evolving field that holds promise to provide insights into genotype-phenotype relationships in cancers, diabetes and other complex diseases. One of the major informatics challenges is providing tools that link metabolite data with other types of high-throughput molecular data (e.g. transcriptomics, proteomics), and incorporate prior knowledge of pathways and molecular interactions.


RESULTS
We describe a new, substantially redesigned version of our tool Metscape that allows users to enter experimental data for metabolites, genes and pathways and display them in the context of relevant metabolic networks. Metscape 2 uses an internal relational database that integrates data from KEGG and EHMN databases. The new version of the tool allows users to identify enriched pathways from expression profiling data, build and analyze the networks of genes and metabolites, and visualize changes in the gene/metabolite data. We demonstrate the applications of Metscape to annotate molecular pathways for human and mouse metabolites implicated in the pathogenesis of sepsis-induced acute lung injury, for the analysis of gene expression and metabolite data from pancreatic ductal adenocarcinoma, and for identification of the candidate metabolites involved in cancer and inflammation.


AVAILABILITY
Metscape is part of the National Institutes of Health-supported National Center for Integrative Biomedical Informatics (NCIBI) suite of tools, freely available at http://metscape.ncibi.org. It can be downloaded from http://cytoscape.org or installed via Cytoscape plugin manager.


CONTACT
metscape-help@umich.edu; akarnovs@umich.edu


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",c12,International Conference on Statistical and Scientific Database Management,cp12,accepted,f931,2002,2002-03-13
s1149,p1149,Are graph databases ready for bioinformatics?,Contact: Lars.Juhl.Jensen@gmail.com,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f932,2019,2019-03-05
s1150,p1150,Bioinformatics tools for secretome analysis.,Abstract content goes here ...,j211,Biochimica et Biophysica Acta,jv211,accepted,f933,2004,2004-09-15
s1151,p1151,Bio2RDF: Towards a mashup to build bioinformatics knowledge systems,Abstract content goes here ...,j189,Journal of Biomedical Informatics,jv189,accepted,f934,2012,2012-11-22
s1152,p1152,Coronavirus Genomics and Bioinformatics Analysis,"The drastic increase in the number of coronaviruses discovered and coronavirus genomes being sequenced have given us an unprecedented opportunity to perform genomics and bioinformatics analysis on this family of viruses. Coronaviruses possess the largest genomes (26.4 to 31.7 kb) among all known RNA viruses, with G + C contents varying from 32% to 43%. Variable numbers of small ORFs are present between the various conserved genes (ORF1ab, spike, envelope, membrane and nucleocapsid) and downstream to nucleocapsid gene in different coronavirus lineages. Phylogenetically, three genera, Alphacoronavirus, Betacoronavirus and Gammacoronavirus, with Betacoronavirus consisting of subgroups A, B, C and D, exist. A fourth genus, Deltacoronavirus, which includes bulbul coronavirus HKU11, thrush coronavirus HKU12 and munia coronavirus HKU13, is emerging. Molecular clock analysis using various gene loci revealed that the time of most recent common ancestor of human/civet SARS related coronavirus to be 1999–2002, with estimated substitution rate of 4×10−4 to 2×10−2 substitutions per site per year. Recombination in coronaviruses was most notable between different strains of murine hepatitis virus (MHV), between different strains of infectious bronchitis virus, between MHV and bovine coronavirus, between feline coronavirus (FCoV) type I and canine coronavirus generating FCoV type II, and between the three genotypes of human coronavirus HKU1 (HCoV-HKU1). Codon usage bias in coronaviruses were observed, with HCoV-HKU1 showing the most extreme bias, and cytosine deamination and selection of CpG suppressed clones are the two major independent biological forces that shape such codon usage bias in coronaviruses.",j212,Viruses,jv212,accepted,f935,2001,2001-11-01
s1154,p1154,Bioinformatics Tools for Mass Spectroscopy-Based Metabolomic Data Processing and Analysis,"Biological systems are increasingly being studied in a holistic manner, using omics approaches, to provide quantitative and qualitative descriptions of the diverse collection of cellular components. Among the omics approaches, metabolomics, which deals with the quantitative global profiling of small molecules or metabolites, is being used extensively to explore the dynamic response of living systems, such as organelles, cells, tissues, organs and whole organisms, under diverse physiological and pathological conditions. This technology is now used routinely in a number of applications, including basic and clinical research, agriculture, microbiology, food science, nutrition, pharmaceutical research, environmental science and the development of biofuels. Of the multiple analytical platforms available to perform such analyses, nuclear magnetic resonance and mass spectrometry have come to dominate, owing to the high resolution and large datasets that can be generated with these techniques. The large multidimensional datasets that result from such studies must be processed and analyzed to render this data meaningful. Thus, bioinformatics tools are essential for the efficient processing of huge datasets, the characterization of the detected signals, and to align multiple datasets and their features. This paper provides a state-of-the-art overview of the data processing tools available, and reviews a collection of recent reports on the topic. Data conversion, pre-processing, alignment, normalization and statistical analysis are introduced, with their advantages and disadvantages, and comparisons are made to guide the reader.",j213,Current Bioinformatics,jv213,accepted,f936,2002,2002-02-23
s1155,p1155,Bioinformatics: Databases and Systems,Abstract content goes here ...,j156,Springer US,jv156,accepted,f937,2010,2010-06-16
s1157,p1157,Computational intelligence techniques in bioinformatics,Abstract content goes here ...,c60,IEEE International Conference on Software Engineering and Formal Methods,cp60,accepted,f938,2011,2011-11-01
s1158,p1158,bioNerDS: exploring bioinformatics’ database and software use through literature mining,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f939,2007,2007-01-25
s1159,p1159,4273π: Bioinformatics education on low cost ARM hardware,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f940,2007,2007-12-21
s1161,p1161,The Chemistry Development Kit (CDK): An Open-Source Java Library for Chemo-and Bioinformatics,"The Chemistry Development Kit (CDK) is a freely available open-source Java library for Structural Chemo-and Bioinformatics. Its architecture and capabilities as well as the development as an open-source project by a team of international collaborators from academic and industrial institutions is described. The CDK provides methods for many common tasks in molecular informatics, including 2D and 3D rendering of chemical structures, I/O routines, SMILES parsing and generation, ring searches, isomorphism checking, structure diagram generation, etc. Application scenarios as well as access information for interested users and potential contributors are given.",j214,Journal of chemical information and computer sciences,jv214,accepted,f941,2022,2022-07-25
s1162,p1162,Cloud BioLinux: pre-configured and on-demand bioinformatics computing for the genomics community,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f942,2007,2007-05-22
s1163,p1163,A Bioinformatics Method Identifies Prominent Off-targeted Transcripts in RNAi Screens,Abstract content goes here ...,j6,Nature Methods,jv6,accepted,f943,2010,2010-04-09
s1164,p1164,An overview of the Hadoop/MapReduce/HBase framework and its current applications in bioinformatics,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f944,2007,2007-02-15
s1165,p1165,Bioinformatics clouds for big data manipulation,Abstract content goes here ...,j215,Biology Direct,jv215,accepted,f945,2006,2006-06-21
s1166,p1166,Provenance in bioinformatics workflows,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f946,2007,2007-12-22
s1168,p1168,Bpipe: a tool for running and managing bioinformatics pipelines,"SUMMARY
Bpipe is a simple, dedicated programming language for defining and executing bioinformatics pipelines. It specializes in enabling users to turn existing pipelines based on shell scripts or command line tools into highly flexible, adaptable and maintainable workflows with a minimum of effort. Bpipe ensures that pipelines execute in a controlled and repeatable fashion and keeps audit trails and logs to ensure that experimental results are reproducible. Requiring only Java as a dependency, Bpipe is fully self-contained and cross-platform, making it very easy to adopt and deploy into existing environments.


AVAILABILITY AND IMPLEMENTATION
Bpipe is freely available from http://bpipe.org under a BSD License.",c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f947,2006,2006-09-18
s1169,p1169,PATRIC: the Comprehensive Bacterial Bioinformatics Resource with a Focus on Human Pathogenic Species,"ABSTRACT Funded by the National Institute of Allergy and Infectious Diseases, the Pathosystems Resource Integration Center (PATRIC) is a genomics-centric relational database and bioinformatics resource designed to assist scientists in infectious-disease research. Specifically, PATRIC provides scientists with (i) a comprehensive bacterial genomics database, (ii) a plethora of associated data relevant to genomic analysis, and (iii) an extensive suite of computational tools and platforms for bioinformatics analysis. While the primary aim of PATRIC is to advance the knowledge underlying the biology of human pathogens, all publicly available genome-scale data for bacteria are compiled and continually updated, thereby enabling comparative analyses to reveal the basis for differences between infectious free-living and commensal species. Herein we summarize the major features available at PATRIC, dividing the resources into two major categories: (i) organisms, genomes, and comparative genomics and (ii) recurrent integration of community-derived associated data. Additionally, we present two experimental designs typical of bacterial genomics research and report on the execution of both projects using only PATRIC data and tools. These applications encompass a broad range of the data and analysis tools available, illustrating practical uses of PATRIC for the biologist. Finally, a summary of PATRIC's outreach activities, collaborative endeavors, and future research directions is provided.",j216,Infection and Immunity,jv216,accepted,f948,2004,2004-10-16
s1171,p1171,BioJava: an open-source framework for bioinformatics in 2012,"Motivation: BioJava is an open-source project for processing of biological data in the Java programming language. We have recently released a new version (3.0.5), which is a major update to the code base that greatly extends its functionality. Results: BioJava now consists of several independent modules that provide state-of-the-art tools for protein structure comparison, pairwise and multiple sequence alignments, working with DNA and protein sequences, analysis of amino acid properties, detection of protein modifications and prediction of disordered regions in proteins as well as parsers for common file formats using a biologically meaningful data model. Availability: BioJava is an open-source project distributed under the Lesser GPL (LGPL). BioJava can be downloaded from the BioJava website (http://www.biojava.org). BioJava requires Java 1.6 or higher. All inquiries should be directed to the BioJava mailing lists. Details are available at http://biojava.org/wiki/BioJava:MailingLists Contact: andreas.prlic@gmail.com",c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f949,2016,2016-06-15
s1172,p1172,A bioinformatics view of zinc enzymes.,Abstract content goes here ...,j217,Journal of Inorganic Biochemistry,jv217,accepted,f950,2013,2013-03-03
s1173,p1173,The Roots of Bioinformatics in Theoretical Biology,"From the late 1980s onward, the term “bioinformatics” mostly has been used to refer to computational methods for comparative analysis of genome data. However, the term was originally more widely defined as the study of informatic processes in biotic systems. In this essay, I will trace this early history (from a personal point of view) and I will argue that the original meaning of the term is re-emerging.",c8,The Compass,cp8,accepted,f951,2016,2016-01-25
s1176,p1176,"Probing Native Protein Structures by Chemical Cross-linking, Mass Spectrometry, and Bioinformatics*","Chemical cross-linking of reactive groups in native proteins and protein complexes in combination with the identification of cross-linked sites by mass spectrometry has been in use for more than a decade. Recent advances in instrumentation, cross-linking protocols, and analysis software have led to a renewed interest in this technique, which promises to provide important information about native protein structure and the topology of protein complexes. In this article, we discuss the critical steps of chemical cross-linking and its implications for (structural) biology: reagent design and cross-linking protocols, separation and mass spectrometric analysis of cross-linked samples, dedicated software for data analysis, and the use of cross-linking data for computational modeling. Finally, the impact of protein cross-linking on various biological disciplines is highlighted.",j218,Molecular & Cellular Proteomics,jv218,accepted,f952,2003,2003-03-31
s1177,p1177,A review of the stability of feature selection techniques for bioinformatics data,"Feature selection is an important step in data mining and is used in various domains including genetics, medicine, and bioinformatics. Choosing the important features (genes) is essential for the discovery of new knowledge hidden within the genetic code as well as the identification of important biomarkers. Although feature selection methods can help sort through large numbers of genes based on their relevance to the problem at hand, the results generated tend to be unstable and thus cannot be reproduced in other experiments. Relatedly, research interest in the stability of feature ranking methods has grown recently and researchers have produced experimental designs for testing the stability of feature selection, creating new metrics for measuring stability and new techniques designed to improve the stability of the feature selection process. In this paper, we will introduce the role of stability in feature selection with DNA microarray data. We list various ways of improving feature ranking stability, and discuss feature selection techniques, specifically explaining ensemble feature ranking and presenting various ensemble feature ranking aggregation methods. Finally, we discuss experimental procedures such as dataset perturbation, fixed overlap partitioning, and cross validation procedures that help researchers analyze and measure the stability of feature ranking methods. Throughout this work, we investigate current research in the field and discuss possible avenues of continuing such research efforts.",c71,IEEE International Conference on Information Reuse and Integration,cp71,accepted,f953,2022,2022-09-21
s1178,p1178,Comprehensive Decision Tree Models in Bioinformatics,"Purpose Classification is an important and widely used machine learning technique in bioinformatics. Researchers and other end-users of machine learning software often prefer to work with comprehensible models where knowledge extraction and explanation of reasoning behind the classification model are possible. Methods This paper presents an extension to an existing machine learning environment and a study on visual tuning of decision tree classifiers. The motivation for this research comes from the need to build effective and easily interpretable decision tree models by so called one-button data mining approach where no parameter tuning is needed. To avoid bias in classification, no classification performance measure is used during the tuning of the model that is constrained exclusively by the dimensions of the produced decision tree. Results The proposed visual tuning of decision trees was evaluated on 40 datasets containing classical machine learning problems and 31 datasets from the field of bioinformatics. Although we did not expected significant differences in classification performance, the results demonstrate a significant increase of accuracy in less complex visually tuned decision trees. In contrast to classical machine learning benchmarking datasets, we observe higher accuracy gains in bioinformatics datasets. Additionally, a user study was carried out to confirm the assumption that the tree tuning times are significantly lower for the proposed method in comparison to manual tuning of the decision tree. Conclusions The empirical results demonstrate that by building simple models constrained by predefined visual boundaries, one not only achieves good comprehensibility, but also very good classification performance that does not differ from usually more complex models built using default settings of the classical decision tree algorithm. In addition, our study demonstrates the suitability of visually tuned decision trees for datasets with binary class attributes and a high number of possibly redundant attributes that are very common in bioinformatics.",j108,PLoS ONE,jv108,accepted,f954,2006,2006-10-23
s1180,p1180,Machine learning in bioinformatics,"This article reviews machine learning methods for bioinformatics. It presents modelling methods, such as supervised classification, clustering and probabilistic graphical models for knowledge discovery, as well as deterministic and stochastic heuristics for optimization. Applications in genomics, proteomics, systems biology, evolution and text mining are also shown.",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f955,2018,2018-02-10
s1181,p1181,Data Mining in Bioinformatics,Abstract content goes here ...,c50,International Conference on Automated Software Engineering,cp50,accepted,f956,2008,2008-10-30
s1183,p1183,Cancer bioinformatics: A new approach to systems clinical medicine,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f957,2007,2007-02-25
s1184,p1184,Rise and Demise of Bioinformatics? Promise and Progress,"The field of bioinformatics and computational biology has gone through a number of transformations during the past 15 years, establishing itself as a key component of new biology. This spectacular growth has been challenged by a number of disruptive changes in science and technology. Despite the apparent fatigue of the linguistic use of the term itself, bioinformatics has grown perhaps to a point beyond recognition. We explore both historical aspects and future trends and argue that as the field expands, key questions remain unanswered and acquire new meaning while at the same time the range of applications is widening to cover an ever increasing number of biological disciplines. These trends appear to be pointing to a redefinition of certain objectives, milestones, and possibly the field itself.",c100,ACM SIGMOD Conference,cp100,accepted,f958,2010,2010-07-26
s1185,p1185,Bioinformatics for personal genome interpretation,"An international consortium released the first draft sequence of the human genome 10 years ago. Although the analysis of this data has suggested the genetic underpinnings of many diseases, we have not yet been able to fully quantify the relationship between genotype and phenotype. Thus, a major current effort of the scientific community focuses on evaluating individual predispositions to specific phenotypic traits given their genetic backgrounds. Many resources aim to identify and annotate the specific genes responsible for the observed phenotypes. Some of these use intra-species genetic variability as a means for better understanding this relationship. In addition, several online resources are now dedicated to collecting single nucleotide variants and other types of variants, and annotating their functional effects and associations with phenotypic traits. This information has enabled researchers to develop bioinformatics tools to analyze the rapidly increasing amount of newly extracted variation data and to predict the effect of uncharacterized variants. In this work, we review the most important developments in the field--the databases and bioinformatics tools that will be of utmost importance in our concerted effort to interpret the human variome.",c77,Networks,cp77,accepted,f959,2019,2019-09-19
s1186,p1186,Fast Parallel Markov Clustering in Bioinformatics Using Massively Parallel Computing on GPU with CUDA and ELLPACK-R Sparse Format,"Markov clustering (MCL) is becoming a key algorithm within bioinformatics for determining clusters in networks. However, with increasing vast amount of data on biological networks, performance and scalability issues are becoming a critical limiting factor in applications. Meanwhile, GPU computing, which uses CUDA tool for implementing a massively parallel computing environment in the GPU card, is becoming a very powerful, efficient, and low-cost option to achieve substantial performance gains over CPU approaches. The use of on-chip memory on the GPU is efficiently lowering the latency time, thus, circumventing a major issue in other parallel computing environments, such as MPI. We introduce a very fast Markov clustering algorithm using CUDA (CUDA-MCL) to perform parallel sparse matrix-matrix computations and parallel sparse Markov matrix normalizations, which are at the heart of MCL. We utilized ELLPACK-R sparse format to allow the effective and fine-grain massively parallel processing to cope with the sparse nature of interaction networks data sets in bioinformatics applications. As the results show, CUDA-MCL is significantly faster than the original MCL running on CPU. Thus, large-scale parallel computation on off-the-shelf desktop-machines, that were previously only possible on supercomputing architectures, can significantly change the way bioinformaticians and biologists deal with their data.",j219,IEEE/ACM Transactions on Computational Biology & Bioinformatics,jv219,accepted,f960,2001,2001-10-25
s1187,p1187,Translational Bioinformatics: Linking the Molecular World to the Clinical World,Abstract content goes here ...,j220,Clinical pharmacology and therapy,jv220,accepted,f961,2022,2022-07-15
s1189,p1189,Threshold-based feature selection techniques for high-dimensional bioinformatics data,Abstract content goes here ...,j201,Network Modeling Analysis in Health Informatics and Bioinformatics,jv201,accepted,f962,2013,2013-08-16
s1190,p1190,myExperiment: a repository and social network for the sharing of bioinformatics workflows,"myExperiment (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, myExperiment allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to myExperiment and enable them to be shared in a secure manner. Since its release in 2007, myExperiment currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about myExperiment including its REST web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org.",c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f963,2016,2016-08-18
s1191,p1191,Bioinformatics for the Human Microbiome Project,"Microbes inhabit virtually all sites of the human body, yet we know very little about the role they play in our health. In recent years, there has been increasing interest in studying human-associated microbial communities, particularly since microbial dysbioses have now been implicated in a number of human diseases [1]–[3]. Dysbiosis, the disruption of the normal microbial community structure, however, is impossible to define without first establishing what “normal microbial community structure” means within the healthy human microbiome. Recent advances in sequencing technologies have made it feasible to perform large-scale studies of microbial communities, providing the tools necessary to begin to address this question [4], [5]. This led to the implementation of the Human Microbiome Project (HMP) in 2007, an initiative funded by the National Institutes of Health Roadmap for Biomedical Research and constructed as a large, genome-scale community research project [6]. Any such project must plan for data analysis, computational methods development, and the public availability of tools and data; here, we provide an overview of the corresponding bioinformatics organization, history, and results from the HMP (Figure 1).",c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f964,2016,2016-11-17
s1192,p1192,Advances in Omics and Bioinformatics Tools for Systems Analyses of Plant Functions,"Omics and bioinformatics are essential to understanding the molecular systems that underlie various plant functions. Recent game-changing sequencing technologies have revitalized sequencing approaches in genomics and have produced opportunities for various emerging analytical applications. Driven by technological advances, several new omics layers such as the interactome, epigenome and hormonome have emerged. Furthermore, in several plant species, the development of omics resources has progressed to address particular biological properties of individual species. Integration of knowledge from omics-based research is an emerging issue as researchers seek to identify significance, gain biological insights and promote translational research. From these perspectives, we provide this review of the emerging aspects of plant systems research based on omics and bioinformatics analyses together with their associated resources and technological advances.",j221,Plant and Cell Physiology,jv221,accepted,f965,2014,2014-05-19
s1193,p1193,Personalized cloud-based bioinformatics services for research and education: use cases and the elasticHPC package,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f966,2007,2007-05-23
s1194,p1194,VectorBase: improvements to a bioinformatics resource for invertebrate vector genomics,"VectorBase (http://www.vectorbase.org) is a NIAID-supported bioinformatics resource for invertebrate vectors of human pathogens. It hosts data for nine genomes: mosquitoes (three Anopheles gambiae genomes, Aedes aegypti and Culex quinquefasciatus), tick (Ixodes scapularis), body louse (Pediculus humanus), kissing bug (Rhodnius prolixus) and tsetse fly (Glossina morsitans). Hosted data range from genomic features and expression data to population genetics and ontologies. We describe improvements and integration of new data that expand our taxonomic coverage. Releases are bi-monthly and include the delivery of preliminary data for emerging genomes. Frequent updates of the genome browser provide VectorBase users with increasing options for visualizing their own high-throughput data. One major development is a new population biology resource for storing genomic variations, insecticide resistance data and their associated metadata. It takes advantage of improved ontologies and controlled vocabularies. Combined, these new features ensure timely release of multiple types of data in the public domain while helping overcome the bottlenecks of bioinformatics and annotation by engaging with our user community.",c22,International Conference on Data Technologies and Applications,cp22,accepted,f967,2020,2020-04-01
s1195,p1195,Bioinformatics for High Throughput Sequencing,Abstract content goes here ...,j222,Springer: New York,jv222,accepted,f968,2022,2022-05-27
s1196,p1196,Aging and microRNA expression in human skeletal muscle: a microarray and bioinformatics analysis.,"A common characteristic of aging is loss of skeletal muscle (sarcopenia), which can lead to falls and fractures. MicroRNAs (miRNAs) are novel posttranscriptional modulators of gene expression with potential roles as regulators of skeletal muscle mass and function. The purpose of this study was to profile miRNA expression patterns in aging human skeletal muscle with a miRNA array followed by in-depth functional and network analysis. Muscle biopsy samples from 36 men [young: 31 ± 2 (n = 19); older: 73 ± 3 (n = 17)] were 1) analyzed for expression of miRNAs with a miRNA array, 2) validated with TaqMan quantitative real-time PCR assays, and 3) identified (and later validated) for potential gene targets with the bioinformatics knowledge base software Ingenuity Pathways Analysis. Eighteen miRNAs were differentially expressed in older humans (P < 0.05 and >500 expression level). Let-7 family members Let-7b and Let-7e were significantly elevated and further validated in older subjects (P < 0.05). Functional and network analysis from Ingenuity determined that gene targets of the Let-7s were associated with molecular networks involved in cell cycle control such as cellular proliferation and differentiation. We confirmed with real-time PCR that mRNA expression of cell cycle regulators CDK6, CDC25A, and CDC34 were downregulated in older compared with young subjects (P < 0.05). In addition, PAX7 mRNA expression was lower in older subjects (P < 0.05). These data suggest that aging is characterized by a higher expression of Let-7 family members that may downregulate genes related to cellular proliferation. We propose that higher Let-7 expression may be an indicator of impaired cell cycle function possibly contributing to reduced muscle cell renewal and regeneration in older human muscle.",j223,Physiological Genomics,jv223,accepted,f969,2006,2006-12-04
s1197,p1197,Principal component analysis based methods in bioinformatics studies,"In analysis of bioinformatics data, a unique challenge arises from the high dimensionality of measurements. Without loss of generality, we use genomic study with gene expression measurements as a representative example but note that analysis techniques discussed in this article are also applicable to other types of bioinformatics studies. Principal component analysis (PCA) is a classic dimension reduction approach. It constructs linear combinations of gene expressions, called principal components (PCs). The PCs are orthogonal to each other, can effectively explain variation of gene expressions, and may have a much lower dimensionality. PCA is computationally simple and can be realized using many existing software packages. This article consists of the following parts. First, we review the standard PCA technique and their applications in bioinformatics data analysis. Second, we describe recent 'non-standard' applications of PCA, including accommodating interactions among genes, pathways and network modules and conducting PCA with estimating equations as opposed to gene expressions. Third, we introduce several recently proposed PCA-based techniques, including the supervised PCA, sparse PCA and functional PCA. The supervised PCA and sparse PCA have been shown to have better empirical performance than the standard PCA. The functional PCA can analyze time-course gene expression data. Last, we raise the awareness of several critical but unsolved problems related to PCA. The goal of this article is to make bioinformatics researchers aware of the PCA technique and more importantly its most recent development, so that this simple yet effective dimension reduction technique can be better employed in bioinformatics data analysis.",c16,Knowledge Discovery and Data Mining,cp16,accepted,f970,2003,2003-03-17
s1198,p1198,"A Bioinformatics Approach to the Identification, Classification, and Analysis of Hydroxyproline-Rich Glycoproteins[W][OA]","Hydroxyproline-rich glycoproteins (HRGPs) are a superfamily of plant cell wall proteins that function in diverse aspects of plant growth and development. This superfamily consists of three members: hyperglycosylated arabinogalactan proteins (AGPs), moderately glycosylated extensins (EXTs), and lightly glycosylated proline-rich proteins (PRPs). Hybrid and chimeric versions of HRGP molecules also exist. In order to “mine” genomic databases for HRGPs and to facilitate and guide research in the field, the BIO OHIO software program was developed that identifies and classifies AGPs, EXTs, PRPs, hybrid HRGPs, and chimeric HRGPs from proteins predicted from DNA sequence data. This bioinformatics program is based on searching for biased amino acid compositions and for particular protein motifs associated with known HRGPs. HRGPs identified by the program are subsequently analyzed to elucidate the following: (1) repeating amino acid sequences, (2) signal peptide and glycosylphosphatidylinositol lipid anchor addition sequences, (3) similar HRGPs via Basic Local Alignment Search Tool, (4) expression patterns of their genes, (5) other HRGPs, glycosyl transferase, prolyl 4-hydroxylase, and peroxidase genes coexpressed with their genes, and (6) gene structure and whether genetic mutants exist in their genes. The program was used to identify and classify 166 HRGPs from Arabidopsis (Arabidopsis thaliana) as follows: 85 AGPs (including classical AGPs, lysine-rich AGPs, arabinogalactan peptides, fasciclin-like AGPs, plastocyanin AGPs, and other chimeric AGPs), 59 EXTs (including SP5 EXTs, SP5/SP4 EXTs, SP4 EXTs, SP4/SP3 EXTs, a SP3 EXT, “short” EXTs, leucine-rich repeat-EXTs, proline-rich extensin-like receptor kinases, and other chimeric EXTs), 18 PRPs (including PRPs and chimeric PRPs), and AGP/EXT hybrid HRGPs.",j224,Plant Physiology,jv224,accepted,f971,2020,2020-08-06
s1199,p1199,"Pseudo Amino Acid Composition and its Applications in Bioinformatics, Proteomics and System Biology","With the avalanche of protein sequences generated in the post-genomic age, it is highly desired to develop automated methods for efficiently identifying various attributes of uncharacterized proteins. This is one of the most im- portant tasks facing us today in bioinformatics, and the information thus obtained will have important impacts on the de- velopment of proteomics and system biology. To realize that, one of the keys is to find an effective model to represent the sample of a protein. The most straightforward model in this regard is its entire amino acid sequence; however, the entire sequence model would fail to work when the query protein did not have significant homology to proteins of known char- acteristics. Thus, various non-sequential models or discrete models were proposed. The simplest discrete model is the amino acid (AA) composition. Using it to represent a protein, however, all the sequence-order information would be com- pletely lost. To cope with such a dilemma, the concept of pseudo amino acid (PseAA) composition was introduced. Its es- sence is to keep using a discrete model to represent a protein yet without completely losing its sequence-order informa- tion. Therefore, in a broad sense, the PseAA composition of a protein is actually a set of discrete numbers that is de- rived from its amino acid sequence and that is different from the classical AA composition and able to harbour some sort of sequence order or pattern information. Ever since the first PseAA composition was formulated to predict protein sub- cellular localization and membrane protein types, it has stimulated many different modes of PseAA composition for studying various kinds of problems in proteins and proteins-related systems. In this review, we shall give a brief and sys- tematic introduction of various modes of PseAA composition and their applications. Meanwhile, the challenges for find- ing the optimal PseAA composition are also briefly discussed.",c102,International Conference on Biometrics,cp102,accepted,f972,2022,2022-08-29
s1200,p1200,"A Bioinformatics Approach to the Identification, Classification, and Analysis of Hydroxyproline-Rich Glycoproteins[W][OA]","Hydroxyproline-rich glycoproteins (HRGPs) are a superfamily of plant cell wall proteins that function in diverse aspects of plant growth and development. This superfamily consists of three members: hyperglycosylated arabinogalactan proteins (AGPs), moderately glycosylated extensins (EXTs), and lightly glycosylated proline-rich proteins (PRPs). Hybrid and chimeric versions of HRGP molecules also exist. In order to “mine” genomic databases for HRGPs and to facilitate and guide research in the field, the BIO OHIO software program was developed that identifies and classifies AGPs, EXTs, PRPs, hybrid HRGPs, and chimeric HRGPs from proteins predicted from DNA sequence data. This bioinformatics program is based on searching for biased amino acid compositions and for particular protein motifs associated with known HRGPs. HRGPs identified by the program are subsequently analyzed to elucidate the following: (1) repeating amino acid sequences, (2) signal peptide and glycosylphosphatidylinositol lipid anchor addition sequences, (3) similar HRGPs via Basic Local Alignment Search Tool, (4) expression patterns of their genes, (5) other HRGPs, glycosyl transferase, prolyl 4-hydroxylase, and peroxidase genes coexpressed with their genes, and (6) gene structure and whether genetic mutants exist in their genes. The program was used to identify and classify 166 HRGPs from Arabidopsis (Arabidopsis thaliana) as follows: 85 AGPs (including classical AGPs, lysine-rich AGPs, arabinogalactan peptides, fasciclin-like AGPs, plastocyanin AGPs, and other chimeric AGPs), 59 EXTs (including SP5 EXTs, SP5/SP4 EXTs, SP4 EXTs, SP4/SP3 EXTs, a SP3 EXT, “short” EXTs, leucine-rich repeat-EXTs, proline-rich extensin-like receptor kinases, and other chimeric EXTs), 18 PRPs (including PRPs and chimeric PRPs), and AGP/EXT hybrid HRGPs.",j224,Plant Physiology,jv224,accepted,f973,2020,2020-07-21
s1201,p1201,"Pseudo Amino Acid Composition and its Applications in Bioinformatics, Proteomics and System Biology","With the avalanche of protein sequences generated in the post-genomic age, it is highly desired to develop automated methods for efficiently identifying various attributes of uncharacterized proteins. This is one of the most im- portant tasks facing us today in bioinformatics, and the information thus obtained will have important impacts on the de- velopment of proteomics and system biology. To realize that, one of the keys is to find an effective model to represent the sample of a protein. The most straightforward model in this regard is its entire amino acid sequence; however, the entire sequence model would fail to work when the query protein did not have significant homology to proteins of known char- acteristics. Thus, various non-sequential models or discrete models were proposed. The simplest discrete model is the amino acid (AA) composition. Using it to represent a protein, however, all the sequence-order information would be com- pletely lost. To cope with such a dilemma, the concept of pseudo amino acid (PseAA) composition was introduced. Its es- sence is to keep using a discrete model to represent a protein yet without completely losing its sequence-order informa- tion. Therefore, in a broad sense, the PseAA composition of a protein is actually a set of discrete numbers that is de- rived from its amino acid sequence and that is different from the classical AA composition and able to harbour some sort of sequence order or pattern information. Ever since the first PseAA composition was formulated to predict protein sub- cellular localization and membrane protein types, it has stimulated many different modes of PseAA composition for studying various kinds of problems in proteins and proteins-related systems. In this review, we shall give a brief and sys- tematic introduction of various modes of PseAA composition and their applications. Meanwhile, the challenges for find- ing the optimal PseAA composition are also briefly discussed.",c88,Symposium on the Theory of Computing,cp88,accepted,f974,2014,2014-10-15
s1202,p1202,Current Protocols in Bioinformatics,"1. Please read the rough pages and mark any changes right in the text. 2. If you have large inserts to add, please supply us with a disk and hard copy of the insert(s) and indicate where they should go.",c75,International Conference on Machine Learning,cp75,accepted,f975,2005,2005-03-05
s1203,p1203,Clinical Bioinformatics: challenges and opportunities,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f976,2007,2007-12-12
s1204,p1204,"Proteins : Structure , Function , and Bioinformatics","After printing the PDF file, please read the page proofs carefully and: 1) indicate changes or corrections in the margin of the page proofs; 2) answer all queries (footnotes A,B,C, etc.) on the last page of the PDF proof; 3) proofread any tables and equations carefully; 4) check that any Greek, especially ""mu"", has translated correctly. Within 48 hours, please return the following to the address given below: 1) original PDF set of page proofs, 2) Reprint Order form, 3) Return fax form Return to: Your article will be published online via our EarlyView service within a few days of correction receipt. Your prompt attention to and return of page proofs is crucial to faster publication of your work. If you experience technical problems, please contact Doug Frank (",c21,Grid Computing Environments,cp21,accepted,f977,2005,2005-09-20
s1205,p1205,"GProX, a User-Friendly Platform for Bioinformatics Analysis and Visualization of Quantitative Proteomics Data*","Recent technological advances have made it possible to identify and quantify thousands of proteins in a single proteomics experiment. As a result of these developments, the analysis of data has become the bottleneck of proteomics experiment. To provide the proteomics community with a user-friendly platform for comprehensive analysis, inspection and visualization of quantitative proteomics data we developed the Graphical Proteomics Data Explorer (GProX)1. The program requires no special bioinformatics training, as all functions of GProX are accessible within its graphical user-friendly interface which will be intuitive to most users. Basic features facilitate the uncomplicated management and organization of large data sets and complex experimental setups as well as the inspection and graphical plotting of quantitative data. These are complemented by readily available high-level analysis options such as database querying, clustering based on abundance ratios, feature enrichment tests for e.g. GO terms and pathway analysis tools. A number of plotting options for visualization of quantitative proteomics data is available and most analysis functions in GProX create customizable high quality graphical displays in both vector and bitmap formats. The generic import requirements allow data originating from essentially all mass spectrometry platforms, quantitation strategies and software to be analyzed in the program. GProX represents a powerful approach to proteomics data analysis providing proteomics experimenters with a toolbox for bioinformatics analysis of quantitative proteomics data. The program is released as open-source and can be freely downloaded from the project webpage at http://gprox.sourceforge.net.",j218,Molecular & Cellular Proteomics,jv218,accepted,f978,2003,2003-09-15
s1206,p1206,International Journal of Knowledge Discovery in Bioinformatics,Abstract content goes here ...,c81,IEEE Annual Symposium on Foundations of Computer Science,cp81,accepted,f979,2004,2004-11-16
s1207,p1207,Bioinformatics and systems biology of the lipidome.,"Lipids play an important role in physiology and pathophysiology of living systems. Until a few decades ago, the number of lipid molecules that were chemically characterized was a few hundred at most and were catalogued in monographs and compendia.1 Since the advent of the era of the genome and the proteome, there has been increasing recognition that other macromolecules like lipids and polysaccharides in living systems display considerable structural diversity and systematic efforts are underway to identify, characterize and catalog these molecules. With mass spectrometric techniques coming of age, several thousand distinct molecular species have been identified from living species and the roles of several of these are beginning to be characterized.2 Unlike genes and proteins, whose defined alphabets provide the framework for ontologies and classification at the sequence level, lipids and polysaccharides have been characterized for the large part by popular names, with no foundations for systematic classification. 
 
The past two decades have witnessed two major advances in lipid biology. In the first, mass spectrometry has enabled the identification of thousands of lipid molecular species from cells and tissues and this has pointed to the important need for developing a systematic ontology that can rationally name and catalog the molecules. Second, the ability to investigate the functional roles of lipid molecules through systematic phenotypic studies has led to the identification of lipids as extremely important players in physiology and pathophysiology of living species.3 In combination with proteins and nucleic acids, lipids are integrally involved in biochemical networks that lead to phenotypes such as homeostasis, differentiation, and death of cells and tissues. Any approach to systems characterization of living systems, of necessity, has to include lipids along with other macromolecules and all complex cellular pathways involving lipid molecular species. Systems biology now extends in its scope to identify biosynthetic and metabolic lipid networks, cellular signaling networks that explicitly include lipid molecules and transcriptional and epigenetic networks where lipids play an integral role.4 
 
Several large scale projects to characterize lipids and their functional roles have been initiated as exemplified by the LIPID MAPS5 effort. The LIPID MAPS is an exemplar systems biology project that measures cell-wide lipid changes in an attempt to reconstruct biochemical pathways associated with lipid processing and signaling. The cell-wide measurements of components of these pathways include mass spectrometric measurements of lipid changes in response to stimulus in mammalian cells, changes in transcription profiles in response to stimulus and in select cases proteomic changes in response to stimulus. Figure 1 shows a schematic of the LIPID MAPS experiments related to different lipid categories/pathways and the subsequent processing of the experimental data generated. Network reconstruction efforts rely on organization, analysis and integration of these data and this requires a strong bioinformatics and systems biology effort. The former has to include development of a systematic and universal classification and nomenclature system, design and development of lipid and lipid-gene, lipid-protein databases with appropriate functional annotations, and efficient query and analysis systems that can be broadly useful to the biology research community. The latter has to include methods for analysis of large scale lipid measurements in cells, reconstruction of lipid metabolic and biosynthetic pathways, and quantitative models of lipid fluxes in cells under varied perturbations. In this review, we will provide a comprehensive summary of extant developments in lipid bioinformatics and systems biology and discuss the outlook for the future integration of lipidomics into cellular and organismic biology. The sections that follow are delineated into the informatics approaches specific to lipid biology followed by an overview and exemplar approach to analysis of large scale lipidomic data towards a systems description of mammalian cells. 
 
 
 
Figure 1 
 
Overview of the process of performing a quantitative lipid analysis of macrophage cell sample (in this example, a time-course experiment using bone marrow derived macrophages). Extraction methods, LC/GC purification methods, MS acquisition strategies ... 
 
 
 
 
2. Classification, Ontology, Nomenclature and Structure Representation of Lipid Molecules 
The first step towards classification of lipids is the establishment of an ontology that is extensible, flexible and scalable. One must be able to classify, name and represent these molecules in a logical manner which is amenable to data basing and computational manipulation. Lipids have been loosely defined as biological substances that are generally hydrophobic in nature and in many cases soluble in organic solvents.6 These chemical features are present in a broad range of molecules such as fatty acids, phospholipids, sterols, sphingolipids, terpenes and others. In view of the fact that lipids comprise an extremely heterogeneous collection of molecules from a structural and functional standpoint, it is not surprising that there are significant differences with regard to the scope and organization of current classification schemes. 
 
2.1. Classification, Ontology and Nomenclature 
In order to address the lack of a consistent classification and nomenclature methodology for lipids, LIPID MAPS consortium members have developed a comprehensive classification system for lipids.7 The consortium has taken a more chemistry-based approach and defines lipids as hydrophobic or amphipathic small molecules that may originate entirely or in part by carbanion based condensations of thioesters (such as fatty acids and polyketides) and/or by carbocation based condensations of isoprene units (such as prenols and sterols). Figure 2 shows the mechanisms of lipid biosynthesis.8 Based on this classification system, lipids have been divided into eight categories: Fatty acyls, Glycerolipids, Glycerophospholipids, Sphingolipids, Sterol lipids, Prenol lipids, Saccharolipids, and Polyketides. Each category is further divided into classes and subclasses. Additionally, following the existing rules and recommendations proposed by the International Union of Biochemistry and Applied Chemists and the International Union of Biochemistry and Molecular Biology (IUPAC-IUBMB) commission on Biochemical Nomenclature, a consistent nomenclature scheme has also been developed to provide systematic names for various classes and subclasses of lipids.7 
 
 
 
Figure 2 
 
Mechanisms of lipid biosynthesis. Biosynthesis of ketoacyl- and isoprene-containing lipids proceeds by carbanion and carbocation-mediated chain extension, respectively.8 
 
 
 
All lipids in the LIPID MAPS Structure Database (LMSD) are classified and annotated using this comprehensive classification and nomenclature system developed by the LIPID MAPS consortium.",j10,Chemical Reviews,jv10,accepted,f980,2021,2021-02-12
s1208,p1208,Multiobjective Genetic Algorithms for Clustering - Applications in Data Mining and Bioinformatics,Abstract content goes here ...,c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,cp20,accepted,f981,2014,2014-01-06
s1209,p1209,Decision tree and ensemble learning algorithms with their applications in bioinformatics.,Abstract content goes here ...,j225,Advances in Experimental Medicine and Biology,jv225,accepted,f982,2014,2014-07-01
s1210,p1210,BioStar: An Online Question & Answer Resource for the Bioinformatics Community,"Although the era of big data has produced many bioinformatics tools and databases, using them effectively often requires specialized knowledge. Many groups lack bioinformatics expertise, and frequently find that software documentation is inadequate while local colleagues may be overburdened or unfamiliar with specific applications. Too often, such problems create data analysis bottlenecks that hinder the progress of biological research. In order to help address this deficiency, we present BioStar, a forum based on the Stack Exchange platform where experts and those seeking solutions to problems of computational biology exchange ideas. The main strengths of BioStar are its large and active group of knowledgeable users, rapid response times, clear organization of questions and responses that limit discussion to the topic at hand, and ranking of questions and answers that help identify their usefulness. These rankings, based on community votes, also contribute to a reputation score for each user, which serves to keep expert contributors engaged. The BioStar community has helped to answer over 2,300 questions from over 1,400 users (as of June 10, 2011), and has played a critical role in enabling and expediting many research projects. BioStar can be accessed at http://www.biostars.org/.",c72,Intelligent Systems in Molecular Biology,cp72,accepted,f983,2003,2003-06-13
s1211,p1211,Clinical bioinformatics: a new emerging science,Abstract content goes here ...,j226,Journal of Clinical Bioinformatics,jv226,accepted,f984,2002,2002-02-10
s1213,p1213,dbOGAP - An Integrated Bioinformatics Resource for Protein O-GlcNAcylation,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f985,2007,2007-06-15
s1214,p1214,Bioinformatics Tools and Novel Challenges in Long Non-Coding RNAs (lncRNAs) Functional Analysis,"The advent of next generation sequencing revealed that a fraction of transcribed RNAs (short and long RNAs) is non-coding. Long non-coding RNAs (lncRNAs) have a crucial role in regulating gene expression and in epigenetics (chromatin and histones remodeling). LncRNAs may have different roles: gene activators (signaling), repressors (decoy), cis and trans gene expression regulators (guides) and chromatin modificators (scaffolds) without the need to be mutually exclusive. LncRNAs are also implicated in a number of diseases. The huge amount of inhomogeneous data produced so far poses several bioinformatics challenges spanning from the simple annotation to the more complex functional annotation. In this review, we report and discuss several bioinformatics resources freely available and dealing with the study of lncRNAs. To our knowledge, this is the first review summarizing all the available bioinformatics resources on lncRNAs appeared in the literature after the completion of the human genome project. Therefore, the aim of this review is to provide a little guide for biologists and bioinformaticians looking for dedicated resources, public repositories and other tools for lncRNAs functional analysis.",j178,International Journal of Molecular Sciences,jv178,accepted,f986,2017,2017-05-27
s1216,p1216,Bioinformatics challenges of new sequencing technology.,Abstract content goes here ...,j227,Trends in Genetics,jv227,accepted,f987,2018,2018-04-26
s1217,p1217,"Omics technologies, data and bioinformatics principles.",Abstract content goes here ...,c16,Knowledge Discovery and Data Mining,cp16,accepted,f988,2003,2003-06-15
s1218,p1218,Microarray bioinformatics.,Abstract content goes here ...,c77,Networks,cp77,accepted,f989,2019,2019-01-15
s1220,p1220,Using bioinformatics to predict the functional impact of SNVs,"MOTIVATION
The past decade has seen the introduction of fast and relatively inexpensive methods to detect genetic variation across the genome and exponential growth in the number of known single nucleotide variants (SNVs). There is increasing interest in bioinformatics approaches to identify variants that are functionally important from millions of candidate variants. Here, we describe the essential components of bioinformatics tools that predict functional SNVs.


RESULTS
Bioinformatics tools have great potential to identify functional SNVs, but the black box nature of many tools can be a pitfall for researchers. Understanding the underlying methods, assumptions and biases of these tools is essential to their intelligent application.",c49,International Symposium on Search Based Software Engineering,cp49,accepted,f990,2012,2012-08-22
s1221,p1221,Tools and collaborative environments for bioinformatics research,"Advanced research requires intensive interaction among a multitude of actors, often possessing different expertise and usually working at a distance from each other. The field of collaborative research aims to establish suitable models and technologies to properly support these interactions. In this article, we first present the reasons for an interest of Bioinformatics in this context by also suggesting some research domains that could benefit from collaborative research. We then review the principles and some of the most relevant applications of social networking, with a special attention to networks supporting scientific collaboration, by also highlighting some critical issues, such as identification of users and standardization of formats. We then introduce some systems for collaborative document creation, including wiki systems and tools for ontology development, and review some of the most interesting biological wikis. We also review the principles of Collaborative Development Environments for software and show some examples in Bioinformatics. Finally, we present the principles and some examples of Learning Management Systems. In conclusion, we try to devise some of the goals to be achieved in the short term for the exploitation of these technologies.",c92,Advances in Soft Computing,cp92,accepted,f991,2009,2009-03-19
s1222,p1222,Bioinformatics Resources and Tools for Phage Display,"Databases and computational tools for mimotopes have been an important part of phage display study. Five special databases and eighteen algorithms, programs and web servers and their applications are reviewed in this paper. Although these bioinformatics resources have been widely used to exclude target-unrelated peptides, characterize small molecules-protein interactions and map protein-protein interactions, a lot of problems are still waiting to be solved. With the improvement of these tools, they are expected to serve the phage display community better.",j229,Molecules,jv229,accepted,f992,2002,2002-08-08
s1223,p1223,Translational bioinformatics: linking knowledge across biological and clinical realms,"Nearly a decade since the completion of the first draft of the human genome, the biomedical community is positioned to usher in a new era of scientific inquiry that links fundamental biological insights with clinical knowledge. Accordingly, holistic approaches are needed to develop and assess hypotheses that incorporate genotypic, phenotypic, and environmental knowledge. This perspective presents translational bioinformatics as a discipline that builds on the successes of bioinformatics and health informatics for the study of complex diseases. The early successes of translational bioinformatics are indicative of the potential to achieve the promise of the Human Genome Project for gaining deeper insights to the genetic underpinnings of disease and progress toward the development of a new generation of therapies.",c106,Chinese Conference on Biometric Recognition,cp106,accepted,f993,2016,2016-04-27
s1226,p1226,Mobyle: a new full web bioinformatics framework,"Motivation: For the biologist, running bioinformatics analyses involves a time-consuming management of data and tools. Users need support to organize their work, retrieve parameters and reproduce their analyses. They also need to be able to combine their analytic tools using a safe data flow software mechanism. Finally, given that scientific tools can be difficult to install, it is particularly helpful for biologists to be able to use these tools through a web user interface. However, providing a web interface for a set of tools raises the problem that a single web portal cannot offer all the existing and possible services: it is the user, again, who has to cope with data copy among a number of different services. A framework enabling portal administrators to build a network of cooperating services would therefore clearly be beneficial. Results: We have designed a system, Mobyle, to provide a flexible and usable Web environment for defining and running bioinformatics analyses. It embeds simple yet powerful data management features that allow the user to reproduce analyses and to combine tools using a hierarchical typing system. Mobyle offers invocation of services distributed over remote Mobyle servers, thus enabling a federated network of curated bioinformatics portals without the user having to learn complex concepts or to install sophisticated software. While being focused on the end user, the Mobyle system also addresses the need, for the bioinfomatician, to automate remote services execution: PlayMOBY is a companion tool that automates the publication of BioMOBY web services, using Mobyle program definitions. Availability: The Mobyle system is distributed under the terms of the GNU GPLv2 on the project web site (http://bioweb2.pasteur.fr/projects/mobyle/). It is already deployed on three servers: http://mobyle.pasteur.fr, http://mobyle.rpbs.univ-paris-diderot.fr and http://lipm-bioinfo.toulouse.inra.fr/Mobyle. The PlayMOBY companion is distributed under the terms of the CeCILL license, and is available at http://lipm-bioinfo.toulouse.inra.fr/biomoby/PlayMOBY/. Contact: mobyle-support@pasteur.fr; mobyle-support@rpbs.univ-paris-diderot.fr; letondal@pasteur.fr Supplementary information:Supplementary data are available at Bioinformatics online.",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f994,2017,2017-09-14
s1227,p1227,The evolution of bioinformatics in toxicology: advancing toxicogenomics.,"As one reflects back through the past 50 years of scientific research, a significant accomplishment was the advance into the genomic era. Basic research scientists have uncovered the genetic code and the foundation of the most fundamental building blocks for the molecular activity that supports biological structure and function. Accompanying these structural and functional discoveries is the advance of techniques and technologies to probe molecular events, in time, across environmental and chemical exposures, within individuals, and across species. The field of toxicology has kept pace with advances in molecular study, and the past 50 years recognizes significant growth and explosive understanding of the impact of the compounds and environment to basic cellular and molecular machinery. The advancement of molecular techniques applied in a whole-genomic capacity to the study of toxicant effects, toxicogenomics, is no doubt a significant milestone for toxicological research. Toxicogenomics has also provided an avenue for advancing a joining of multidisciplinary sciences including engineering and informatics in traditional toxicological research. This review will cover the evolution of the field of toxicogenomics in the context of informatics integration its current promise, and limitations.",j231,Toxicological Sciences,jv231,accepted,f995,2019,2019-01-15
s1228,p1228,Comparative bioinformatics analysis of the mammalian and bacterial glycomes,"A comparative analysis of bacterial and mammalian glycomes based on the statistical analysis of two major carbohydrate databases, Bacterial CarbohydrateStructure Data Base (BCSDB) and GLYCOSCIENCES.de (GS), is presented. An in-depth comparison of these two glycomes reveals both striking differences and unexpected similarities. Within the prokaryotic kingdom, we focus on the glycomes of seven classes of pathogenic bacteria with respect to (i) their most abundant monosaccharide units; (ii) disaccharide pairs; (iii) carbohydrate modifications; (iv) occurrence and use of sialic acids; and (v) class-specific monosaccharides. The aim of this work is to gain insights into unique carbohydrate patterns in bacteria. Data interpretation reveals significant trends in the composition of specific carbohydrate classes as result of evolution-driven structural adaptations of bacterial pathogens and symbionts to their mammalian hosts. The differences are discussed in light of their value for biomedical applications, such as the targeting of unique glycosyl transferases, vaccine development, and devising novel diagnostic tools.",c63,IEEE International Software Metrics Symposium,cp63,accepted,f996,2008,2008-11-19
s1229,p1229,Visual gene developer: a fully programmable bioinformatics software for synthetic gene optimization,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f997,2007,2007-02-08
s1231,p1231,Structural bioinformatics and its impact to biomedical science.,"During the last two decades, the number of sequence-known proteins has increased rapidly. In contrast, the corresponding increment for structure-known proteins is much slower. The unbalanced situation has critically limited our ability to understand the molecular mechanism of proteins and conduct structure-based drug design by timely using the updated information of newly found sequences. Therefore, it is highly desired to develop an automated method for fast deriving the 3D (3-dimensional) structure of a protein from its sequence. Under such a circumstance, the structural bioinformatics was emerging naturally as the time required. In this review, three main strategies developed in structural bioinformatics, i.e., pure energetic approach, heuristic approach, and homology modeling approach, as well as their underlying principles, are briefly introduced. Meanwhile, a series of demonstrations are presented to show how the structural bioinformatics has been applied to timely derive the 3D structures of some functionally important proteins, helping to understand their action mechanisms and stimulating the course of drug discovery. Also, the limitation of these approaches and the future challenges of structural bioinformatics are briefly addressed.",j232,Current Medicinal Chemistry,jv232,accepted,f998,2015,2015-11-09
s1232,p1232,Bioinformatics analysis of disordered proteins in prokaryotes,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f999,2007,2007-09-10
s1233,p1233,Bioinformatics of the TULIP domain superfamily.,"Proteins of the BPI (bactericidal/permeability-increasing protein)-like family contain either one or two tandem copies of a fold that usually provides a tubular cavity for the binding of lipids. Bioinformatic analyses show that, in addition to its known members, which include BPI, LBP [LPS (lipopolysaccharide)-binding protein)], CETP (cholesteryl ester-transfer protein), PLTP (phospholipid-transfer protein) and PLUNC (palate, lung and nasal epithelium clone) protein, this family also includes other, more divergent groups containing hypothetical proteins from fungi, nematodes and deep-branching unicellular eukaryotes. More distantly, BPI-like proteins are related to a family of arthropod proteins that includes hormone-binding proteins (Takeout-like; previously described to adopt a BPI-like fold), allergens and several groups of uncharacterized proteins. At even greater evolutionary distance, BPI-like proteins are homologous with the SMP (synaptotagmin-like, mitochondrial and lipid-binding protein) domains, which are found in proteins associated with eukaryotic membrane processes. In particular, SMP domain-containing proteins of yeast form the ERMES [ER (endoplasmic reticulum)-mitochondria encounter structure], required for efficient phospholipid exchange between these organelles. This suggests that SMP domains themselves bind lipids and mediate their exchange between heterologous membranes. The most distant group of homologues we detected consists of uncharacterized animal proteins annotated as TM (transmembrane) 24. We propose to group these families together into one superfamily that we term as the TULIP (tubular lipid-binding) domain superfamily.",j233,Biochemical Society Transactions,jv233,accepted,f1000,2015,2015-12-24
s1234,p1234,Systems Biology: The Next Frontier for Bioinformatics,"Biochemical systems biology augments more traditional disciplines, such as genomics, biochemistry and molecular biology, by championing (i) mathematical and computational modeling; (ii) the application of traditional engineering practices in the analysis of biochemical systems; and in the past decade increasingly (iii) the use of near-comprehensive data sets derived from ‘omics platform technologies, in particular “downstream” technologies relative to genome sequencing, including transcriptomics, proteomics and metabolomics. The future progress in understanding biological principles will increasingly depend on the development of temporal and spatial analytical techniques that will provide high-resolution data for systems analyses. To date, particularly successful were strategies involving (a) quantitative measurements of cellular components at the mRNA, protein and metabolite levels, as well as in vivo metabolic reaction rates, (b) development of mathematical models that integrate biochemical knowledge with the information generated by high-throughput experiments, and (c) applications to microbial organisms. The inevitable role bioinformatics plays in modern systems biology puts mathematical and computational sciences as an equal partner to analytical and experimental biology. Furthermore, mathematical and computational models are expected to become increasingly prevalent representations of our knowledge about specific biochemical systems.",c41,Software Product Lines Conference,cp41,accepted,f1001,2002,2002-11-08
s1235,p1235,BioRuby: bioinformatics software for the Ruby programming language,"Summary: The BioRuby software toolkit contains a comprehensive set of free development tools and libraries for bioinformatics and molecular biology, written in the Ruby programming language. BioRuby has components for sequence analysis, pathway analysis, protein modelling and phylogenetic analysis; it supports many widely used data formats and provides easy access to databases, external programs and public web services, including BLAST, KEGG, GenBank, MEDLINE and GO. BioRuby comes with a tutorial, documentation and an interactive environment, which can be used in the shell, and in the web browser. Availability: BioRuby is free and open source software, made available under the Ruby license. BioRuby runs on all platforms that support Ruby, including Linux, Mac OS X and Windows. And, with JRuby, BioRuby runs on the Java Virtual Machine. The source code is available from http://www.bioruby.org/. Contact: katayama@bioruby.org",c39,International Conference on Global Software Engineering,cp39,accepted,f1002,2020,2020-08-19
s1236,p1236,Genomics and Bioinformatics Resources for Crop Improvement,"Recent remarkable innovations in platforms for omics-based research and application development provide crucial resources to promote research in model and applied plant species. A combinatorial approach using multiple omics platforms and integration of their outcomes is now an effective strategy for clarifying molecular systems integral to improving plant productivity. Furthermore, promotion of comparative genomics among model and applied plants allows us to grasp the biological properties of each species and to accelerate gene discovery and functional analyses of genes. Bioinformatics platforms and their associated databases are also essential for the effective design of approaches making the best use of genomic resources, including resource integration. We review recent advances in research platforms and resources in plant omics together with related databases and advances in technology.",j221,Plant and Cell Physiology,jv221,accepted,f1003,2014,2014-05-11
s1237,p1237,CloudBLAST: Combining MapReduce and Virtualization on Distributed Resources for Bioinformatics Applications,"This paper proposes and evaluates an approach to the parallelization, deployment and management of bioinformatics applications that integrates several emerging technologies for distributed computing. The proposed approach uses the MapReduce paradigm to parallelize tools and manage their execution, machine virtualization to encapsulate their execution environments and commonly used data sets into flexibly deployable virtual machines, and network virtualization to connect resources behind firewalls/NATs while preserving the necessary performance and the communication environment. An implementation of this approach is described and used to demonstrate and evaluate the proposed approach. The implementation integrates Hadoop, Virtual Workspaces, and ViNe as the MapReduce, virtual machine and virtual network technologies, respectively, to deploy the commonly used bioinformatics tool NCBI BLAST on a WAN-based test bed consisting of clusters at two distinct locations, the University of Florida and the University of Chicago. This WAN-based implementation, called CloudBLAST, was evaluated against both non-virtualized and LAN-based implementations in order to assess the overheads of machine and network virtualization, which were shown to be insignificant. To compare the proposed approach against an MPI-based solution, CloudBLAST performance was experimentally contrasted against the publicly available mpiBLAST on the same WAN-based test bed. Both versions demonstrated performance gains as the number of available processors increased, with CloudBLAST delivering speedups of 57 against 52.4 of MPI version, when 64 processors on 2 sites were used. The results encourage the use of the proposed approach for the execution of large-scale bioinformatics applications on emerging distributed environments that provide access to computing resources as a service.",c11,Hawaii International Conference on System Sciences,cp11,accepted,f1004,2006,2006-09-15
s1239,p1239,BioJava: an open-source framework for bioinformatics,"Summary: BioJava is a mature open-source project that provides a framework for processing of biological data. BioJava contains powerful analysis and statistical routines, tools for parsing common file formats and packages for manipulating sequences and 3D structures. It enables rapid bioinformatics application development in the Java programming language. Availability: BioJava is an open-source project distributed under the Lesser GPL (LGPL). BioJava can be downloaded from the BioJava website (http://www.biojava.org). BioJava requires Java 1.5 or higher. Contact: andreas.prlic@gmail.com. All queries should be directed to the BioJava mailing lists. Details are available at http://biojava.org/wiki/BioJava:MailingLists.",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f1005,2018,2018-10-10
s1240,p1240,Bioinformatics approaches for genomics and post genomics applications of next-generation sequencing,"Technical advances such as the development of molecular cloning, Sanger sequencing, PCR and oligonucleotide microarrays are key to our current capacity to sequence, annotate and study complete organismal genomes. Recent years have seen the development of a variety of so-called 'next-generation' sequencing platforms, with several others anticipated to become available shortly. The previously unimaginable scale and economy of these methods, coupled with their enthusiastic uptake by the scientific community and the potential for further improvements in accuracy and read length, suggest that these technologies are destined to make a huge and ongoing impact upon genomic and post-genomic biology. However, like the analysis of microarray data and the assembly and annotation of complete genome sequences from conventional sequencing data, the management and analysis of next-generation sequencing data requires (and indeed has already driven) the development of informatics tools able to assemble, map, and interpret huge quantities of relatively or extremely short nucleotide sequence data. Here we provide a broad overview of bioinformatics approaches that have been introduced for several genomics and functional genomics applications of next-generation sequencing.",c77,Networks,cp77,accepted,f1006,2019,2019-03-18
s1241,p1241,Combinatorial pattern discovery in biological sequences: The TEIRESIAS algorithm [published erratum appears in Bioinformatics 1998;14(2): 229],"MOTIVATION
The discovery of motifs in biological sequences is an important problem.


RESULTS
This paper presents a new algorithm for the discovery of rigid patterns (motifs) in biological sequences. Our method is combinatorial in nature and able to produce all patterns that appear in at least a (user-defined) minimum number of sequences, yet it manages to be very efficient by avoiding the enumeration of the entire pattern space. Furthermore, the reported patterns are maximal: any reported pattern cannot be made more specific and still keep on appearing at the exact same positions within the input sequences. The effectiveness of the proposed approach is showcased on a number of test cases which aim to: (i) validate the approach through the discovery of previously reported patterns; (ii) demonstrate the capability to identify automatically highly selective patterns particular to the sequences under consideration. Finally, experimental analysis indicates that the algorithm is output sensitive, i.e. its running time is quasi-linear to the size of the generated output.",c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f1007,2018,2018-07-18
s1242,p1242,PhosphoSite: A bioinformatics resource dedicated to physiological protein phosphorylation,"PhosphoSite™ is a curated, web‐based bioinformatics resource dedicated to physiologic sites of protein phosphorylation in human and mouse. PhosphoSite is populated with information derived from published literature as well as high‐throughput discovery programs. PhosphoSite provides information about the phosphorylated residue and its surrounding sequence, orthologous sites in other species, location of the site within known domains and motifs, and relevant literature references. Links are also provided to a number of external resources for protein sequences, structure, post‐translational modifications and signaling pathways, as well as sources of phospho‐specific antibodies and probes. As the amount of information in the underlying knowledgebase expands, users will be able to systematically search for the kinases, phosphatases, ligands, treatments, and receptors that have been shown to regulate the phosphorylation status of the sites, and pathways in which the phosphorylation sites function. As it develops into a comprehensive resource of known in vivo phosphorylation sites, we expect that PhosphoSite will be a valuable tool for researchers seeking to understand the role of intracellular signaling pathways in a wide variety of biological processes.",j235,Proteomics,jv235,accepted,f1008,2006,2006-10-15
s1243,p1243,Bioinformatics: the machine learning approach,"In this book Pierre Baldi and Soren Brunak present the key machine learning approaches and apply them to the computational problems encountered in the analysis of biological data. The book is aimed both at biologists and biochemists who need to understand new data-driven algorithms and at those with a primary background in physics, mathematics, statistics, or computer science who need to know more about applications in molecular biology.",c80,International Conference on Learning Representations,cp80,accepted,f1009,2005,2005-03-10
s1244,p1244,Bioinformatics - The Machine Learning Approach,Abstract content goes here ...,c29,International Conference on Software Engineering,cp29,accepted,f1010,2015,2015-07-12
s1246,p1246,State of the nation in data integration for bioinformatics,Abstract content goes here ...,j189,Journal of Biomedical Informatics,jv189,accepted,f1011,2012,2012-06-10
s1247,p1247,BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs,"MOTIVATION
Genomics has revolutionized biological research, but quality assessment of the resulting assembled sequences is complicated and remains mostly limited to technical measures like N50.


RESULTS
We propose a measure for quantitative assessment of genome assembly and annotation completeness based on evolutionarily informed expectations of gene content. We implemented the assessment procedure in open-source software, with sets of Benchmarking Universal Single-Copy Orthologs, named BUSCO.


AVAILABILITY AND IMPLEMENTATION
Software implemented in Python and datasets available for download from http://busco.ezlab.org.


CONTACT
evgeny.zdobnov@unige.ch


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",c114,IEEE International Conference on Robotics and Automation,cp114,accepted,f1012,2011,2011-07-05
s1248,p1248,WIWS: a protein structure bioinformatics Web service collection,"The WHAT IF molecular-modelling and drug design program is widely distributed in the world of protein structure bioinformatics. Although originally designed as an interactive application, its highly modular design and inbuilt control language have recently enabled its deployment as a collection of programmatically accessible web services. We report here a collection of WHAT IF-based protein structure bioinformatics web services: these relate to structure quality, the use of symmetry in crystal structures, structure correction and optimization, adding hydrogens and optimizing hydrogen bonds and a series of geometric calculations. The freely accessible web services are based on the industry standard WS-I profile and the EMBRACE technical guidelines, and are available via both REST and SOAP paradigms. The web services run on a dedicated computational cluster; their function and availability is monitored daily.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1013,2007,2007-03-14
s1249,p1249,Trimmomatic: a flexible trimmer for Illumina sequence data,"Motivation: Although many next-generation sequencing (NGS) read preprocessing tools already existed, we could not find any tool or combination of tools that met our requirements in terms of flexibility, correct handling of paired-end data and high performance. We have developed Trimmomatic as a more flexible and efficient preprocessing tool, which could correctly handle paired-end data. Results: The value of NGS read preprocessing is demonstrated for both reference-based and reference-free tasks. Trimmomatic is shown to produce output that is at least competitive with, and in many cases superior to, that produced by other tools, in all scenarios tested. Availability and implementation: Trimmomatic is licensed under GPL V3. It is cross-platform (Java 1.5+ required) and available at http://www.usadellab.org/cms/index.php?page=trimmomatic Contact: usadel@bio1.rwth-aachen.de Supplementary information: Supplementary data are available at Bioinformatics online.",c41,Software Product Lines Conference,cp41,accepted,f1014,2002,2002-10-06
s1251,p1251,Incorporating Genomics and Bioinformatics across the Life Sciences Curriculum,"Community Page Incorporating Genomics and Bioinformatics across the Life Sciences Curriculum Jayna L. Ditty 1 , Christopher A. Kvaal 2 , Brad Goodner 3 , Sharyn K. Freyermuth 4 , Cheryl Bailey 5 , Robert A. Britton 6 , Stuart G. Gordon 7 , Sabine Heinhorst 8 , Kelynne Reed 9 , Zhaohui Xu 10 , Erin R. Sanders-Lorenz 11 , Seth Axen 12 , Edwin Kim 12 , Mitrick Johns 13 , Kathleen Scott 14 , Cheryl A. Kerfeld 12,15 * 1 Department of Biology, University of St. Thomas, St. Paul, Minnesota, United States of America, 2 Department of Biological Sciences, St. Cloud State University, St. Cloud, Minnesota, United States of America, 3 Department of Biology, Hiram College, Hiram, Ohio, United States of America, 4 Biochemistry Department, University of Missouri- Columbia, Columbia, Missouri, United States of America, 5 Department of Biochemistry, University of Nebraska-Lincoln, Lincoln, Nebraska, United States of America, 6 Department of Microbiology and Molecular Genetics, Michigan State University, East Lansing, Michigan, United States of America, 7 Department of Biology, Presbyterian College, Clinton, South Carolina, United States of America, 8 Department of Chemistry and Biochemistry, The University of Southern Mississippi, Hattiesburg, Mississippi, United States of America, 9 Biology Department, Austin College, Sherman, Texas, United States of America, 10 Department of Biological Sciences, Bowling Green State University, Bowling Green, Ohio, United States of America, 11 Department of Microbiology, Immunology and Molecular Genetics, University of California – Los Angeles, Los Angeles, California, United States of America, 12 Department of Energy-Joint Genome Institute, Walnut Creek, California, United States of America, 13 Department of Biological Sciences, Northern Illinois University, DeKalb, Illinois, United States of America, 14 Department of Integrative Biology, University of South Florida, Tampa, Florida, United States of America, 15 Department of Plant and Microbial Biology, University of California Berkley, Berkeley, California, United States of America Introduction Undergraduate life sciences education needs an overhaul, as clearly described in the National Research Council of the National Academies’ publication BIO 2010: Transforming Undergraduate Education for Future Research Biologists. Among BIO 2010’s top recommendations is the need to involve students in working with real data and tools that reflect the nature of life sciences research in the 21st century [1]. Education research studies support the importance of utilizing primary literature, designing and implementing experiments, and analyzing results in the context of a bona fide scientific question [1–12] in cultivating the analytical skills necessary to become a scientist. Incorporating these basic scientific methodologies in under- graduate education leads to increased undergraduate and post-graduate reten- tion in the sciences [13–16]. Toward this end, many undergraduate teaching orga- nizations offer training and suggestions for faculty to update and improve their teaching approaches to help students learn as scientists, through design and discovery (e.g., Council of Undergraduate Research [www.cur.org] and Project Kaleidoscope [ www.pkal.org]). With the advent of genome sequencing and bioinformatics, many scientists now formulate biological questions and inter- pret research results in the context of genomic information. Just as the use of bioinformatic tools and databases changed the way scientists investigate problems, it must change how scientists teach to create new opportunities for students to gain experiences reflecting the influence of genomics, proteomics, and bioinformatics on modern life sciences research [17–41]. Educators have responded by incorpo- rating bioinformatics into diverse life science curricula [42–44]. While these published exercises in, and guidelines for, bioinformatics curricula are helpful and inspirational, faculty new to the area of bioinformatics inevitably need training in the theoretical underpinnings of the algo- rithms [45]. Moreover, effectively inte- grating bioinformatics into courses or independent research projects requires infrastructure for organizing and assessing student work. Here, we present a new platform for faculty to keep current with the rapidly changing field of bioinfor- matics, the Integrated Microbial Genomes Annotation Collaboration Toolkit (IMG- ACT) (Figure 1). It was developed by instructors from both research-intensive and predominately undergraduate institu- tions in collaboration with the Department of Energy-Joint Genome Institute (DOE- JGI) as a means to innovate and update undergraduate education and faculty de- velopment. The IMG-ACT program pro- vides a cadre of tools, including access to a clearinghouse of genome sequences, bioin- formatics databases, data storage, instruc- tor course management, and student notebooks for organizing the results of their bioinformatic investigations. In the process, IMG-ACT makes it feasible to provide undergraduate research opportu- nities to a greater number and diversity of students, in contrast to the traditional mentor-to-student apprenticeship model for undergraduate research, which can be too expensive and time-consuming to provide for every undergraduate. The IMG-ACT serves as the hub for the network of faculty and students that use the system for microbial genome analysis. Open access of the IMG-ACT infrastructure to participating schools en- sures that all types of higher education institutions can utilize it. With the infra- structure in place, faculty can focus their efforts on the pedagogy of bioinformatics, involvement of students in research, and use of this tool for their own research agenda. What the original faculty mem- bers of the IMG-ACT development team present here is an overview of how the IMG-ACT program has affected our Citation: Ditty JL, Kvaal CA, Goodner B, Freyermuth SK, Bailey C, et al. (2010) Incorporating Genomics and Bioinformatics across the Life Sciences Curriculum. PLoS Biol 8(8): e1000448. doi:10.1371/journal.pbio.1000448 Published August 10, 2010 Copyright: s 2010 Ditty et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Funding: No specific funding was received for this work. The Community Page is a forum for organizations and societies to highlight their efforts to enhance the dissemination and value of scientific knowledge. Competing Interests: The authors have declared that no competing interests exist. Abbreviations: IMG-ACT; Integrated Microbial Genomes Annotation Collaboration Toolkit * E-mail: ckerfeld@lbl.gov PLoS Biology | www.plosbiology.org August 2010 | Volume 8 | Issue 8 | e1000448",j60,PLoS Biology,jv60,accepted,f1015,2001,2001-05-12
s1253,p1253,myGrid: personalised bioinformatics on the information grid,"MOTIVATION
The (my)Grid project aims to exploit Grid technology, with an emphasis on the Information Grid, and provide middleware layers that make it appropriate for the needs of bioinformatics. (my)Grid is building high level services for data and application integration such as resource discovery, workflow enactment and distributed query processing. Additional services are provided to support the scientific method and best practice found at the bench but often neglected at the workstation, notably provenance management, change notification and personalisation.


RESULTS
We give an overview of these services and their metadata. In particular, semantically rich metadata expressed using ontologies necessary to discover, select and compose services into dynamic workflows.",c72,Intelligent Systems in Molecular Biology,cp72,accepted,f1016,2003,2003-03-04
s1254,p1254,Over-optimism in bioinformatics: an illustration,"MOTIVATION
In statistical bioinformatics research, different optimization mechanisms potentially lead to 'over-optimism' in published papers. So far, however, a systematic critical study concerning the various sources underlying this over-optimism is lacking.


RESULTS
We present an empirical study on over-optimism using high-dimensional classification as example. Specifically, we consider a 'promising' new classification algorithm, namely linear discriminant analysis incorporating prior knowledge on gene functional groups through an appropriate shrinkage of the within-group covariance matrix. While this approach yields poor results in terms of error rate, we quantitatively demonstrate that it can artificially seem superior to existing approaches if we 'fish for significance'. The investigated sources of over-optimism include the optimization of datasets, of settings, of competing methods and, most importantly, of the method's characteristics. We conclude that, if the improvement of a quantitative criterion such as the error rate is the main contribution of a paper, the superiority of new algorithms should always be demonstrated on independent validation data.


AVAILABILITY
The R codes and relevant data can be downloaded from http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/020_professuren/boulesteix/overoptimism/, such that the study is completely reproducible.",c112,Very Large Data Bases Conference,cp112,accepted,f1017,2018,2018-05-14
s1255,p1255,Machine learning: an indispensable tool in bioinformatics.,Abstract content goes here ...,c72,Intelligent Systems in Molecular Biology,cp72,accepted,f1018,2003,2003-09-02
s1257,p1257,"The Phyre2 web portal for protein modeling, prediction and analysis",Abstract content goes here ...,j172,Nature Protocols,jv172,accepted,f1019,2010,2010-07-19
s1258,p1258,Bioinformatics and molecular modeling in glycobiology,Abstract content goes here ...,c105,Biometrics and Identity Management,cp105,accepted,f1020,2006,2006-07-03
s1260,p1260,"Bioinformatics training: a review of challenges, actions and support requirements","As bioinformatics becomes increasingly central to research in the molecular life sciences, the need to train non-bioinformaticians to make the most of bioinformatics resources is growing. Here, we review the key challenges and pitfalls to providing effective training for users of bioinformatics services, and discuss successful training strategies shared by a diverse set of bioinformatics trainers. We also identify steps that trainers in bioinformatics could take together to advance the state of the art in current training practices. The ideas presented in this article derive from the first Trainer Networking Session held under the auspices of the EU-funded SLING Integrating Activity, which took place in November 2009.",c78,Neural Information Processing Systems,cp78,accepted,f1021,2012,2012-01-09
s1261,p1261,Protein Bioinformatics: From Sequence to Function,"One of the most pressing tasks in biotechnology today is to unlock the function of each of the thousands of new genes identified everyday. Scientists do this by analyzing and interpreting proteins, which are considered the task force of a gene. This single source reference covers all aspects of proteins, explaining fundamentals, synthesizing the latest literature, and demonstrating the most important bioinformatics tools available today for protein analysis, interpretation and prediction. Students and researchers of biotechnology, bioinformatics, proteomics, protein engineering, biophysics, computational biology, molecular modeling, and drug design will find this a ready reference for staying current and productive in this fast evolving interdisciplinary field. It explains all aspects of proteins including sequence and structure analysis, prediction of protein structures, protein folding, protein stability, and protein interactions. It teaches readers how to analyze their own datasets using available online databases, software tools, and web servers, which are listed and updated on the book's web companion page. It presents a cohesive and accessible overview of the field, using illustrations to explain key concepts and detailed exercises for students.",c8,The Compass,cp8,accepted,f1022,2016,2016-12-23
s1262,p1262,Machine Learning Approaches to Bioinformatics,"This book covers a wide range of subjects in applying machine learning approaches for bioinformatics projects. The book succeeds on two key unique features. First, it introduces the most widely used machine learning approaches in bioinformatics and discusses, with evaluations from real case studies, how they are used in individual bioinformatics projects. Second, it introduces state-of-the-art bioinformatics research methods. The theoretical parts and the practical parts are well integrated for readers to follow the existing procedures in individual research. Unlike most of the bioinformatics books on the market, the content coverage is not limited to just one subject. A broad spectrum of relevant topics in bioinformatics including systematic data mining and computational systems biology researches are brought together in this book, thereby offering an efficient and convenient platform for teaching purposes. An essential reference for both final year undergraduates and graduate students in universities, as well as a comprehensive handbook for new researchers, this book will also serve as a practical guide for software development in relevant bioinformatics projects.",c31,International Conference on Evaluation & Assessment in Software Engineering,cp31,accepted,f1023,2022,2022-11-12
s1264,p1264,"DNA barcodes: Genes, genomics, and bioinformatics","It is not a coincidence that DNA barcoding has developed in concert with genomics-based investigations. DNA barcoding (a tool for rapid species identification based on DNA sequences) and genomics (which compares entire genome structure and expression) share an emphasis on large-scale genetic data acquisition that offers new answers to questions previously beyond the reach of traditional disciplines. DNA barcodes consist of a standardized short sequence of DNA (400–800 bp) that in principle should be easily generated and characterized for all species on the planet (1). A massive on-line digital library of barcodes will serve as a standard to which the DNA barcode sequence of an unidentified sample from the forest, garden, or market can be matched. Similar to genomics, which has accelerated the process of recognizing novel genes and comparing gene function, DNA barcoding will allow users to efficiently recognize known species and speed the discovery of species yet to be found in nature. DNA barcoding aims to use the information of one or a few gene regions to identify all species of life, whereas genomics, the inverse of barcoding, describes in one (e.g., humans) or a few selected species the function and interactions across all genes (Fig. 1). The work of Lahaye et al. (2) reported in a recent issue of PNAS brings the application of DNA barcoding one step closer to implementation in plants.",j20,Proceedings of the National Academy of Sciences of the United States of America,jv20,accepted,f1024,2006,2006-09-10
s1265,p1265,Complex heatmaps reveal patterns and correlations in multidimensional genomic data,"UNLABELLED
Parallel heatmaps with carefully designed annotation graphics are powerful for efficient visualization of patterns and relationships among high dimensional genomic data. Here we present the ComplexHeatmap package that provides rich functionalities for customizing heatmaps, arranging multiple parallel heatmaps and including user-defined annotation graphics. We demonstrate the power of ComplexHeatmap to easily reveal patterns and correlations among multiple sources of information with four real-world datasets.


AVAILABILITY AND IMPLEMENTATION
The ComplexHeatmap package and documentation are freely available from the Bioconductor project: http://www.bioconductor.org/packages/devel/bioc/html/ComplexHeatmap.html


CONTACT
m.schlesner@dkfz.de


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",c15,International Conference on Conceptual Structures,cp15,accepted,f1025,2011,2011-10-04
s1266,p1266,Bioinformatics prediction of HIV coreceptor usage,Abstract content goes here ...,j0,Nature Biotechnology,jv0,accepted,f1026,2006,2006-01-29
s1267,p1267,CD-HIT: accelerated for clustering the next-generation sequencing data,"Summary: CD-HIT is a widely used program for clustering biological sequences to reduce sequence redundancy and improve the performance of other sequence analyses. In response to the rapid increase in the amount of sequencing data produced by the next-generation sequencing technologies, we have developed a new CD-HIT program accelerated with a novel parallelization strategy and some other techniques to allow efficient clustering of such datasets. Our tests demonstrated very good speedup derived from the parallelization for up to ∼24 cores and a quasi-linear speedup for up to ∼8 cores. The enhanced CD-HIT is capable of handling very large datasets in much shorter time than previous versions. Availability: http://cd-hit.org. Contact: liwz@sdsc.edu Supplementary information: Supplementary data are available at Bioinformatics online.",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f1027,2018,2018-05-17
s1268,p1268,Gene Expression Atlas at the European Bioinformatics Institute,"The Gene Expression Atlas (http://www.ebi.ac.uk/gxa) is an added-value database providing information about gene expression in different cell types, organism parts, developmental stages, disease states, sample treatments and other biological/experimental conditions. The content of this database derives from curation, re-annotation and statistical analysis of selected data from the ArrayExpress Archive of Functional Genomics Data. A simple interface allows the user to query for differential gene expression either (i) by gene names or attributes such as Gene Ontology terms, or (ii) by biological conditions, e.g. diseases, organism parts or cell types. The gene queries return the conditions where expression has been reported, while condition queries return which genes are reported to be expressed in these conditions. A combination of both query types is possible. The query results are ranked using various statistical measures and by how many independent studies in the database show the particular gene-condition association. Currently, the database contains information about more than 200 000 genes from nine species and almost 4500 biological conditions studied in over 30 000 assays from over 1000 independent studies.",c76,International Conference on Artificial Neural Networks,cp76,accepted,f1028,2013,2013-04-21
s1270,p1270,The EMBL-EBI search and sequence analysis tools APIs in 2019,"Abstract The EMBL-EBI provides free access to popular bioinformatics sequence analysis applications as well as to a full-featured text search engine with powerful cross-referencing and data retrieval capabilities. Access to these services is provided via user-friendly web interfaces and via established RESTful and SOAP Web Services APIs (https://www.ebi.ac.uk/seqdb/confluence/display/JDSAT/EMBL-EBI+Web+Services+APIs+-+Data+Retrieval). Both systems have been developed with the same core principles that allow them to integrate an ever-increasing volume of biological data, making them an integral part of many popular data resources provided at the EMBL-EBI. Here, we describe the latest improvements made to the frameworks which enhance the interconnectivity between public EMBL-EBI resources and ultimately enhance biological data discoverability, accessibility, interoperability and reusability.",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f1029,2002,2002-01-21
s1271,p1271,Pharmacogenomics and bioinformatics: PharmGKB.,"The NIH initiated the PharmGKB in April 2000. The primary mission was to create a repository of primary data, tools to track associations between genes and drugs, and to catalog the location and frequency of genetic variations known to impact drug response. Over the past 10 years, new technologies have shifted research from candidate gene pharmacogenetics to phenotype-based pharmacogenomics with a consequent explosion of data. PharmGKB has refocused on curating knowledge rather than housing primary genotype and phenotype data, and now, captures more complex relationships between genes, variants, drugs, diseases and pathways. Going forward, the challenges are to provide the tools and knowledge to plan and interpret genome-wide pharmacogenomics studies, predict gene-drug relationships based on shared mechanisms and support data-sharing consortia investigating clinical applications of pharmacogenomics.",j239,Pharmacogenomics (London),jv239,accepted,f1030,2015,2015-11-29
s1272,p1272,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.",j240,International Journal of Systematic and Evolutionary Microbiology,jv240,accepted,f1031,2016,2016-04-10
s1273,p1273,Multiobjective Optimization in Bioinformatics and Computational Biology,"This paper reviews the application of multiobjective optimization in the fields of bioinformatics and computational biology. A survey of existing work, organized by application area, forms the main body of the review, following an introduction to the key concepts in multiobjective optimization. An original contribution of the review is the identification of five distinct ""contexts,"" giving rise to multiple objectives: These are used to explain the reasons behind the use of multiobjective optimization in each application area and also to point the way to potential future uses of the technique",j219,IEEE/ACM Transactions on Computational Biology & Bioinformatics,jv219,accepted,f1032,2001,2001-10-16
s1275,p1275,The Roots of Bioinformatics,"Every new scientific discipline or methodology reaches a point in its maturation where it is fruitful for it to turn its gaze inward, as well as backward. Such introspection helps to clarify the essential structure of a field of study, facilitating communication, pedagogy, standardization, and the like, while retrospection aids this process by accounting for its beginnings and underpinnings. 
 
In this spirit, PLoS Computational Biology is launching a new series of themed articles tracing the roots of bioinformatics. Essays from prominent workers in the field will relate how selected scientific, technological, economic, and even cultural threads came to influence the development of the field we know today. These are not intended to be review articles, nor personal reminiscences, but rather narratives from individual perspectives about the origins and foundations of bioinformatics, and are expected to provide both historical and technical insights. Ideally, these articles will offer an archival record of the field's development, as well as a human face on an important segment of science, for the benefit of current and future workers. 
 
Upcoming articles, already commissioned, will cover the roots of bioinformatics in structural biology, in evolutionary biology, and in artificial intelligence, with more in the works. These topics are obviously very broad, and so are likely to be subdivided or otherwise revisited in future installments by authors with varying perspectives. Topics and authors will be chosen at the discretion of the editors along lines broadly corresponding to the usual content of this journal. 
 
The author, having been asked to serve as Series Editor by the Editor-in-Chief, will endeavor to maintain a uniform flow of articles solicited from luminaries in the field. As a starting point to the series, I offer below a few vignettes and reflections on some longer-term influences that have shaped the discipline. I first consider the unique status of bioinformatics vis-a-vis science and technology, and then explore historical trends in biology and related fields that anticipated and prepared the way for bioinformatics. Examining the context of key moments when computers were first taken up by early adopters reveals how deep the roots of bioinformatics go.",c65,Formal Concept Analysis,cp65,accepted,f1033,2008,2008-07-06
s1276,p1276,Integrative Analysis of Complex Cancer Genomics and Clinical Profiles Using the cBioPortal,"The cBioPortal enables integration, visualization, and analysis of multidimensional cancer genomic and clinical data. The cBioPortal for Cancer Genomics (http://cbioportal.org) provides a Web resource for exploring, visualizing, and analyzing multidimensional cancer genomics data. The portal reduces molecular profiling data from cancer tissues and cell lines into readily understandable genetic, epigenetic, gene expression, and proteomic events. The query interface combined with customized data storage enables researchers to interactively explore genetic alterations across samples, genes, and pathways and, when available in the underlying data, to link these to clinical outcomes. The portal provides graphical summaries of gene-level data from multiple platforms, network visualization and analysis, survival analysis, patient-centric queries, and software programmatic access. The intuitive Web interface of the portal makes complex cancer genomics profiles accessible to researchers and clinicians without requiring bioinformatics expertise, thus facilitating biological discoveries. Here, we provide a practical guide to the analysis and visualization features of the cBioPortal for Cancer Genomics.",j241,Science Signaling,jv241,accepted,f1034,2012,2012-04-12
s1277,p1277,"Anatomy Ontologies for Bioinformatics, Principles and Practice",Abstract content goes here ...,c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1035,2007,2007-05-11
s1278,p1278,Metabolomics technology and bioinformatics,"Metabolomics is the global analysis of all or a large number of cellular metabolites. Like other functional genomics research, metabolomics generates large amounts of data. Handling, processing and analysis of this data is a clear challenge and requires specialized mathematical, statistical and bioinformatics tools. Metabolomics needs for bioinformatics span through data and information management, raw analytical data processing, metabolomics standards and ontology, statistical analysis and data mining, data integration and mathematical modelling of metabolic networks within a framework of systems biology. The major approaches in metabolomics, along with the modern analytical tools used for data generation, are reviewed in the context of these specific bioinformatics needs.",c102,International Conference on Biometrics,cp102,accepted,f1036,2022,2022-01-19
s1279,p1279,Phylogenetic diversity (PD) and biodiversity conservation: some bioinformatics challenges,"Biodiversity conservation addresses information challenges through estimations encapsulated in measures of diversity. A quantitative measure of phylogenetic diversity, “PD”, has been defined as the minimum total length of all the phylogenetic branches required to span a given set of taxa on the phylogenetic tree (Faith 1992a). While a recent paper incorrectly characterizes PD as not including information about deeper phylogenetic branches, PD applications over the past decade document the proper incorporation of shared deep branches when assessing the total PD of a set of taxa. Current PD applications to macroinvertebrate taxa in streams of New South Wales, Australia illustrate the practical importance of this definition. Phylogenetic lineages, often corresponding to new, “cryptic”, taxa, are restricted to a small number of stream localities. A recent case of human impact causing loss of taxa in one locality implies a higher PD value for another locality, because it now uniquely represents a deeper branch. This molecular-based phylogenetic pattern supports the use of DNA barcoding programs for biodiversity conservation planning. Here, PD assessments side-step the contentious use of barcoding-based “species” designations. Bioinformatics challenges include combining different phylogenetic evidence, optimization problems for conservation planning, and effective integration of phylogenetic information with environmental and socioeconomic data.",c64,Experimental Software Engineering Network,cp64,accepted,f1037,2014,2014-09-26
s1280,p1280,WGCNA: an R package for weighted correlation network analysis,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1038,2007,2007-05-21
s1281,p1281,Cytoscape 2.8: new features for data integration and network visualization,"Summary: Cytoscape is a popular bioinformatics package for biological network visualization and data integration. Version 2.8 introduces two powerful new features—Custom Node Graphics and Attribute Equations—which can be used jointly to greatly enhance Cytoscape's data integration and visualization capabilities. Custom Node Graphics allow an image to be projected onto a node, including images generated dynamically or at remote locations. Attribute Equations provide Cytoscape with spreadsheet-like functionality in which the value of an attribute is computed dynamically as a function of other attributes and network properties. Availability and implementation: Cytoscape is a desktop Java application released under the Library Gnu Public License (LGPL). Binary install bundles and source code for Cytoscape 2.8 are available for download from http://cytoscape.org. Contact: msmoot@ucsd.edu",c88,Symposium on the Theory of Computing,cp88,accepted,f1039,2014,2014-03-21
s1283,p1283,Cache-Oblivious Dynamic Programming for Bioinformatics,"We present efficient cache-oblivious algorithms for some well-studied string problems in bioinformatics including the longest common subsequence, global pairwise sequence alignment and three-way sequence alignment (or median), both with affine gap costs, and RNA secondary structure prediction with simple pseudoknots. For each of these problems, we present cache-oblivious algorithms that match the best-known time complexity, match or improve the best-known space complexity, and improve significantly over the cache-efficiency of earlier algorithms. We present experimental results which show that our cache-oblivious algorithms run faster than software and implementations based on previous best algorithms for these problems.",j219,IEEE/ACM Transactions on Computational Biology & Bioinformatics,jv219,accepted,f1040,2001,2001-10-09
s1284,p1284,Bismark: a flexible aligner and methylation caller for Bisulfite-Seq applications,"Summary: A combination of bisulfite treatment of DNA and high-throughput sequencing (BS-Seq) can capture a snapshot of a cell's epigenomic state by revealing its genome-wide cytosine methylation at single base resolution. Bismark is a flexible tool for the time-efficient analysis of BS-Seq data which performs both read mapping and methylation calling in a single convenient step. Its output discriminates between cytosines in CpG, CHG and CHH context and enables bench scientists to visualize and interpret their methylation data soon after the sequencing run is completed. Availability and implementation: Bismark is released under the GNU GPLv3+ licence. The source code is freely available from www.bioinformatics.bbsrc.ac.uk/projects/bismark/. Contact: felix.krueger@bbsrc.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",c62,International Conference on Software Reuse,cp62,accepted,f1041,2006,2006-12-05
s1285,p1285,The Protein Data Bank,"The Protein Data Bank [PDB; Berman, Westbrook et al. (2000), Nucleic Acids Res. 28, 235-242; http://www.pdb.org/] is the single worldwide archive of primary structural data of biological macromolecules. Many secondary sources of information are derived from PDB data. It is the starting point for studies in structural bioinformatics. This article describes the goals of the PDB, the systems in place for data deposition and access, how to obtain further information and plans for the future development of the resource. The reader should come away with an understanding of the scope of the PDB and what is provided by the resource.",c1,Technical Symposium on Computer Science Education,cp1,accepted,f1042,2002,2002-03-01
s1287,p1287,Over-optimism in bioinformatics research,"The problem of ”false research findings” in medical research has focused much attention in the last few years (Ioannidis, 2005). One of the main problems, termed as ”fishing for significance” in the present letter, is that researchers often (consciously or subconsciously) report results that are in fact the product of an intensive optimization, i.e. of multiple comparisons. Such results are typically unlikely to be reproduced in an independent study and have a high probability to be false (Ioannidis, 2005). The ”fishing for significance” problem is enhanced by the so-called ”publication bias”: positive results have a much higher chance to get published than negative results, as already acknowledged fifty years ago (Sterling, 1959). In a word, many false positive results are produced through multiple comparisons, and false positives have higher chance to get published than true negatives. Moreover, the difficulty to publish negative results obviously encourages authors to find something positive in their study by performing numerous analyses until one of them yields positive results by chance, i.e. to fish for significance. Although this issue is by far less acknowledged and publicly admitted than in the medical context, the same types of problems occur in biostatistics and bioinformatics research.",c83,International Conference on Computer Graphics and Interactive Techniques,cp83,accepted,f1043,2015,2015-06-12
s1288,p1288,Bioinformatics for Next Generation Sequencing Data,"The emergence of next-generation sequencing (NGS) platforms imposes increasing demands on statistical methods and bioinformatic tools for the analysis and the management of the huge amounts of data generated by these technologies. Even at the early stages of their commercial availability, a large number of softwares already exist for analyzing NGS data. These tools can be fit into many general categories including alignment of sequence reads to a reference, base-calling and/or polymorphism detection, de novo assembly from paired or unpaired reads, structural variant detection and genome browsing. This manuscript aims to guide readers in the choice of the available computational tools that can be used to face the several steps of the data analysis workflow.",j244,Genes,jv244,accepted,f1044,2005,2005-08-29
s1290,p1290,Introduction to bioinformatics,"Bioinformatics is a multifaceted discipline combining many scientific fields including computational biology, statistics, mathematics, molecular biology, and genetics. Bioinformatics enables biomedical investigators to exploit existing and emerging computational technologies to seamlessly store, mine, retrieve, and analyze data from genomics and proteomics technologies. This is achieved by creating unified data models, standardizing data interfaces, developing structured vocabularies, generating new data visualization methods, and capturing detailed metadata that describes various aspects of the experimental design and analysis methods. Already there are a number of related undertakings that are dividing the field into more specialized groups. Clinical Bioinformatics and Biomedical Informatics are emerging as transitional fields to promote the utilization of genomics and proteomics data combined with medical history and demographic data towards personalized medicine, molecular diagnostics, pharmacogenomics and predicting outcomes of therapeutic interventions. The field of bioinformatics will continue to evolve through the incorporation of diverse technologies and methodologies that draw experts from disparate fields to create the latest computational and informational tools specifically design for the biomedical research enterprise.",c94,Vision,cp94,accepted,f1045,2020,2020-10-10
s1291,p1291,pROC: an open-source package for R and S+ to analyze and compare ROC curves,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1046,2007,2007-09-09
s1292,p1292,Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences,"MOTIVATION
In 2001 and 2002, we published two papers (Bioinformatics, 17, 282-283, Bioinformatics, 18, 77-82) describing an ultrafast protein sequence clustering program called cd-hit. This program can efficiently cluster a huge protein database with millions of sequences. However, the applications of the underlying algorithm are not limited to only protein sequences clustering, here we present several new programs using the same algorithm including cd-hit-2d, cd-hit-est and cd-hit-est-2d. Cd-hit-2d compares two protein datasets and reports similar matches between them; cd-hit-est clusters a DNA/RNA sequence database and cd-hit-est-2d compares two nucleotide datasets. All these programs can handle huge datasets with millions of sequences and can be hundreds of times faster than methods based on the popular sequence comparison and database search tools, such as BLAST.",c63,IEEE International Software Metrics Symposium,cp63,accepted,f1047,2008,2008-10-06
s1293,p1293,An insight-based methodology for evaluating bioinformatics visualizations,"High-throughput experiments, such as gene expression microarrays in the life sciences, result in very large data sets. In response, a wide variety of visualization tools have been created to facilitate data analysis. A primary purpose of these tools is to provide biologically relevant insight into the data. Typically, visualizations are evaluated in controlled studies that measure user performance on predetermined tasks or using heuristics and expert reviews. To evaluate and rank bioinformatics visualizations based on real-world data analysis scenarios, we developed a more relevant evaluation method that focuses on data insight. This paper presents several characteristics of insight that enabled us to recognize and quantify it in open-ended user tests. Using these characteristics, we evaluated five microarray visualization tools on the amount and types of insight they provide and the time it takes to acquire it. The results of the study guide biologists in selecting a visualization tool based on the type of their microarray data, visualization designers on the key role of user interaction techniques, and evaluators on a new approach for evaluating the effectiveness of visualizations for providing insight. Though we used the method to analyze bioinformatics visualizations, it can be applied to other domains.",j245,IEEE Transactions on Visualization and Computer Graphics,jv245,accepted,f1048,2009,2009-04-07
s1295,p1295,trimAl: a tool for automated alignment trimming in large-scale phylogenetic analyses,"Summary: Multiple sequence alignments are central to many areas of bioinformatics. It has been shown that the removal of poorly aligned regions from an alignment increases the quality of subsequent analyses. Such an alignment trimming phase is complicated in large-scale phylogenetic analyses that deal with thousands of alignments. Here, we present trimAl, a tool for automated alignment trimming, which is especially suited for large-scale phylogenetic analyses. trimAl can consider several parameters, alone or in multiple combinations, for selecting the most reliable positions in the alignment. These include the proportion of sequences with a gap, the level of amino acid similarity and, if several alignments for the same set of sequences are provided, the level of consistency across different alignments. Moreover, trimAl can automatically select the parameters to be used in each specific alignment so that the signal-to-noise ratio is optimized. Availability: trimAl has been written in C++, it is portable to all platforms. trimAl is freely available for download (http://trimal.cgenomics.org) and can be used online through the Phylemon web server (http://phylemon2.bioinfo.cipf.es/). Supplementary Material is available at http://trimal.cgenomics.org/publications. Contact: tgabaldon@crg.es",c30,IEEE Aerospace Conference,cp30,accepted,f1049,2006,2006-06-06
s1297,p1297,Penalized feature selection and classification in bioinformatics,"In bioinformatics studies, supervised classification with high-dimensional input variables is frequently encountered. Examples routinely arise in genomic, epigenetic and proteomic studies. Feature selection can be employed along with classifier construction to avoid over-fitting, to generate more reliable classifier and to provide more insights into the underlying causal relationships. In this article, we provide a review of several recently developed penalized feature selection and classification techniques--which belong to the family of embedded feature selection methods--for bioinformatics studies with high-dimensional input. Classification objective functions, penalty functions and computational algorithms are discussed. Our goal is to make interested researchers aware of these feature selection and classification methods that are applicable to high-dimensional bioinformatics data.",c77,Networks,cp77,accepted,f1050,2019,2019-12-21
s1298,p1298,I-TASSER server for protein 3D structure prediction,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1051,2007,2007-11-03
s1299,p1299,Recent developments in the MAFFT multiple sequence alignment program,"The accuracy and scalability of multiple sequence alignment (MSA) of DNAs and proteins have long been and are still important issues in bioinformatics. To rapidly construct a reasonable MSA, we developed the initial version of the MAFFT program in 2002. MSA software is now facing greater challenges in both scalability and accuracy than those of 5 years ago. As increasing amounts of sequence data are being generated by large-scale sequencing projects, scalability is now critical in many situations. The requirement of accuracy has also entered a new stage since the discovery of functional noncoding RNAs (ncRNAs); the secondary structure should be considered for constructing a high-quality alignment of distantly related ncRNAs. To deal with these problems, in 2007, we updated MAFFT to Version 6 with two new techniques: the PartTree algorithm and the Four-way consistency objective function. The former improved the scalability of progressive alignment and the latter improved the accuracy of ncRNA alignment. We review these and other techniques that MAFFT uses and suggest possible future directions of MSA software as a basis of comparative analyses. MAFFT is available at http://align.bmr.kyushu-u.ac.jp/mafft/software/.",c30,IEEE Aerospace Conference,cp30,accepted,f1052,2006,2006-06-01
s1300,p1300,I-TASSER server for protein 3D structure prediction,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1053,2007,2007-03-30
s1302,p1302,Roary: rapid large-scale prokaryote pan genome analysis,"Summary A typical prokaryote population sequencing study can now consist of hundreds or thousands of isolates. Interrogating these datasets can provide detailed insights into the genetic structure of of prokaryotic genomes. We introduce Roary, a tool that rapidly builds large-scale pan genomes, identifying the core and dispensable accessory genes. Roary makes construction of the pan genome of thousands of prokaryote samples possible on a standard desktop without compromising on the accuracy of results. Using a single CPU Roary can produce a pan genome consisting of 1000 isolates in 4.5 hours using 13 GB of RAM, with further speedups possible using multiple processors. Availability and implementation Roary is implemented in Perl and is freely available under an open source GPLv3 license from http://sanger-pathogens.github.io/Roary Contact roary@sanger.ac.uk Supplementary information Supplementary data are available at Bioinformatics online.",j153,bioRxiv,jv153,accepted,f1054,2020,2020-06-13
s1303,p1303,MultiQC: summarize analysis results for multiple tools and samples in a single report,"Motivation: Fast and accurate quality control is essential for studies involving next-generation sequencing data. Whilst numerous tools exist to quantify QC metrics, there is no common approach to flexibly integrate these across tools and large sample sets. Assessing analysis results across an entire project can be time consuming and error prone; batch effects and outlier samples can easily be missed in the early stages of analysis. Results: We present MultiQC, a tool to create a single report visualising output from multiple tools across many samples, enabling global trends and biases to be quickly identified. MultiQC can plot data from many common bioinformatics tools and is built to allow easy extension and customization. Availability and implementation: MultiQC is available with an GNU GPLv3 license on GitHub, the Python Package Index and Bioconda. Documentation and example reports are available at http://multiqc.info Contact: phil.ewels@scilifelab.se",c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f1055,2015,2015-05-09
s1304,p1304,What is Bioinformatics? A Proposed Definition and Overview of the Field,"Summary Background: The recent flood of data from genome sequences and functional genomics has given rise to new field, bioinformatics, which combines elements of biology and computer science. Objectives: Here we propose a definition for this new field and review some of the research that is being pursued, particularly in relation to transcriptional regulatory systems. Methods: Our definition is as follows: Bioinformatics is conceptualizing biology in terms of macromolecules (in the sense of physical-chemistry) and then applying “informatics” techniques (derived from disciplines such as applied maths, computer science, and statistics) to understand and organize the information associated with these molecules, on a large-scale. Results and Conclusions: Analyses in bioinformatics predominantly focus on three types of large datasets available in molecular biology: macromolecular structures, genome sequences, and the results of functional genomics experiments (eg expression data). Additional information includes the text of scientific papers and “relationship data” from metabolic pathways, taxonomy trees, and protein-protein interaction networks. Bioinformatics employs a wide range of computational techniques including sequence and structural alignment, database design and data mining, macromolecular geometry, phylogenetic tree construction, prediction of protein structure and function, gene finding, and expression data clustering. The emphasis is on approaches integrating a variety of computational methods and heterogeneous data sources. Finally, bioinformatics is a practical discipline. We survey some representative applications, such as finding homologues, designing drugs, and performing large-scale censuses. Additional information pertinent to the review is available over the web at http://bioinfo.mbb.yale.edu/what-is-it.",j246,Methods of Information in Medicine,jv246,accepted,f1056,2009,2009-04-25
s1308,p1308,Bioinformatics identification of MurJ (MviN) as the peptidoglycan lipid II flippase in Escherichia coli,"Peptidoglycan is a cell-wall glycopeptide polymer that protects bacteria from osmotic lysis. Whereas in Gram-positive bacteria it also serves as scaffold for many virulence factors, in Gram-negative bacteria, peptidoglycan is an anchor for the outer membrane. For years, we have known the enzymes required for the biosynthesis of peptidoglycan; what was missing was the flippase that translocates the lipid-anchored precursors across the cytoplasmic membrane before their polymerization into mature peptidoglycan. Using a reductionist bioinformatics approach, I have identified the essential inner-membrane protein MviN (renamed MurJ) as a likely candidate for the peptidoglycan flippase in Escherichia coli. Here, I present genetic and biochemical data that confirm the requirement of MurJ for peptidoglycan biosynthesis and that are in agreement with a role of MurJ as a flippase. Because of its essential nature, MurJ could serve as a target in the continuing search for antimicrobial compounds.",j20,Proceedings of the National Academy of Sciences of the United States of America,jv20,accepted,f1057,2006,2006-05-02
s1310,p1310,High-throughput functional annotation and data mining with the Blast2GO suite,"Functional genomics technologies have been widely adopted in the biological research of both model and non-model species. An efficient functional annotation of DNA or protein sequences is a major requirement for the successful application of these approaches as functional information on gene products is often the key to the interpretation of experimental results. Therefore, there is an increasing need for bioinformatics resources which are able to cope with large amount of sequence data, produce valuable annotation results and are easily accessible to laboratories where functional genomics projects are being undertaken. We present the Blast2GO suite as an integrated and biologist-oriented solution for the high-throughput and automatic functional annotation of DNA or protein sequences based on the Gene Ontology vocabulary. The most outstanding Blast2GO features are: (i) the combination of various annotation strategies and tools controlling type and intensity of annotation, (ii) the numerous graphical features such as the interactive GO-graph visualization for gene-set function profiling or descriptive charts, (iii) the general sequence management features and (iv) high-throughput capabilities. We used the Blast2GO framework to carry out a detailed analysis of annotation behaviour through homology transfer and its impact in functional genomics research. Our aim is to offer biologists useful information to take into account when addressing the task of functionally characterizing their sequence data.",j102,Nucleic Acids Research,jv102,accepted,f1058,2002,2002-09-02
s1314,p1314,"Learning with Kernels: support vector machines, regularization, optimization, and beyond","From the Publisher: 
In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. 
Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.",c77,Networks,cp77,accepted,f1059,2019,2019-06-02
s1316,p1316,ExPASy: the proteomics server for in-depth protein knowledge and analysis,"The ExPASy (the Expert Protein Analysis System) World Wide Web server (http://www.expasy.org), is provided as a service to the life science community by a multidisciplinary team at the Swiss Institute of Bioinformatics (SIB). It provides access to a variety of databases and analytical tools dedicated to proteins and proteomics. ExPASy databases include SWISS-PROT and TrEMBL, SWISS-2DPAGE, PROSITE, ENZYME and the SWISS-MODEL repository. Analysis tools are available for specific tasks relevant to proteomics, similarity searches, pattern and profile searches, post-translational modification prediction, topology prediction, primary, secondary and tertiary structure analysis and sequence alignment. These databases and tools are tightly interlinked: a special emphasis is placed on integration of database entries with related resources developed at the SIB and elsewhere, and the proteomics tools have been designed to read the annotations in SWISS-PROT in order to enhance their predictions. ExPASy started to operate in 1993, as the first WWW server in the field of life sciences. In addition to the main site in Switzerland, seven mirror sites in different continents currently serve the user community.",c65,Formal Concept Analysis,cp65,accepted,f1060,2008,2008-04-06
s1317,p1317,An innovative approach for testing bioinformatics programs using metamorphic testing,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1061,2007,2007-01-10
s1318,p1318,Cloud Technologies for Bioinformatics Applications,"Executing large number of independent jobs or jobs comprising of large number of tasks that perform minimal intertask communication is a common requirement in many domains. Various technologies ranging from classic job schedulers to the latest cloud technologies such as MapReduce can be used to execute these ""many-tasks” in parallel. In this paper, we present our experience in applying two cloud technologies Apache Hadoop and Microsoft DryadLINQ to two bioinformatics applications with the above characteristics. The applications are a pairwise Alu sequence alignment application and an Expressed Sequence Tag (EST) sequence assembly program. First, we compare the performance of these cloud technologies using the above applications and also compare them with traditional MPI implementation in one application. Next, we analyze the effect of inhomogeneous data on the scheduling mechanisms of the cloud technologies. Finally, we present a comparison of performance of the cloud technologies under virtual and nonvirtual hardware platforms.",j249,IEEE Transactions on Parallel and Distributed Systems,jv249,accepted,f1062,2015,2015-12-03
s1319,p1319,Nextstrain: real-time tracking of pathogen evolution,"Summary Understanding the spread and evolution of pathogens is important for effective public health measures and surveillance. Nextstrain consists of a database of viral genomes, a bioinformatics pipeline for phylodynamics analysis, and an interactive visualisation platform. Together these present a real-time view into the evolution and spread of a range of viral pathogens of high public health importance. The visualization integrates sequence data with other data types such as geographic information, serology, or host species. Nextstrain compiles our current understanding into a single accessible location, publicly available for use by health professionals, epidemiologists, virologists and the public alike. Availability and implementation All code (predominantly JavaScript and Python) is freely available from github.com/nextstrain and the web-application is available at nextstrain.org.",j153,bioRxiv,jv153,accepted,f1063,2020,2020-08-04
s1320,p1320,BIGSdb: Scalable analysis of bacterial genome variation at the population level,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1064,2007,2007-03-21
s1322,p1322,CD-HIT Suite: a web server for clustering and comparing biological sequences,"Summary: CD-HIT is a widely used program for clustering and comparing large biological sequence datasets. In order to further assist the CD-HIT users, we significantly improved this program with more functions and better accuracy, scalability and flexibility. Most importantly, we developed a new web server, CD-HIT Suite, for clustering a user-uploaded sequence dataset or comparing it to another dataset at different identity levels. Users can now interactively explore the clusters within web browsers. We also provide downloadable clusters for several public databases (NCBI NR, Swissprot and PDB) at different identity levels. Availability: Free access at http://cd-hit.org Contact: liwz@sdsc.edu Supplementary information: Supplementary data are available at Bioinformatics online.",c90,Computer Vision and Pattern Recognition,cp90,accepted,f1065,2008,2008-08-19
s1324,p1324,ConsensusClusterPlus: a class discovery tool with confidence assessments and item tracking,"Summary: Unsupervised class discovery is a highly useful technique in cancer research, where intrinsic groups sharing biological characteristics may exist but are unknown. The consensus clustering (CC) method provides quantitative and visual stability evidence for estimating the number of unsupervised classes in a dataset. ConsensusClusterPlus implements the CC method in R and extends it with new functionality and visualizations including item tracking, item-consensus and cluster-consensus plots. These new features provide users with detailed information that enable more specific decisions in unsupervised class discovery. Availability: ConsensusClusterPlus is open source software, written in R, under GPL-2, and available through the Bioconductor project (http://www.bioconductor.org/). Contact: mwilkers@med.unc.edu Supplementary Information: Supplementary data are available at Bioinformatics online.",c49,International Symposium on Search Based Software Engineering,cp49,accepted,f1066,2012,2012-08-09
s1326,p1326,Deriving the consequences of genomic variants with the Ensembl API and SNP Effect Predictor,"Summary: A tool to predict the effect that newly discovered genomic variants have on known transcripts is indispensible in prioritizing and categorizing such variants. In Ensembl, a web-based tool (the SNP Effect Predictor) and API interface can now functionally annotate variants in all Ensembl and Ensembl Genomes supported species. Availability: The Ensembl SNP Effect Predictor can be accessed via the Ensembl website at http://www.ensembl.org/. The Ensembl API (http://www.ensembl.org/info/docs/api/api_installation.html for installation instructions) is open source software. Contact: wm2@ebi.ac.uk; fiona@ebi.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",c7,European Conference on Modelling and Simulation,cp7,accepted,f1067,2015,2015-04-01
s1327,p1327,"Reactome: a database of reactions, pathways and biological processes","Reactome (http://www.reactome.org) is a collaboration among groups at the Ontario Institute for Cancer Research, Cold Spring Harbor Laboratory, New York University School of Medicine and The European Bioinformatics Institute, to develop an open source curated bioinformatics database of human pathways and reactions. Recently, we developed a new web site with improved tools for pathway browsing and data analysis. The Pathway Browser is an Systems Biology Graphical Notation (SBGN)-based visualization system that supports zooming, scrolling and event highlighting. It exploits PSIQUIC web services to overlay our curated pathways with molecular interaction data from the Reactome Functional Interaction Network and external interaction databases such as IntAct, BioGRID, ChEMBL, iRefIndex, MINT and STRING. Our Pathway and Expression Analysis tools enable ID mapping, pathway assignment and overrepresentation analysis of user-supplied data sets. To support pathway annotation and analysis in other species, we continue to make orthology-based inferences of pathways in non-human species, applying Ensembl Compara to identify orthologs of curated human proteins in each of 20 other species. The resulting inferred pathway sets can be browsed and analyzed with our Species Comparison tool. Collaborations are also underway to create manually curated data sets on the Reactome framework for chicken, Drosophila and rice.",c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f1068,2004,2004-10-16
s1328,p1328,GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database,Abstract Summary The Genome Taxonomy Database Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the GTDB. GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10 156 bacterial and archaeal metagenome-assembled genomes. Availability and implementation GTDB-Tk is implemented in Python and licenced under the GNU General Public Licence v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtk. Supplementary information Supplementary data are available at Bioinformatics online.,c69,International Conference on Parallel Processing,cp69,accepted,f1069,2010,2010-07-08
s1329,p1329,Using MetaboAnalyst 4.0 for Comprehensive and Integrative Metabolomics Data Analysis,"MetaboAnalyst (https://www.metaboanalyst.ca) is an easy‐to‐use web‐based tool suite for comprehensive metabolomic data analysis, interpretation, and integration with other omics data. Since its first release in 2009, MetaboAnalyst has evolved significantly to meet the ever‐expanding bioinformatics demands from the rapidly growing metabolomics community. In addition to providing a variety of data processing and normalization procedures, MetaboAnalyst supports a wide array of functions for statistical, functional, as well as data visualization tasks. Some of the most widely used approaches include PCA (principal component analysis), PLS‐DA (partial least squares discriminant analysis), clustering analysis and visualization, MSEA (metabolite set enrichment analysis), MetPA (metabolic pathway analysis), biomarker selection via ROC (receiver operating characteristic) curve analysis, as well as time series and power analysis. The current version of MetaboAnalyst (4.0) features a complete overhaul of the user interface and significantly expanded underlying knowledge bases (compound database, pathway libraries, and metabolite sets). Three new modules have been added to support pathway activity prediction directly from mass peaks, biomarker meta‐analysis, and network‐based multi‐omics data integration. To enable more transparent and reproducible analysis of metabolomic data, we have released a companion R package (MetaboAnalystR) to complement the web‐based application. This article provides an overview of the main functional modules and the general workflow of MetaboAnalyst 4.0, followed by 12 detailed protocols: © 2019 by John Wiley & Sons, Inc.",c60,IEEE International Conference on Software Engineering and Formal Methods,cp60,accepted,f1070,2011,2011-07-04
s1330,p1330,GAPIT: genome association and prediction integrated tool,"SUMMARY
Software programs that conduct genome-wide association studies and genomic prediction and selection need to use methodologies that maximize statistical power, provide high prediction accuracy and run in a computationally efficient manner. We developed an R package called Genome Association and Prediction Integrated Tool (GAPIT) that implements advanced statistical methods including the compressed mixed linear model (CMLM) and CMLM-based genomic prediction and selection. The GAPIT package can handle large datasets in excess of 10 000 individuals and 1 million single-nucleotide polymorphisms with minimal computational time, while providing user-friendly access and concise tables and graphs to interpret results.


AVAILABILITY
http://www.maizegenetics.net/GAPIT.


CONTACT
zhiwu.zhang@cornell.edu


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",c72,Intelligent Systems in Molecular Biology,cp72,accepted,f1071,2003,2003-05-17
s1331,p1331,ChromHMM: automating chromatin-state discovery and characterization,Abstract content goes here ...,j6,Nature Methods,jv6,accepted,f1072,2010,2010-01-10
s1332,p1332,Gene Ontology Consortium: going forward,"The Gene Ontology (GO; http://www.geneontology.org) is a community-based bioinformatics resource that supplies information about gene product function using ontologies to represent biological knowledge. Here we describe improvements and expansions to several branches of the ontology, as well as updates that have allowed us to more efficiently disseminate the GO and capture feedback from the research community. The Gene Ontology Consortium (GOC) has expanded areas of the ontology such as cilia-related terms, cell-cycle terms and multicellular organism processes. We have also implemented new tools for generating ontology terms based on a set of logical rules making use of templates, and we have made efforts to increase our use of logical definitions. The GOC has a new and improved web site summarizing new developments and documentation, serving as a portal to GO data. Users can perform GO enrichment analysis, and search the GO for terms, annotations to gene products, and associated metadata across multiple species using the all-new AmiGO 2 browser. We encourage and welcome the input of the research community in all biological areas in our continued effort to improve the Gene Ontology.",c78,Neural Information Processing Systems,cp78,accepted,f1073,2012,2012-11-25
s1333,p1333,Scalable web services for the PSIPRED Protein Analysis Workbench,"Here, we present the new UCL Bioinformatics Group’s PSIPRED Protein Analysis Workbench. The Workbench unites all of our previously available analysis methods into a single web-based framework. The new web portal provides a greatly streamlined user interface with a number of new features to allow users to better explore their results. We offer a number of additional services to enable computationally scalable execution of our prediction methods; these include SOAP and XML-RPC web server access and new HADOOP packages. All software and services are available via the UCL Bioinformatics Group website at http://bioinf.cs.ucl.ac.uk/.",c88,Symposium on the Theory of Computing,cp88,accepted,f1074,2014,2014-10-21
s1334,p1334,"The Human Gene Mutation Database: building a comprehensive mutation repository for clinical and molecular genetics, diagnostic testing and personalized genomic medicine",Abstract content goes here ...,j250,Human Genetics,jv250,accepted,f1075,2001,2001-03-09
s1336,p1336,ShinyGO: a graphical gene-set enrichment tool for animals and plants,"MOTIVATION
Gene lists are routinely produced from various genome-wide studies. Enrichment analysis can link these gene lists with underlying molecular pathways and functional categories such as gene ontology (GO) and other databases.


RESULTS
To complement existing tools, we developed ShinyGO based on a large annotation database derived from Ensembl and STRING-db for 59 plant, 256 animal, 115 archaeal, and 1678 bacterial species. ShinyGO's novel features include graphical visualization of enrichment results and gene characteristics, and application program interface (API) access to KEGG and STRING for the retrieval of pathway diagrams and protein-protein interaction networks. ShinyGO is an intuitive, graphical web application that can help researchers gain actionable insights from gene lists.


AVAILABILITY
http://ge-lab.org/go/.


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",c59,British Computer Society Conference on Human-Computer Interaction,cp59,accepted,f1076,2019,2019-07-02
s1337,p1337,The Austronesian Basic Vocabulary Database: From Bioinformatics to Lexomics,"Phylogenetic methods have revolutionised evolutionary biology and have recently been applied to studies of linguistic and cultural evolution. However, the basic comparative data on the languages of the world required for these analyses is often widely dispersed in hard to obtain sources. Here we outline how our Austronesian Basic Vocabulary Database (ABVD) helps remedy this situation by collating wordlists from over 500 languages into one web-accessible database. We describe the technology underlying the ABVD and discuss the benefits that an evolutionary bioinformatic approach can provide. These include facilitating computational comparative linguistic research, answering questions about human prehistory, enabling syntheses with genetic data, and safe-guarding fragile linguistic information.",c74,IEEE International Conference on Tools with Artificial Intelligence,cp74,accepted,f1077,2003,2003-08-04
s1338,p1338,The European Bioinformatics Institute’s data resources,"The wide uptake of next-generation sequencing and other ultra-high throughput technologies by life scientists with a diverse range of interests, spanning fundamental biological research, medicine, agriculture and environmental science, has led to unprecedented growth in the amount of data generated. It has also put the need for unrestricted access to biological data at the centre of biology. The European Bioinformatics Institute (EMBL-EBI) is unique in Europe and is one of only two organisations worldwide providing access to a comprehensive, integrated set of these collections. Here, we describe how the EMBL-EBI’s biomolecular databases are evolving to cope with increasing levels of submission, a growing and diversifying user base, and the demand for new types of data. All of the resources described here can be accessed from the EMBL-EBI website: http://www.ebi.ac.uk",c84,The Web Conference,cp84,accepted,f1078,2006,2006-03-27
s1340,p1340,"Epidemiological, clinical and virological characteristics of 74 cases of coronavirus-infected disease 2019 (COVID-19) with gastrointestinal symptoms","Objective The SARS-CoV-2-infected disease (COVID-19) outbreak is a major threat to human beings. Previous studies mainly focused on Wuhan and typical symptoms. We analysed 74 confirmed COVID-19 cases with GI symptoms in the Zhejiang province to determine epidemiological, clinical and virological characteristics. Design COVID-19 hospital patients were admitted in the Zhejiang province from 17 January 2020 to 8 February 2020. Epidemiological, demographic, clinical, laboratory, management and outcome data of patients with GI symptoms were analysed using multivariate analysis for risk of severe/critical type. Bioinformatics were used to analyse features of SARS-CoV-2 from Zhejiang province. Results Among enrolled 651 patients, 74 (11.4%) presented with at least one GI symptom (nausea, vomiting or diarrhoea), average age of 46.14 years, 4-day incubation period and 10.8% had pre-existing liver disease. Of patients with COVID-19 with GI symptoms, 17 (22.97%) and 23 (31.08%) had severe/critical types and family clustering, respectively, significantly higher than those without GI symptoms, 47 (8.14%) and 118 (20.45%). Of patients with COVID-19 with GI symptoms, 29 (39.19%), 23 (31.08%), 8 (10.81%) and 16 (21.62%) had significantly higher rates of fever >38.5°C, fatigue, shortness of breath and headache, respectively. Low-dose glucocorticoids and antibiotics were administered to 14.86% and 41.89% of patients, respectively. Sputum production and increased lactate dehydrogenase/glucose levels were risk factors for severe/critical type. Bioinformatics showed sequence mutation of SARS-CoV-2 with m6A methylation and changed binding capacity with ACE2. Conclusion We report COVID-19 cases with GI symptoms with novel features outside Wuhan. Attention to patients with COVID-19 with non-classic symptoms should increase to protect health providers.",j251,Gut,jv251,accepted,f1079,2005,2005-12-17
s1341,p1341,TAMBIS: Transparent Access to Multiple Bioinformatics Information Sources,"UNLABELLED
TAMBIS (Transparent Access to Multiple Bioinformatics Information Sources) is an application that allows biologists to ask rich and complex questions over a range of bioinformatics resources. It is based on a model of the knowledge of the concepts and their relationships in molecular biology and bioinformatics.


AVAILABILITY
TAMBIS is available as an applet from http://img.cs.man.ac.uk/tambis SUPPLEMENTARY: A full manual, tutorial and videos can be found at http://img.cs.man.ac.uk/tambis.


CONTACT
tambis@cs.man.ac.uk",c72,Intelligent Systems in Molecular Biology,cp72,accepted,f1080,2003,2003-02-18
s1342,p1342,Ontology-based Knowledge Representation for Bioinformatics,"Much of biology works by applying prior knowledge ('what is known') to an unknown entity, rather than the application of a set of axioms that will elicit knowledge. In addition, the complex biological data stored in bioinformatics databases often require the addition of knowledge to specify and constrain the values held in that database. One way of capturing knowledge within bioinformatics applications and databases is the use of ontologies. An ontology is the concrete form of a conceptualisation of a community's knowledge of a domain. This paper aims to introduce the reader to the use of ontologies within bioinformatics. A description of the type of knowledge held in an ontology will be given.The paper will be illustrated throughout with examples taken from bioinformatics and molecular biology, and a survey of current biological ontologies will be presented. From this it will be seen that the use to which the ontology is put largely determines the content of the ontology. Finally, the paper will describe the process of building an ontology, introducing the reader to the techniques and methods currently in use and the open research questions in ontology development.",c32,International Conference on Software Technology: Methods and Tools,cp32,accepted,f1081,2010,2010-04-24
s1343,p1343,bold: The Barcode of Life Data System (http://www.barcodinglife.org),"The Barcode of Life Data System (bold) is an informatics workbench aiding the acquisition, storage, analysis and publication of DNA barcode records. By assembling molecular, morphological and distributional data, it bridges a traditional bioinformatics chasm. bold is freely available to any researcher with interests in DNA barcoding. By providing specialized services, it aids the assembly of records that meet the standards needed to gain BARCODE designation in the global sequence databases. Because of its web-based delivery and flexible data security model, it is also well positioned to support projects that involve broad research alliances. This paper provides a brief introduction to the key elements of bold, discusses their functional capabilities, and concludes by examining computational resources and future prospects.",j252,Molecular Ecology Notes,jv252,accepted,f1082,2009,2009-09-23
s1345,p1345,Bioinformatics for DNA Sequence Analysis,Abstract content goes here ...,c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f1083,2017,2017-11-16
s1346,p1346,Identification and analysis of occludin phosphosites: a combined mass spectrometry and bioinformatics approach.,"The molecular function of occludin, an integral membrane component of tight junctions, remains unclear. VEGF-induced phosphorylation sites were mapped on occludin by combining MS data analysis with bioinformatics. In vivo phosphorylation of Ser490 was validated and protein interaction studies combined with crystal structure analysis suggest that Ser490 phosphorylation attenuates the interaction between occludin and ZO-1. This study demonstrates that combining MS data and bioinformatics can successfully identify novel phosphorylation sites from limiting samples.",j253,Journal of Proteome Research,jv253,accepted,f1084,2003,2003-04-04
s1347,p1347,Representation Learning on Graphs with Jumping Knowledge Networks,"Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of ""neighboring"" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance.",c75,International Conference on Machine Learning,cp75,accepted,f1085,2005,2005-01-08
s1348,p1348,DrugBank: a comprehensive resource for in silico drug discovery and exploration,"DrugBank is a unique bioinformatics/cheminformatics resource that combines detailed drug (i.e. chemical) data with comprehensive drug target (i.e. protein) information. The database contains >4100 drug entries including >800 FDA approved small molecule and biotech drugs as well as >3200 experimental drugs. Additionally, >14,000 protein or drug target sequences are linked to these drug entries. Each DrugCard entry contains >80 data fields with half of the information being devoted to drug/chemical data and the other half devoted to drug target or protein data. Many data fields are hyperlinked to other databases (KEGG, PubChem, ChEBI, PDB, Swiss-Prot and GenBank) and a variety of structure viewing applets. The database is fully searchable supporting extensive text, sequence, chemical structure and relational query searches. Potential applications of DrugBank include in silico drug target discovery, drug design, drug docking or screening, drug metabolism prediction, drug interaction prediction and general pharmaceutical education. DrugBank is available at http://redpoll.pharmacy.ualberta.ca/drugbank/.",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f1086,2008,2008-07-21
s1349,p1349,Bioinformatics : applications in life and environmental sciences,Abstract content goes here ...,c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f1087,2006,2006-07-23
s1350,p1350,A Quick Guide for Developing Effective Bioinformatics Programming Skills,"Bioinformatics programming skills are becoming a necessity across many facets of biology and medicine, owed in part to the continuing explosion of biological data aggregation and the complexity and scale of questions now being addressed through modern bioinformatics. Although many are now receiving formal training in bioinformatics through various university degree and certificate programs, this training is often focused strongly on bioinformatics methodology, leaving many important and practical aspects of bioinformatics to self-education and experience. The following set of guidelines distill several key principals of effective bioinformatics programming, which the authors learned through insights gained across many years of combined experience developing popular bioinformatics software applications and database systems in both academic and commercial settings [1]–[6]. Successful adoption of these principals will serve both beginner and experienced bioinformaticians alike in career development and pursuit of professional and scientific goals.",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f1088,2002,2002-03-24
s1351,p1351,Bioinformatics analysis of microarray data.,Abstract content goes here ...,c109,International Conference on Mobile Data Management,cp109,accepted,f1089,2014,2014-12-27
s1353,p1353,Nonnegative Matrix and Tensor Factorizations - Applications to Exploratory Multi-way Data Analysis and Blind Source Separation,"This book provides a broad survey of models and efficient algorithms for Nonnegative Matrix Factorization (NMF). This includes NMFs various extensions and modifications, especially Nonnegative Tensor Factorizations (NTF) and Nonnegative Tucker Decompositions (NTD). NMF/NTF and their extensions are increasingly used as tools in signal and image processing, and data analysis, having garnered interest due to their capability to provide new insights and relevant information about the complex latent relationships in experimental data sets. It is suggested that NMF can provide meaningful components with physical interpretations; for example, in bioinformatics, NMF and its extensions have been successfully applied to gene expression, sequence analysis, the functional characterization of genes, clustering and text mining. As such, the authors focus on the algorithms that are most useful in practice, looking at the fastest, most robust, and suitable for large-scale models. Key features: Acts as a single source reference guide to NMF, collating information that is widely dispersed in current literature, including the authors own recently developed techniques in the subject area. Uses generalized cost functions such as Bregman, Alpha and Beta divergences, to present practical implementations of several types of robust algorithms, in particular Multiplicative, Alternating Least Squares, Projected Gradient and Quasi Newton algorithms. Provides a comparative analysis of the different methods in order to identify approximation error and complexity. Includes pseudo codes and optimized MATLAB source codes for almost all algorithms presented in the book. The increasing interest in nonnegative matrix and tensor factorizations, as well as decompositions and sparse representation of data, will ensure that this book is essential reading for engineers, scientists, researchers, industry practitioners and graduate students across signal and image processing; neuroscience; data mining and data analysis; computer science; bioinformatics; speech processing; biomedical engineering; and multimedia.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f1090,2011,2011-11-05
s1354,p1354,Nitric oxide-responsive genes and promoters in Arabidopsis thaliana: a bioinformatics approach.,"Due to its high reactivity and its ability to diffuse and permeate the cell membrane, nitric oxide (NO) and its exchangeable redox-activated species are unique biological messengers in animals and in plants. Although an increasing number of reports indicate that NO is an essential molecule in several physiological processes, there is not a clear picture of its method of action. Studies on the transcriptional changes induced by NO permitted identification of genes involved in different functional processes such as signal transduction, defence and cell death, transport, basic metabolism, and reactive oxygen species (ROS) production and degradation. The co-expression of these genes can be explained by the co-operation of a set of transcription factors that bind a common region in the promoter of the regulated genes. The present report describes the search for a common transcription factor-binding site (TFBS) in promoter regions of NO-regulated genes, based on microarray analyses. Using Genomatix Gene2Promotor and MatInspector, eight families of TFBSs were found to occur at least 15% more often in the promoter regions of the responsive genes in comparison with the promoter regions of 28,447 Arabidopsis control genes. Most of these TFBSs, such as ocs element-like sequences and WRKY, have already been reported to be involved in particular stress responses. Furthermore, the promoter regions of genes involved in jasmonic acid (JA) biosynthesis were analysed for a common TFBS module, since some genes responsible for JA biosynthesis are induced by NO, and an interaction between NO and JA signalling has already been described.",j255,Journal of Experimental Botany,jv255,accepted,f1091,2010,2010-10-11
s1355,p1355,Bioinformatics construction of the human cell surfaceome,"Cell surface proteins are excellent targets for diagnostic and therapeutic interventions. By using bioinformatics tools, we generated a catalog of 3,702 transmembrane proteins located at the surface of human cells (human cell surfaceome). We explored the genetic diversity of the human cell surfaceome at different levels, including the distribution of polymorphisms, conservation among eukaryotic species, and patterns of gene expression. By integrating expression information from a variety of sources, we were able to identify surfaceome genes with a restricted expression in normal tissues and/or differential expression in tumors, important characteristics for putative tumor targets. A high-throughput and efficient quantitative real-time PCR approach was used to validate 593 surfaceome genes selected on the basis of their expression pattern in normal and tumor samples. A number of candidates were identified as potential diagnostic and therapeutic targets for colorectal tumors and glioblastoma. Several candidate genes were also identified as coding for cell surface cancer/testis antigens. The human cell surfaceome will serve as a reference for further studies aimed at characterizing tumor targets at the surface of human cells.",j20,Proceedings of the National Academy of Sciences of the United States of America,jv20,accepted,f1092,2006,2006-06-14
s1356,p1356,Biodoop: Bioinformatics on Hadoop,"Bioinformatics applications currently require both processing of huge amounts of data and heavy computation. Fulfilling these requirements calls for simple ways to implement parallel computing. MapReduce is a general-purpose parallelization model that seems particularly well-suited to this task and for which an open source implementation (Hadoop) is available. Here we report on its application to three relevant algorithms: BLAST, GSEA and GRAMMAR. The first is characterized by relatively low-weight computation on large data sets, while the second requires heavy processing of relatively small data sets. The third one can be considered as containing a mixture of these two computational flavors. Our results are encouraging and indicate that the framework could have a wide range of bioinformatics applications while maintaining good computational efficiency, scalability and ease of maintenance.",c37,Asia-Pacific Software Engineering Conference,cp37,accepted,f1093,2008,2008-09-25
s1357,p1357,XMPP for cloud computing in bioinformatics supporting discovery and invocation of asynchronous web services,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1094,2007,2007-11-28
s1358,p1358,PROVEAN web server: a tool to predict the functional effect of amino acid substitutions and indels,"UNLABELLED
We present a web server to predict the functional effect of single or multiple amino acid substitutions, insertions and deletions using the prediction tool PROVEAN. The server provides rapid analysis of protein variants from any organisms, and also supports high-throughput analysis for human and mouse variants at both the genomic and protein levels.


AVAILABILITY AND IMPLEMENTATION
The web server is freely available and open to all users with no login requirements at http://provean.jcvi.org.


CONTACT
achan@jcvi.org


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",c95,IEEE International Conference on Computer Vision,cp95,accepted,f1095,2017,2017-03-23
s1359,p1359,Bandage: interactive visualization of de novo genome assemblies,"Summary While de novo assembly graphs contain assembled contigs (nodes), the connections between those contigs (edges) are difficult for users to access. Bandage (a Bioinformatics Application for Navigating De novo Assembly Graphs Easily) is a tool for visualising assembly graphs with connections. Users can zoom in to specific areas of the graph and interact with it by moving nodes, adding labels, changing colours and extracting sequences. BLAST searches can be performed within the Bandage GUI and the hits are displayed as highlights in the graph. By displaying connections between contigs, Bandage presents new possibilities for analysing de novo assemblies that are not possible through investigation of contigs alone. Availability and implementation Source code and binaries are freely available at https://github.com/rrwick/Bandage. Bandage is implemented in C++ and supported on Linux, OS X and Windows. Contact rrwick@gmail.com Supplementary information A full feature list and screenshots are available at Bioinformatics online and http://rrwick.github.io/Bandage.",j153,bioRxiv,jv153,accepted,f1096,2020,2020-05-24
s1361,p1361,Pathview: an R/Bioconductor package for pathway-based data integration and visualization,"Summary: Pathview is a novel tool set for pathway-based data integration and visualization. It maps and renders user data on relevant pathway graphs. Users only need to supply their data and specify the target pathway. Pathview automatically downloads the pathway graph data, parses the data file, maps and integrates user data onto the pathway and renders pathway graphs with the mapped data. Although built as a stand-alone program, Pathview may seamlessly integrate with pathway and functional analysis tools for large-scale and fully automated analysis pipelines. Availability: The package is freely available under the GPLv3 license through Bioconductor and R-Forge. It is available at http://bioconductor.org/packages/release/bioc/html/pathview.html and at http://Pathview.r-forge.r-project.org/. Contact: luo_weijun@yahoo.com Supplementary information: Supplementary data are available at Bioinformatics online.",c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f1097,2018,2018-10-24
s1362,p1362,AliView: a fast and lightweight alignment viewer and editor for large datasets,"Summary: AliView is an alignment viewer and editor designed to meet the requirements of next-generation sequencing era phylogenetic datasets. AliView handles alignments of unlimited size in the formats most commonly used, i.e. FASTA, Phylip, Nexus, Clustal and MSF. The intuitive graphical interface makes it easy to inspect, sort, delete, merge and realign sequences as part of the manual filtering process of large datasets. AliView also works as an easy-to-use alignment editor for small as well as large datasets. Availability and implementation: AliView is released as open-source software under the GNU General Public License, version 3.0 (GPLv3), and is available at GitHub (www.github.com/AliView). The program is cross-platform and extensively tested on Linux, Mac OS X and Windows systems. Downloads and help are available at http://ormbunkar.se/aliview Contact: anders.larsson@ebc.uu.se Supplementary information: Supplementary data are available at Bioinformatics online.",c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f1098,2015,2015-06-14
s1365,p1365,TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data,"The Cancer Genome Atlas (TCGA) research network has made public a large collection of clinical and molecular phenotypes of more than 10 000 tumor patients across 33 different tumor types. Using this cohort, TCGA has published over 20 marker papers detailing the genomic and epigenomic alterations associated with these tumor types. Although many important discoveries have been made by TCGA's research network, opportunities still exist to implement novel methods, thereby elucidating new biological pathways and diagnostic markers. However, mining the TCGA data presents several bioinformatics challenges, such as data retrieval and integration with clinical data and other molecular data types (e.g. RNA and DNA methylation). We developed an R/Bioconductor package called TCGAbiolinks to address these challenges and offer bioinformatics solutions by using a guided workflow to allow users to query, download and perform integrative analyses of TCGA data. We combined methods from computer science and statistics into the pipeline and incorporated methodologies developed in previous TCGA marker studies and in our own group. Using four different TCGA tumor types (Kidney, Brain, Breast and Colon) as examples, we provide case studies to illustrate examples of reproducibility, integrative analysis and utilization of different Bioconductor packages to advance and accelerate novel discoveries.",j102,Nucleic Acids Research,jv102,accepted,f1099,2002,2002-01-20
s1366,p1366,RASTtk: A modular and extensible implementation of the RAST algorithm for building custom annotation pipelines and annotating batches of genomes,Abstract content goes here ...,j130,Scientific Reports,jv130,accepted,f1100,2006,2006-01-10
s1367,p1367,Tools and Algorithms for the Construction and Analysis of Systems,Abstract content goes here ...,j75,Lecture Notes in Computer Science,jv75,accepted,f1101,2015,2015-03-08
s1369,p1369,BIRI: a new approach for automatically discovering and indexing available public bioinformatics resources from the literature,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1102,2007,2007-02-05
s1371,p1371,BOLD : The Barcode of Life Data System,"The Barcode of Life Data System ( BOLD ) is an informatics workbench aiding the acquisition, storage, analysis and publication of DNA barcode records. By assembling molecular, morphological and distributional data, it bridges a traditional bioinformatics chasm. BOLD is freely available to any researcher with interests in DNA barcoding. By providing specialized services, it aids the assembly of records that meet the standards needed to gain BARCODE designation in the global sequence databases. Because of its web-based delivery and flexible data security model, it is also well positioned to support projects that involve broad research alliances. This paper provides a brief introduction to the key elements of BOLD , discusses their functional capabilities, and concludes by examining computational resources and future prospects.",c61,Jahrestagung der Gesellschaft für Informatik,cp61,accepted,f1103,2017,2017-09-19
s1372,p1372,SNAP: a web-based tool for identification and annotation of proxy SNPs using HapMap,"SUMMARY
The interpretation of genome-wide association results is confounded by linkage disequilibrium between nearby alleles. We have developed a flexible bioinformatics query tool for single-nucleotide polymorphisms (SNPs) to identify and to annotate nearby SNPs in linkage disequilibrium (proxies) based on HapMap. By offering functionality to generate graphical plots for these data, the SNAP server will facilitate interpretation and comparison of genome-wide association study results, and the design of fine-mapping experiments (by delineating genomic regions harboring associated variants and their proxies).


AVAILABILITY
SNAP server is available at http://www.broad.mit.edu/mpg/snap/.",c57,IEEE International Conference on Engineering of Complex Computer Systems,cp57,accepted,f1104,2003,2003-01-03
s1374,p1374,"Bias in random forest variable importance measures: Illustrations, sources and a solution",Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1105,2007,2007-10-28
s1375,p1375,PRGdb: a bioinformatics platform for plant resistance gene analysis,"PRGdb is a web accessible open-source (http://www.prgdb.org) database that represents the first bioinformatic resource providing a comprehensive overview of resistance genes (R-genes) in plants. PRGdb holds more than 16 000 known and putative R-genes belonging to 192 plant species challenged by 115 different pathogens and linked with useful biological information. The complete database includes a set of 73 manually curated reference R-genes, 6308 putative R-genes collected from NCBI and 10463 computationally predicted putative R-genes. Thanks to a user-friendly interface, data can be examined using different query tools. A home-made prediction pipeline called Disease Resistance Analysis and Gene Orthology (DRAGO), based on reference R-gene sequence data, was developed to search for plant resistance genes in public datasets such as Unigene and Genbank. New putative R-gene classes containing unknown domain combinations were discovered and characterized. The development of the PRG platform represents an important starting point to conduct various experimental tasks. The inferred cross-link between genomic and phenotypic information allows access to a large body of information to find answers to several biological questions. The database structure also permits easy integration with other data types and opens up prospects for future implementations.",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1106,2015,2015-11-05
s1377,p1377,Modeling and Simulation of Genetic Regulatory Systems: A Literature Review,"In order to understand the functioning of organisms on the molecular level, we need to know which genes are expressed, when and where in the organism, and to which extent. The regulation of gene expression is achieved through genetic regulatory systems structured by networks of interactions between DNA, RNA, proteins, and small molecules. As most genetic regulatory networks of interest involve many components connected through interlocking positive and negative feedback loops, an intuitive understanding of their dynamics is hard to obtain. As a consequence, formal methods and computer tools for the modeling and simulation of genetic regulatory networks will be indispensable. This paper reviews formalisms that have been employed in mathematical biology and bioinformatics to describe genetic regulatory systems, in particular directed graphs, Bayesian networks, Boolean networks and their generalizations, ordinary and partial differential equations, qualitative differential equations, stochastic equations, an...",c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f1107,2015,2015-11-28
s1378,p1378,Web Services at the European Bioinformatics Institute,"We present a new version of the European Bioinformatics Institute Web Services, a complete suite of SOAP-based web tools for structural and functional analysis, with new and improved applications. New functionality has been added to most of the services already available, and an improved version of the underlying framework has allowed us to include more applications. Information on the EBI Web Services, tutorials and clients can be found at http://www.ebi.ac.uk/Tools/webservices.",c3,Frontiers in Education Conference,cp3,accepted,f1108,2016,2016-02-21
s1380,p1380,Blast2GO: A Comprehensive Suite for Functional Analysis in Plant Genomics,"Functional annotation of novel sequence data is a primary requirement for the utilization of functional genomics approaches in plant research. In this paper, we describe the Blast2GO suite as a comprehensive bioinformatics tool for functional annotation of sequences and data mining on the resulting annotations, primarily based on the gene ontology (GO) vocabulary. Blast2GO optimizes function transfer from homologous sequences through an elaborate algorithm that considers similarity, the extension of the homology, the database of choice, the GO hierarchy, and the quality of the original annotations. The tool includes numerous functions for the visualization, management, and statistical analysis of annotation results, including gene set enrichment analysis. The application supports InterPro, enzyme codes, KEGG pathways, GO direct acyclic graphs (DAGs), and GOSlim. Blast2GO is a suitable tool for plant genomics research because of its versatility, easy installation, and friendly use.",j257,International Journal of Plant Genomics,jv257,accepted,f1109,2004,2004-09-29
s1381,p1381,"The Sanger FASTQ file format for sequences with quality scores, and the Solexa/Illumina FASTQ variants","FASTQ has emerged as a common file format for sharing sequencing read data combining both the sequence and an associated per base quality score, despite lacking any formal definition to date, and existing in at least three incompatible variants. This article defines the FASTQ format, covering the original Sanger standard, the Solexa/Illumina variants and conversion between them, based on publicly available information such as the MAQ documentation and conventions recently agreed by the Open Bioinformatics Foundation projects Biopython, BioPerl, BioRuby, BioJava and EMBOSS. Being an open access publication, it is hoped that this description, with the example files provided as Supplementary Data, will serve in future as a reference for this important file format.",j102,Nucleic Acids Research,jv102,accepted,f1110,2002,2002-10-28
s1382,p1382,Cloning of a human parvovirus by molecular screening of respiratory tract samples.,"The identification of new virus species is a key issue for the study of infectious disease but is technically very difficult. We developed a system for large-scale molecular virus screening of clinical samples based on host DNA depletion, random PCR amplification, large-scale sequencing, and bioinformatics. The technology was applied to pooled human respiratory tract samples. The first experiments detected seven human virus species without the use of any specific reagent. Among the detected viruses were one coronavirus and one parvovirus, both of which were at that time uncharacterized. The parvovirus, provisionally named human bocavirus, was in a retrospective clinical study detected in 17 additional patients and associated with lower respiratory tract infections in children. The molecular virus screening procedure provides a general culture-independent solution to the problem of detecting unknown virus species in single or pooled samples. We suggest that a systematic exploration of the viruses that infect humans, ""the human virome,"" can be initiated.",j20,Proceedings of the National Academy of Sciences of the United States of America,jv20,accepted,f1111,2006,2006-11-13
s1385,p1385,A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics,"Inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. Clearly, the widely used standard covariance and correlation estimators are ill-suited for this purpose. As statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the Ledoit-Wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity.Subsequently, we apply this improved covariance estimator (which has guaranteed minimum mean squared error, is well-conditioned, and is always positive definite even for small sample sizes) to the problem of inferring large-scale gene association networks. We show that it performs very favorably compared to competing approaches both in simulations as well as in application to real expression data.",j258,Statistical Applications in Genetics and Molecular Biology,jv258,accepted,f1112,2018,2018-10-31
s1387,p1387,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,cp99,accepted,f1113,2015,2015-12-03
s1388,p1388,Ontology-driven indexing of public datasets for translational bioinformatics,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1114,2007,2007-01-12
s1389,p1389,Bioinformatics for next generation sequencing.,Abstract content goes here ...,j260,Bioinformatics,jv260,accepted,f1115,2010,2010-08-05
s1390,p1390,An Extensive Class of Small RNAs in Caenorhabditis elegans,"The lin-4 and let-7 antisense RNAs are temporal regulators that control the timing of developmental events inCaenorhabditis elegans by inhibiting translation of target mRNAs. let-7 RNA is conserved among bilaterian animals, suggesting that this class of small RNAs [microRNAs (miRNAs)] is evolutionarily ancient. Using bioinformatics and cDNA cloning, we found 15 new miRNA genes in C. elegans. Several of these genes express small transcripts that vary in abundance during C. elegans larval development, and three of them have apparent homologs in mammals and/or insects. Small noncoding RNAs of the miRNA class appear to be numerous and diverse.",j97,Science,jv97,accepted,f1116,2012,2012-01-03
s1391,p1391,MITOMASTER: a bioinformatics tool for the analysis of mitochondrial DNA sequences,"We have developed a computer system, MITOMASTER, to make analysis of human mitochondrial DNA (mtDNA) sequences efficient, accurate, and easily available. From imported sequences, the system identifies nucleotide variants, determines the haplogroup, rules out possible pseudogene contamination, identifies novel DNA sequence variants, and evaluates the potential biological significance of each variant. This system should be beneficial for mtDNA analyses of biomedical physicians and investigators, population biologists and forensic scientists. MITOMASTER can be accessed at http://mammag.web.uci.edu/twiki/bin/view/Mitomaster. Hum Mutat 0,1–6, 2008. © 2008 Wiley‐Liss, Inc.",j209,Human Mutation,jv209,accepted,f1117,2005,2005-07-02
s1392,p1392,A Survey on Deep Transfer Learning,Abstract content goes here ...,c76,International Conference on Artificial Neural Networks,cp76,accepted,f1118,2013,2013-01-10
s1394,p1394,ThunderSTORM: a comprehensive ImageJ plug-in for PALM and STORM data analysis and super-resolution imaging,"Summary: ThunderSTORM is an open-source, interactive and modular plug-in for ImageJ designed for automated processing, analysis and visualization of data acquired by single-molecule localization microscopy methods such as photo-activated localization microscopy and stochastic optical reconstruction microscopy. ThunderSTORM offers an extensive collection of processing and post-processing methods so that users can easily adapt the process of analysis to their data. ThunderSTORM also offers a set of tools for creation of simulated data and quantitative performance evaluation of localization algorithms using Monte Carlo simulations. Availability and implementation: ThunderSTORM and the online documentation are both freely accessible at https://code.google.com/p/thunder-storm/ Contact: guy.hagen@lf1.cuni.cz Supplementary information: Supplementary data are available at Bioinformatics online.",c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,cp86,accepted,f1119,2012,2012-01-12
s1395,p1395,BOLD : The Barcode of Life Data System (www.barcodinglife.org),"The Barcode of Life Data System ( BOLD ) is an informatics workbench aiding the acquisition, storage, analysis and publication of DNA barcode records. By assembling molecular, morphological and distributional data, it bridges a traditional bioinformatics chasm. BOLD is freely available to any researcher with interests in DNA barcoding. By providing specialized services, it aids the assembly of records that meet the standards needed to gain BARCODE designation in the global sequence databases. Because of its web-based delivery and flexible data security model, it is also well positioned to support projects that involve broad research alliances. This paper provides a brief introduction to the key elements of BOLD , discusses their functional capabilities, and concludes by examining computational resources and future prospects.",c31,International Conference on Evaluation & Assessment in Software Engineering,cp31,accepted,f1120,2022,2022-12-27
s1396,p1396,Activities at the Universal Protein Resource (UniProt),"The mission of the Universal Protein Resource (UniProt) (http://www.uniprot.org) is to provide the scientific community with a comprehensive, high-quality and freely accessible resource of protein sequences and functional annotation. It integrates, interprets and standardizes data from literature and numerous resources to achieve the most comprehensive catalog possible of protein information. The central activities are the biocuration of the UniProt Knowledgebase and the dissemination of these data through our Web site and web services. UniProt is produced by the UniProt Consortium, which consists of groups from the European Bioinformatics Institute (EBI), the SIB Swiss Institute of Bioinformatics (SIB) and the Protein Information Resource (PIR). UniProt is updated and distributed every 4 weeks and can be accessed online for searches or downloads.",c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f1121,2015,2015-06-19
s1397,p1397,The Bioperl toolkit: Perl modules for the life sciences.,"The Bioperl project is an international open-source collaboration of biologists, bioinformaticians, and computer scientists that has evolved over the past 7 yr into the most comprehensive library of Perl modules available for managing and manipulating life-science information. Bioperl provides an easy-to-use, stable, and consistent programming interface for bioinformatics application programmers. The Bioperl modules have been successfully and repeatedly used to reduce otherwise complex tasks to only a few lines of code. The Bioperl object model has been proven to be flexible enough to support enterprise-level applications such as EnsEMBL, while maintaining an easy learning curve for novice Perl programmers. Bioperl is capable of executing analyses and processing results from programs such as BLAST, ClustalW, or the EMBOSS suite. Interoperation with modules written in Python and Java is supported through the evolving BioCORBA bridge. Bioperl provides access to data stores such as GenBank and SwissProt via a flexible series of sequence input/output modules, and to the emerging common sequence data storage format of the Open Bioinformatics Database Access project. This study describes the overall architecture of the toolkit, the problem domains that it addresses, and gives specific examples of how the toolkit can be used to solve common life-sciences problems. We conclude with a discussion of how the open-source nature of the project has contributed to the development effort.",j187,Genome Research,jv187,accepted,f1122,2006,2006-03-29
s1398,p1398,"Oncomine 3.0: genes, pathways, and networks in a collection of 18,000 cancer gene expression profiles.","DNA microarrays have been widely applied to cancer transcriptome analysis; however, the majority of such data are not easily accessible or comparable. Furthermore, several important analytic approaches have been applied to microarray analysis; however, their application is often limited. To overcome these limitations, we have developed Oncomine, a bioinformatics initiative aimed at collecting, standardizing, analyzing, and delivering cancer transcriptome data to the biomedical research community. Our analysis has identified the genes, pathways, and networks deregulated across 18,000 cancer gene expression microarrays, spanning the majority of cancer types and subtypes. Here, we provide an update on the initiative, describe the database and analysis modules, and highlight several notable observations. Results from this comprehensive analysis are available at http://www.oncomine.org.",c15,International Conference on Conceptual Structures,cp15,accepted,f1123,2011,2011-11-23
s1399,p1399,Toward almost closed genomes with GapFiller,Abstract content goes here ...,j5,Genome Biology,jv5,accepted,f1124,2020,2020-05-28
s1400,p1400,The Proteomics Identifications (PRIDE) database and associated tools: status in 2013,"The PRoteomics IDEntifications (PRIDE, http://www.ebi.ac.uk/pride) database at the European Bioinformatics Institute is one of the most prominent data repositories of mass spectrometry (MS)-based proteomics data. Here, we summarize recent developments in the PRIDE database and related tools. First, we provide up-to-date statistics in data content, splitting the figures by groups of organisms and species, including peptide and protein identifications, and post-translational modifications. We then describe the tools that are part of the PRIDE submission pipeline, especially the recently developed PRIDE Converter 2 (new submission tool) and PRIDE Inspector (visualization and analysis tool). We also give an update about the integration of PRIDE with other MS proteomics resources in the context of the ProteomeXchange consortium. Finally, we briefly review the quality control efforts that are ongoing at present and outline our future plans.",c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f1125,2022,2022-07-12
s1401,p1401,Repetitive DNA and next-generation sequencing: computational challenges and solutions,Abstract content goes here ...,j175,Nature reviews genetics,jv175,accepted,f1126,2007,2007-08-13
s1402,p1402,The MPI Bioinformatics Toolkit for protein sequence analysis,"The MPI Bioinformatics Toolkit is an interactive web service which offers access to a great variety of public and in-house bioinformatics tools. They are grouped into different sections that support sequence searches, multiple alignment, secondary and tertiary structure prediction and classification. Several public tools are offered in customized versions that extend their functionality. For example, PSI-BLAST can be run against regularly updated standard databases, customized user databases or selectable sets of genomes. Another tool, Quick2D, integrates the results of various secondary structure, transmembrane and disorder prediction programs into one view. The Toolkit provides a friendly and intuitive user interface with an online help facility. As a key feature, various tools are interconnected so that the results of one tool can be forwarded to other tools. One could run PSI-BLAST, parse out a multiple alignment of selected hits and send the results to a cluster analysis tool. The Toolkit framework and the tools developed in-house will be packaged and freely available under the GNU Lesser General Public Licence (LGPL). The Toolkit can be accessed at .",c76,International Conference on Artificial Neural Networks,cp76,accepted,f1127,2013,2013-01-18
s1403,p1403,An Introduction to Conditional Random Fields,"Many tasks involve predicting a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling. They combine the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This survey describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in many areas, including natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large-scale CRFs. We do not assume previous knowledge of graphical modeling, so this survey is intended to be useful to practitioners in a wide variety of fields.",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f1128,2002,2002-05-01
s1404,p1404,GenomeScope: fast reference‐free genome profiling from short reads,"Summary: GenomeScope is an open‐source web tool to rapidly estimate the overall characteristics of a genome, including genome size, heterozygosity rate and repeat content from unprocessed short reads. These features are essential for studying genome evolution, and help to choose parameters for downstream analysis. We demonstrate its accuracy on 324 simulated and 16 real datasets with a wide range in genome sizes, heterozygosity levels and error rates. Availability and Implementation: http://genomescope.org, https://github.com/schatzlab/genomescope.git. Contact: mschatz@jhu.edu Supplementary information: Supplementary data are available at Bioinformatics online.",c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f1129,2006,2006-10-06
s1406,p1406,Understanding Bioinformatics,Abstract content goes here ...,j261,Biologia plantarum,jv261,accepted,f1130,2020,2020-04-04
s1408,p1408,[Structural bioinformatics].,Abstract content goes here ...,j262,"Tanpakushitsu kakusan koso. Protein, nucleic acid, enzyme",jv262,accepted,f1131,2011,2011-05-04
s1409,p1409,Parallel Clustering Algorithm for Large Data Sets with Applications in Bioinformatics,"Large sets of bioinformatical data provide a challenge in time consumption while solving the cluster identification problem, and that is why a parallel algorithm is so needed for identifying dense clusters in a noisy background. Our algorithm works on a graph representation of the data set to be analyzed. It identifies clusters through the identification of densely intraconnected subgraphs. We have employed a minimum spanning tree (MST) representation of the graph and solve the cluster identification problem using this representation. The computational bottleneck of our algorithm is the construction of an MST of a graph, for which a parallel algorithm is employed. Our high-level strategy for the parallel MST construction algorithm is to first partition the graph, then construct MSTs for the partitioned subgraphs and auxiliary bipartite graphs based on the subgraphs, and finally merge these MSTs to derive an MST of the original graph. The computational results indicate that when running on 150 CPUs, our algorithm can solve a cluster identification problem on a data set with 1,000,000 data points almost 100 times faster than on single CPU, indicating that this program is capable of handling very large data clustering problems in an efficient manner. We have implemented the clustering algorithm as the software CLUMP.",j219,IEEE/ACM Transactions on Computational Biology & Bioinformatics,jv219,accepted,f1132,2001,2001-05-15
s1410,p1410,From Protein Structure to Function with Bioinformatics,Abstract content goes here ...,c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1133,2015,2015-09-25
s1412,p1412,ChEBI: An Open Bioinformatics and Cheminformatics Resource,"Chemical Entities of Biological Interest (ChEBI) is a freely available dictionary of molecular entities focused on “small” chemical compounds. This unit provides a detailed guide to browsing, searching, downloading, and programmatic access to the ChEBI database. Curr. Protoc. Bioinform. 26:14.9.1‐14.9.20. © 2009 by John Wiley & Sons, Inc.",c43,ACM Symposium on Applied Computing,cp43,accepted,f1134,2001,2001-04-26
s1414,p1414,The Obesity-Associated FTO Gene Encodes a 2-Oxoglutarate-Dependent Nucleic Acid Demethylase,"Variants in the FTO (fat mass and obesity associated) gene are associated with increased body mass index in humans. Here, we show by bioinformatics analysis that FTO shares sequence motifs with Fe(II)- and 2-oxoglutarate–dependent oxygenases. We find that recombinant murine Fto catalyzes the Fe(II)- and 2OG-dependent demethylation of 3-methylthymine in single-stranded DNA, with concomitant production of succinate, formaldehyde, and carbon dioxide. Consistent with a potential role in nucleic acid demethylation, Fto localizes to the nucleus in transfected cells. Studies of wild-type mice indicate that Fto messenger RNA (mRNA) is most abundant in the brain, particularly in hypothalamic nuclei governing energy balance, and that Fto mRNA levels in the arcuate nucleus are regulated by feeding and fasting. Studies can now be directed toward determining the physiologically relevant FTO substrate and how nucleic acid methylation status is linked to increased fat mass.",j97,Science,jv97,accepted,f1135,2012,2012-08-26
s1415,p1415,Automated comparative protein structure modeling with SWISS‐MODEL and Swiss‐PdbViewer: A historical perspective,"SWISS‐MODEL pioneered the field of automated modeling as the first protein modeling service on the Internet. In combination with the visualization tool Swiss‐PdbViewer, the Internet‐based Workspace and the SWISS‐MODEL Repository, it provides a fully integrated sequence to structure analysis and modeling platform. This computational environment is made freely available to the scientific community with the aim to hide the computational complexity of structural bioinformatics and encourage bench scientists to make use of the ever‐increasing structural information available. Indeed, over the last decade, the availability of structural information has significantly increased for many organisms as a direct consequence of the complementary nature of comparative protein modeling and experimental structure determination. This has a very positive and enabling impact on many different applications in biomedical research as described in this paper.",j263,Electrophoresis,jv263,accepted,f1136,2001,2001-04-27
s1416,p1416,The abundance and variety of carbohydrate-active enzymes in the human gut microbiota,Abstract content goes here ...,j234,Nature Reviews Microbiology,jv234,accepted,f1137,2011,2011-07-09
s1417,p1417,Briefings in bioinformatics.,Abstract content goes here ...,j264,Briefings in Bioinformatics,jv264,accepted,f1138,2019,2019-03-03
s1419,p1419,R Programming for Bioinformatics,"From the co-developer of R and lead founder of the Bioconductor Project Thanks to its data handling and modeling capabilities and its flexibility, R is becoming the most widely used software in bioinformatics. R Programming for Bioinformatics builds the programming skills needed to use R for solving bioinformatics and computational biology problems. Drawing on the authors experiences as an R expert, the book begins with coverage on the general properties of the R language, several unique programming aspects of R, and object-oriented programming in R. It presents methods for data input and output as well as database interactions. The author also examines different facets of string handling and manipulations, discusses the interfacing of R with other languages, and describes how to write software packages. He concludes with a discussion on the debugging and profiling of R code.",c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,cp86,accepted,f1139,2012,2012-05-24
s1420,p1420,A comparison of common programming languages used in bioinformatics,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1140,2007,2007-12-03
s1421,p1421,Bringing Web 2.0 to bioinformatics,"Enabling deft data integration from numerous, voluminous and heterogeneous data sources is a major bioinformatic challenge. Several approaches have been proposed to address this challenge, including data warehousing and federated databasing. Yet despite the rise of these approaches, integration of data from multiple sources remains problematic and toilsome. These two approaches follow a user-to-computer communication model for data exchange, and do not facilitate a broader concept of data sharing or collaboration among users. In this report, we discuss the potential of Web 2.0 technologies to transcend this model and enhance bioinformatics research. We propose a Web 2.0-based Scientific Social Community (SSC) model for the implementation of these technologies. By establishing a social, collective and collaborative platform for data creation, sharing and integration, we promote a web services-based pipeline featuring web services for computer-to-computer data exchange as users add value. This pipeline aims to simplify data integration and creation, to realize automatic analysis, and to facilitate reuse and sharing of data. SSC can foster collaboration and harness collective intelligence to create and discover new knowledge. In addition to its research potential, we also describe its potential role as an e-learning platform in education. We discuss lessons from information technology, predict the next generation of Web (Web 3.0), and describe its potential impact on the future of bioinformatics studies.",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f1141,2005,2005-12-06
s1422,p1422,Swarm Intelligence Algorithms in Bioinformatics,Abstract content goes here ...,c56,European Conference on Software Process Improvement,cp56,accepted,f1142,2016,2016-02-22
s1424,p1424,"Improved structure, function and compatibility for CellProfiler: modular high-throughput image analysis software","UNLABELLED
There is a strong and growing need in the biology research community for accurate, automated image analysis. Here, we describe CellProfiler 2.0, which has been engineered to meet the needs of its growing user base. It is more robust and user friendly, with new algorithms and features to facilitate high-throughput work. ImageJ plugins can now be run within a CellProfiler pipeline.


AVAILABILITY AND IMPLEMENTATION
CellProfiler 2.0 is free and open source, available at http://www.cellprofiler.org under the GPL v. 2 license. It is available as a packaged application for Macintosh OS X and Microsoft Windows and can be compiled for Linux.


CONTACT
anne@broadinstitute.org


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",c6,Americas Conference on Information Systems,cp6,accepted,f1143,2007,2007-09-03
s1425,p1425,A flexible R package for nonnegative matrix factorization,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1144,2007,2007-02-21
s1426,p1426,Viewpoint Paper: Translational Bioinformatics: Coming of Age,"The American Medical Informatics Association (AMIA) recently augmented the scope of its activities to encompass translational bioinformatics as a third major domain of informatics. The AMIA has defined translational bioinformatics as ""... the development of storage, analytic, and interpretive methods to optimize the transformation of increasingly voluminous biomedical data into proactive, predictive, preventative, and participatory health."" In this perspective, I will list eight reasons why this is an excellent time to be studying translational bioinformatics, including the significant increase in funding opportunities available for informatics from the United States National Institutes of Health, and the explosion of publicly-available data sets of molecular measurements. I end with the significant challenges we face in building a community of future investigators in Translational Bioinformatics.",c39,International Conference on Global Software Engineering,cp39,accepted,f1145,2020,2020-03-10
s1427,p1427,Comparison and evaluation of Chinese research performance in the field of bioinformatics,Abstract content goes here ...,j104,Scientometrics,jv104,accepted,f1146,2010,2010-03-05
s1428,p1428,"RCSB Protein Data Bank: biological macromolecular structures enabling research and education in fundamental biology, biomedicine, biotechnology and energy","Abstract The Research Collaboratory for Structural Bioinformatics Protein Data Bank (RCSB PDB, rcsb.org), the US data center for the global PDB archive, serves thousands of Data Depositors in the Americas and Oceania and makes 3D macromolecular structure data available at no charge and without usage restrictions to more than 1 million rcsb.org Users worldwide and 600 000 pdb101.rcsb.org education-focused Users around the globe. PDB Data Depositors include structural biologists using macromolecular crystallography, nuclear magnetic resonance spectroscopy and 3D electron microscopy. PDB Data Consumers include researchers, educators and students studying Fundamental Biology, Biomedicine, Biotechnology and Energy. Recent reorganization of RCSB PDB activities into four integrated, interdependent services is described in detail, together with tools and resources added over the past 2 years to RCSB PDB web portals in support of a ‘Structural View of Biology.’",c14,International Conference on Exploring Services Science,cp14,accepted,f1147,2016,2016-12-02
s1430,p1430,An Overview of Multi-task Learning,"As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.",c112,Very Large Data Bases Conference,cp112,accepted,f1148,2018,2018-06-09
s1431,p1431,GoMiner: a resource for biological interpretation of genomic and proteomic data,Abstract content goes here ...,j5,Genome Biology,jv5,accepted,f1149,2020,2020-06-08
s1432,p1432,"Bio-jETI: a service integration, design, and provisioning platform for orchestrated bioinformatics processes",Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1150,2007,2007-04-11
s1433,p1433,Bioinformatics Algorithms: Techniques and Applications,"This book introduces algorithmic techniques in bioinformatics, emphasizing their application to solving novel problems in post-genomic molecular biology. Beginning with a thought-provoking discussion on the role of algorithms in twenty-first-century bioinformatics education, the book covers: general algorithmic techniques, algorithms and tools for genome and sequence analysis, microarray design and analysis, algorithmic issues arising in the analysis of genetic variation across human population, and algorithmic approaches in structural and systems biology.",c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f1151,2006,2006-08-16
s1434,p1434,An Introduction to Bioinformatics for Glycomics Research,"Carbohydrates are considered the thirdclass of information-encoding biologicalmacromolecules. ‘‘Glycomics,’’ the scientificattempt to characterize and study carbohy-drates, is a rapidly emerging branch ofscience, for which informatics is just begin-ning. Glycomics requires sophisticated algo-rithmic approaches. Several algorithms andmodels have been developed for glycobiol-ogy research in the past several years. Thistutorial will provide a brief introduction tothe field of glycome informatics, which willinclude a primer on glycobiology as well asdescriptions of the algorithms and modelsthat have been developed in this field.The four essential molecular buildingblocks of cells are nucleic acids, proteins,lipids, and carbohydrates, often referred toas glycans. Nucleotide and protein sequenc-es are at the heart of nearly all bioinfor-matics applications and research, whereasglycan and lipid structures have been widelyneglected in bioinformatics. However, gly-cans are the most abundant and structurallydiverse biopolymers formed in nature.Bound to proteins, as glycoproteins, theyare known to affectthefunctions of proteins.More than half of all protein sequencesdeposited in the SWISS-PROT databankinclude potential glycosylation sites and thusmay be glycoproteins. Based on an analysisof well-annotated and characterized glyco-proteins inSWISS-PROT,itwas concludedthat more than half of all proteins areglycosylated [1].The development and use of informaticstools and databases for glycobiology andglycomics research has increased consider-ably in recent years. However, the generaldevelopment in this field can still beconsidered as being in its infancy whencompared to the genomics and proteomicsareas. In terms of bioinformatics in glyco-biology, there are several paths of researchthat are currently in progress. The develop-ment of algorithms to reliably support thecharacterization of glycan structures forhigh-throughput applications is the mostimmediate demand of the glycomics com-munity. Additionally, several major glyco-related projects (Consortium for FunctionalGlycomics [2], KEGG Glycan [3], GLY-COSCIENCES.de [4]) are maturing andprovide well-structured glyco-related datathat are awaiting data mining and analysis.With the exciting new developments incarbohydrate arrays and automated MSannotation, the analysis of the glycome hasreached a new level of sophistication, whichrequires broader informatics support. Thistutorial aims to give an overview of thecurrent status of carbohydrate databases, thenewest analytical techniques, as well as theinformatics needed for rapid progress inglycomics research.",c10,Big Data,cp10,accepted,f1152,2021,2021-02-23
s1435,p1435,Protein secondary structure analyses from circular dichroism spectroscopy: methods and reference databases.,"Circular dichroism (CD) spectroscopy has been a valuable method for the analysis of protein secondary structures for many years. With the advent of synchrotron radiation circular dichroism (SRCD) and improvements in instrumentation for conventional CD, lower wavelength data are obtainable and the information content of the spectra increased. In addition, new computation and bioinformatics methods have been developed and new reference databases have been created, which greatly improve and facilitate the analyses of CD spectra. This article discusses recent developments in the analysis of protein secondary structures, including features of the DICHROWEB analysis webserver.",j266,Biopolymers,jv266,accepted,f1153,2021,2021-05-23
s1436,p1436,Bioinformatics applications for pathway analysis of microarray data.,Abstract content goes here ...,j267,Current Opinion in Biotechnology,jv267,accepted,f1154,2017,2017-05-03
s1440,p1440,Data analysis and bioinformatics tools for tandem mass spectrometry in proteomics.,"Data processing is a central and critical component of a successful proteomics experiment, and is often the most time-consuming step. There have been considerable advances in the field of proteomics informatics in the past 5 years, spurred mainly by free and open-source software tools. Along with the gains afforded by new software, the benefits of making raw data and processed results freely available to the community in data repositories are finally in evidence. In this review, we provide an overview of the general analysis approaches, software tools, and repositories that are enabling successful proteomics research via tandem mass spectrometry.",j223,Physiological Genomics,jv223,accepted,f1155,2006,2006-03-22
s1443,p1443,Bayesian methods in bioinformatics and computational systems biology,"Bayesian methods are valuable, inter alia, whenever there is a need to extract information from data that are uncertain or subject to any kind of error or noise (including measurement error and experimental error, as well as noise or random variation intrinsic to the process of interest). Bayesian methods offer a number of advantages over more conventional statistical techniques that make them particularly appropriate for complex data. It is therefore no surprise that Bayesian methods are becoming more widely used in the fields of genetics, genomics, bioinformatics and computational systems biology, where making sense of complex noisy data is the norm. This review provides an introduction to the growing literature in this area, with particular emphasis on recent developments in Bayesian bioinformatics relevant to computational systems biology.",c25,International Conference on Contemporary Computing,cp25,accepted,f1156,2014,2014-09-22
s1445,p1445,Hidden Markov Models in Bioinformatics,"Hidden Markov Models (HMMs) became recently important and popular among bioinformatics researchers, and many software tools are based on them. In this survey, we first consider in some detail the mathematical foundations of HMMs, we describe the most important algorithms, and provide useful comparisons, pointing out advantages and drawbacks. We then consider the major bioinformatics applications, such as alignment, labeling, and profiling of sequences, protein structure prediction, and pattern recognition. We finally provide a critical appraisal of the use and perspectives of HMMs in bioinformatics.",c44,International Workshop on Green and Sustainable Software,cp44,accepted,f1157,2008,2008-09-13
s1446,p1446,Assigning protein functions by comparative genome analysis: protein phylogenetic profiles.,"Determining protein functions from genomic sequences is a central goal of bioinformatics. We present a method based on the assumption that proteins that function together in a pathway or structural complex are likely to evolve in a correlated fashion. During evolution, all such functionally linked proteins tend to be either preserved or eliminated in a new species. We describe this property of correlated evolution by characterizing each protein by its phylogenetic profile, a string that encodes the presence or absence of a protein in every known genome. We show that proteins having matching or similar profiles strongly tend to be functionally linked. This method of phylogenetic profiling allows us to predict the function of uncharacterized proteins.",j20,Proceedings of the National Academy of Sciences of the United States of America,jv20,accepted,f1158,2006,2006-06-29
s1447,p1447,DNA secondary structures: stability and function of G-quadruplex structures,Abstract content goes here ...,j175,Nature reviews genetics,jv175,accepted,f1159,2007,2007-01-24
s1448,p1448,"The Taverna workflow suite: designing and executing workflows of Web Services on the desktop, web or in the cloud","The Taverna workflow tool suite (http://www.taverna.org.uk) is designed to combine distributed Web Services and/or local tools into complex analysis pipelines. These pipelines can be executed on local desktop machines or through larger infrastructure (such as supercomputers, Grids or cloud environments), using the Taverna Server. In bioinformatics, Taverna workflows are typically used in the areas of high-throughput omics analyses (for example, proteomics or transcriptomics), or for evidence gathering methods involving text mining or data mining. Through Taverna, scientists have access to several thousand different tools and resources that are freely available from a large range of life science institutions. Once constructed, the workflows are reusable, executable bioinformatics protocols that can be shared, reused and repurposed. A repository of public workflows is available at http://www.myexperiment.org. This article provides an update to the Taverna tool suite, highlighting new features and developments in the workbench and the Taverna Server.",c21,Grid Computing Environments,cp21,accepted,f1160,2005,2005-11-16
s1449,p1449,"A Semantic Web for bioinformatics: goals, tools, systems, applications",Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1161,2007,2007-05-19
s1450,p1450,Introduction to bioinformatics,"This textbook is a solid introduction to the science of Bioinformatics, an integration of computing skills and biological methods.",c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,cp20,accepted,f1162,2014,2014-02-26
s1452,p1452,PlasmoDB: a functional genomic database for malaria parasites,"PlasmoDB (http://PlasmoDB.org) is a functional genomic database for Plasmodium spp. that provides a resource for data analysis and visualization in a gene-by-gene or genome-wide scale. PlasmoDB belongs to a family of genomic resources that are housed under the EuPathDB (http://EuPathDB.org) Bioinformatics Resource Center (BRC) umbrella. The latest release, PlasmoDB 5.5, contains numerous new data types from several broad categories—annotated genomes, evidence of transcription, proteomics evidence, protein function evidence, population biology and evolution. Data in PlasmoDB can be queried by selecting the data of interest from a query grid or drop down menus. Various results can then be combined with each other on the query history page. Search results can be downloaded with associated functional data and registered users can store their query history for future retrieval or analysis.",c1,Technical Symposium on Computer Science Education,cp1,accepted,f1163,2002,2002-07-09
s1453,p1453,Bioclipse: an open source workbench for chemo- and bioinformatics,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1164,2007,2007-11-19
s1455,p1455,Recurrent Fusion of TMPRSS2 and ETS Transcription Factor Genes in Prostate Cancer,"Recurrent chromosomal rearrangements have not been well characterized in common carcinomas. We used a bioinformatics approach to discover candidate oncogenic chromosomal aberrations on the basis of outlier gene expression. Two ETS transcription factors, ERG and ETV1, were identified as outliers in prostate cancer. We identified recurrent gene fusions of the 5′ untranslated region of TMPRSS2 to ERG or ETV1 in prostate cancer tissues with outlier expression. By using fluorescence in situ hybridization, we demonstrated that 23 of 29 prostate cancer samples harbor rearrangements in ERG or ETV1. Cell line experiments suggest that the androgen-responsive promoter elements of TMPRSS2 mediate the overexpression of ETS family members in prostate cancer. These results have implications in the development of carcinomas and the molecular diagnosis and treatment of prostate cancer.",j97,Science,jv97,accepted,f1165,2012,2012-07-27
s1456,p1456,The New Bioinformatics: Integrating Ecological Data from the Gene to the Biosphere,"Bioinformatics, the application of computational tools to the management and analysis of biological data, has stimulated rapid research advances in genomics through the development of data archives such as GenBank, and similar progress is just beginning within ecology. One reason for the belated adoption of informatics approaches in ecology is the breadth of ecologically pertinent data (from genes to the biosphere) and its highly heterogeneous nature. The variety of formats, logical structures, and sampling methods in ecology create significant challenges. Cultural barriers further impede progress, especially for the creation and adoption of data standards. Here we describe informatics frameworks for ecology, from subject-specific data warehouses, to generic data collections that use detailed metadata descriptions and formal ontologies to catalog and cross-reference information. Combining these approaches with automated data integration techniques and scientific workflow systems will maximize the value of data and open new frontiers for research in ecology.",c77,Networks,cp77,accepted,f1166,2019,2019-05-09
s1457,p1457,"Computational Intelligence in Biomedicine and Bioinformatics, Current Trends and Applications",Abstract content goes here ...,c1,Technical Symposium on Computer Science Education,cp1,accepted,f1167,2002,2002-08-03
s1458,p1458,Bioinformatics for Geneticists: A Bioinformatics Primer for the Analysis of Genetic Data,"Michael R. Barnes, ed 554 pages. England, UK: John Wiley & Sons; 2007. 2nd ed. $90.00. Paperback. ISBN 978-0-470-02620-5

This book, now in its second edition for more than a year, is positioned at the intersection of disciplines including genetics, bioinformatics (the melding of computer science and biology), biomedical research, and molecular biology. Over 19 chapters, the authors cover an impressive terrain. The focus is mainly on human genetics and genomics, with research in other species also presented, particularly where it supports and advances our understanding of human genetics. Although a thoughtful discussion of the relevant literature and techniques is found in each chapter, the book is not overly technical and does not present advanced mathematical, statistical, or genetic concepts in great depth. Instead, the focus is on practical applications, available tools, software, and databases, and the presentation of supporting real world research examples. The end result is one of the best available and most accessible texts on bioinformatics and genetics in the postgenome age.

This book is …",c24,Decision Support Systems,cp24,accepted,f1168,2013,2013-05-26
s1459,p1459,Integrated Bioinformatics for Radiation-Induced Pathway Analysis from Proteomics and Microarray Data.,"Functional analysis and interpretation of large-scale proteomics and gene expression data require effective use of bioinformatics tools and public knowledge resources coupled with expert-guided examination. An integrated bioinformatics approach was used to analyze cellular pathways in response to ionizing radiation. ATM, or ataxia-telangiectasia mutated , a serine-threonine protein kinase, plays critical roles in radiation responses, including cell cycle arrest and DNA repair. We analyzed radiation responsive pathways based on 2D-gel/MS proteomics and microarray gene expression data from fibroblasts expressing wild type or mutant ATM gene. The analysis showed that metabolism was significantly affected by radiation in an ATM dependent manner. In particular, purine metabolic pathways were differentially changed in the two cell lines. The expression of ribonucleoside-diphosphate reductase subunit M2 (RRM2) was increased in ATM-wild type cells at both mRNA and protein levels, but no changes were detected in ATM-mutated cells. Increased expression of p53 was observed 30min after irradiation of the ATM-wild type cells. These results suggest that RRM2 is a downstream target of the ATM-p53 pathway that mediates radiation-induced DNA repair. We demonstrated that the integrated bioinformatics approach facilitated pathway analysis, hypothesis generation and target gene/protein identification.",j270,Journal of Proteomics & Bioinformatics,jv270,accepted,f1169,2005,2005-09-16
s1460,p1460,Fungal community analysis by high-throughput sequencing of amplified markers – a user's guide,"Novel high-throughput sequencing methods outperform earlier approaches in terms of resolution and magnitude. They enable identification and relative quantification of community members and offer new insights into fungal community ecology. These methods are currently taking over as the primary tool to assess fungal communities of plant-associated endophytes, pathogens, and mycorrhizal symbionts, as well as free-living saprotrophs. Taking advantage of the collective experience of six research groups, we here review the different stages involved in fungal community analysis, from field sampling via laboratory procedures to bioinformatics and data interpretation. We discuss potential pitfalls, alternatives, and solutions. Highlighted topics are challenges involved in: obtaining representative DNA/RNA samples and replicates that encompass the targeted variation in community composition, selection of marker regions and primers, options for amplification and multiplexing, handling of sequencing errors, and taxonomic identification. Without awareness of methodological biases, limitations of markers, and bioinformatics challenges, large-scale sequencing projects risk yielding artificial results and misleading conclusions.",j271,New Phytologist,jv271,accepted,f1170,2002,2002-03-20
s1461,p1461,Accelerating String Set Matching in FPGA Hardware for Bioinformatics Research,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1171,2007,2007-09-08
s1462,p1462,Bioinformatics for Vaccinology,"CONTENTS Preface Acknowledgements Exordium: Vaccines: a Very, Very Short Introduction 1 Vaccines: Their Place in History Smallpox in History Variolation Variolation in History Variolation Comes to Britain Lady Mary Wortley Montagu Variolation and the Sublime Porte The Royal Experiment The Boston Connection Variolation Takes Hold The Suttonian Method Variolation in Europe The Coming of Vaccination Edward Jenner Cowpox Vaccination Vindicated Louis Pasteur Vaccination Becomes a Science Meister, Pasteur, and Rabies A Vaccine for Every Disease In the Time of Cholera Haffkine and Cholera Bubonic Plague The Changing Face of Disease Almroth Wright and Typhoid Tuberculosis, Koch, and Calmette Vaccine BCG Poliomyelitis Salk and Sabin Diptheria Whooping Cough Many Diseases, Many Vaccines Smallpox: Endgame Further Reading 2 Vaccines: Need and Opportunity Eradication and Reservoirs The Ongoing Burden of Disease Lifespans The Evolving Nature of Disease Economics, Climate, and Disease Three Threats Tuberculosis in the 21st Century HIV and AIDS Malaria: Then and Now Influenza Bioterrorism Vaccines as Medicines Vaccines and the Pharmaceutical Industry Making Vaccines The Coming of the Vaccine Industry 3 Vaccines: How They Work Challenging the Immune System The Threat from Bacteria: Robust, Diverse, and Endemic MiCrobes, Diversity, and Metagenomics The Intrinsic Complexity of the Bacterial Threat Microbes and Humankind The Nature of Vaccines Types of Vaccine Carbohydrate Vaccines Epitopic Vaccines Adjuvants and Vaccine Delivery Emerging Immunovaccinology The Immune System Innate Immunity Adaptive Immunity The Microbiome and Mucosal Immunity Cellular Components of Immunity Cellular Immunity The T Cell Repertoire Epitopes: The Immunological Quantum The Major Histocompatility Complex MHC Nomenclature Peptide Binding by the MHC The Structure of the MHC Antigen Presentation The Proteasome Transporter Associated with Antigen Processing Class II Processing Seek Simplicity and Then Distrust It Cross Presentation T Cell Receptor T Cell Activation Immunological Synapse Signal 1, Signal 2, Immunodominance Humoral Immunity Further Reading 4 Vaccines: Data and Databases Making Sense of Data Knowledge in a Box The Science of -Omes and -Omics The Proteome Systems Biology The Immunome Databases and Databanks The Relational Database The XML Database The Protein Universe Much Data, Many Databases What Proteins Do What Proteins Are The Amino Acid World The Chiral Nature of Amino Acids Naming the Amino Acids The Amino Acid Alphabet Defining Amino Acid Properties Size, Charge, and Hydrogen Bonding Hydrophobicity, Lipophilicity, and Partitioning Understanding Partitioning Charges, Ionization, and pKa Many Kinds of Property Mapping the World of Sequences Biological Sequence Databases Nucleic Acid Sequence Databases Protein Sequence Databases Annotating Databases Text Mining Ontologies Secondary Sequence Databases Other Databases Databases in Immunology Host Databases Pathogen Databases Functional Immunological Databases Composite, Integrated Databases Allergen Databases Further Reading Reference 5 Vaccines: Data Driven Prediction of Binders, Epitopes and Immunogenicity Towards Epitope-Based Vaccines T Cell Epitope Prediction Predicting MHC Binding Binding is Biology Quantifying Binding Entropy, Enthalpy, and Entropy-Enthalpy Compensation Experimental Measurement of Binding Modern Measurement Methods Isothermal Titration Calorimetry Long and Short of Peptide Binding The Class I Peptide Repertoire Practicalities of Binding Prediction Binding Becomes Recognition Immunoinformatics Lends a Hand Motif Based Prediction The Imperfect Motif Other Approaches to Binding Prediction Representing Sequences Computer Science Lends a Hand Artificial Neural Networks Hidden Markov Model Support Vector Machines Robust Multivariate Statistics Partial Least Squares Quantitative Structure Activity Relationships Other Techniques and Sequence Representations Amino Acid Properties Direct Epitope Prediction Predicting Antigen Presentation Predicting Class II MHC Binding Assessing Prediction Accuracy Roc Plots Quantitative Accuracy Prediction Assessment Protocols Comparing Predictions Prediction Versus Experiment Predicting B Cell Epitopes Peak Profiles and Smoothing Early Methods Imperfect B Cell Prediction References 6 Vaccines: Structural Approaches Structure and Function Structure and Function Types of Protein Structure Protein Folding Ramachandran Plots Local Structures Protein Families, Protein Folds Comparing Structures Experimental Structure Determination Structural Genomics Protein Structure Databases Other Databases Immunological Structural Databases Small Molecule Databases Protein Homology Modelling Using Homology Modelling Predicting MHC Supertypes Application to Alloreactivity 3D-QSAR Protein Docking Predicting B Cell Epitopes with Docking Virtual Screening Limitations to Virtual Screening Predicting Epitopes with Virtual Screening Virtual Screening and Adjuvant Discovery Adjuvants and Innate Immunity Small Molecule Adjuvants Molecular Dynamics and Immunology Molecular Dynamics Methodology Molecular Dynamics and Binding Immunological Applications Limitations of Molecular Dynamics Molecular Dynamics and High Performance Computing References 7 Vaccines: Computational Solutions Vaccines and the World Bioinformatics and the Challenge for Vaccinology Predicting Immunogenicity Computational Vaccinology The Threat Remains Beyond Empirical Vaccinology Designing New Vaccines The Perfect Vaccine Conventional Approaches Genome Sequences Size of a Genome Reverse Vaccinology Finding Antigens The Success of Reverse Vaccinology Tumour Vaccines Prediction and Personalised Medicine Imperfect Data Forecasting and the Future of Computational Vaccinology Index",c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f1172,2006,2006-12-04
s1463,p1463,MorphoLibJ: integrated library and plugins for mathematical morphology with ImageJ,"MOTIVATION
Mathematical morphology (MM) provides many powerful operators for processing 2D and 3D images. However, most MM plugins currently implemented for the popular ImageJ/Fiji platform are limited to the processing of 2D images.


RESULTS
The MorphoLibJ library proposes a large collection of generic tools based on MM to process binary and grey-level 2D and 3D images, integrated into user-friendly plugins. We illustrate how MorphoLibJ can facilitate the exploitation of 3D images of plant tissues.


AVAILABILITY AND IMPLEMENTATION
MorphoLibJ is freely available at http://imagej.net/MorphoLibJ CONTACT: david.legland@nantes.inra.frSupplementary information: Supplementary data are available at Bioinformatics online.",c58,Australian Software Engineering Conference,cp58,accepted,f1173,2021,2021-08-29
s1464,p1464,GOSemSim: an R package for measuring semantic similarity among GO terms and gene products,"SUMMARY
The semantic comparisons of Gene Ontology (GO) annotations provide quantitative ways to compute similarities between genes and gene groups, and have became important basis for many bioinformatics analysis approaches. GOSemSim is an R package for semantic similarity computation among GO terms, sets of GO terms, gene products and gene clusters. Four information content (IC)- and a graph-based methods are implemented in the GOSemSim package, multiple species including human, rat, mouse, fly and yeast are also supported. The functions provided by the GOSemSim offer flexibility for applications, and can be easily integrated into high-throughput analysis pipelines.


AVAILABILITY
GOSemSim is released under the GNU General Public License within Bioconductor project, and freely available at http://bioconductor.org/packages/2.6/bioc/html/GOSemSim.html.",c40,IEEE International Conference on Software Maintenance and Evolution,cp40,accepted,f1174,2013,2013-03-10
s1465,p1465,The myGrid ontology: bioinformatics service discovery,"(my)Grid supports in silico experiments in the life sciences, enabling the design and enactment of workflows as well as providing components to assist service discovery, data and metadata management. The (my)Grid ontology is one component in a larger semantic discovery framework for the identification of the highly distributed and heterogeneous bioinformatics services in the public domain. From an initial model of formal OWL-DL semantics throughout, we now adopt a spectrum of expressivity and reasoning for different tasks in service annotation and discovery. Here, we discuss the development and use of the (my)Grid ontology and our experiences in semantic service discovery.",j272,International Journal of Bioinformatics Research and Applications,jv272,accepted,f1175,2021,2021-01-26
s1466,p1466,Bioinformatics approaches in the study of cancer.,"A revolution is underway in the approach to studying the genetic basis of cancer. Massive amounts of data are now being generated via high-throughput techniques such as DNA microarray technology and new computational algorithms have been developed to aid in analysis. At the same time, standards-based repositories, including the Stanford Microarray Database and the Gene Expression Omnibus have been developed to store and disseminate the results of microarray experiments. Bioinformatics, the convergence of biology, information science, and computation, has played a key role in these developments. Recently developed techniques include Module Maps, SLAMS (Stepwise Linkage Analysis of Microarray Signatures), and COPA (Cancer Outlier Profile Analysis). What these techniques have in common is the application of novel algorithms to find high-level gene expression patterns across heterogeneous microarray experiments. Large-scale initiatives are underway as well. The Cancer Genome Atlas (TCGA) project is a logical extension of the Human Genome Project and is meant to produce a comprehensive atlas of genetic changes associated with cancer. The Cancer Biomedical Informatics Grid (caBIG), led by the NCI, also represents a colossal initiative involving virtually all aspects of cancer research and may help to transform the way cancer research is conducted and data are shared.",j273,Current molecular medicine,jv273,accepted,f1176,2002,2002-08-12
s1467,p1467,A Tutorial on Hierarchical Classification with Applications in Bioinformatics.,"In Machine Learning and Data Mining, most of the works in classification problems deal with flat classification, where each instance is classified in one of a set of possible classes and there is no hierarchical relationship between the classes. There are, however, more complex classification problems where the classes to be predicted are hierarchically related. This chapter presents a tutorial on the hierarchical classification techniques found in the literature. We also discuss how hierarchical classification techniques have been applied to the area of Bioinformatics (particularly the prediction of protein function), where hierarchical classification problems are often found. INTRODUCTION Classification is one of the most important problems in Machine Learning (ML) and Data Mining (DM). In general, a classification problem can be formally defined as: Given a set of training examples composed of pairs {xi, yi}, find a function f(x) that maps each xi to its associated class yi, i = 1, 2, ..., n, where n is the total number of training examples. After training, the predictive accuracy of the classification function induced is evaluated by using it to classify a set of unlabeled examples, unseen during training. This evaluation measures the generalization ability (predictive accuracy) of the classification function induced. The vast majority of classification problems addressed in the literature involves flat classification, where each example is assigned to a class out of a finite (and usually small) set of classes. By contrast, in hierarchical classification problems, the classes are disposed in a hierarchical structure, such as a tree or a Directed Acyclic Graph (DAG). In these structures, the nodes represent classes. Figure 1 illustrates the difference between flat and hierarchical classification problems. To keep the example simple, Figure 1(b) shows a tree-structured class hierarchy. The more complex case of DAG-structured class hierarchies will be discussed later. In Figure 1, each node – except the root nodes – is labeled with the number of a class. In Figure 1(b), class 1 is divided into two sub-classes, 1.1 and 1.2, and class 3 is divided into three sub-classes. The root nodes are labeled “any class”, to denote the case where the class of an example is unknown. Figure 1 clearly shows that flat classification problems are actually a particular case of hierarchical classification problems where there is a single level of classes, i.e., where no class is divided into sub-classes.",c114,IEEE International Conference on Robotics and Automation,cp114,accepted,f1177,2011,2011-09-24
s1468,p1468,Biowep: a workflow enactment portal for bioinformatics applications,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1178,2007,2007-08-02
s1469,p1469,"Bioinformatics and the cell - modern computational approaches in genomics, proteomics and transcriptomics",Blast and Fasta.- Sequence alignment.- Contig assembly.- DNA replication and viral evolution.- Gene and motif prediction.- Hidden Markov Models.- Gibbs Sampler.- Bioinformatics and vertebrate mitochondria.- Characterizing translation efficiency.- Protein isoelectric point.- Bioinformatics and Two-Dimensional Protein Separation.- Self-Organizing Map and other clustering Algorithms.- Molecular Phylogenetics.- Fundamentals of Proteomics.,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f1179,2019,2019-07-14
s1470,p1470,"Bioinformatics: Genes, Proteins and Computers",1. Molecular Evolution. 2. Gene Finding. 3. Sequence Comparison Methods. 4. Amino Acid Residue Conservation. 5. Function Prediction From Protein Sequence. 6. Protein Structure Comparison. 7. Protein Structure Classifications. 8. Comparative Modelling. 9. Protein Structure Prediction. 10. From Protein Structure to Function. 11. From Structure-Based Genome Annotation to Understanding Genes and Proteins. 12. Global Approaches for Studying Protein-Protein Interactions. 13. Predicting The Structure of Protein-Biomolecular Interactions. 14. Experimental Use of DNA Arrays. 15. Mining Gene Expression Data 16. Proteomics. 17. Data Management of Biological Information. 18. Internet Technologies for Bioinformatics.,c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f1180,2004,2004-06-26
s1472,p1472,BioWeka - extending the Weka framework for bioinformatics,"UNLABELLED
Given the growing amount of biological data, data mining methods have become an integral part of bioinformatics research. Unfortunately, standard data mining tools are often not sufficiently equipped for handling raw data such as e.g. amino acid sequences. One popular and freely available framework that contains many well-known data mining algorithms is the Waikato Environment for Knowledge Analysis (Weka). In the BioWeka project, we introduce various input formats for bioinformatics data and bioinformatics methods like alignments to Weka. This allows users to easily combine them with Weka's classification, clustering, validation and visualization facilities on a single platform and therefore reduces the overhead of converting data between different data formats as well as the need to write custom evaluation procedures that can deal with many different programs. We encourage users to participate in this project by adding their own components and data formats to BioWeka.


AVAILABILITY
The software, documentation and tutorial are available at http://www.bioweka.org.",c35,EUROMICRO Conference on Software Engineering and Advanced Applications,cp35,accepted,f1181,2005,2005-09-07
s1473,p1473,National Institute of Allergy and Infectious Diseases Bioinformatics Resource Centers: New Assets for Pathogen Informatics,The National Institute of Allergy and Infectious Diseases (NIAID) began a new bioinformatic venture in July 2004 intended to integrate the vast amount of genomic and other biological data that are both available and being produced by the rapid increase in biodefense research. Eight Bioinformatics,j216,Infection and Immunity,jv216,accepted,f1182,2004,2004-02-25
s1474,p1474,Biskit - A software platform for structural bioinformatics,"UNLABELLED
Biskit is a modular, object-oriented python library that provides intuitive classes for many typical tasks of structural bioinformatics research. It facilitates the manipulation and analysis of macromolecular structures, protein complexes and molecular dynamics trajectories. At the same time, Biskit offers a software platform for the rapid integration of external programs and new algorithms into complex structural bioinformatics workflows. Calculations are thus often delegated to established programs like Xplor, Amber, Hex, Prosa, Hmmer and Modeller; interfaces to further software can be easily added. Moreover, Biskit simplifies the parallelization of time consuming calculations via PVM (Parallel Virtual Machine).


AVAILABILITY
The latest snapshot of Biskit, documentation and examples are freely available under the GNU General Public License at http://biskit.sf.net (alternate url http://biskit.pasteur.fr).",c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f1183,2018,2018-03-22
s1475,p1475,"Multiancestry genome-wide association study of 520,000 subjects identifies 32 loci associated with stroke and stroke subtypes",Abstract content goes here ...,j274,Nature Genetics,jv274,accepted,f1184,2020,2020-10-22
s1476,p1476,Development of human protein reference database as an initial platform for approaching systems biology in humans.,"Human Protein Reference Database (HPRD) is an object database that integrates a wealth of information relevant to the function of human proteins in health and disease. Data pertaining to thousands of protein-protein interactions, posttranslational modifications, enzyme/substrate relationships, disease associations, tissue expression, and subcellular localization were extracted from the literature for a nonredundant set of 2750 human proteins. Almost all the information was obtained manually by biologists who read and interpreted >300,000 published articles during the annotation process. This database, which has an intuitive query interface allowing easy access to all the features of proteins, was built by using open source technologies and will be freely available at http://www.hprd.org to the academic community. This unified bioinformatics platform will be useful in cataloging and mining the large number of proteomic interactions and alterations that will be discovered in the postgenomic era.",j187,Genome Research,jv187,accepted,f1185,2006,2006-07-18
s1478,p1478,"Correction to ‘The STRING database in 2021: customizable protein–protein networks, and functional characterization of user-uploaded gene/measurement sets’","1Department of Molecular Life Sciences and Swiss Institute of Bioinformatics, University of Zurich, 8057 Zurich, Switzerland, 2Novo Nordisk Foundation Center for Protein Research, University of Copenhagen, 2200 Copenhagen N, Denmark, 3TurkuNLP Group, Department of Future Technologies, University of Turku, 20014 Turun Yliopisto, Finland, 4Structural and Computational Biology Unit, European Molecular Biology Laboratory, 69117 Heidelberg, Germany, 5Molecular Medicine Partnership Unit, University of Heidelberg and European Molecular Biology Laboratory, 69117 Heidelberg, Germany, 6Max Delbrück Centre for Molecular Medicine, 13125 Berlin, Germany and 7Department of Bioinformatics, Biozentrum, University of Würzburg, 97074 Würzburg, Germany",j102,Nucleic Acids Research,jv102,accepted,f1186,2002,2002-04-22
s1479,p1479,Bioconductor: an open source framework for bioinformatics and computational biology.,Abstract content goes here ...,j275,Methods in Enzymology,jv275,accepted,f1187,2002,2002-11-30
s1481,p1481,A gene expression database for the molecular pharmacology of cancer,Abstract content goes here ...,j274,Nature Genetics,jv274,accepted,f1188,2020,2020-07-01
s1482,p1482,Harnessing bioinformatics to discover new vaccines.,Abstract content goes here ...,j276,Drug Discovery Today,jv276,accepted,f1189,2021,2021-11-05
s1483,p1483,Identification through bioinformatics of two new macrophage proinflammatory human chemokines: MIP-3alpha and MIP-3beta.,"An increasing number of proinflammatory peptides, known as chemokines, are constantly being described and characterized. Because of their proven biologic functions in allergy, AIDS and, in general, inflammatory processes, these proteins have recently gained more attention. In this study we report the identification through bioinformatics of two new human chemokines: MIP-3alpha and MIP-3beta. Both of them belong to the beta- or CC chemokine family. Expression studies indicate that MIP-3alpha is predominantly expressed in lymph nodes, appendix, PBL, fetal liver, fetal lung and several cell lines. However, MIP-3beta expression is restricted to lymph nodes, thymus and appendix. Interestingly enough, both chemokines manifested a pattern of expression strongly regulated by IL-10. In contrast with other CC chemokines, MIP-3beta maps to chromosome 9. Here we show the importance of bioinformatics to discover new molecules with possible therapeutic effects and regulatory functions.",j277,Journal of Immunology,jv277,accepted,f1190,2007,2007-05-18
s1484,p1484,"Data Mining. Multimedia, Soft Computing, and Bioinformatics",Preface. 1. Introduction to Data Mining. 2. Soft Computing. 3. Multimedia Data Compression. 4. String Matching. 5. Classification in Data Mining. 6. Clustering in Data Mining. 7. Association Rules. 8. Rule Mining with Soft Computing. 9. Multimedia Data Mining. 10. Bioinformatics: An Application. Index. About the Authors.,j247,IEEE Transactions on Neural Networks,jv247,accepted,f1191,2006,2006-02-18
s1485,p1485,Detailed estimation of bioinformatics prediction reliability through the Fragmented Prediction Performance Plots,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1192,2007,2007-11-12
s1486,p1486,GSVA: gene set variation analysis for microarray and RNA-Seq data,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1193,2007,2007-02-08
s1487,p1487,Structural and evolutionary bioinformatics of the SPOUT superfamily of methyltransferases,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1194,2007,2007-08-01
s1488,p1488,Viruses and interferon: a fight for supremacy,Abstract content goes here ...,j278,Nature reviews. Immunology,jv278,accepted,f1195,2018,2018-08-01
s1490,p1490,A genomic view of alternative splicing,Abstract content goes here ...,j274,Nature Genetics,jv274,accepted,f1196,2020,2020-02-02
s1491,p1491,FatiGO: a web tool for finding significant associations of Gene Ontology terms with groups of genes,"We present a simple but powerful procedure to extract Gene Ontology (GO) terms that are significantly over- or under-represented in sets of genes within the context of a genome-scale experiment (DNA microarray, proteomics, etc.). Said procedure has been implemented as a web application, FatiGO, allowing for easy and interactive querying. FatiGO, which takes the multiple-testing nature of statistical contrast into account, currently includes GO associations for diverse organisms (human, mouse, fly, worm and yeast) and the TrEMBL/Swissprot GOAnnotations@EBI correspondences from the European Bioinformatics Institute.",c41,Software Product Lines Conference,cp41,accepted,f1197,2002,2002-10-17
s1492,p1492,Spectral clustering and its use in bioinformatics,Abstract content goes here ...,c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f1198,2012,2012-10-26
s1494,p1494,Bioinformatics for geneticists : a bioinformatics primer for the analysis of genetic data,"Foreword. Preface. Contributors. Glossary. SECTION I AN INTRODUCTION TO BIOINFORMATICS FOR THE GENETICIST. 1 Bioinformatics challenges for the geneticist (Michael R. Barnes). 1.1 Introduction. 1.2 The role of bioinformatics in genetics research. 1.3 Genetics in the post-genome era. 1.4 Conclusions. References. 2 Managing and manipulating genetic data (Karl W. Broman and Simon C. Heath). 2.1 Introduction. 2.2 Basic principles. 2.3 Data entry and storage. 2.4 Data manipulation. 2.5 Examples of code. 2.6 Resources. 2.7 Summary. References. SECTION II MASTERING GENES, GENOMES AND GENETIC VARIATION DATA. 3 The HapMap - A haplotype map of the human genome (Ellen M. Brown and Bryan J. Barratt). 3.1 Introduction. 3.2 Accessing the data. 3.3 Application of HapMap data in association studies. 3.4 Future Perspectives. References. 4 Assembling a view of the human genome (Colin A. M. Semple). 4.1 Introduction. 4.2 Genomic sequence assembly. 4.3 Annotation from a distance: the generalities. 4.4 Annotation up close and personal: the specifics. 4.5 Annotation: the next generation. References. 5 Finding, delineating and analysing genes (Christopher Southan and Michael R. Barnes). 5.1 Introduction. 5.2 Why learn to predict and analyse genes in the complete genome era? 5.3 The evidence cascade for gene products. 5.4 Dealing with the complexities of gene models. 5.5 Locating known genes in the human genome. 5.6 Genome portal inspection. 5.7 Analysing novel genes. 5.8 Conclusions and prospects. References. 6 Comparative genomics (Martin S. Taylor and Richard R. Copley). 6.1 Introduction. 6.2 The Genomic landscape. 6.3 Concepts. 6.4 Practicalities. 6.5 Technology. 6.6 Applications. 6.7 Challenges and future directions. 6.8 Conclusion. References. SECTION III BIOINFORMATICS FOR GENETIC STUDY DESIGN AND ANALYSIS. 7 Identifying mutations in single gene disorders (David P. Kelsell, Diana Blaydon and Charles A. Mein). 7.1 Introduction. 7.2 Clinical Ascertainment. 7.3 Genome-wide mapping of monogenic diseases. 7.4 The nature of mutation in monogenic diseases. 7.5 Considering epigenetic effects in mendelian traits. 7.6 Summary. References. 8 From Genome Scan Culprit Gene (Ian C. Gray). 8.1 Introduction. 8.2 Theoretical and practical considerations. 8.3 A stepwise approach to locus refinement and candidate gene identification. 8.4 Conclusion. 8.5 A list of the software tools and Web links mentioned in this chapter. References. 9 Integrating Genetics, Genomics and Epigenomics to Identify. Disease Genes (Michael R. Barnes). 9.1 Introduction. 9.2 Dealing with the (draft) human genome sequence. 9.3 Progressing loci of interest with genomic information. 9.4 In silico characterization of the IBD5 locus - a case study. 9.5 Drawing together biological rationale - hypothesis building. 9.6 Identification of potentially functional polymorphisms. 9.7 Conclusions. References. 10 Tools for statistical genetics (Aruna Bansal, Charlotte Vignal and Ralph McGinnis). 10.1 Introduction. 10.2 Linkage analysis. 10.3 Association analysis. 10.4 Linkage disequilibrium. 10.5 Quantitative trait locus (QTL) mapping in experimental crosses. 10.6 Closing remarks. References. SECTION IV MOVING FROM ASSOCIATED GENES TO DISEASE ALLELES. 11 Predictive functional analysis of polymorphisms: An overview (Mary Plumpton and Michael R. Barnes). 11.1 Introduction. 11.2 Principles of predictive functional analysis of polymorphisms. 11.3 The anatomy of promoter regions and regulatory elements. 11.4 The anatomy of genes. 11.5 Pseudogenes and regulatory mRNA. 11.6 Analysis of novel regulatory elements and motifs in. nucleotide sequences. 11.7 Functional analysis of non-synonymous coding polymorphisms. 11.8 Integrated tools for functional analysis of genetic variation. 11.9 A note of caution on the prioritization of in silico predictions for. further laboratory investigation. 11.10 Conclusions. References. 12 Functional in silico analysis of gene regulatory polymorphism (Chaolin Zhang, Xiaoyue Zhao, Michael Q. Zhang). 12.1 Introduction. 12.2 Predicting regulatory regions. 12.3 Modelling and predicting transcription factor-binding sites. 12.4 Predicting regulatory elements for splicing regulation. 12.5 Evaluating the functional importance of. regulatory polymorphisms. References. 13 Amino-acid properties and consequences of substitutions (Matthew J. Betts and Robert B. Russell). 13.1 Introduction. 13.2 Protein features relevant to amino-acid behaviour. 13.3 Amino-acid classifications. 13.4 Properties of the amino acids. 13.5 Amino-acid quick reference. 13.6 Studies of how mutations affect function. 13.7 A summary of the thought process. References. 14 Non-coding RNA bioinformatics (James Brown, Steve Deharo, Barry Dancis, Michael R. Barnes and Philippe Sanseau). 14.1 Introduction. 14.2 The non-coding (nc) RNA universe. 14.3 Computational analysis of ncRNA. 14.4 ncRNA variation in disease. 14.5 Assessing the impact of variation in ncRNA. 14.6 Data resources to support small ncRNA analysis. 14.7 Conclusions. References. SECTION V ANALYSIS AT THE GENETIC AND GENOMIC DATA INTERFACE. 15 What are microarrays? (Catherine A. Ball and Gavin Sherlock). 15.1 Introduction. 15.2 Principles of the application of microarray technology. 15.3 Complementary approaches to microarray analysis. 15.4 Differences between data repository and research database. 15.5 Descriptions of freely available research database packages. References. 16 Combining quantitative trait and gene-expression data (Elissa J. Chesler). 16.1 Introduction: the genetic regulation of endophenotypes. 16.2 Transcript abundance as a complex phenotype. 16.3 Scaling up genetic analysis and mapping models for microarrays. 16.4 Genetic correlation analysis. 16.5 Systems genetic analysis. 16.6 Using expression QTLs to identify candidate genes for the regulation of complex phenotypes. 16.7 Conclusions. References. 17 Bioinformatics and cancer genetics (Joel Greshock). 17.1 Introduction. 17.2 Cancer genomes. 17.3 Approaches to studying cancer genetics. 17.4 General resources for cancer genetics. 17.5 Cancer genes and mutations. 17.6 Copy number alterations in cancer. 17.7 Loss of heterozygosity in cancer. 17.8 Gene-expression data in cancer. 17.9 Multiplatform gene target identification. 17.10 The epigenetics of cancer. 17.11 Tumour modelling. 17.12 Conclusions. References. 18 Needle in a haystack? dealing with 500 SNP genome scans (Michael R. Barnes and Paul S. Derwent). 18.1 Introduction. 18.2 Genome scan analysis issues. 18.3 Ultra-high-density genome-scanning technologies. 18.4 Bioinformatics for genome scan analysis. 18.5 Conclusions. References. 19 A bioinformatics perspective on genetics in drug discovery and development (Christopher D. Southan, Magnus Ulvsb ack and Michael R. Barnes). 19.1 Introduction. 19.2 Target genetics. 19.3 Pharmacogenetics (PGx). 19.4 Conclusions: toward 'personalized medicine'. References. Appendix I. Appendix II. Index.",c88,Symposium on the Theory of Computing,cp88,accepted,f1199,2014,2014-06-15
s1495,p1495,Transparent access to multiple bioinformatics information sources,"This paper describes the Transparent Access to Multiple Bioinformatics Information Sources project, known as TAMBIS, in which a domain ontology for molecular biology and bioinformatics is used in a retrieval-based information integration system for biologists. The ontology, represented using a description logic and managed by a terminology server, is used both to drive a visual query interface and as a global schema against which complex intersource queries are expressed. These source-independent declarative queries are then rewritten into collections of ordered source-dependent queries for execution by a middleware layer. In bioinformatics, the majority of data sources are not databases but tools with limited accessible interfaces. The ontology helps manage the interoperation between these resources. The paper emphasizes the central role that is played by the ontology in the system. The project distinguishes itself from others in the following ways: the ontology, developed by a biologist, is substantial; the retrieval interface is sophisticated; the description logic is managed by a sophisticated terminology server. A full pilot application is available as a JavaTM applet integrating five sources concerned with proteins. This pilot is currently undergoing field trials with working biologists and is being used to answer real questions in biology, one of which is used as a case study throughout the paper.",j279,IBM Systems Journal,jv279,accepted,f1200,2001,2001-02-11
s1496,p1496,Bioinformatics—an introduction for computer scientists,"The article aims to introduce computer scientists to the new field of bioinformatics. This area has arisen from the needs of biologists to utilize and help interpret the vast amounts of data that are constantly being gathered in genomic research---and its more recent counterparts, proteomics and functional genomics. The ultimate goal of bioinformatics is to develop in silico models that will complement in vitro and in vivo biological experiments. The article provides a bird's eye view of the basic concepts in molecular cell biology, outlines the nature of the existing data, and describes the kind of computer algorithms and techniques that are necessary to understand cell behavior. The underlying motivation for many of the bioinformatics approaches is the evolution of organisms and the complexity of working with incomplete and noisy data. The topics covered include: descriptions of the current software especially developed for biologists, computer and mathematical cell models, and areas of computer science that play an important role in bioinformatics.",c37,Asia-Pacific Software Engineering Conference,cp37,accepted,f1201,2008,2008-01-30
s1497,p1497,"Classical Nuclear Localization Signals: Definition, Function, and Interaction with Importin α*","The best understood system for the transport of macromolecules between the cytoplasm and the nucleus is the classical nuclear import pathway. In this pathway, a protein containing a classical basic nuclear localization signal (NLS) is imported by a heterodimeric import receptor consisting of the β-karyopherin importin β, which mediates interactions with the nuclear pore complex, and the adaptor protein importin α, which directly binds the classical NLS. Here we review recent studies that have advanced our understanding of this pathway and also take a bioinformatics approach to analyze the likely prevalence of this system in vivo. Finally, we describe how a predicted NLS within a protein of interest can be confirmed experimentally to be functionally important.",j280,Journal of Biological Chemistry,jv280,accepted,f1202,2007,2007-10-19
s1499,p1499,Grid Computing for Bioinformatics and Computational Biology,"Preface. Chapter 1: Open computing Grid for molecular sciences (M. Romberg, E. Benfenati, and W. Dubitzky). Chapter 2: Designing high-performance concurrent strategies for biological sequence alignment problems on networked computing platforms (B. Veeravalli). Chapter 3: Optimized cluster-enabled HMMER searches (J. P. Walters, J. Landman, and V. Chaudhary). Chapter 4: Expanding the rich of Grid computing: combining Globus and BOINC based systems (D. S. Myers, A. L. Bazinet, and M. P. Cummings). Chapter 5: Hierarchical Grid computing for high performance bioinformatics (B. Schmidt, C.X. Chen and W. Liu). Chapter 6:Multiple sequence alignment and phylogenetic inference (D. Trystram, and J. Zola). Chapter 7: Data syndication techniques for bioinformatics applications (C. Wang, A. Y. Zomaya, and B. B. Zhou). Chapter 8: Conformational sampling and docking on Grids (A. Tantar, N. Melab, and E-G. Talbi). Chapter 9: Deployment of Grid life sciences applications (V. Breton, N. Jacq , V. Kasam, and J. Salzemann). Chapter 10: Grid-based interactive decision support in biomedicine (A. Tirado-Ramos, P. M. A. Sloot, and M. Bubak). Chapter 11: Database-driven grid computing and distributed web applications: a comparison (H. De Sterck, A.Papo, C. Zhang, M. Hamady, and R. Knight). Chapter 12: A semantic mediation architecture for a clinical Data Grid (K. Kumpf, A. Wohrer, S. Benkner, G. Engelbrecht, and Jochen Fingberg). Chapter 13: Bioinformatics applications in Grid computing environments (A. Boukerche, A. C. Magalhaes and Alves De Melo). Chapter 14: Recent advances in solving the protein threading problem (R. Andonov, G. Collet, J-F. Gibrat, A. Marin, V. Poirriez, and N. Yanev). Chapter 15: DNA fragment assembly using Grid systems (A. J. Nebro, G. Luque, and E. Alba). Chapter 16: Seeing is knowing: Visualization of parameter-parameter dependencies in biomedical network models (A. Konagaya, R. Azuma, R. Umetsu, S. Ohki, F. Konishi, K. Matsumura, and S. Yoshikawa).",c41,Software Product Lines Conference,cp41,accepted,f1203,2002,2002-05-17
s1500,p1500,Spectral graph theory,"With every graph (or digraph) one can associate several different matrices. We have already seen the vertex-edge incidence matrix, the Laplacian and the adjacency matrix of a graph. Here we shall concentrate mainly on the adjacency matrix of (undirected) graphs, and also discuss briefly the Laplacian. We shall show that spectral properies (the eigenvalues and eigenvectors) of these matrices provide useful information about the structure of the graph. It turns out that for regular graphs, the information one can deduce from one matrix representation (e.g., the adjacency matrix) is similar to the information one can deduce from other representations (such as the Laplacian). We remark that for nonregular graphs, this is not the case, and the choice of matrix representation may make a significant difference. We shall not elaborate on this issue further, as our main concern here will be either with regular or nearly regular graph. The adjacency matrix of a connected undirected graph is nonnegative, symmetric and irreducible (namely, it cannot be decomposed into two diagonal blocks and two off-diagonal blocks, one of which is all-0). As such, standard results n linear algebra, including the Perron-Frobenius theorem, imply that:",c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f1204,2012,2012-08-18
s1504,p1504,Introduction to graph theory,Abstract content goes here ...,c77,Networks,cp77,accepted,f1205,2019,2019-01-28
s1505,p1505,Application of Graph Theory for Identifying Connectivity Patterns in Human Brain Networks: A Systematic Review,"Background: Analysis of the human connectome using functional magnetic resonance imaging (fMRI) started in the mid-1990s and attracted increasing attention in attempts to discover the neural underpinnings of human cognition and neurological disorders. In general, brain connectivity patterns from fMRI data are classified as statistical dependencies (functional connectivity) or causal interactions (effective connectivity) among various neural units. Computational methods, especially graph theory-based methods, have recently played a significant role in understanding brain connectivity architecture. Objectives: Thanks to the emergence of graph theoretical analysis, the main purpose of the current paper is to systematically review how brain properties can emerge through the interactions of distinct neuronal units in various cognitive and neurological applications using fMRI. Moreover, this article provides an overview of the existing functional and effective connectivity methods used to construct the brain network, along with their advantages and pitfalls. Methods: In this systematic review, the databases Science Direct, Scopus, arXiv, Google Scholar, IEEE Xplore, PsycINFO, PubMed, and SpringerLink are employed for exploring the evolution of computational methods in human brain connectivity from 1990 to the present, focusing on graph theory. The Cochrane Collaboration's tool was used to assess the risk of bias in individual studies. Results: Our results show that graph theory and its implications in cognitive neuroscience have attracted the attention of researchers since 2009 (as the Human Connectome Project launched), because of their prominent capability in characterizing the behavior of complex brain systems. Although graph theoretical approach can be generally applied to either functional or effective connectivity patterns during rest or task performance, to date, most articles have focused on the resting-state functional connectivity. Conclusions: This review provides an insight into how to utilize graph theoretical measures to make neurobiological inferences regarding the mechanisms underlying human cognition and behavior as well as different brain disorders.",j281,Frontiers in Neuroscience,jv281,accepted,f1206,2005,2005-08-15
s1506,p1506,Spectral Graph Theory,"Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index.",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f1207,2005,2005-04-30
s1507,p1507,Graph Theory,"Gaph Teory Fourth Edition Th is standard textbook of modern graph theory, now in its fourth edition, combines the authority of a classic with the engaging freshness of style that is the hallmark of active mathematics. It covers the core material of the subject with concise yet reliably complete proofs, while offering glimpses of more advanced methods in each fi eld by one or two deeper results, again with proofs given in full detail.",c59,British Computer Society Conference on Human-Computer Interaction,cp59,accepted,f1208,2019,2019-06-29
s1508,p1508,Introduction to Graph Theory,"1. Fundamental Concepts. What Is a Graph? Paths, Cycles, and Trails. Vertex Degrees and Counting. Directed Graphs. 2. Trees and Distance. Basic Properties. Spanning Trees and Enumeration. Optimization and Trees. 3. Matchings and Factors. Matchings and Covers. Algorithms and Applications. Matchings in General Graphs. 4. Connectivity and Paths. Cuts and Connectivity. k-connected Graphs. Network Flow Problems. 5. Coloring of Graphs. Vertex Colorings and Upper Bounds. Structure of k-chromatic Graphs. Enumerative Aspects. 6. Planar Graphs. Embeddings and Euler's Formula. Characterization of Planar Graphs. Parameters of Planarity. 7. Edges and Cycles. Line Graphs and Edge-Coloring. Hamiltonian Cycles. Planarity, Coloring, and Cycles. 8. Additional Topics (Optional). Perfect Graphs. Matroids. Ramsey Theory. More Extremal Problems. Random Graphs. Eigenvalues of Graphs. Appendix A: Mathematical Background. Appendix B: Optimization and Complexity. Appendix C: Hints for Selected Exercises. Appendix D: Glossary of Terms. Appendix E: Supplemental Reading. Appendix F: References. Indices.",c7,European Conference on Modelling and Simulation,cp7,accepted,f1209,2015,2015-08-25
s1509,p1509,A Guide to Conquer the Biological Network Era Using Graph Theory,"Networks are one of the most common ways to represent biological systems as complex sets of binary interactions or relations between different bioentities. In this article, we discuss the basic graph theory concepts and the various graph types, as well as the available data structures for storing and reading graphs. In addition, we describe several network properties and we highlight some of the widely used network topological features. We briefly mention the network patterns, motifs and models, and we further comment on the types of biological and biomedical networks along with their corresponding computer- and human-readable file formats. Finally, we discuss a variety of algorithms and metrics for network analyses regarding graph drawing, clustering, visualization, link prediction, perturbation, and network alignment as well as the current state-of-the-art tools. We expect this review to reach a very broad spectrum of readers varying from experts to beginners while encouraging them to enhance the field further.",j282,Frontiers in Bioengineering and Biotechnology,jv282,accepted,f1210,2010,2010-02-05
s1511,p1511,Graph Theory,Abstract content goes here ...,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,cp79,accepted,f1211,2020,2020-09-13
s1512,p1512,Graph theory methods: applications in brain networks,"Network neuroscience is a thriving and rapidly expanding field. Empirical data on brain networks, from molecular to behavioral scales, are ever increasing in size and complexity. These developments lead to a strong demand for appropriate tools and methods that model and analyze brain network data, such as those provided by graph theory. This brief review surveys some of the most commonly used and neurobiologically insightful graph measures and techniques. Among these, the detection of network communities or modules, and the identification of central network elements that facilitate communication and signal transfer, are particularly salient. A number of emerging trends are the growing use of generative models, dynamic (time-varying) and multilayer networks, as well as the application of algebraic topology. Overall, graph theory methods are centrally important to understanding the architecture, development, and evolution of brain networks.",j283,Dialogues in Clinical Neuroscience,jv283,accepted,f1212,2014,2014-02-25
s1513,p1513,"Electrical Networks and Algebraic Graph Theory: Models, Properties, and Applications","Algebraic graph theory is a cornerstone in the study of electrical networks ranging from miniature integrated circuits to continental-scale power systems. Conversely, many fundamental results of algebraic graph theory were laid out by early electrical circuit analysts. In this paper, we survey some fundamental and historic as well as recent results on how algebraic graph theory informs electrical network analysis, dynamics, and design. In particular, we review the algebraic and spectral properties of graph adjacency, Laplacian, incidence, and resistance matrices and how they relate to the analysis, network reduction, and dynamics of certain classes of electrical networks. We study these relations for models of increasing complexity ranging from static resistive direct current (dc) circuits, over dynamic resistor..inductor..capacitor (RLC) circuits, to nonlinear alternating current (ac) power flow. We conclude this paper by presenting a set of fundamental open questions at the intersection of algebraic graph theory and electrical networks.",j168,Proceedings of the IEEE,jv168,accepted,f1213,2019,2019-04-08
s1516,p1516,Thermal Modeling in Metal Additive Manufacturing Using Graph Theory,"The goal of this work is to predict the effect of part geometry and process parameters on the instantaneous spatiotemporal distribution of temperature, also called the thermal field or temperature history, in metal parts as they are being built layer-by-layer using additive manufacturing (AM) processes. In pursuit of this goal, the objective of this work is to develop and verify a graph theory-based approach for predicting the temperature distribution in metal AM parts. This objective is consequential to overcome the current poor process consistency and part quality in AM. One of the main reasons for poor part quality in metal AM processes is ascribed to the nature of temperature distribution in the part. For instance, steep thermal gradients created in the part during printing leads to defects, such as warping and thermal stress-induced cracking. Existing nonproprietary approaches to predict the temperature distribution in AM parts predominantly use mesh-based finite element analyses that are computationally tortuous—the simulation of a few layers typically requires several hours, if not days. Hence, to alleviate these challenges in metal AM processes, there is a need for efficient computational models to predict the temperature distribution, and thereby guide part design and selection of process parameters instead of expensive empirical testing. Compared with finite element analyses techniques, the proposed mesh-free graph theory-based approach facilitates prediction of the temperature distribution within a few minutes on a desktop computer. To explore these assertions, we conducted the following two studies: (1) comparing the heat diffusion trends predicted using the graph theory approach with finite element analysis, and analytical heat transfer calculations based on Green’s functions for an elementary cuboid geometry which is subjected to an impulse heat input in a certain part of its volume and (2) simulating the laser powder bed fusion metal AM of three-part geometries with (a) Goldak’s moving heat source finite element method, (b) the proposed graph theory approach, and (c) further comparing the thermal trends predicted from the last two approaches with a commercial solution. From the first study, we report that the thermal trends approximated by the graph theory approach are found to be accurate within 5% of the Green’s functions-based analytical solution (in terms of the symmetric mean absolute percentage error). Results from the second study show that the thermal trends predicted for the AM parts using graph theory approach agree with finite element analyses, and the computational time for predicting the temperature distribution was significantly reduced with graph theory. For instance, for one of the AM part geometries studied, the temperature trends were predicted in less than 18 min within 10% error using the graph theory approach compared with over 180 min with finite element analyses. Although this paper is restricted to theoretical development and verification of the graph theory approach, our forthcoming research will focus on experimental validation through in-process thermal measurements.",j284,Journal of manufacturing science and engineering,jv284,accepted,f1214,2005,2005-09-19
s1517,p1517,Graph Theory,"Mathematics acts an important and essential need in different fields. One of the significant roles in mathematics is played by graph theory that is used in structural models and innovative methods, models in various disciplines for better strategic decisions. In mathematics, graph theory is the study through graphs by which the structural relationship studied with a pair wise relationship between different objects. The different types of network theory or models or model of the network are called graphs. These graphs do not form a part of analytical geometry, but they are called graph theory, which is points connected by lines. The various concepts of graph theory have varied applications in diverse fields. The chapter will deal with graph theory and its application in various financial market decisions. The topological properties of the network of stocks will provide a deeper understanding and a good conclusion to the market structure and connectivity. The chapter is very useful for academicians, market researchers, financial analysts, and economists.",c39,International Conference on Global Software Engineering,cp39,accepted,f1215,2020,2020-04-10
s1518,p1518,Novel reliable routing method for engineering of internet of vehicles based on graph theory,"
Purpose
The communication link in the engineering of Internet of Vehicle (IOV) is more frequent than the communication link in the Mobile ad hoc Network (MANET). Therefore, the highly dynamic network routing reliability problem is a research hotspot to be solved.


Design/methodology/approach
The graph theory is used to model the MANET communication diagram on the highway and propose a new reliable routing method for internet of vehicles based on graph theory.


Findings
The expanded graph theory can help capture the evolution characteristics of the network topology and predetermine the reliable route to promote quality of service (QoS) in the routing process. The program can find the most reliable route from source to the destination from the MANET graph theory.


Originality/value
The good performance of the proposed method is verified and compared with the related algorithms of the literature.
",j285,Engineering computations,jv285,accepted,f1216,2001,2001-04-22
s1519,p1519,Graph Theory with Applications,Abstract content goes here ...,c32,International Conference on Software Technology: Methods and Tools,cp32,accepted,f1217,2010,2010-06-01
s1520,p1520,A GENERAL POSITION PROBLEM IN GRAPH THEORY,"The paper introduces a graph theory variation of the general position problem: given a graph $G$ , determine a largest set $S$ of vertices of $G$ such that no three vertices of $S$ lie on a common geodesic. Such a set is a max-gp-set of $G$ and its size is the gp-number $\text{gp}(G)$ of $G$ . Upper bounds on $\text{gp}(G)$ in terms of different isometric covers are given and used to determine the gp-number of several classes of graphs. Connections between general position sets and packings are investigated and used to give lower bounds on the gp-number. It is also proved that the general position problem is NP-complete.",j286,Bulletin of the Australian Mathematical Society,jv286,accepted,f1218,2012,2012-11-05
s1521,p1521,Topics in graph theory,"A graph is a system G = (V, E) consisting of a set V of vertices and a set E (disjoint from V ) of edges, together with an incidence function End : E → M2(V ), where M2(V ) is set of all 2-element sub-multisets of V . We usually write V = V (G), E = E(G), and End = EndG. For each edge e ∈ E with End(e) = {u, v}, we called u, v the end-vertices of e, and say that the edge e is incident with the vertices u, v, or the vertices u, v are incident with the edge e, or the vertices u, v are adjacent by the edge e. Sometimes it is more convenient to just write the incidence relation as e = uv. If u = v, the edge e is called a loop; if u 6= v, the edge is called a link. Two edges are said to be parallel if their end vertices are the same. Parallel edges are also referred to multiple edges. A simple graph is a graph without loops and multiple edges. When we emphasize that a graph may have loops and multiple edges, we refer the graph as a multigraph. A graph is said to be (i) finite if it has finite number of vertices and edges; (ii) null if it has no vertices, and consequently has no edges; (iii) trivial if it has only one vertex with possible loops; (iv) empty if its has no edges; and (v) nontrivial if it is not trivial. A complete graph is a simple graph that every pair of vertices are adjacent. A complete graph with n vertices is denoted by Kn. A graph G is said to be bipartite if its vertex set V (G) can be partitioned into two disjoint nonempty parts X,Y such that every edge has one end-vertex in X and the other in Y ; such a partition {X,Y } is called a bipartition of G, and such a bipartite graph is denoted by G[X,Y ]. A bipartite graph G[X,Y ] is called a complete bipartite graph if each vertex in X is joined to every vertex in Y ; we abbreviate G[X,Y ] to Km,n if |X| = m and |Y | = n. Let G be a graph. Two vertices of G are called neighbors each other if they are adjacent. For each vertex v ∈ V (G), the set of neighbors of v in G is denoted by Nv(G), the number of edges incident with v (loops counted twice) is called the degree of v in G, denoted deg (v) or deg G(v). A vertex of degree 0 is called an isolated vertex; a vertex of degree 1 is called a leaf. A graph is said to be regular if its every vertex has the same degree. A graph is said to be k-regular if its every vertex has degree k. We always have",c50,International Conference on Automated Software Engineering,cp50,accepted,f1219,2008,2008-06-22
s1522,p1522,Graph Theory,Abstract content goes here ...,j287,Oberwolfach Reports,jv287,accepted,f1220,2017,2017-05-02
s1523,p1523,BRAPH: A graph theory software for the analysis of brain connectivity,"The brain is a large-scale complex network whose workings rely on the interaction between its various regions. In the past few years, the organization of the human brain network has been studied extensively using concepts from graph theory, where the brain is represented as a set of nodes connected by edges. This representation of the brain as a connectome can be used to assess important measures that reflect its topological architecture. We have developed a freeware MatLab-based software (BRAPH – BRain Analysis using graPH theory) for connectivity analysis of brain networks derived from structural magnetic resonance imaging (MRI), functional MRI (fMRI), positron emission tomography (PET) and electroencephalogram (EEG) data. BRAPH allows building connectivity matrices, calculating global and local network measures, performing non-parametric permutations for group comparisons, assessing the modules in the network, and comparing the results to random networks. By contrast to other toolboxes, it allows performing longitudinal comparisons of the same patients across different points in time. Furthermore, even though a user-friendly interface is provided, the architecture of the program is modular (object-oriented) so that it can be easily expanded and customized. To demonstrate the abilities of BRAPH, we performed structural and functional graph theory analyses in two separate studies. In the first study, using MRI data, we assessed the differences in global and nodal network topology in healthy controls, patients with amnestic mild cognitive impairment, and patients with Alzheimer’s disease. In the second study, using resting-state fMRI data, we compared healthy controls and Parkinson’s patients with mild cognitive impairment.",j153,bioRxiv,jv153,accepted,f1221,2020,2020-04-22
s1524,p1524,Modern Graph Theory,Abstract content goes here ...,c113,International Conference on Image Analysis and Processing,cp113,accepted,f1222,2002,2002-11-30
s1525,p1525,Graph Theory and Brain Connectivity in Alzheimer’s Disease,This article presents a review of recent advances in neuroscience research in the specific area of brain connectivity as a potential biomarker of Alzheimer’s disease with a focus on the application of graph theory. The review will begin with a brief overview of connectivity and graph theory. Then resent advances in connectivity as a biomarker for Alzheimer’s disease will be presented and analyzed.,j288,The Neuroscientist,jv288,accepted,f1223,2022,2022-06-18
s1526,p1526,Graph Theory,Abstract content goes here ...,c83,International Conference on Computer Graphics and Interactive Techniques,cp83,accepted,f1224,2015,2015-08-05
s1527,p1527,Graph Theory 1736 1936,"Thank you very much for downloading graph theory 1736 1936. Maybe you have knowledge that, people have search hundreds times for their favorite readings like this graph theory 1736 1936, but end up in malicious downloads. Rather than enjoying a good book with a cup of coffee in the afternoon, instead they cope with some malicious bugs inside their laptop. graph theory 1736 1936 is available in our digital library an online access to it is set as public so you can get it instantly. Our books collection spans in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the graph theory 1736 1936 is universally compatible with any devices to read.",c1,Technical Symposium on Computer Science Education,cp1,accepted,f1225,2002,2002-01-11
s1528,p1528,Cytoscape.js: a graph theory library for visualisation and analysis,"Summary: Cytoscape.js is an open-source JavaScript-based graph library. Its most common use case is as a visualization software component, so it can be used to render interactive graphs in a web browser. It also can be used in a headless manner, useful for graph operations on a server, such as Node.js. Availability and implementation: Cytoscape.js is implemented in JavaScript. Documentation, downloads and source code are available at http://js.cytoscape.org. Contact: gary.bader@utoronto.ca",c97,Interspeech,cp97,accepted,f1226,2004,2004-11-11
s1529,p1529,Study of biological networks using graph theory,Abstract content goes here ...,j289,Saudi Journal of Biological Sciences,jv289,accepted,f1227,2004,2004-05-01
s1530,p1530,Graph Theory-Based Pinning Synchronization of Stochastic Complex Dynamical Networks,"This paper is concerned with the adaptive pinning synchronization problem of stochastic complex dynamical networks (CDNs). Based on algebraic graph theory and Lyapunov theory, pinning controller design conditions are derived, and the rigorous convergence analysis of synchronization errors in the probability sense is also conducted. Compared with the existing results, the topology structures of stochastic CDN are allowed to be unknown due to the use of graph theory. In particular, it is shown that the selection of nodes for pinning depends on the unknown lower bounds of coupling strengths. Finally, an example on a Chua’s circuit network is given to validate the effectiveness of the theoretical results.",c107,British Machine Vision Conference,cp107,accepted,f1228,2012,2012-12-01
s1531,p1531,"Network science and the human brain: Using graph theory to understand the brain and one of its hubs, the amygdala, in health and disease","Over the past 15 years, the emerging field of network science has revealed the key features of brain networks, which include small‐world topology, the presence of highly connected hubs, and hierarchical modularity. The value of network studies of the brain is underscored by the range of network alterations that have been identified in neurological and psychiatric disorders, including epilepsy, depression, Alzheimer's disease, schizophrenia, and many others. Here we briefly summarize the concepts of graph theory that are used to quantify network properties and describe common experimental approaches for analysis of brain networks of structural and functional connectivity. These range from tract tracing to functional magnetic resonance imaging, diffusion tensor imaging, electroencephalography, and magnetoencephalography. We then summarize the major findings from the application of graph theory to nervous systems ranging from Caenorhabditis elegans to more complex primate brains, including man. Focusing, then, on studies involving the amygdala, a brain region that has attracted intense interest as a center for emotional processing, fear, and motivation, we discuss the features of the amygdala in brain networks for fear conditioning and emotional perception. Finally, to highlight the utility of graph theory for studying dysfunction of the amygdala in mental illness, we review data with regard to changes in the hub properties of the amygdala in brain networks of patients with depression. We suggest that network studies of the human brain may serve to focus attention on regions and connections that act as principal drivers and controllers of brain function in health and disease.†Published 2016",j290,Journal of Neuroscience Research,jv290,accepted,f1229,2016,2016-04-09
s1532,p1532,Band connectivity for topological quantum chemistry: Band structures as a graph theory problem,"The conventional theory of solids is well suited to describing band structures locally near isolated points in momentum space, but struggles to capture the full, global picture necessary for understanding topological phenomena. In part of a recent paper [B. Bradlyn et al., Nature 547, 298 (2017)], we have introduced the way to overcome this difficulty by formulating the problem of sewing together many disconnected local ""k-dot-p"" band structures across the Brillouin zone in terms of graph theory. In the current manuscript we give the details of our full theoretical construction. We show that crystal symmetries strongly constrain the allowed connectivities of energy bands, and we employ graph-theoretic techniques such as graph connectivity to enumerate all the solutions to these constraints. The tools of graph theory allow us to identify disconnected groups of bands in these solutions, and so identify topologically distinct insulating phases.",c37,Asia-Pacific Software Engineering Conference,cp37,accepted,f1230,2008,2008-08-15
s1533,p1533,Applying Graph Theory in Ecological Research,"Graph theory can be applied to ecological questions in many ways, and more insights can be gained by expanding the range of graph theoretical concepts applied to a specific system. But how do you know which methods might be used? And what do you do with the graph once it has been obtained? This book provides a broad introduction to the application of graph theory in different ecological systems, providing practical guidance for researchers in ecology and related fields. Readers are guided through the creation of an appropriate graph for the system being studied, including the application of spatial, spatio-temporal, and more abstract structural process graphs. Simple figures accompany the explanations to add clarity, and a broad range of ecological phenomena from many ecological systems are covered. This is the ideal book for graduate students and researchers looking to apply graph theoretical methods in their work.",c39,International Conference on Global Software Engineering,cp39,accepted,f1231,2020,2020-01-25
s1534,p1534,"Handbook of graph theory, combinatorial optimization, and algorithms","Basic Concepts and Algorithms Basic Concepts in Graph Theory and Algorithms Subramanian Arumugam and Krishnaiyan ""KT"" Thulasiraman Basic Graph Algorithms Krishnaiyan ""KT"" Thulasiraman Depth-First Search and Applications Krishnaiyan ""KT"" Thulasiraman Flows in Networks Maximum Flow Problem F. Zeynep Sargut, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Minimum Cost Flow Problem Balachandran Vaidyanathan, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Multi-Commodity Flows Balachandran Vaidyanathan, Ravindra K. Ahuja, James B. Orlin, and Thomas L. Magnanti Algebraic Graph Theory Graphs and Vector Spaces Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Incidence, Cut, and Circuit Matrices of a Graph Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Adjacency Matrix and Signal Flow Graphs Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Adjacency Spectrum and the Laplacian Spectrum of a Graph R. Balakrishnan Resistance Networks, Random Walks, and Network Theorems Krishnaiyan ""KT"" Thulasiraman and Mamta Yadav Structural Graph Theory Connectivity Subramanian Arumugam and Karam Ebadi Connectivity Algorithms Krishnaiyan ""KT"" Thulasiraman Graph Connectivity Augmentation Andras Frank and Tibor Jordan Matchings Michael D. Plummer Matching Algorithms Krishnaiyan ""KT"" Thulasiraman Stable Marriage Problem Shuichi Miyazaki Domination in Graphs Subramanian Arumugam and M. Sundarakannan Graph Colorings Subramanian Arumugam and K. Raja Chandrasekar Planar Graphs Planarity and Duality Krishnaiyan ""KT"" Thulasiraman and M.N.S. Swamy Edge Addition Planarity Testing Algorithm John M. Boyer Planarity Testing Based on PC-Trees Wen-Lian Hsu Graph Drawing Md. Saidur Rahman and Takao Nishizeki Interconnection Networks Introduction to Interconnection Networks S.A. Choudum, Lavanya Sivakumar, and V. Sunitha Cayley Graphs S. Lakshmivarahan, Lavanya Sivakumar, and S.K. Dhall Graph Embedding and Interconnection Networks S.A. Choudum, Lavanya Sivakumar, and V. Sunitha Special Graphs Program Graphs Krishnaiyan ""KT"" Thulasiraman Perfect Graphs Chinh T. Hoang and R. Sritharan Tree-Structured Graphs Andreas Brandstadt and Feodor F. Dragan Partitioning Graph and Hypergraph Partitioning Sachin B. Patkar and H. Narayanan Matroids Matroids H. Narayanan and Sachin B. Patkar Hybrid Analysis and Combinatorial Optimization H. Narayanan Probabilistic Methods, Random Graph Models, and Randomized Algorithms Probabilistic Arguments in Combinatorics C.R. Subramanian Random Models and Analyses for Chemical Graphs Daniel Pascua, Tina M. Kouri, and Dinesh P. Mehta Randomized Graph Algorithms: Techniques and Analysis Surender Baswana and Sandeep Sen Coping with NP-Completeness General Techniques for Combinatorial Approximation Sartaj Sahni epsilon-Approximation Schemes for the Constrained Shortest Path Problem Krishnaiyan ""KT"" Thulasiraman Constrained Shortest Path Problem: Lagrangian Relaxation-Based Algorithmic Approaches Ying Xiao and Krishnaiyan ""KT"" Thulasiraman Algorithms for Finding Disjoint Paths with QoS Constraints Alex Sprintson and Ariel Orda Set-Cover Approximation Neal E. Young Approximation Schemes for Fractional Multicommodity Flow Problems George Karakostas Approximation Algorithms for Connectivity Problems Ramakrishna Thurimella Rectilinear Steiner Minimum Trees Tao Huang and Evangeline F.Y. Young Fixed-Parameter Algorithms and Complexity Venkatesh Raman and Saket Saurabh",c3,Frontiers in Education Conference,cp3,accepted,f1232,2016,2016-10-18
s1535,p1535,An Introduction to Bipolar Single Valued Neutrosophic Graph Theory,"In this paper, we first define the concept of bipolar single neutrosophic graphs as the generalization of bipolar fuzzy graphs, N-graphs, intuitionistic fuzzy graph, single valued neutrosophic graphs and bipolar intuitionistic fuzzy graphs.",c88,Symposium on the Theory of Computing,cp88,accepted,f1233,2014,2014-09-08
s1536,p1536,Wavelets on Graphs via Spectral Graph Theory,Abstract content goes here ...,c71,IEEE International Conference on Information Reuse and Integration,cp71,accepted,f1234,2022,2022-06-16
s1538,p1538,Molecular Orbital Calculations Using Chemical Graph Theory,"molecular orbital calculations using chemical graph theory is available in our digital library an online access to it is set as public so you can get it instantly. Our book servers hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Kindly say, the molecular orbital calculations using chemical graph theory is universally compatible with any devices to read.",c57,IEEE International Conference on Engineering of Complex Computer Systems,cp57,accepted,f1235,2003,2003-06-25
s1539,p1539,A Brief Introduction to Spectral Graph Theory,"Spectral graph theory starts by associating matrices to graphs, notably, the adjacency matrix and the laplacian matrix. The general theme is then, firstly, to compute or estimate the eigenvalues of such matrices, and secondly, to relate the eigenvalues to structural properties of graphs. As it turns out, the spectral perspective is a powerful tool. Some of its loveliest applications concern facts that are, in principle, purely graph-theoretic or combinatorial. To give just one example, spectral ideas are a key ingredient in the proof of the so-called Friendship Theorem: if, in a group of people, any two persons have exactly one common friend, then there is a person who is everybody’s friend. This text is an introduction to spectral graph theory, but it could also be seen as an invitation to algebraic graph theory. On the one hand, there is, of course, the linear algebra that underlies the spectral ideas in graph theory. On the other hand, most of our examples are graphs of algebraic origin. The two recurring sources are Cayley graphs of groups, and graphs built out of finite fields. In the study of such graphs, some further algebraic ingredients (e.g., characters) naturally come up. The table of contents gives, as it should, a good glimpse of where is this text going. Very broadly, the first half is devoted to graphs, finite fields, and how they come together. This part is meant as an appealing and meaningful motivation. It provides a context that frames and fuels much of the second, spectral, half. Most sections have one or two exercises. Their position within the text is a hint. The exercises are optional, in the sense that virtually nothing in the main body depends on them. But the exercises are often of the non-trivial variety, and they should enhance the text in an interesting way. The hope is that the reader will enjoy them. We assume a basic familiarity with linear algebra, finite fields, and groups, but not necessarily with graph theory. This, again, betrays our algebraic perspective. This text is based on a course I taught in Göttingen, in the Fall of 2015. I would like to thank Jerome Baum for his help with some of the drawings. The present version is preliminary, and comments are welcome (email: bogdan.nica@gmail.com).",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f1236,2002,2002-06-20
s1540,p1540,Quantum Zero-Error Source-Channel Coding and Non-Commutative Graph Theory,"Alice and Bob receive a bipartite state (possibly entangled) from some finite collection or from some subspace. Alice sends a message to Bob through a noisy quantum channel such that Bob may determine the initial state, with zero chance of error. This framework encompasses, for example, teleportation, dense coding, entanglement assisted quantum channel capacity, and one-way communication complexity of function evaluation. With classical sources and channels, this problem can be analyzed using graph homomorphisms. We show this quantum version can be analyzed using homomorphisms on non-commutative graphs (an operator space generalization of graphs). Previously the Lovász ϑ number has been generalized to non-commutative graphs; we show this to be a homomorphism monotone, thus providing bounds on quantum source-channel coding. We generalize the Schrijver and Szegedy numbers, and show these to be monotones as well. As an application, we construct a quantum channel whose entanglement assisted zero-error one-shot capacity can only be unlocked using a non-maximally entangled state. These homomorphisms allow definition of a chromatic number for non-commutative graphs. Many open questions are presented regarding the possibility of a more fully developed theory.",j291,IEEE Transactions on Information Theory,jv291,accepted,f1237,2010,2010-03-04
s1541,p1541,Graph Theory And Sparse Matrix Computation,"Thank you for downloading graph theory and sparse matrix computation. As you may know, people have look numerous times for their chosen readings like this graph theory and sparse matrix computation, but end up in infectious downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they juggled with some infectious virus inside their laptop. graph theory and sparse matrix computation is available in our book collection an online access to it is set as public so you can get it instantly. Our books collection hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the graph theory and sparse matrix computation is universally compatible with any devices to read.",c11,Hawaii International Conference on System Sciences,cp11,accepted,f1238,2006,2006-08-10
s1543,p1543,Algorithmic graph theory and perfect graphs,Abstract content goes here ...,c43,ACM Symposium on Applied Computing,cp43,accepted,f1239,2001,2001-02-18
s1544,p1544,Three conjectures in extremal spectral graph theory,Abstract content goes here ...,c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f1240,2006,2006-08-19
s1545,p1545,Rational exponents in extremal graph theory,"Given a family of graphs $\mathcal{H}$, the extremal number $\textrm{ex}(n, \mathcal{H})$ is the largest $m$ for which there exists a graph with $n$ vertices and $m$ edges containing no graph from the family $\mathcal{H}$ as a subgraph. We show that for every rational number $r$ between $1$ and $2$, there is a family of graphs $\mathcal{H}_r$ such that $\textrm{ex}(n, \mathcal{H}_r) = \Theta(n^r)$. This solves a longstanding problem in the area of extremal graph theory.",c76,International Conference on Artificial Neural Networks,cp76,accepted,f1241,2013,2013-07-14
s1546,p1546,Presurgery resting‐state local graph‐theory measures predict neurocognitive outcomes after brain surgery in temporal lobe epilepsy,This study determined the ability of resting‐state functional connectivity (rsFC) graph‐theory measures to predict neurocognitive status postsurgery in patients with temporal lobe epilepsy (TLE) who underwent anterior temporal lobectomy (ATL).,j292,Epilepsia,jv292,accepted,f1242,2015,2015-02-11
s1549,p1549,Neutrosophic Graphs: A New Dimension to Graph Theory,"In this book authors for the first time have made a through study of neutrosophic graphs. This study reveals that these neutrosophic graphs give a new dimension to graph theory. The important feature of this book is it contains over 200 neutrosophic graphs to provide better understanding of this concepts. Further these graphs happen to behave in a unique way inmost cases, for even the edge colouring problem is different from the classical one. Several directions and dimensions in graph theory are obtained from this study.",c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f1243,2006,2006-06-27
s1551,p1551,Modern Graph Theory,Abstract content goes here ...,c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1244,2007,2007-02-12
s1552,p1552,Functional connectivity and graph theory in preclinical Alzheimer's disease,Abstract content goes here ...,j294,Neurobiology of Aging,jv294,accepted,f1245,2006,2006-06-21
s1553,p1553,An Introduction To The Theory Of Graph Spectra,"an introduction to the theory of graph spectra is available in our book collection an online access to it is set as public so you can download it instantly. Our digital library hosts in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the an introduction to the theory of graph spectra is universally compatible with any devices to read.",c90,Computer Vision and Pattern Recognition,cp90,accepted,f1246,2008,2008-09-13
s1554,p1554,A Survey on some Applications of Graph Theory in Cryptography,"Abstract Graph theory is rapidly moving into the main stream of research because of its applications in diverse fields such as biochemistry (genomics), coding theory, communication networks and their security etc. In particular researchers are exploring the concepts of graph theory that can be used in different areas of Cryptography. In this paper a review of the works carried out in the field of Cryptography which use the concepts of Graph Theory, is given. Some of the Cryptographic Algorithms based on general graph theory concepts, Extremal Graph Theory and Expander Graphs are analyzed.",c14,International Conference on Exploring Services Science,cp14,accepted,f1247,2016,2016-11-30
s1555,p1555,Algorithmic graph theory and perfect graphs,Abstract content goes here ...,c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f1248,2019,2019-08-16
s1556,p1556,Extremal Graph Theory,3 Third Lecture 11 3.1 Applications of the Zarankiewicz Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 The Turán Problem for Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.3 The Girth Problem and Moore’s Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.4 Application of Moore’s Bound to Graph Spanners . . . . . . . . . . . . . . . . . . . . . . . 14,c111,International Society for Music Information Retrieval Conference,cp111,accepted,f1249,2001,2001-09-24
s1557,p1557,Chemical Graph Theory,This chapter on chemical graph theory forms part of the natural science and processes section of the handbook,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f1250,2018,2018-12-16
s1560,p1560,"Handbook of Graph Theory, Second Edition","In the ten years since the publication of the best-selling first edition, more than 1,000 graph theory papers have been published each year. Reflecting these advances, Handbook of Graph Theory, Second Edition provides comprehensive coverage of the main topics in pure and applied graph theory. This second editionover 400 pages longer than its predecessorincorporates 14 new sections. Each chapter includes lists of essential definitions and facts, accompanied by examples, tables, remarks, and, in some cases, conjectures and open problems. A bibliography at the end of each chapter provides an extensive guide to the research literature and pointers to monographs. In addition, a glossary is included in each chapter as well as at the end of each section. This edition also contains notes regarding terminology and notation. With 34 new contributors, this handbook is the most comprehensive single-source guide to graph theory. It emphasizes quick accessibility to topics for non-experts and enables easy cross-referencing among chapters.",c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f1251,2016,2016-10-22
s1562,p1562,"Descriptive Complexity, Canonisation, and Definable Graph Structure Theory","Descriptive complexity theory establishes a connection between the computational complexity of algorithmic problems (the computational resources required to solve the problems) and their descriptive complexity (the language resources required to describe the problems). This ground-breaking book approaches descriptive complexity from the angle of modern structural graph theory, specifically graph minor theory. It develops a ‘definable structure theory’ concerned with the logical definability of graph-theoretic concepts such as tree decompositions and embeddings. The first part starts with an introduction to the background, from logic, complexity, and graph theory, and develops the theory up to first applications in descriptive complexity theory and graph isomorphism testing. It may serve as the basis for a graduate-level course. The second part is more advanced and mainly devoted to the proof of a single, previously unpublished theorem: properties of graphs with excluded minors are decidable in polynomial time if, and only if, they are definable in fixed-point logic with counting.",c63,IEEE International Software Metrics Symposium,cp63,accepted,f1252,2008,2008-06-11
s1563,p1563,Complex brain networks: graph theoretical analysis of structural and functional systems,Abstract content goes here ...,j296,Nature Reviews Neuroscience,jv296,accepted,f1253,2005,2005-08-03
s1565,p1565,Algorithm Design Using Spectral Graph Theory,"Spectral graph theory is the interplay between linear algebra and combinatorial graph theory. Laplace’s equation and its discrete form, the Laplacian matrix, appear ubiquitously in mathematical physics. Due to the recent discovery of very fast solvers for these equations, they are also becoming increasingly useful in combinatorial optimization, computer vision, computer graphics, and machine learning. In this thesis, we develop highly efficient and parallelizable algorithms for solving linear systems involving graph Laplacian matrices. These solvers can also be extended to symmetric diagonally dominant matrices and M -matrices, both of which are closely related to graph Laplacians. Our algorithms build upon two decades of progress on combinatorial preconditioning, which connects numerical and combinatorial algorithms through spectral graph theory. They in turn rely on tools from numerical analysis, metric embeddings, and random matrix theory. We give two solver algorithms that take diametrically opposite approaches. The first is motivated by combinatorial algorithms, and aims to gradually break the problem into several smaller ones. It represents major simplifications over previous solver constructions, and has theoretical running time comparable to sorting. The second is motivated by numerical analysis, and aims to rapidly improve the algebraic connectivity of the graph. It is the first highly efficient solver for Laplacian linear systems that parallelizes almost completely. Our results improve the performances of applications of fast linear system solvers ranging from scientific computing to algorithmic graph theory. We also show that these solvers can be used to address broad classes of image processing tasks, and give some preliminary experimental results.",c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,cp20,accepted,f1254,2014,2014-12-01
s1566,p1566,Fuzzy Graph Theory: A Survey,"A fuzzy graph (f-graph) is a pair G : ( σ, �) where σ is a fuzzy subset of a set S andis a fuzzy relation on σ. A fuzzy graph H : ( τ, υ) is called a partial fuzzy subgraph of G : ( σ, �) if τ (u) ≤ σ(u) for every u and υ (u, v) ≤ �(u, v) for every u and v . In particular we call a partial fuzzy subgraph H : ( τ, υ) a fuzzy subgraph of G : ( σ, � ) if τ (u) = σ(u) for every u in τ * and υ (u, v) = �(u, v) for every arc (u, v) in υ*. A connected f-graph G : ( σ, �) is a fuzzy tree(f-tree) if it has a fuzzy spannin g subgraph F : (σ, υ), which is a tree, where for all arcs (x, y) not i n F there exists a path from x to y in F whose strength is more than �(x, y). A path P of length n is a sequence of disti nct nodes u0, u 1, ..., u n such that �(u i1 , u i) > 0, i = 1, 2, ..., n and the degree of membershi p of a weakest arc is defined as its strength. If u 0 = u n and n ≥ 3, then P is called a cycle and a cycle P is called a fuzzy cycle(f-cycle) if it cont ains more than one weakest arc . The strength of connectedness between two nodes x and y is defined as the maximum of the strengths of all paths between x and y and is denot ed by CONN G(x, y). An x y path P is called a strongest x y",c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,cp20,accepted,f1255,2014,2014-06-20
s1567,p1567,Introduction to Graph Theory and Algebraic Graph Theory,Abstract content goes here ...,c92,Advances in Soft Computing,cp92,accepted,f1256,2009,2009-02-19
s1568,p1568,Graph Theory: An Introductory Course,"From the reviews: ""Bla Bollob's introductory course on graph theory deserves to be considered as a watershed in the development of this theory as a serious academic subject...The book has chapters on electrical networks, flows, connectivity and matchings, extremal problems, colouring, Ramsey theory, random graphs, and graphs and groups. Each chapter starts at a measured and gentle pace. Classical results are proved and new insight is provided, with the examples at the end of each chapter fully supplementing the text...Even so this allows an introduction not only to some of the deeper results but, more vitally, provides outlines of, and firm insights into, their proofs. Thus in an elementary text book, we gain an overall understanding of well-known standard results, and yet at the same time constant hints of, and guidelines into, the higher levels of the subject. It is this aspect of the book which should guarantee it a permanent place in the literature.""",c95,IEEE International Conference on Computer Vision,cp95,accepted,f1257,2017,2017-01-30
s1569,p1569,"Network meta‐analysis, electrical networks and graph theory","Network meta‐analysis is an active field of research in clinical biostatistics. It aims to combine information from all randomized comparisons among a set of treatments for a given medical condition. We show how graph‐theoretical methods can be applied to network meta‐analysis. A meta‐analytic graph consists of vertices (treatments) and edges (randomized comparisons). We illustrate the correspondence between meta‐analytic networks and electrical networks, where variance corresponds to resistance, treatment effects to voltage, and weighted treatment effects to current flows. Based thereon, we then show that graph‐theoretical methods that have been routinely applied to electrical networks also work well in network meta‐analysis. In more detail, the resulting consistent treatment effects induced in the edges can be estimated via the Moore–Penrose pseudoinverse of the Laplacian matrix. Moreover, the variances of the treatment effects are estimated in analogy to electrical effective resistances. It is shown that this method, being computationally simple, leads to the usual fixed effect model estimate when applied to pairwise meta‐analysis and is consistent with published results when applied to network meta‐analysis examples from the literature. Moreover, problems of heterogeneity and inconsistency, random effects modeling and including multi‐armed trials are addressed. Copyright © 2012 John Wiley & Sons, Ltd.",j297,Research Synthesis Methods,jv297,accepted,f1258,2001,2001-10-30
s1570,p1570,Comparing Brain Networks of Different Size and Connectivity Density Using Graph Theory,"Graph theory is a valuable framework to study the organization of functional and anatomical connections in the brain. Its use for comparing network topologies, however, is not without difficulties. Graph measures may be influenced by the number of nodes (N) and the average degree (k) of the network. The explicit form of that influence depends on the type of network topology, which is usually unknown for experimental data. Direct comparisons of graph measures between empirical networks with different N and/or k can therefore yield spurious results. We list benefits and pitfalls of various approaches that intend to overcome these difficulties. We discuss the initial graph definition of unweighted graphs via fixed thresholds, average degrees or edge densities, and the use of weighted graphs. For instance, choosing a threshold to fix N and k does eliminate size and density effects but may lead to modifications of the network by enforcing (ignoring) non-significant (significant) connections. Opposed to fixing N and k, graph measures are often normalized via random surrogates but, in fact, this may even increase the sensitivity to differences in N and k for the commonly used clustering coefficient and small-world index. To avoid such a bias we tried to estimate the N,k-dependence for empirical networks, which can serve to correct for size effects, if successful. We also add a number of methods used in social sciences that build on statistics of local network structures including exponential random graph models and motif counting. We show that none of the here-investigated methods allows for a reliable and fully unbiased comparison, but some perform better than others.",j108,PLoS ONE,jv108,accepted,f1259,2006,2006-04-17
s1571,p1571,A First Course in Graph Theory,"Because of its inherent simplicity, graph theory has a wide range of applications in engineering, and in physical sciences. It has of course uses in social sciences, in linguistics and in numerous other areas. In fact, a graph can be used to represent almost any physical situation involving discrete objects and the relationship among them. Now with the solutions to engineering and other problems becoming so complex leading to larger graphs, it is virtually difficult to analyze without the use of computers. This book is recommended in IIT Kharagpur, West Bengal for B.Tech Computer Science, NIT Arunachal Pradesh, NIT Nagaland, NIT Agartala, NIT Silchar, Gauhati University, Dibrugarh University, North Eastern Regional Institute of Management, Assam Engineering College, West Bengal Univerity of Technology (WBUT) for B.Tech, M.Tech Computer Science, University of Burdwan, West Bengal for B.Tech. Computer Science, Jadavpur University, West Bengal for M.Sc. Computer Science, Kalyani College of Engineering, West Bengal for B.Tech. Computer Science. Key Features: This book provides a rigorous yet informal treatment of graph theory with an emphasis on computational aspects of graph theory and graph-theoretic algorithms. Numerous applications to actual engineering problems are incorpo-rated with software design and optimization topics.",c19,ACM Conference on Economics and Computation,cp19,accepted,f1260,2002,2002-06-06
s1572,p1572,Link Prediction Based on Graph Neural Networks,"Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a `heuristic' that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel $\gamma$-decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the $\gamma$-decaying theory, we propose a new algorithm to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems.",c78,Neural Information Processing Systems,cp78,accepted,f1261,2012,2012-07-09
s1573,p1573,"Exponential Random Graph Models for Social Networks: Theory, Methods, and Applications","result of the social structure of American society.’’ This is true of the particular form or version of organized crime which he describes— it did not emanate from a transplanted Sicilian Mafia. That explanation, however, does not account for the many faces of organized crime in Australia, China, Russia, Japan, and many other places. Nor does it account for the growing phenomenon of transnational organized crime. Apart from any points of disagreement, this is a serious work of scholarship on a subject that has too often gotten short shrift in that respect. Organized Crime in Chicago is a good addition to the organized crime literature.",c60,IEEE International Conference on Software Engineering and Formal Methods,cp60,accepted,f1262,2011,2011-05-16
s1574,p1574,Test-Retest Reliability of Graph Theory Measures of Structural Brain Connectivity,Abstract content goes here ...,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,cp79,accepted,f1263,2020,2020-03-31
s1576,p1576,Graph Neural Networks Exponentially Lose Expressive Power for Node Classification,"Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erdős -- Renyi graph. We show that when the Erdős -- Renyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ""information loss"" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at this https URL.",c80,International Conference on Learning Representations,cp80,accepted,f1264,2005,2005-12-20
s1577,p1577,Recent developments in graph Ramsey theory,"Given a graph $H$, the Ramsey number $r(H)$ is the smallest natural number $N$ such that any two-colouring of the edges of $K_N$ contains a monochromatic copy of $H$. The existence of these numbers has been known since 1930 but their quantitative behaviour is still not well understood. Even so, there has been a great deal of recent progress on the study of Ramsey numbers and their variants, spurred on by the many advances across extremal combinatorics. In this survey, we will describe some of this progress.",c75,International Conference on Machine Learning,cp75,accepted,f1265,2005,2005-05-04
s1579,p1579,Using graph theory to analyze biological networks,Abstract content goes here ...,j265,BioData Mining,jv265,accepted,f1266,2009,2009-04-05
s1580,p1580,Spectral Graph Theory,Abstract content goes here ...,c56,European Conference on Software Process Improvement,cp56,accepted,f1267,2016,2016-04-25
s1581,p1581,Graph theory and molecular orbitals. Total φ-electron energy of alternant hydrocarbons,Abstract content goes here ...,c28,International Conference on Collaboration Technologies and Systems,cp28,accepted,f1268,2009,2009-08-04
s1584,p1584,"An $L^{p}$ theory of sparse graph convergence II: LD convergence, quotients and right convergence","We extend the LpLp theory of sparse graph limits, which was introduced in a companion paper, by analyzing different notions of convergence. Under suitable restrictions on node weights, we prove the equivalence of metric convergence, quotient convergence, microcanonical ground state energy convergence, microcanonical free energy convergence and large deviation convergence. Our theorems extend the broad applicability of dense graph convergence to all sparse graphs with unbounded average degree, while the proofs require new techniques based on uniform upper regularity. Examples to which our theory applies include stochastic block models, power law graphs and sparse versions of WW-random graphs.",c112,Very Large Data Bases Conference,cp112,accepted,f1269,2018,2018-02-22
s1585,p1585,Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties.,"The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with 10^{4} data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.",j91,Physical Review Letters,jv91,accepted,f1270,2006,2006-08-28
s1586,p1586,"Algorithms, Graph Theory, and Linear Equations in Laplacian Matrices","The Laplacian matrices of graphs are fundamental. In addition to facilitating the application of linear algebra to graph theory, they arise in many practical problems. In this talk we survey recent progress on the design of provably fast algorithms for solving linear equations in the Laplacian matrices of graphs. These algorithms motivate and rely upon fascinating primitives in graph theory, including low-stretch spanning trees, graph sparsifiers, ultra-sparsifiers, and local graph clustering. These are all connected by a definition of what it means for one graph to approximate another. While this definition is dictated by Numerical Linear Algebra, it proves useful and natural from a graph theoretic perspective. Mathematics Subject Classification (2010). Primary 68Q25; Secondary 65F08.",c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f1271,2006,2006-07-06
s1588,p1588,Local Higher-Order Graph Clustering,"Local graph clustering methods aim to find a cluster of nodes by exploring a small region of the graph. These methods are attractive because they enable targeted clustering around a given seed node and are faster than traditional global graph clustering methods because their runtime does not depend on the size of the input graph. However, current local graph partitioning methods are not designed to account for the higher-order structures crucial to the network, nor can they effectively handle directed networks. Here we introduce a new class of local graph clustering methods that address these issues by incorporating higher-order network information captured by small subgraphs, also called network motifs. We develop the Motif-based Approximate Personalized PageRank (MAPPR) algorithm that finds clusters containing a seed node with minimal \emph{motif conductance}, a generalization of the conductance metric for network motifs. We generalize existing theory to prove the fast running time (independent of the size of the graph) and obtain theoretical guarantees on the cluster quality (in terms of motif conductance). We also develop a theory of node neighborhoods for finding sets that have small motif conductance, and apply these results to the case of finding good seed nodes to use as input to the MAPPR algorithm. Experimental validation on community detection tasks in both synthetic and real-world networks, shows that our new framework MAPPR outperforms the current edge-based personalized PageRank methodology.",c16,Knowledge Discovery and Data Mining,cp16,accepted,f1272,2003,2003-03-23
s1589,p1589,Some new results in extremal graph theory,"In recent years several classical results in extremal graph theory have been improved in a uniform way and their proofs have been simplified and streamlined. These results include a new Erd\H{o}s-Stone-Bollob\'as theorem, several stability theorems, several saturation results and bounds for the number of graphs with large forbidden subgraphs. Another recent trend is the expansion of spectral extremal graph theory, in which extremal properties of graphs are studied by means of eigenvalues of various matrices. One particular achievement in this area is the casting of the central results above in spectral terms, often with additional enhancement. In addition, new, specific spectral results were found that have no conventional analogs. All of the above material is scattered throughout various journals, and since it may be of some interest, the purpose of this survey is to present the best of these results in a uniform, structured setting, together with some discussions of the underpinning ideas.",c114,IEEE International Conference on Robotics and Automation,cp114,accepted,f1273,2011,2011-07-11
s1591,p1591,Systemic Risk in Energy Derivative Markets: A Graph-Theory Analysis,"This article uses graph theory to provide novel evidence regarding market integration, a favorable condition for systemic risk to appear in. Relying on daily futures returns covering a 12-year period, we examine cross- and inter-market linkages, both within the commodity complex and between commodities and other financial assets. In such a high dimensional analysis, graph theory enables us to understand the dynamic behavior of our price system. We show that energy markets - as a whole - stand at the heart of this system. We also establish that crude oil is itself at the center of the energy complex. Further, we provide evidence that commodity markets have become more integrated over time.",c14,International Conference on Exploring Services Science,cp14,accepted,f1274,2016,2016-08-13
s1592,p1592,Applications of Graph Theory in Computer Science,"Graphs are among the most ubiquitous models of both natural and human-made structures. They can be used to model many types of relations and process dynamics in computer science, physical, biological and social systems. Many problems of practical interest can be represented by graphs. In general graphs theory has a wide range of applications in diverse fields. This paper explores different elements involved in graph theory including graph representations using computer systems and graph-theoretic data structures such as list structure and matrix structure. The emphasis of this paper is on graph applications in computer science. To demonstrate the importance of graph theory in computer science, this article addresses most common applications for graph theory in computer science. These applications are presented especially to project the idea of graph theory and to demonstrate its importance in computer science.",c10,Big Data,cp10,accepted,f1275,2021,2021-04-24
s1593,p1593,"Graph Theory, Combinatorics and Algorithms: Interdisciplinary Applications","Graph Theory, Combinatorics and Algorithms: Interdisciplinary Applications focuses on discrete mathematics and combinatorial algorithms interacting with real world problems in computer science, operations research, applied mathematics and engineering.The book containseleven chapters written by experts in their respective fields, and covers a wide spectrum of high-interest problems across these discipline domains. Among the contributing authors are Richard Karp of UC Berkeley and Robert Tarjan of Princeton; both are at the pinnacle of research scholarship in Graph Theory and Combinatorics. The chapters from the contributing authors focus on ""real world"" applications, all of which will be of considerable interest across the areas of Operations Research, Computer Science, Applied Mathematics, and Engineering. These problems include Internet congestion control, high-speed communication networks, multi-object auctions, resource allocation, software testing, data structures, etc. In sum, this is a book focused on major, contemporary problems, written by the top research scholars in the field, using cutting-edge mathematical and computational techniques.",c71,IEEE International Conference on Information Reuse and Integration,cp71,accepted,f1276,2022,2022-09-14
s1594,p1594,Relational Pooling for Graph Representations,"This work generalizes graph neural networks (GNNs) beyond those based on the Weisfeiler-Lehman (WL) algorithm, graph Laplacians, and diffusions. Our approach, denoted Relational Pooling (RP), draws from the theory of finite partial exchangeability to provide a framework with maximal representation power for graphs. RP can work with existing graph representation models and, somewhat counterintuitively, can make them even more powerful than the original WL isomorphism test. Additionally, RP allows architectures like Recurrent Neural Networks and Convolutional Neural Networks to be used in a theoretically sound approach for graph classification. We demonstrate improved performance of RP-based graph representations over state-of-the-art methods on a number of tasks.",c75,International Conference on Machine Learning,cp75,accepted,f1277,2005,2005-08-30
s1595,p1595,Graph Embedding and Extensions: A General Framework for Dimensionality Reduction,"A large family of algorithms - supervised or unsupervised; stemming from statistics or geometry theory - has been designed to provide different solutions to the problem of dimensionality reduction. Despite the different motivations of these algorithms, we present in this paper a general formulation known as graph embedding to unify them within a common framework. In graph embedding, each algorithm can be considered as the direct graph embedding or its linear/kernel/tensor extension of a specific intrinsic graph that describes certain desired statistical or geometric properties of a data set, with constraints from scale normalization or a penalty graph that characterizes a statistical or geometric property that should be avoided. Furthermore, the graph embedding framework can be used as a general platform for developing new dimensionality reduction algorithms. By utilizing this framework as a tool, we propose a new supervised dimensionality reduction algorithm called marginal Fisher analysis in which the intrinsic graph characterizes the intraclass compactness and connects each data point with its neighboring points of the same class, while the penalty graph connects the marginal points and characterizes the interclass separability. We show that MFA effectively overcomes the limitations of the traditional linear discriminant analysis algorithm due to data distribution assumptions and available projection directions. Real face recognition experiments show the superiority of our proposed MFA in comparison to LDA, also for corresponding kernel and tensor extensions",j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,jv299,accepted,f1278,2020,2020-07-20
s1596,p1596,Topological Graph Theory,Introduction Voltage Graphs and Covering Spaces Surfaces and Graph Imbeddings Imbedded Voltage Graphs and Current Graphs Map Colorings The Genus of A Group References.,c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f1279,2019,2019-08-11
s1597,p1597,Handbook of graph theory,"Introduction to Graphs Fundamentals of Graph Theory, Jonathan L. Gross and Jay Yellen Families of Graphs and Digraphs, Lowell W. Beineke History of Graph Theory, Robin J. Wilson Graph Representation Computer Representation of Graphs, Alfred V. Aho Graph Isomorphism, Brendan D. McKay The Reconstruction Problem, Josef Lauri Recursively Constructed Graphs, Richard B. Borie, R. Gary Parker, and Craig A. Tovey Structural Graph Theory, Maria Chudnovsky Directed Graphs Basic Digraph Models and Properties, Jay Yellen Directed Acyclic Graphs, Stephen B. Maurer Tournaments, K.B. Reid Connectivity and Traversability Connectivity Properties and Structure, Camino Balbuena, Josep Fabrega, and Miguel Angel Fiol Eulerian Graphs, Herbert Fleischner Chinese Postman Problems, R. Gary Parker and Richard B. Borie DeBruijn Graphs and Sequences, A.K. Dewdney Hamiltonian Graphs, Ronald J. Gould Traveling Salesman Problems, Gregory Gutin Further Topics in Connectivity, Josep Fabrega and Miguel Angel Fiol Colorings and Related Topics Graph Coloring, Zsolt Tuza Further Topics in Graph Coloring, Zsolt Tuza Independence and Cliques, Gregory Gutin Factors and Factorization, Michael Plummer Applications to Timetabling, Edmund Burke, Dominique de Werra, and Jeffrey Kingston Graceful Labelings, Joseph A. Gallian Algebraic Graph Theory Automorphisms, Mark E. Watkins Cayley Graphs, Brian Alspach Enumeration, Paul K. Stockmeyer Graphs and Vector Spaces, Krishnaiyan ""KT"" Thulasiraman Spectral Graph Theory, Michael Doob Matroidal Methods in Graph Theory, James Oxley Topological Graph Theory Graphs on Surfaces, Tomaz Pisanski and Primoz Potocnik Minimum Genus and Maximum Genus, Jianer Chen Genus Distributions, Jonathan L. Gross Voltage Graphs, Jonathan L. Gross The Genus of a Group, Thomas W. Tucker Maps, Roman Nedela and Martin Skoviera Representativity, Dan Archdeacon Triangulations, Seiya Negami Graphs and Finite Geometries, Arthur T. White Crossing Numbers, R. Bruce Richter and Gelasio Salazar Analytic Graph Theory Extremal Graph Theory, Bela Bollobas and Vladimir Nikiforov Random Graphs, Nicholas Wormald Ramsey Graph Theory, Ralph J. Faudree The Probabilistic Method, Alan Frieze and Po-Shen Loh Graph Limits, Bojan Mohar Graphical Measurement Distance in Graphs, Gary Chartrand and Ping Zhang Domination in Graphs, Teresa W. Haynes and Michael A. Henning Tolerance Graphs, Martin Charles Golumbic Bandwidth, Robert C. Brigham Pursuit-Evasion Problems, Richard B. Borie, Sven Koenig, and Craig A. Tovey Graphs in Computer Science Searching, Harold N. Gabow Dynamic Graph Algorithms, Camil Demetrescu, Irene Finocchi, and Giuseppe F. Italiano Drawings of Graphs, Emilio Di Giacomo, Giuseppe Liotta, and Roberto Tamassia Algorithms on Recursively Constructed Graphs, Richard B. Borie, R. Gary Parker, and Craig A. Tovey Fuzzy Graphs, John N. Mordeson and D.S. Malik Expander Graphs, Mike Krebs and Anthony Shaheen Visibility Graphs, Alice M. Dean and Joan P. Hutchinson Networks and Flows Maximum Flows, Clifford Stein Minimum Cost Flows, Lisa Fleischer Matchings and Assignments, Jay Sethuraman and Douglas R. Shier Communication Networks Complex Networks, Anthony Bonato and Fan Chung Broadcasting and Gossiping, Hovhannes A. Harutyunyan, Arthur L. Liestman, Joseph G. Peters, and Dana Richards Communication Network Design Models, Prakash Mirchandani and David Simchi-Levi Network Science for Graph Theorists, David C. Arney and Steven B. Horton Natural Science and Processes Chemical Graph Theory, Ernesto Estrada and Danail Bonchev Ties between Graph Theory and Biology, Jacek Blazewicz, Marta Kasprzak, and Nikos Vlassis Index A Glossary appears at the end of each chapter.",c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f1280,2006,2006-06-12
s1598,p1598,Introduction to Graph and Hypergraph Theory,Preface Basic Definitions and Concepts Trees and Bipartite Graphs Chordal Graphs Planar Graphs Graph Coloring Traversals and Flows Basic Hypergraph Concepts Hypertrees and Chordal Hypergraphs Some Other Remarkable Hypergraph Classes Hypergraph Coloring Modeling with Hypergraphs Appendix Index.,c84,The Web Conference,cp84,accepted,f1281,2006,2006-09-05
s1599,p1599,A graph‐theory algorithm for rapid protein side‐chain prediction,"Fast and accurate side‐chain conformation prediction is important for homology modeling, ab initio protein structure prediction, and protein design applications. Many methods have been presented, although only a few computer programs are publicly available. The SCWRL program is one such method and is widely used because of its speed, accuracy, and ease of use. A new algorithm for SCWRL is presented that uses results from graph theory to solve the combinatorial problem encountered in the side‐chain prediction problem. In this method, side chains are represented as vertices in an undirected graph. Any two residues that have rotamers with nonzero interaction energies are considered to have an edge in the graph. The resulting graph can be partitioned into connected subgraphs with no edges between them. These subgraphs can in turn be broken into biconnected components, which are graphs that cannot be disconnected by removal of a single vertex. The combinatorial problem is reduced to finding the minimum energy of these small biconnected components and combining the results to identify the global minimum energy conformation. This algorithm is able to complete predictions on a set of 180 proteins with 34,342 side chains in <7 min of computer time. The total χ1 and χ1 + 2 dihedral angle accuracies are 82.6% and 73.7% using a simple energy function based on the backbone‐dependent rotamer library and a linear repulsive steric energy. The new algorithm will allow for use of SCWRL in more demanding applications such as sequence design and ab initio structure prediction, as well addition of a more complex energy function and conformational flexibility, leading to increased accuracy.",j179,Protein Science,jv179,accepted,f1282,2001,2001-03-31
s1600,p1600,A graph‐theory algorithm for rapid protein side‐chain prediction,"Fast and accurate side‐chain conformation prediction is important for homology modeling, ab initio protein structure prediction, and protein design applications. Many methods have been presented, although only a few computer programs are publicly available. The SCWRL program is one such method and is widely used because of its speed, accuracy, and ease of use. A new algorithm for SCWRL is presented that uses results from graph theory to solve the combinatorial problem encountered in the side‐chain prediction problem. In this method, side chains are represented as vertices in an undirected graph. Any two residues that have rotamers with nonzero interaction energies are considered to have an edge in the graph. The resulting graph can be partitioned into connected subgraphs with no edges between them. These subgraphs can in turn be broken into biconnected components, which are graphs that cannot be disconnected by removal of a single vertex. The combinatorial problem is reduced to finding the minimum energy of these small biconnected components and combining the results to identify the global minimum energy conformation. This algorithm is able to complete predictions on a set of 180 proteins with 34,342 side chains in <7 min of computer time. The total χ1 and χ1 + 2 dihedral angle accuracies are 82.6% and 73.7% using a simple energy function based on the backbone‐dependent rotamer library and a linear repulsive steric energy. The new algorithm will allow for use of SCWRL in more demanding applications such as sequence design and ab initio structure prediction, as well addition of a more complex energy function and conformational flexibility, leading to increased accuracy.",j179,Protein Science,jv179,accepted,f1283,2001,2001-06-22
s1601,p1601,Elements of Information Theory,"Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.",c56,European Conference on Software Process Improvement,cp56,accepted,f1284,2016,2016-12-25
s1602,p1602,APPLICATIONS OF GRAPH THEORY IN COMPUTER SCIENCE AN OVERVIEW,"The field of mathematics plays vital role in various fields. One of the important areas in mathematics is graph theory which is used in structural models. This structural arrangements of various objects or technologies lead to new inventions and modifications in the existing environment for enhancement in those fields. The field graph theory started its journey from the problem of Koinsberg bridge in 1735. This paper gives an overview of the applications of graph theory in heterogeneous fields to some extent but mainly focuses on the computer science applications that uses graph theoretical concepts. Various papers based on graph theory have been studied related to scheduling concepts, computer science applications and an overview has been presented here.",c25,International Conference on Contemporary Computing,cp25,accepted,f1285,2014,2014-02-22
s1603,p1603,Graph Theory in the Information Age,"I n the past decade, graph theory has gone through a remarkable shift and a profound transformation. The change is in large part due to the humongous amount of information that we are confronted with. A main way to sort through massive data sets is to build and examine the network formed by interrelations. For example, Google’s successful Web search algorithms are based on the WWW graph, which contains all Web pages as vertices and hyperlinks as edges. There are all sorts of information networks, such as biological networks built from biological databases and social networks formed by email, phone calls, instant messaging, etc., as well as various types of physical networks. Of particular interest to mathematicians is the collaboration graph, which is based on the data from Mathematical Reviews. In the collaboration graph, every mathematician is a vertex, and two mathematicians who wrote a joint paper are connected by an edge. Figure 1 illustrates a portion of the collaboration graph consisting of about 5,000 vertices, representing mathematicians with Erdős number 2 (i.e., mathematicians who wrote a paper with a coauthor of Paul Erdős). Graph theory has two hundred years of history studying the basic mathematical structures called graphs. A graph G consists of a collection V of vertices and a collection E of edges that connect pairs of vertices. In the past, graph theory has",c107,British Machine Vision Conference,cp107,accepted,f1286,2012,2012-06-28
s1604,p1604,Graph Theory and Complex Networks: An Introduction,"Chapter Description 01: Introduction History, background 02: Foundations Basic terminology and properties of graphs 03: Extensions Directed & weighted graphs, colorings 04: Network traversal Walking through graphs (cf. traveling) 05: Trees Graphs without cycles; routing algorithms 06: Network analysis Basic metrics for analyzing large graphs 07: Random networks Introduction modeling real-world networks 08: Computer networks The Internet & WWW seen as a huge graph 09: Social networks Communities seen as graphs",c106,Chinese Conference on Biometric Recognition,cp106,accepted,f1287,2016,2016-03-06
s1605,p1605,Assessing the vulnerability of supply chains using graph theory,Abstract content goes here ...,c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f1288,2012,2012-05-01
s1607,p1607,Functional neural network analysis in frontotemporal dementia and Alzheimer's disease using EEG and graph theory,Abstract content goes here ...,j300,BMC Neuroscience,jv300,accepted,f1289,2005,2005-05-29
s1610,p1610,Urban-Area and Building Detection Using SIFT Keypoints and Graph Theory,"Very high resolution satellite images provide valuable information to researchers. Among these, urban-area boundaries and building locations play crucial roles. For a human expert, manually extracting this valuable information is tedious. One possible solution to extract this information is using automated techniques. Unfortunately, the solution is not straightforward if standard image processing and pattern recognition techniques are used. Therefore, to detect the urban area and buildings in satellite images, we propose the use of scale invariant feature transform (SIFT) and graph theoretical tools. SIFT keypoints are powerful in detecting objects under various imaging conditions. However, SIFT is not sufficient for detecting urban areas and buildings alone. Therefore, we formalize the problem in terms of graph theory. In forming the graph, we represent each keypoint as a vertex of the graph. The unary and binary relationships between these vertices (such as spatial distance and intensity values) lead to the edges of the graph. Based on this formalism, we extract the urban area using a novel multiple subgraph matching method. Then, we extract separate buildings in the urban area using a novel graph cut method. We form a diverse and representative test set using panchromatic 1-m-resolution Ikonos imagery. By extensive testings, we report very promising results on automatically detecting urban areas and buildings.",j99,IEEE Transactions on Geoscience and Remote Sensing,jv99,accepted,f1290,2013,2013-03-20
s1611,p1611,A property of eigenvectors of nonnegative symmetric matrices and its application to graph theory,Abstract content goes here ...,c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f1291,2018,2018-10-28
s1613,p1613,An Optimal Graph Theoretic Approach to Data Clustering: Theory and Its Application to Image Segmentation,"A novel graph theoretic approach for data clustering is presented and its application to the image segmentation problem is demonstrated. The data to be clustered are represented by an undirected adjacency graph G with arc capacities assigned to reflect the similarity between the linked vertices. Clustering is achieved by removing arcs of G to form mutually exclusive subgraphs such that the largest inter-subgraph maximum flow is minimized. For graphs of moderate size ( approximately 2000 vertices), the optimal solution is obtained through partitioning a flow and cut equivalent tree of G, which can be efficiently constructed using the Gomory-Hu algorithm (1961). However for larger graphs this approach is impractical. New theorems for subgraph condensation are derived and are then used to develop a fast algorithm which hierarchically constructs and partitions a partially equivalent tree of much reduced size. This algorithm results in an optimal solution equivalent to that obtained by partitioning the complete equivalent tree and is able to handle very large graphs with several hundred thousand vertices. The new clustering algorithm is applied to the image segmentation problem. The segmentation is achieved by effectively searching for closed contours of edge elements (equivalent to minimum cuts in G), which consist mostly of strong edges, while rejecting contours containing isolated strong edges. This method is able to accurately locate region boundaries and at the same time guarantees the formation of closed edge contours. >",j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,jv299,accepted,f1292,2020,2020-08-02
s1614,p1614,Network Analysis of World Subway Systems Using Updated Graph Theory,"This paper demonstrates that network topologies play a key role in attracting people to use public transit; ridership is not solely determined by cultural characteristics (North American versus European versus Asian) or city design (transit oriented versus automobile oriented). The analysis considers 19 subway systems worldwide: those in Toronto, Ontario, Canada; Montreal, Quebec, Canada; Chicago, Illinois; New York City; Washington, D.C.; San Francisco, California; Mexico City, Mexico; London; Paris; Lyon, France; Madrid, Spain; Berlin; Athens, Greece; Stockholm, Sweden; Moscow; Tokyo; Osaka, Japan; Seoul, South Korea; and Singapore. The relationship between ridership and network design was studied by using updated graph theory concepts. Ridership was computed as the annual number of boardings per capita. Network design was measured according to three major indicators. The first is a measure of transit coverage and is based on the total number of stations and land area. The second relates to the maximum number of transfers necessary to go from one station to another and is called directness. The third attempts to get an overall view of transfer possibilities to travel in the network to appreciate a sense of mobility; it is termed connectivity. Multiple-regression analysis showed a strong relationship between these three indicators and ridership, achieving a goodness of fit (adjusted R2 value) of .725. The importance of network design is significant and should be considered in future public transportation projects.",c60,IEEE International Conference on Software Engineering and Formal Methods,cp60,accepted,f1293,2011,2011-12-14
s1615,p1615,Discovering genetic ancestry using spectral graph theory,"As one approach to uncovering the genetic underpinnings of complex disease, individuals are measured at a large number of genetic variants (usually SNPs) across the genome and these SNP genotypes are assessed for association with disease status. We propose a new statistical method called Spectral‐GEM for the analysis of genome‐wide association studies; the goal of Spectral‐GEM is to quantify the ancestry of the sample from such genotypic data. Ignoring structure due to differential ancestry can lead to an excess of spurious findings and reduce power. Ancestry is commonly estimated using the eigenvectors derived from principal component analysis (PCA). To develop an alternative to PCA we draw on connections between multidimensional scaling and spectral graph theory. Our approach, based on a spectral embedding derived from the normalized Laplacian of a graph, can produce more meaningful delineation of ancestry than by using PCA. Often the results from Spectral‐GEM are straightforward to interpret and therefore useful in association analysis. We illustrate the new algorithm with an analysis of the POPRES data [Nelson et al., 2008]. Genet. Epidemiol. 34:51–59, 2010. © 2009 Wiley‐Liss, Inc.",j301,Genetic Epidemiology,jv301,accepted,f1294,2015,2015-02-09
s1616,p1616,Insights into the Organization of Biochemical Regulatory Networks Using Graph Theory Analyses*,"Graph theory has been a valuable mathematical modeling tool to gain insights into the topological organization of biochemical networks. There are two types of insights that may be obtained by graph theory analyses. The first provides an overview of the global organization of biochemical networks; the second uses prior knowledge to place results from multivariate experiments, such as microarray data sets, in the context of known pathways and networks to infer regulation. Using graph analyses, biochemical networks are found to be scale-free and small-world, indicating that these networks contain hubs, which are proteins that interact with many other molecules. These hubs may interact with many different types of proteins at the same time and location or at different times and locations, resulting in diverse biological responses. Groups of components in networks are organized in recurring patterns termed network motifs such as feedback and feed-forward loops. Graph analysis revealed that negative feedback loops are less common and are present mostly in proximity to the membrane, whereas positive feedback loops are highly nested in an architecture that promotes dynamical stability. Cell signaling networks have multiple pathways from some input receptors and few from others. Such topology is reminiscent of a classification system. Signaling networks display a bow-tie structure indicative of funneling information from extracellular signals and then dispatching information from a few specific central intracellular signaling nexuses. These insights show that graph theory is a valuable tool for gaining an understanding of global regulatory features of biochemical networks.",j280,Journal of Biological Chemistry,jv280,accepted,f1295,2007,2007-03-12
s1617,p1617,Discrete Mathematics and Graph Theory,"This comprehensive and self-contained text provides a thorough understanding of the concepts and applications of discrete mathematics and graph theory. It is written in such a manner that beginners can develop an interest in the subject. Besides providing the essentials, it also provides problem-solving techniques and develops the skill of how to think logically. Organized into two parts. The first part on discrete mathematics covers a wide range of topics such as predicate logic, recurrences, generating function, combinatorics, partially-ordered sets, lattices, Boolean algebra, finite state machines, finite fields, elementary number theory and discrete probability. The second part on graph theory covers planarity, colouring and partitioning, directed and algebraic graphs.",c11,Hawaii International Conference on System Sciences,cp11,accepted,f1296,2006,2006-06-18
s1618,p1618,Spectral Graph Theory and Network Dependability,"The paper introduces methods of graph theory for ranking substations of an electric power grid. In particular, spectral graph theory is used and several ranking algorithms are described. The procedure is illustrated on a practical numerical example",c23,International Conference on Open and Big Data,cp23,accepted,f1297,2012,2012-10-04
s1619,p1619,Computational Discrete Mathematics: Combinatorics and Graph Theory with Mathematica ®,"With examples of all 450 functions in action plus tutorial text on the mathematics, this book is the definitive guide to Experimenting with Combinatorica, a widely used software package for teaching and research in discrete mathematics. Three interesting classes of exercises are provided--theorem/proof, programming exercises, and experimental explorations--ensuring great flexibility in teaching and learning the material. The Combinatorica user community ranges from students to engineers, researchers in mathematics, computer science, physics, economics, and the humanities. Recipient of the EDUCOM Higher Education Software Award, Combinatorica is included with every copy of the popular computer algebra system Mathematica.",c80,International Conference on Learning Representations,cp80,accepted,f1298,2005,2005-06-20
s1620,p1620,Graph theory with applications to engineering and computer science,"Introductory Graph Theory with ApplicationsGraph Theory with ApplicationsResearch Topics in Graph Theory and Its ApplicationsChemical Graph TheoryMathematical Foundations and Applications of Graph EntropyGraph Theory with Applications to Engineering and Computer ScienceGraphs Theory and ApplicationsQuantitative Graph TheoryApplied Graph TheoryChemical Graph TheoryA First Course in Graph TheoryGraph TheoryGraph Theory with ApplicationsGraph Theory with ApplicationsSpectra of GraphsFuzzy Graph Theory with Applications to Human TraffickingApplications of Graph TheoryChemical Applications of Graph TheoryRecent Advancements in Graph TheoryA Textbook of Graph TheoryGraph Theory and Its Engineering ApplicationsGraph Theory, Combinatorics, and ApplicationsAdvanced Graph Theory and CombinatoricsTopics in Intersection Graph TheoryGraph Theory with Applications to Engineering and Computer ScienceGraph Theory and Its Applications, Second EditionHandbook of Research on Advanced Applications of Graph Theory in Modern SocietyGraph Theory with Applications to Algorithms and Computer ScienceGraph TheoryGraph Theory with Algorithms and its ApplicationsGraph TheoryGraph Theory with ApplicationsGraph Theory ApplicationsHandbook of Graph TheoryGraph Theory and Its Applications to Problems of SocietyBasic Graph Theory with ApplicationsTen",j168,Proceedings of the IEEE,jv168,accepted,f1299,2019,2019-04-21
s1621,p1621,Recent Results in the Theory of Graph Spectra,Abstract content goes here ...,c21,Grid Computing Environments,cp21,accepted,f1300,2005,2005-12-29
s1622,p1622,Application of the graph theory and matrix methods to contractor ranking,Abstract content goes here ...,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f1301,2021,2021-01-31
s1624,p1624,On an extremal problem in graph theory,"G ( n;l ) will denote a graph of n vertices and l edges. Let f 0 ( n, k ) be the smallest integer such that there is a G ( n;f 0 (n, k )) in which for every set of k vertices there is a vertex joined to each of these. Thus for example f o = 3 since in a triangle each pair of vertices is joined to a third. It can readily be checked that f o = 5 (the extremal graph consists of a complete 4-gon with one edge removed). In general we will prove: Let n > k , and then f 0 ( n, k ) = f(n, k) .",c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f1302,2022,2022-12-04
s1625,p1625,Efficient Sampling Set Selection for Bandlimited Graph Signals Using Graph Spectral Proxies,"We study the problem of selecting the best sampling set for bandlimited reconstruction of signals on graphs. A frequency domain representation for graph signals can be defined using the eigenvectors and eigenvalues of variation operators that take into account the underlying graph connectivity. Smoothly varying signals defined on the nodes are of particular interest in various applications, and tend to be approximately bandlimited in the frequency basis. Sampling theory for graph signals deals with the problem of choosing the best subset of nodes for reconstructing a bandlimited signal from its samples. Most approaches to this problem require a computation of the frequency basis (i.e., the eigenvectors of the variation operator), followed by a search procedure using the basis elements. This can be impractical, in terms of storage and time complexity, for real datasets involving very large graphs. We circumvent this issue in our formulation by introducing quantities called graph spectral proxies, defined using the powers of the variation operator, in order to approximate the spectral content of graph signals. This allows us to formulate a direct sampling set selection approach that does not require the computation and storage of the basis elements. We show that our approach also provides stable reconstruction when the samples are noisy or when the original signal is only approximately bandlimited. Furthermore, the proposed approach is valid for any choice of the variation operator, thereby covering a wide range of graphs and applications. We demonstrate its effectiveness through various numerical experiments.",j13,IEEE Transactions on Signal Processing,jv13,accepted,f1303,2013,2013-10-15
s1626,p1626,A Graph‐Theory Framework for Evaluating Landscape Connectivity and Conservation Planning,"Abstract:  Connectivity of habitat patches is thought to be important for movement of genes, individuals, populations, and species over multiple temporal and spatial scales. We used graph theory to characterize multiple aspects of landscape connectivity in a habitat network in the North Carolina Piedmont (U.S.A).. We compared this landscape with simulated networks with known topology, resistance to disturbance, and rate of movement. We introduced graph measures such as compartmentalization and clustering, which can be used to identify locations on the landscape that may be especially resilient to human development or areas that may be most suitable for conservation. Our analyses indicated that for songbirds the Piedmont habitat network was well connected. Furthermore, the habitat network had commonalities with planar networks, which exhibit slow movement, and scale‐free networks, which are resistant to random disturbances. These results suggest that connectivity in the habitat network was high enough to prevent the negative consequences of isolation but not so high as to allow rapid spread of disease. Our graph‐theory framework provided insight into regional and emergent global network properties in an intuitive and visual way and allowed us to make inferences about rates and paths of species movements and vulnerability to disturbance. This approach can be applied easily to assessing habitat connectivity in any fragmented or patchy landscape.",j103,Conservation Biology,jv103,accepted,f1304,2006,2006-01-19
s1628,p1628,Graph Theory. An Algorithmic Approach,Abstract content goes here ...,c33,International Conference on Agile Software Development,cp33,accepted,f1305,2022,2022-04-01
s1629,p1629,Fundamentals of Algebraic Graph Transformation,Abstract content goes here ...,c109,International Conference on Mobile Data Management,cp109,accepted,f1306,2014,2014-07-04
s1630,p1630,Graph theory and molecular orbitals,Abstract content goes here ...,c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f1307,2019,2019-09-26
s1631,p1631,What can graph theory tell us about word learning and lexical retrieval?,"PURPOSE
Graph theory and the new science of networks provide a mathematically rigorous approach to examine the development and organization of complex systems. These tools were applied to the mental lexicon to examine the organization of words in the lexicon and to explore how that structure might influence the acquisition and retrieval of phonological word-forms.


METHOD
Pajek, a program for large network analysis and visualization (V. Batagelj & A. Mvrar, 1998), was used to examine several characteristics of a network derived from a computerized database of the adult lexicon. Nodes in the network represented words, and a link connected two nodes if the words were phonological neighbors.


RESULTS
The average path length and clustering coefficient suggest that the phonological network exhibits small-world characteristics. The degree distribution was fit better by an exponential rather than a power-law function. Finally, the network exhibited assortative mixing by degree. Some of these structural characteristics were also found in graphs that were formed by 2 simple stochastic processes suggesting that similar processes might influence the development of the lexicon.


CONCLUSIONS
The graph theoretic perspective may provide novel insights about the mental lexicon and lead to future studies that help us better understand language development and processing.",j302,"Journal of Speech, Language and Hearing Research",jv302,accepted,f1308,2022,2022-01-21
s1632,p1632,Graph Theory and Interconnection Networks,"The advancement of large scale integrated circuit technology has enabled the construction of complex interconnection networks. Graph theory provides a fundamental tool for designing and analyzing such networks. Graph Theory and Interconnection Networks provides a thorough understanding of these interrelated topics. After a brief introduction to graph terminology, this book presents well-known interconnection networks as examples of graphs, followed by in-depth coverage of Hamiltonian graphs. Different types of problems illustrate the wide range of available methods for solving such problems. The text also explores recent progress on the diagnosability of graphs under various models.",c23,International Conference on Open and Big Data,cp23,accepted,f1309,2012,2012-09-16
s1633,p1633,Spectral Graph Theory and its Applications,"Spectral graph theory is the study of the eigenvalues and eigenvectors of matrices associated with graphs. In this tutorial, we will try to provide some intuition as to why these eigenvectors and eigenvalues have combinatorial significance, and will sitn'ey some of their applications.",c81,IEEE Annual Symposium on Foundations of Computer Science,cp81,accepted,f1310,2004,2004-09-23
s1634,p1634,Topics in Graph Theory: Graphs and Their Cartesian Product,"From specialists in the field, learn about interesting connections and recent developments in the field of graph theory by looking in particular at Cartesian products arguably the most important of the four standard graph products. Many new results in this area appear for the first time in print in this book. Written in an accessible way, this book can be used for personal study in advanced applications of graph theory or for an advanced graph theory course.",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f1311,2018,2018-12-20
s1635,p1635,Proteins as networks: usefulness of graph theory in protein science.,"The network paradigm is based on the derivation of emerging properties of studied systems by their representation as oriented graphs: any system is traced back to a set of nodes (its constituent elements) linked by edges (arcs) correspondent to the relations existing between the nodes. This allows for a straightforward quantitative formalization of systems by means of the computation of mathematical descriptors of such graphs (graph theory). The network paradigm is particularly useful when it is clear which elements of the modelled system must play the role of nodes and arcs respectively, and when topological constraints have a major role with respect to kinetic ones. In this review we demonstrate how nodes and arcs of protein topology are characterized at different levels of definition: 1. Recurrence matrix of hydrophobicity patterns along the sequence 2. Contact matrix of alpha carbons of 3D structures 3. Correlation matrix of motions of different portion of the molecule in molecular dynamics. These three conditions represent different but potentially correlated reticular systems that can be profitably analysed by means of network analysis tools.",j303,Current protein and peptide science,jv303,accepted,f1312,2006,2006-11-20
s1636,p1636,GRAPH THEORY AND COMPLEX NETWORKS,"In the past ten years,the fast development of complex network theory has provided a good support for the study of complexity and complex systems,since they describe clearly the important characteristics of complex systems,and show bright prospects in theory and applications.This paper presents mainly the application of graph theory to complex networks,especially to the synchronization problem of complex networks First,its application to the estimations of smallest nonzero,largest eigenvalues and synchronizability index of certain graphs are commented,followed by the effects of subgraph and graph eigenvector in the estimation of synchronizability index.Furthermore,the complexity between the relationships of synchronizability and network structural parameters are discussed via two simple graphs,and the effects of complementary graph, edge-addition and graph operation on the synchronization of complex networks are elaborated.Finally,some possible development directions in complex networks are predicted from the viewpoint of graph and control theory.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1313,2007,2007-12-03
s1637,p1637,Applications of graph theory to landscape genetics,"We investigated the relationships among landscape quality, gene flow, and population genetic structure of fishers (Martes pennanti) in ON, Canada. We used graph theory as an analytical framework considering each landscape as a network node. The 34 nodes were connected by 93 edges. Network structure was characterized by a higher level of clustering than expected by chance, a short mean path length connecting all pairs of nodes, and a resiliency to the loss of highly connected nodes. This suggests that alleles can be efficiently spread through the system and that extirpations and conservative harvest are not likely to affect their spread. Two measures of node centrality were negatively related to both the proportion of immigrants in a node and node snow depth. This suggests that central nodes are producers of emigrants, contain high‐quality habitat (i.e., deep snow can make locomotion energetically costly) and that fishers were migrating from high to low quality habitat. A method of community detection on networks delineated five genetic clusters of nodes suggesting cryptic population structure. Our analyses showed that network models can provide system‐level insight into the process of gene flow with implications for understanding how landscape alterations might affect population fitness and evolutionary potential.",j304,Evolutionary Applications,jv304,accepted,f1314,2011,2011-08-08
s1638,p1638,A BIRD'S EYE VIEW OF THE CUT METHOD AND A SURVEY OF ITS APPLICATIONS IN CHEMICAL GRAPH THEORY,"A general description of the cut method is presented and an overview of its applications in chemical graph theory is given. Applications include the Wiener index, the Szeged index, the hyper-Wiener index, the PI index, the weighted Wiener index, Wiener-type indices, and classes of chemical graphs such as trees, benzenoid graphs and phenylenes. A computation of the Wiener index of an arbitrary connected graph using its canonical metric representation is described. Algorithmic issues are also briefly mentioned as well as are the recently introduced CI index and related polynomials.",c97,Interspeech,cp97,accepted,f1315,2004,2004-08-14
s1639,p1639,From Graph Theory to Models of Economic Networks. A Tutorial.,Abstract content goes here ...,c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f1316,2006,2006-09-06
s1641,p1641,Graph Theory Based Algorithms for Water Distribution Network Sectorization Projects,"Water distribution network sectorization projects, in process in Mexican cities in the last two decades, consist in dividing the large interconnected city distribution network in smaller networks with one (or two in exceptional cases) supply points. Known water distribution network models are routinely applied to revise any proposed sectorization. For large network sectorization projects, however, additional algorithmic capabilities are needed, such as connectivity and source contribution analysis. This paper presents algorithms of that type, based on graph theory, for obtaining the number of independent sectors in a network layout, the set of nodes belonging to each sector, the set of disconnected nodes, and the source to node contribution. The algorithms are implemented in a computer AutoCAD-based system. Real sectorization project in two cities, where the proposed algorithms are applied, are then commented.",c111,International Society for Music Information Retrieval Conference,cp111,accepted,f1317,2001,2001-12-16
s1643,p1643,Algebraic Graph Theory,1. Introduction to algebraic graph theory Part I. Linear Algebra in Graphic Thoery: 2. The spectrum of a graph 3. Regular graphs and line graphs 4. Cycles and cuts 5. Spanning trees and associated structures 6. The tree-number 7. Determinant expansions 8. Vertex-partitions and the spectrum Part II. Colouring Problems: 9. The chromatic polynomial 10. Subgraph expansions 11. The multiplicative expansion 12. The induced subgraph expansion 13. The Tutte polynomial 14. Chromatic polynomials and spanning trees Part III. Symmetry and Regularity: 15. Automorphisms of graphs 16. Vertex-transitive graphs 17. Symmetric graphs 18. Symmetric graphs of degree three 19. The covering graph construction 20. Distance-transitive graphs 21. Feasibility of intersection arrays 22. Imprimitivity 23. Minimal regular graphs with given girth References Index.,c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f1318,2017,2017-04-13
s1644,p1644,Graph Theory: A Problem Oriented Approach,Preface A. Basic Concepts B. Isomorphic graphs C. Bipartite graphs D. Trees and forests E. Spanning tree algorithms F. Euler paths G. Hamilton paths and cycles H. Planar graphs I. Independence and covering J. Connections and obstructions K. Vertex coloring L. Edge coloring M. Matching theory for bipartite graphs N. Applications of matching theory O. Cycle-Free digraphs Answers to selected problems.,c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f1319,2018,2018-05-07
s1645,p1645,"Algorithmic Graph Theory and Perfect Graphs (Annals of Discrete Mathematics, Vol 57)",Abstract content goes here ...,c18,Conference on Innovative Data Systems Research,cp18,accepted,f1320,2012,2012-05-23
s1647,p1647,AN EXTREMAL PROBLEM IN GRAPH THEORY,"G(?z; I) will denote a graph of n vertices and 1 edges. Let fO(lz, K) be the smallest integer such that there is a G (n; f,, (n, k)) in which for every set of K vertices there is a vertex joined to each of these. Thus for example fO(3, 2) = 3 since in a triangle each pair of vertices is joined to a third. It can readily be checked that f,(4, 2) = 5 (the extremal graph consists of a complete 4-gon with one edge removed). In general we will prove: Let n > k, and",c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f1321,2020,2020-09-25
s1649,p1649,Graph theory and networks in Biology.,"A survey of the use of graph theoretical techniques in Biology is presented. In particular, recent work on identifying and modelling the structure of bio-molecular networks is discussed, as well as the application of centrality measures to interaction networks and research on the hierarchical structure of such networks and network motifs. Work on the link between structural network properties and dynamics is also described, with emphasis on synchronisation and disease propagation.",j307,IET Systems Biology,jv307,accepted,f1322,2016,2016-09-02
s1650,p1650,Discrete Signal Processing on Graphs: Sampling Theory,"We propose a sampling theory for signals that are supported on either directed or undirected graphs. The theory follows the same paradigm as classical sampling theory. We show that perfect recovery is possible for graph signals bandlimited under the graph Fourier transform. The sampled signal coefficients form a new graph signal, whose corresponding graph structure preserves the first-order difference of the original graph signal. For general graphs, an optimal sampling operator based on experimentally designed sampling is proposed to guarantee perfect recovery and robustness to noise; for graphs whose graph Fourier transforms are frames with maximal robustness to erasures as well as for Erdös-Rényi graphs, random sampling leads to perfect recovery with high probability. We further establish the connection to the sampling theory of finite discrete-time signal processing and previous work on signal recovery on graphs. To handle full-band graph signals, we propose a graph filter bank based on sampling theory on graphs. Finally, we apply the proposed sampling theory to semi-supervised classification of online blogs and digit images, where we achieve similar or better performance with fewer labeled samples compared to previous work.",j13,IEEE Transactions on Signal Processing,jv13,accepted,f1323,2013,2013-12-06
s1651,p1651,Characterizing brain anatomical connections using diffusion weighted MRI and graph theory,Abstract content goes here ...,j308,NeuroImage,jv308,accepted,f1324,2005,2005-09-19
s1652,p1652,Bondary-connectivity via graph theory,"We generalize theorems of Kesten and Deuschel-Pisztora about the connectedness of the exterior boundary of a connected subset of $\Z^d$, where ""connectedness"" and ""boundary"" are understood with respect to various graphs on the vertices of $\Z^d$. We provide simple and elementary proofs of their results. It turns out that the proper way of viewing these questions is graph theory, instead of topology.",c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f1325,2015,2015-04-01
s1653,p1653,On Graph Theory and Its Application,"Graph theory has around 300 years of history,but many problems haven't been solved.With the development of computer science,graph theory becomes hot point again.In this paper,the application of graph theory is discussed.",c37,Asia-Pacific Software Engineering Conference,cp37,accepted,f1326,2008,2008-05-31
s1654,p1654,Applied Graph Theory in Computer Vision and Pattern Recognition,Abstract content goes here ...,c39,International Conference on Global Software Engineering,cp39,accepted,f1327,2020,2020-08-22
s1655,p1655,Extremal Graph Theory for Metric Dimension and Diameter,"A set of vertices $S$ resolves a connected graph $G$ if every vertex is uniquely determined by its vector of distances to the vertices in $S$. The metric dimension of $G$ is the minimum cardinality of a resolving set of $G$. Let ${\cal G}_{\beta,D}$ be the set of graphs with metric dimension $\beta$ and diameter $D$. It is well-known that the minimum order of a graph in ${\cal G}_{\beta,D}$ is exactly $\beta+D$. The first contribution of this paper is to characterise the graphs in ${\cal G}_{\beta,D}$ with order $\beta+D$ for all values of $\beta$ and $D$. Such a characterisation was previously only known for $D\leq2$ or $\beta\leq1$. The second contribution is to determine the maximum order of a graph in ${\cal G}_{\beta,D}$ for all values of $D$ and $\beta$. Only a weak upper bound was previously known.",j309,Electronic Journal of Combinatorics,jv309,accepted,f1328,2002,2002-11-05
s1656,p1656,Applications of graph theory,"Graph theory is rapidly moving into the mainstream of mathematics mainly because of its applications in diverse fields. In this paper, we discuss certain ways of applying graph theoretical techniques to solve various problems and present the review of some of the applications. (© 2008 WILEY‐VCH Verlag GmbH & Co. KGaA, Weinheim)",c95,IEEE International Conference on Computer Vision,cp95,accepted,f1329,2017,2017-09-30
s1657,p1657,The History of Degenerate (Bipartite) Extremal Graph Problems,Abstract content goes here ...,c75,International Conference on Machine Learning,cp75,accepted,f1330,2005,2005-10-13
s1658,p1658,PEARLS in GRAPH THEORY,Abstract content goes here ...,c10,Big Data,cp10,accepted,f1331,2021,2021-06-09
s1659,p1659,Modern temporal network theory: a colloquium,Abstract content goes here ...,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f1332,2005,2005-05-17
s1662,p1662,A Walk Through Combinatorics: An Introduction to Enumeration and Graph Theory,"Basic Methods: Seven Is More Than Six. The Pigeon-Hole Principle One Step at a Time. The Method of Mathematical Induction Enumerative Combinatorics: There Are a Lot of Them. Elementary Counting Problems No Matter How You Slice It. The Binomial Theorem and Related Identities Divide and Conquer. Partitions Not So Vicious Cycles. Cycles in Permutations You Shall Not Overcount. The Sieve A Function is Worth Many Numbers. Generating Functions Graph Theory: Dots and Lines. The Origins of Graph Theory Staying Connected. Trees Finding a Good Match. Coloring and Matching Do Not Cross. Planar Graphs Horizons: Does It Clique? Ramsey Theory So Hard to Avoid. Subsequence Conditions on Permutations Who Knows What It Looks Like, but It Exists. The Probabilistic Method At Least Some Order. Partial Orders and Lattices The Sooner The Better. Combinatorial Algorithms Does Many Mean More Than One? Computational Complexity.",c22,International Conference on Data Technologies and Applications,cp22,accepted,f1333,2020,2020-03-27
s1663,p1663,Combinatorics and Graph Theory,Abstract content goes here ...,c61,Jahrestagung der Gesellschaft für Informatik,cp61,accepted,f1334,2017,2017-08-18
s1664,p1664,Complex Networks: from Graph Theory to Biology,Abstract content goes here ...,c6,Americas Conference on Information Systems,cp6,accepted,f1335,2007,2007-08-06
s1665,p1665,Application of graph theory to OO software engineering,"Graph Theory, which studies the properties of graphs, has been widely accepted as a core subject in the knowledge of computer scientists. So is Object-Oriented (OO) software engineering, which deals with the analysis, design and implementation of systems employing classes as modules. The latter field can greatly benefit from the application of Graph Theory, since the main mode of representation, namely the class diagram, is essentially a directed graph. The study of graph properties can be valuable in many ways for understanding the characteristics of the underlying software systems. Representative examples for the usefulness of graph theory on OO systems based on recent research results are presented in this paper.",c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f1336,2004,2004-03-13
s1666,p1666,Graph theory,"rHYtHMs | slow art Art & Design Galleries 95 This animation was a happy accident. I was interested in seeing what kinds of shapes could be generated by simple L-systems, so I coded a Python/PyQt application to iterate the replacement rule and draw the resulting string using the typical turtle geometry interpretation. I quickly saw that changing the angle parameter about a degree could produce very different shapes, and that haphazard exploration of the angle parameter might miss interesting shapes. So I added an animation feature to vary the angle systematically and save out the resulting frames. By trial and error, I settled on .01 degree as an increment that produced a small but noticeable change in the picture. I rendered the 18,000 frames into a movie for review and was surprised to find that at 30 frames per second the animation was interesting and fun to watch for 10 minutes, and that the eye could easily pick out single-frame shapes that differed substantially from the neighboring frames. FFDavid Gladstein",c83,International Conference on Computer Graphics and Interactive Techniques,cp83,accepted,f1337,2015,2015-12-11
s1667,p1667,Metric graph theory and geometry: a survey,"The article surveys structural characterizations of several graph classes defined by distance properties, which have in part a general algebraic flavor and can be interpreted as subdirect decomposition. The graphs we feature in the first place are the median graphs and their various kinds of generalizations, e.g., weakly modular graphs, or fiber-complemented graphs, or l1-graphs. Several kinds of l1-graphs admit natural geometric realizations as polyhedral complexes. Particular instances of these graphs also occur in other geometric contexts, for example, as dual polar graphs, basis graphs of (even ∆-)matroids, tope graphs, lopsided sets, or plane graphs with vertex degrees and face sizes bounded from below. Several other classes of graphs, e.g., Helly graphs (as injective objects), or bridged graphs (generalizing chordal graphs), or tree-like graphs such as distance-hereditary graphs occur in the investigation of graphs satisfying some basic properties of the distance function, such as the Helly property for balls, or the convexity of balls or of the neighborhoods of convex sets, etc. Operators between graphs or complexes relate some of the graph classes reported in this survey.",c56,European Conference on Software Process Improvement,cp56,accepted,f1338,2016,2016-12-12
s1671,p1671,Algorithmic graph theory,"Introduction to graph theory algorithmic techniques shortest paths trees and acyclic diagraphs depth first search connectivity and routing graph colouring covers, domination, independent sets, matchings and factors, parallel algorithms computational complexity.",c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f1339,2001,2001-12-30
s1672,p1672,Introduction to Graph Theory,"Introduction * Definitions and examples* Paths and cycles* Trees* Planarity* Colouring graphs* Matching, marriage and Menger's theorem* Matroids Appendix 1: Algorithms Appendix 2: Table of numbers List of symbols Bibliography Solutions to selected exercises Index",c65,Formal Concept Analysis,cp65,accepted,f1340,2008,2008-03-25
s1674,p1674,Graph theory and its applications,"In this paper we will discuss how problems like Page ranking and finding the shortest paths can be solved by using Graph Theory. At its core, graph theory is the study of graphs as mathematical structures. In our paper, we will first cover Graph Theory as a broad topic. Then we will move on to Linear Algebra. Linear Algebra is the study of matrices. We will apply the skills discussed in these two sections to Dijkstra Algorithms which cover how to find the shortest paths in graphs. Finally, we will present PageRank where we will demonstrate how to rank pages based on their importance.",c19,ACM Conference on Economics and Computation,cp19,accepted,f1341,2002,2002-03-20
s1675,p1675,A textbook of graph theory,Basic Results.- Directed Graphs.- Connectivity.- Trees.- Independent Sets and Matchings.- Eulerian and Hamiltonian Graphs.- Graph Colourings.- Planarity.- Triangulated Graphs.- Applications.,c105,Biometrics and Identity Management,cp105,accepted,f1342,2006,2006-12-24
s1676,p1676,Szemeredi''s Regularity Lemma and its applications in graph theory,"Szemer\''edi''s Regularity Lemma is an important tool in discrete mathematics. It says that, in some sense, all graphs can be approximated by random-looking graphs. Therefore the lemma helps in proving theorems for arbitrary graphs whenever the corresponding result is easy for random graphs. Recently quite a few new results were obtained by using the Regularity Lemma, and also some new variants and generalizations appeared. In this survey we describe some typical applications and some generalizations.",c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f1343,2006,2006-01-12
s1677,p1677,Introduction to Graph Theory,"Written by one of the leading authors in the field, this text provides a student-friendly approach to graph theory for undergraduates. Much care has been given to present the material at the most effective level for students taking a first course in graph theory. Gary Chartrand and Ping Zhang's lively and engaging style, historical emphasis, unique examples and clearly-written proof techniques make it a sound yet accessible text that stimulates interest in an evolving subject and exploration in its many applications. This text is part of the Walter Rudin Student Series in Advanced Mathematics.",c91,Workshop on Algorithms and Models for the Web-Graph,cp91,accepted,f1344,2022,2022-12-14
s1678,p1678,Applied and algorithmic graph theory,"Designed as the bridge to cross the widening gap between mathematics and computer science, and planned as the mathematical base for computer science students, this maths text is written for upper-level college students who have had previous coursework involving proofs and proof techniques. The close tie between the theoretical and algorithmic aspects of graph theory, and graphs that lend themselves naturally as models in computer science, results in a need for efficient algorithims to solve any large scale problems. Each algorithm in the text includes explanatory statements that clarify individual steps, a worst-case complexity analysis, and algorithmic correctness proofs. As a result, the student will develop an understanding of the concept of an efficient algorithm.",c84,The Web Conference,cp84,accepted,f1345,2006,2006-07-24
s1679,p1679,Topics in algebraic graph theory,Foreword Peter J. Cameron Introduction 1. Eigenvalues of graphs Michael Doob 2. Graphs and matrices Richard A. Brualdi and Bryan L. Shader 3. Spectral graph theory Dragos Cvetkovic and Peter Rowlinson 4. Graph Laplacians Bojan Mohar 5. Automorphism groups Peter J. Cameron 6. Cayley graphs Brian Alspach 7. Finite symmetric graphs Cheryle E. Praeger 8. Strongly regular graphs Peter J. Cameron 9. Distance-transitive graphs Arjeh M. Cohen 10. Computing with graphs and groups Leonard H. Soicher.,c91,Workshop on Algorithms and Models for the Web-Graph,cp91,accepted,f1346,2022,2022-07-26
s1680,p1680,A LIMIT THEOREM IN GRAPH THEORY,"In this paper G(n ; I) will denote a graph of n vertices and l edges, K„ will denote the complete graph of p vertices G (p ; (PA and K,(p i , . . ., p,) will denote the rchromatic graph with p i vertices of the i-th colour, in which every two vertices of different colour are adjacent . 7r(G) will denote the number of vertices of G and v(G) denotes the number of edges of G . G(n :1) denotes the complementary graph of G(n : l) i . e. G(n ; 1) is the G (ii : (211) -/) which has the samevertices as G(n ; 1)",c105,Biometrics and Identity Management,cp105,accepted,f1347,2006,2006-03-19
s1682,p1682,GRAPH THEORY AND PROBABILITY,"A well-known theorem of Ramsay (8; 9) states that to every n there exists a smallest integer g(n) so that every graph of g(n) vertices contains either a set of n independent points or a complete graph of order n, but there exists a graph of g(n) 1 vertices which does not contain a complete subgraph of n vertices and also does not contain a set of n independent points. (A graph is called complete if every two of its vertices are connected by an edge; a set of points is called independent if no two of its points are connected by an edge.) The determination of g(n) seems a very difficult problem; the best inequalities for g(lz) are (3)",c37,Asia-Pacific Software Engineering Conference,cp37,accepted,f1348,2008,2008-05-12
s1683,p1683,Graph Theory Methods for the Analysis of Neural Connectivity Patterns,Abstract content goes here ...,c100,ACM SIGMOD Conference,cp100,accepted,f1349,2010,2010-04-01
s1684,p1684,Applications of graph theory in chemistry,"Graph theoretical (GT) applications in chemistry underwent a dramatic revival lately. Constitutional (molecular) graphs have points (vertices) representing atoms and lines (edges) symbolizing malent bonds. This review deals with definition. enumeration. and systematic coding or nomenclature of constitutional or steric isomers, valence isomers (especially of annulenes). and condensed polycyclic aromatic hydrocarbons. A few key applications of graph theory in theoretical chemistry are pointed out. The complete set of all poasible monocyclic aromatic and heteroaromatic compounds may be explored by a mmbination of Pauli's principle, P6lya's theorem. and electronegativities. Topological indica and some of their applications are reviewed. Reaction graphs and synthon graphs differ from constitutional graphs i n their meaning of vertices and edges and find other kinds of chemical applications. This paper ends with a review of the use of GT applications for chemical nomenclature (nodal nomenclature and related areas), coding. and information processing/storage/retrieval",j214,Journal of chemical information and computer sciences,jv214,accepted,f1350,2022,2022-07-21
s1685,p1685,PROTEIN STRUCTURE: INSIGHTS FROM GRAPH THEORY,"The sequence and structure of a large body of proteins are becoming increasingly available. It is desirable to explore mathematical tools for ecient extraction of information from such sources. The principles of graph theory, which was earlier applied in elds such as electrical engineering and computer networks are now being adopted to investigate protein structure, folding, stability, function and dynamics. This review deals with a brief account of relevant graphs and graph theoretic concepts. The concepts of protein graph construction are discussed. The manner in which graphs are analyzed and parameters relevant to protein structure are extracted, are explained. The structural and biological information derived from protein structures using these methods is presented.",c112,Very Large Data Bases Conference,cp112,accepted,f1351,2018,2018-04-19
s1686,p1686,Power transfer allocation for open access using graph theory-fundamentals and applications in systems without loopflow,"In this paper, graph theory is used to calculate the contributions of individual generators and loads to line flows and the real power transfer between individual generators and loads that are significant to transmission open access. Related lemmas are proved which present necessary conditions required by the method. Based on AC load flow solution a novel method is suggested which can decide downstream and upstream power flow tracing paths very fast and can calculate the contribution factors of generations and loads to the line flows efficiently. The power transfer between generators and loads can also be determined. The suggested method is suitable for both active and reactive power tracings of real power systems.",c3,Frontiers in Education Conference,cp3,accepted,f1352,2016,2016-10-12
s1687,p1687,Dynamic Modelling of Mechatronic Multibody Systems With Symbolic Computing and Linear Graph Theory,"The application of linear graph theory to the modelling of flexible multibody systems is described. When combined with symbolic computing methods, linear graph theory leads to efficient dynamic models that facilitate real-time simulation of systems of rigid bodies and flexible beams. The natural extension of linear graphs to the modelling of mechatronic multibody systems is presented, along with a recently-developed theory for building complex system models from models of individual subsystems.",c50,International Conference on Automated Software Engineering,cp50,accepted,f1353,2008,2008-05-10
s1689,p1689,SOME UNSOLVED PROBLEMS IN GRAPH THEORY,CONTENTSIntroduction § 1. Fundamental concepts § 2. Isomorphism problems § 3. Metric questions § 4. Thickness and genus of graph § 5. Colouring problems § 6. Parts with given propertiesReferences,c22,International Conference on Data Technologies and Applications,cp22,accepted,f1354,2020,2020-08-31
s1690,p1690,A graph theory interpretation of nodal regions,Abstract content goes here ...,c81,IEEE Annual Symposium on Foundations of Computer Science,cp81,accepted,f1355,2004,2004-06-14
s1691,p1691,Graph Theory 1736-1936,1. Oaths 2. Circuits 3. Trees 4. Chemical graphs 5. Euler's polyhedral formula 6. The four-colour problem - early history 7. Colouring maps on surfaces 8. Ideas from algebra and topology 9. The four-colour problem - to 1936 10. The factorization of graphs Appendix 1: Graph theory since 1936 Appendix 2: Bibliographical notes Appendix 3: Bibliography: 1736-1936,c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f1356,2011,2011-02-02
s1692,p1692,Algorithmic Graph Theory,"Preface 1. Introducing graphs and algorithmic complexity 2. Spanning-trees, branchings and connectivity 3. Planar graphs 4. Networks and flows 5. Matchings 6. Eulerian and Hamiltonian tours 7. Colouring graphs 8. Graph problems and intractability Appendix Author Index Subject Index.",c35,EUROMICRO Conference on Software Engineering and Advanced Applications,cp35,accepted,f1357,2005,2005-07-18
s1694,p1694,"Topology optimization of trusses using genetic algorithm, force method and graph theory","In this article size/topology optimization of trusses is performed using a genetic algorithm (GA), the force method and some concepts of graph theory. One of the main difficulties with optimization with a GA is that the parameters involved are not completely known and the number of operations needed is often quite high. Application of some concepts of the force method, together with theory of graphs, make the generation of a suitable initial population well‐matched with critical paths for the transformation of internal forces feasible. In the process of optimization generated topologically unstable trusses are identified without any matrix manipulation and highly penalized. Identifying a suitable range for the cross‐section of each member for the ground structure in the list of profiles, the length of the substrings representing the cross‐sectional design variables are reduced. Using a contraction algorithm, the length of the strings is further reduced and a GA is performed in a smaller domain of design space. The above process is accompanied by efficient methods for selection, and by using a suitable penalty function in order to reduce the number of numerical operations and to increase the speed of the optimization toward a global optimum. The efficiency of the present method is illustrated using some examples, and compared to those of previous studies. Copyright © 2003 John Wiley & Sons, Ltd.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1358,2007,2007-01-22
s1695,p1695,Pattern Vectors from Algebraic Graph Theory,"Graph structures have proven computationally cumbersome for pattern analysis. The reason for this is that, before graphs can be converted to pattern vectors, correspondences must be established between the nodes of structures which are potentially of different size. To overcome this problem, in this paper, we turn to the spectral decomposition of the Laplacian matrix. We show how the elements of the spectral matrix for the Laplacian can be used to construct symmetric polynomials that are permutation invariants. The coefficients of these polynomials can be used as graph features which can be encoded in a vectorial manner. We extend this representation to graphs in which there are unary attributes on the nodes and binary attributes on the edges by using the spectral decomposition of a Hermitian property matrix that can be viewed as a complex analogue of the Laplacian. To embed the graphs in a pattern space, we explore whether the vectors of invariants can be embedded in a low-dimensional space using a number of alternative strategies, including principal components analysis (PCA), multidimensional scaling (MDS), and locality preserving projection (LPP). Experimentally, we demonstrate that the embeddings result in well-defined graph clusters. Our experiments with the spectral representation involve both synthetic and real-world data. The experiments with synthetic data demonstrate that the distances between spectral feature vectors can be used to discriminate between graphs on the basis of their structure. The real-world experiments show that the method can be used to locate clusters of graphs.",j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,jv299,accepted,f1359,2020,2020-10-17
s1696,p1696,Combinatorics and graph theory,"Graph Theory: Introductory Concepts.- Trees.- Planarity.- Colorings.- Matchings.- Ramsey Theory.- References Combinatorics: Three Basic Problems.- Binomial Coefficients.- The Principle of Inclusion and Exclusion.- Generating Functions.- Polya's Theory of Counting.- More Numbers.- Stable Marriage.- References Infinite Combinatorics and Graph Theory: Pigeons and Trees.- Ramsey Revisited.- ZFC.- The Return of der Koenig.- Ordinals, Cardinals, and Many Pigeons.- Incompleteness and Coardinals.- Weakly Compact Cardinals.- Finite Combinatorics with Infinite Consequences.- Points of Departure.- References.",c16,Knowledge Discovery and Data Mining,cp16,accepted,f1360,2003,2003-02-12
s1698,p1698,Chemical Graph Theory: Introduction and Fundamentals,The Origins of Chemical Graph Theory Elements of Graph Theory for Chemists Nomenclature of Chemical Compounds Polynomials in Graph Theory Enumeration of Isomers Graph Theory and Molecular Orbitals,c7,European Conference on Modelling and Simulation,cp7,accepted,f1361,2015,2015-09-18
s1699,p1699,Application of graph theory to the synchronization in an array of coupled nonlinear oscillators,"In this letter, we show how algebraic graph theory can be used to derive sufficient conditions for an array of resistively coupled nonlinear oscillators to synchronize. These conditions are derived from the connectivity graph, which describes how the oscillators are connected. In particular, we show how such a sufficient condition is dependent on the algebraic connectivity of the connectivity graph. Intuition tells us that if the oscillators are more ""closely connected"" to each other, then they are more likely to synchronize. We discuss how to quantify connectedness in graph-theoretical terms and its relation to algebraic connectivity and show that our results are in accordance with this intuition. We also give an upper bound on the coupling conductance required for synchronization for arbitrary graphs, which is in the order of n/sup 2/, where n is the number of oscillators. >",c11,Hawaii International Conference on System Sciences,cp11,accepted,f1362,2006,2006-12-05
s1700,p1700,On the use of linear graph theory in multibody system dynamics,Abstract content goes here ...,c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f1363,2006,2006-11-16
s1701,p1701,Combinatorics and graph theory,"Graph Theory: Introductory Concepts.- Trees.- Planarity.- Colorings.- Matchings.- Ramsey Theory.- References Combinatorics: Three Basic Problems.- Binomial Coefficients.- The Principle of Inclusion and Exclusion.- Generating Functions.- Polya's Theory of Counting.- More Numbers.- Stable Marriage.- References Infinite Combinatorics and Graph Theory: Pigeons and Trees.- Ramsey Revisited.- ZFC.- The Return of der Koenig.- Ordinals, Cardinals, and Many Pigeons.- Incompleteness and Coardinals.- Weakly Compact Cardinals.- Finite Combinatorics with Infinite Consequences.- Points of Departure.- References.",c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f1364,2021,2021-09-29
s1702,p1702,Introduction to Graph Theory,"In graph theory, the term graph refers to a set of vertices and a set of edges. A vertex can be used to represent any object. Graphs may contain undirected or directed edges. An undirected edge is a set of two vertices. A directed edge is an ordered pair of two vertices where the edge goes from the first vertex to the second vertex. Graphs that contain directed edges are called directed graphs or digraphs.",c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f1365,2001,2001-02-22
s1703,p1703,Application of Graph Theory: Relationship of Eccentric Connectivity Index and Wiener's Index with Anti-inflammatory Activity,"Abstract Graph theory was successfully applied in developing a relationship between chemical structure and biological activity. The relationship of two graph invariants—the eccentric connectivity index and the Wiener's index was investigated with regard to anti-inflammatory activity, for a data set consisting of 76 pyrazole carboxylic acid hydrazide analogues. The values of the eccentric connectivity index and the Wiener's index of each analogue in the data set were computed and active ranges were identified. Subsequently, each analogue was assigned a biological activity that was compared with the anti-inflammatory activity reported as percent reduction in paw swelling. Prediction with an accuracy of ∼90% was obtained using the eccentric connectivity index as compared to 84% in the case of Wiener's index.",c81,IEEE Annual Symposium on Foundations of Computer Science,cp81,accepted,f1366,2004,2004-07-01
s1705,p1705,Dynamics of Flexible Multibody Systems Using Virtual Work and Linear Graph Theory,Abstract content goes here ...,c43,ACM Symposium on Applied Computing,cp43,accepted,f1367,2001,2001-05-20
s1706,p1706,Application of graph theory to the synchronization in an array of coupled nonlinear oscillators,"In this letter, we show how algebraic graph theory can be used to derive sufficient conditions for an array of resistively coupled nonlinear oscillators to synchronize. These conditions are derived from the connectivity graph, which describes how the oscillators are connected. In particular, we show how such a sufficient condition is dependent on the algebraic connectivity of the connectivity graph. Intuition tells us that if the oscillators are more ""closely connected"" to each other, then they are more likely to synchronize. We discuss how to quantify connectedness in graph-theoretical terms and its relation to algebraic connectivity and show that our results are in accordance with this intuition. We also give an upper bound on the coupling conductance required for synchronization for arbitrary graphs, which is in the order of n/sup 2/, where n is the number of oscillators. >",c21,Grid Computing Environments,cp21,accepted,f1368,2005,2005-04-26
s1707,p1707,Reflections on graph theory,"At the occasion of the 250th anniversary of graph theory, we recall some of the basic results and unsolved problems, some of the attractive and surprising methods and results, and some possible future directions in graph theory.",j310,Journal of Graph Theory,jv310,accepted,f1369,2016,2016-08-08
s1708,p1708,Graph theory,"In this unit, you will investigate some interesting applications of polygons and circles. You will first determine the area of any given regular polygon and explore what happens to the shape of a polygon as the number of sides increase. You will also examine the area formula and how it is derived from an infinite number of sides in a regular polygon. You will then look at geometric probability based on length and area. The final part of this unit is investigating graph theory which is a study of networks that connect nodes with straight or curved paths.",c92,Advances in Soft Computing,cp92,accepted,f1370,2009,2009-03-26
s1709,p1709,Graph Theory and Social Networks: A Technical Comment on Connectedness and Connectivity,"Concepts taken from graph theory and other branches of topology have been used by many sociologists and social psychologists, in particular Kurt Lewin and J. L. Moreno. Similar ideas have been used to construct statistical models of nervous systems, and these have been applied by J. S. Coleman and others to the spread of information and other social phenomena. The study of social networks by anthropologists has been based, knowingly or unknowingly, on the basic notions of graph theory, as has the identification and analysis of social cliques. There is little consensus among mathematicians about terminology, and social scientists have drawn fortuitously on various mathematical vocabularies as well as inventing their own technical terms. Applied to social networks, the words `connectedness' and `connectivity' may refer to properties of the distance between persons, the number of paths between them, whether there is a path at all, or the proportion of possible paths actually in existence. These different usages are contrasted by restating them all in the terminology set out in Structural models (1965) by Harary, Norman and Cartwright.",c81,IEEE Annual Symposium on Foundations of Computer Science,cp81,accepted,f1371,2004,2004-10-30
s1710,p1710,The Foundations of Topological Graph Theory,Abstract content goes here ...,c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f1372,2012,2012-07-18
s1711,p1711,SOME RECENT RESULTS ON EXTREMAL PROBLEMS IN GRAPH THEORY (Results),"Three years ago I gave a talk on extremal problems in graph theory at Smolenice [2]. I will refer to this paper as I. I will only discuss results which have been found since the publication of I. ~(9) will denote the number of vertices, V(g) the number of edges of 9. s(a ; I) will denote a graph of n vertices and I edges. The vertices of 3 will be denoted by letters x, y, . . . 9(x,, . . . . xk) will denote the subgraph of 9 spanned by the vertices x1, . . . . xA. u(x), the valence of X, denotes the number of edges incident to x. x(S) will denote the",c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,cp99,accepted,f1373,2015,2015-10-15
s1712,p1712,Landscape graphs: Ecological modeling with graph theory to detect configurations common to diverse landscapes,Abstract content goes here ...,j311,Landscape Ecology,jv311,accepted,f1374,2019,2019-07-11
s1713,p1713,Graph theory and molecular orbitals. XII. Acyclic polyenes,"A graph‐theoretical study of acyclic polyenes is carried out with an emphasis on the influence of branching on several molecular properties. A definition of branching is given and several branching indices are analyzed. The case of polyenes without a Kekule structure is discussed briefly. The main conclusions are: (a) thermodynamic stability of conjugated polyenes decreases with branching, but (b) reactivity, in general, increases with branching.",c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f1375,2019,2019-01-30
s1714,p1714,Algorithmic graph theory and perfect graphs,Abstract content goes here ...,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f1376,2001,2001-05-02
s1715,p1715,A method in graph theory,Abstract content goes here ...,j312,Discrete Mathematics,jv312,accepted,f1377,2013,2013-02-08
s1716,p1716,Problems in combinatorics and graph theory,"Three hundred and sixty-nine problems with fully worked solutions for courses in computer science, combinatorics, and graph theory, designed to provide graded practice to students with as little as a high school algebra background. Originally used to prepare Rumanian candidates for participation in the International Mathematical Olympiads, this book includes both simple problems and complex ones, arranged according to subject. It provides various levels of problems, some of which had been previously available only in research journals. All details of the proofs are given in the solutions.",c10,Big Data,cp10,accepted,f1378,2021,2021-01-07
s1717,p1717,Hybrid Graph Theory and Network Analysis,"This book combines traditional graph theory with the matroid view of graphs in order to throw light on the mathematical approach to network analysis. The authors examine in detail two dual structures associated with a graph, namely circuits and cutsets. These are strongly dependent on one another and together constitute a third, hybrid, vertex-independent structure called a graphoid, whose study is here termed hybrid graph theory. This approach has particular relevance for network analysis. The first account of the subject in book form, the text includes many new results as well as the synthesizing and reworking of much research done over the past thirty years (historically, the study of hybrid aspects of graphs owes much to the foundational work of Japanese researchers). This work will be regarded as the definitive account of the subject, suitable for all working in theoretical network analysis: mathematicians, computer scientists or electrical engineers.",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f1379,2018,2018-07-02
s1718,p1718,Graph Theory Techniques in Model-Based Testing,"Models are a method of representing software behavior. Graph theory is an area of mathematics that can help us use this model information to test applications in many different ways. This paper describes several graph theory techniques, where they came from, and how they can be used to improve software testing.",c88,Symposium on the Theory of Computing,cp88,accepted,f1380,2014,2014-12-01
s1719,p1719,Graph Connections: Relationships between Graph Theory and Other Areas of Mathematics,"The purpose of this book is to inform mathematicians about the applicability of graph theory to other areas of mathematics, from number theory, to linear algebra, knots, neural networks, and finance. This is achieved through a series of expository chapters, each devoted to a different field and written by an expert in that field. The book, however, is more than a collection of essays. Each chapter has been carefully edited to ensure a common level of exposition, with terminology and notation standarised as far as possible.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1381,2007,2007-08-29
s1721,p1721,On domination related concepts in Graph Theory,Abstract content goes here ...,c100,ACM SIGMOD Conference,cp100,accepted,f1382,2010,2010-08-20
s1722,p1722,A SEMINAR ON GRAPH THEORY,"Abstract : The opening six chapters present a coherent body of graph theoretic concepts. The remaining eight chapters report lectures presented by various seminar participants. Topics presented include: Complete Bipartite Graphs, Extremal Problems in Graph Theory, Applications of Probabilistic Methods to Graph Theory, The Minimal Regular Graph Containing a Given Graph, Various Proofs of Cayley's Formula for Counting Trees, On Well-Quasi-Ordering Trees, Universal Graphs and Graphs and Composite Games. (Author)",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f1383,2018,2018-11-05
s1724,p1724,Lattice Constant Systems and Graph Theory,"The two principal systems of lattice constants that have arisen in the study of cooperative phenomena and related problems on crystal lattices are the strong (low‐temperature) and the weak (high‐temperature) systems. The two systems are defined in terms of the concepts of graph theory, and a general theorem relevant to cluster expansions is stated. The interrelation of the two systems is studied and exploited to derive configurational data for the face‐centered cubic lattice. All star graphs with up to seven points (vertices) or nine lines (edges) that are embeddable on the face‐centered cubic are described. A general classification of stars with cyclomatic index 3 is given.",c98,North American Chapter of the Association for Computational Linguistics,cp98,accepted,f1384,2009,2009-03-15
s1725,p1725,Open problems of Paul Erdös in graph theory,"The main treasure that Paul Erdős has left us is his collection of problems, most of which are still open today. These problems are seeds that Paul sowed and watered by giving numerous talks at meetings big and small, near and far. In the past, his problems have spawned many areas in graph theory and beyond (e.g., in number theory, probability, geometry, algorithms and complexity theory). Solutions or partial solutions to Erdős problems usually lead to further questions, often in new directions. These problems provide inspiration and serve as a common focus for all graph theorists. Through the problems, the legacy of Paul Erdős continues (particularly if solving one of these problems results in creating three new problems, for example.) There is a huge literature of almost 1500 papers written by Erdős and his (more than 460) collaborators. Paul wrote many problem papers, some of which appeared in various (really hard-to-find) proceedings. Here is an attempt to collect and organize these problems in the area of graph theory. The list here is by no means complete or exhaustive. Our goal is to state the problems, locate the sources, and provide the references related to these problems. We will include the earliest and latest known references without covering the entire history of the problems because of space limitations (The most up-to-date list of Erdős' papers can be found in [65]; an electronic file is maintained by Jerry Grossman at grossman@oakland.edu.) There are many survey papers on the impact of Paul's work, e.g., see those in the books: A Tribute to Paul Erdős [84], Combinatorics, Paul Erdős is Eighty, Volumes 1 and 2 [83], and The Mathematics of Paul Erdős, Volumes I and II [81]. To honestly follow the unique style of Paul Erdős, we will mention the fact that Erdős often offered monetary awards for solutions to a number of his favorite problems. In November 1996, a committee of Erdős' friends decided no more such awards will be given in Erdős' name. However, the author, with the help of Ron Graham, will honor future claims on the problems in this paper, provided the requirements previously set by Paul are satisfied (e.g., proofs have been verified and published in recognized journals). Throughout this paper, the constants c, c′, c1, c2, · · · and extremal functions f(n), f(n, k), f(n, k, r, t), g(n), · · · are used extensively, although within the context of each problem, the",j310,Journal of Graph Theory,jv310,accepted,f1385,2016,2016-11-26
s1726,p1726,Applications of combinatorics and graph theory to the biological and social sciences,Abstract content goes here ...,c72,Intelligent Systems in Molecular Biology,cp72,accepted,f1386,2003,2003-12-21
s1727,p1727,Computational Graph Theory,Abstract content goes here ...,c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f1387,2020,2020-11-10
s1728,p1728,AN APPLICATION OF GRAPH THEORY TO ALGEBRA,"[Al, * * * , Ak ] 54-0. The original proof of the theorem [1] was elementary but very complicated. In attempting to simplify this proof, I found a more transparent proof based on the use of graph theory.3 One advantage of this approach is that complicated algebraic definitions can be replaced by much simpler geometric definitions merely by drawing a picture of the appropriate graph. Before stating the graph theoretic theorem which implies Theorem 1, I will give some elementary definitions and lemmas from graph theory.",c88,Symposium on the Theory of Computing,cp88,accepted,f1388,2014,2014-01-16
s1729,p1729,Chemical signed graph theory,"Chemical signed graph theory is presented. Each topological orbital of an N-vertex molecular graph is represented by a vertex-signed graph (VSG) that is generated by assigning a sign, either plus or minus, to the vertices without solving the secular matrix equation. The bonding capacity of each VSG is represented by its corresponding edge-signed graph (ESG) and is quantified by the net sign of the ESG. The resulting 2N–1 configurations of VSGs can be divided into several groups according to the net signs of the corresponding ESGs. Summation of an appropriate set of degenerate VSGs is found to yield the conventional, canonical molecular orbitals. The distribution of the number of VSGs with respect to the net sign is found to be binomial, which can be related to bond percolation in statistical physics. © 1994 John Wiley & Sons, Inc.",c90,Computer Vision and Pattern Recognition,cp90,accepted,f1389,2008,2008-04-08
s1730,p1730,Potts model and graph theory,Abstract content goes here ...,c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f1390,2002,2002-06-11
s1731,p1731,Some recent results in topological graph theory,Abstract content goes here ...,c113,International Conference on Image Analysis and Processing,cp113,accepted,f1391,2002,2002-06-11
s1732,p1732,Conference on Graph Theory and Topology in Chemistry.,"Abstract : Prof. R. B. King and Dr. D. H. Rouvray organized an International Conference on Graph Theory and Topology in Chemistry which was held at the University of Georgia, Athens, Georgia, during the period March 15-20, 1987. A volume containing the papers presented at this conference is being published by Elsevier Scientific Publishing Company, Amsterdam, and will appear around the end of 1987. The following items are attached: (1) The program of the conference. (2) The short abstracts of the paper presented at the conference. (3) The contents of the conference volume. (4) The preface of the conference volume. Knots, Marcromolecules and Chemical Dynamics Topological Stereochemistry: Knot Theory of Molecular Graphs Extrinsic Topological Chirality Indices of Molecular Graphs A Topological Approach to the Stereochemistry of Nonrigid Molecules Chirality of Non-Standardly Embedded Mobius Ladders .",c87,European Conference on Computer Vision,cp87,accepted,f1392,2014,2014-09-10
s1733,p1733,Graph theory and molecular orbitals. VII. The role of resonance structures,"The relations between the simplest variants of MO and VB theory are discussed. It is shown that there is a unique principle causing all the cases of congruity between these two theories‐Kekule structures being related to the permutations contained in the molecular graph [Eqs. (6) and (7)]. The class of benzenoid hydrocarbons where both theories are substantially equivalent is rigorously defined using graph theory. A number of topological regularities for these hydrocarbons are proved. Thus, the Dewar‐Longuet‐Higgins equation, the proof of the Ruedenberg's and Pauling's bond orders, the relation between the VB and MO spin and charge density, and Heilbronner's formula are obtained. The limits of validity for all these results are strictly determined.",c33,International Conference on Agile Software Development,cp33,accepted,f1393,2022,2022-12-16
s1734,p1734,Facilities Planning with Graph Theory,Basic concepts of Graph Theory are discussed which are relevant to solving problems of locating economic activities within a service or manufacturing facility. The location problem is formulated in terms of Graph Theory knowledge and a solution procedure proposed. An example is provided and finally boundary conditions are elaborated.,c23,International Conference on Open and Big Data,cp23,accepted,f1394,2012,2012-04-07
s1735,p1735,On some solved and unsolved problems of chemical graph theory,"The development of several novel graph theoretical concepts and their applications in different branches of chemistry are reviewed. After a few introductory remarks we follow with an outline of selected important graph theoretical invariants, introducing some new results and indicating some open problems. We continue with discussing the problem of graph characterization and construction of graphs of chemical interest, with a particular emphasis on large systems. Finally we consider various problems and difficulties associated with special subgraphs, including subgraphs representing Kekule valence structures. The paper ends with a brief review of structure-property and structure-activity correlations, the topic which is one of prime motivations for application of graph theory to chemistry.",c112,Very Large Data Bases Conference,cp112,accepted,f1395,2018,2018-02-06
s1736,p1736,TWO THEOREMS IN GRAPH THEORY.,"Introduction. Given an unoriented graph (or 1-dimensional regular complex), let X be the set of all its vertices and U be the set of all its edges. When the graph is finite, the following problems arise: Problem 1: A set A c X is said to be internally stable if x e A, y e A implies (x, y) o U. The symbol A will denote the number of elements of A. Construct an internally stable set A such that A is maximum. Problem 2: A set B c X is said to be a cover if every edge of U is adjacent to at least one vertex in B. Construct a cover with the minimum number of elements. Problem 3: A set of edges V c U is said to be a matching if two edges of V have no vertex in common. Construct a matching with the maximum number of elements. A particular case of Problem 1 is the chess problem of Gauss: Put eight queens on the board such that no one can take any other. In n-person game theory, if the graph of domination is symmetrical, a maximum internally stable set turns out to be a maximum solution (in the von Neumann-Morgenstern sense1), and the more usual case can be solved by means of the Grundy functions.2 Problem 2 is the set theoretic dual of Problem 1, since the complement of an internally stable set is a cover, and conversely. Particular cases of Problem 3 are the problem of distinct representatives (P. Hall1) and the problem of Petersen (D. K6nig4). In the case where the graph is bipartite, Problem 3 has been solved by algebraic methods by 0. Ore,5 and an efficient algorithm has been given by H. Kuhn.6 Unfortunately, the linear programming duality used by H. Kuhn no longer subsists when the graph is not bipartite. (Note that Problem 2 is the linear program dual to Problem 3 in the bipartite case.) In view of solving the general case, this paper states two theorems: Theorem 1 gives a necessary and sufficient condition for recognizing whether a matching is maximum and provides an algorithm for Problem 3, while Theorem 2 yields an algorithm for Problems 1 and 2. The Theorems.-Consider a graph G = (X, U) with a matching V0; if u e Vo we shall say that edge u is strong, otherwise that u is weak. An alternating chain is a chain which does not use the same edge twice and is such that for any two adjacent edges one is strong and the other is weak. A vertex x which is not adjacent to a strong edge is said to be neutral, the set of all neutral points being N. We shall also consider a graph G constructed from G by adding a vertex a and connecting a to every neutral point with a strong edge. If there exists an alternating chain from a to a vertex x, we shall picture an arrow on the last edge (z, x), directed from z to x. A vertex x (a N) which is not adjacent to a directed edge is said to be inaccessible, the set of all inaccessible points being I. A vertex x (t N) adjacent to a weak edge directed to x and not to a strong edge directed to x is said to be weak, the set of all weak points being W. A vertex x (f N) adjacent to a strong edge directed to x and not to a weak edge directed to x is said to be strong,",j20,Proceedings of the National Academy of Sciences of the United States of America,jv20,accepted,f1396,2006,2006-03-15
s1737,p1737,Graph Theory and Probability,"A well-known theorem of Ramsay (8; 9) states that to every n there exists a smallest integer g(n) so that every graph of g(n) vertices contains either a set of n independent points or a complete graph of order n, but there exists a graph of g(n) — 1 vertices which does not contain a complete subgraph of n vertices and also does not contain a set of n independent points. (A graph is called complete if every two of its vertices are connected by an edge; a set of points is called independent if no two of its points are connected by an edge.) The determination of g(n) seems a very difficult problem; the best inequalities for g(n) are (3) It is not even known that g(n)1/n tends to a limit. The lower bound in (1) has been obtained by combinatorial and probabilistic arguments without an explicit construction.",j313,Canadian Journal of Mathematics - Journal Canadien de Mathematiques,jv313,accepted,f1397,2006,2006-07-25
s1738,p1738,Fractional Graph Theory: A Rational Approach to the Theory of Graphs,General Theory: Hypergraphs. Fractional Matching. Fractional Coloring. Fractional Edge Coloring. Fractional Arboricity and Matroid Methods. Fractional Isomorphism. Fractional Odds and Ends. Appendix. Bibliography. Indexes.,c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f1398,2005,2005-03-25
s1739,p1739,Graph Theory and Q-Analysis,"Structures of graph theory are compared with those of Q-analysis and there are many similarities. The graph and simplicial complex defined by a relation are equivalent in terms of the information they represent, so that the choice between graph theory and Q-analysis depends on which gives the most natural and complete description of a system. The higher dimensional graphs are shown to be simplicial families or complexes. Although network theory is very successful in those physical science applications for which it was developed, it is argued that Q-analysis gives a better description of human network systems as patterns of traffic on a backcloth of simplicial complexes. The q-nearness graph represents the q-nearness of pairs of simplices for a given q-value. It is concluded that known results from graph theory could be applied to the q-nearness graph to assist in the investigation of q-connectivity, to introduce the notion of connection defined by graph cuts, and to assist in computation. The application of the q-nearness graph to q-transmission and shomotopy is investigated.",c114,IEEE International Conference on Robotics and Automation,cp114,accepted,f1399,2011,2011-11-11
s1741,p1741,"ON SOME PROBLEMS IN GRAPH THEORY , COMBINATORIAL ANALYSIS AND COMBINATORIAL NUMBER THEORY","1. G(n) is a graph of n vertices and G(n ; e) is a graph of n vertices and e edges. Is it true that if every induced subgraph of a G(10n) of 5n vertices has more than 2n 2 edges then our G(10n) contains a triangle? It is easy to show that if true this result is best possible . To see this let A i , i =1, 2, . . . , 5, be sets of 2n vertices, put A, = A 6 and join every vertex of A, to every vertex of A; + , . This G(10n ; 20n 2) has of course no triangle and every induced subgraph of 5n vertices contains at least 2n2 edges . Equality is of course possible : choose A,, A, and half the vertices of A, Simonovits pointed out to me that a graph of completely different structure also shows that the conjecture, if true, is best possible . Consider the Petersen graph, which is a G(10 ; 15) . Replace each vertex by a set of n vertices and replace every edge of the Petersen graph by the n 2 edges of a K(n, n) . This gives a G(10n ; 15n 2) and it is easy to see that every induced subgraph of 5n vertices has at least 2n2 edges . The fact that two graphs of different structure are extremal perhaps indicates that the conjecture is either false or difficult to prove . I certainly hope that the latter is the case . It is perhaps tempting to conjecture that my graph has the following extremal property . If a G(10n) has no triangle and every induced subgraph of 5n vertices has at least 2n2 edges, then our graph can have at most 20n2 edges. Perhaps the graph of Simonovits has the smallest number of edges among all extremal graphs; perhaps in fact these two graphs are the only extremal graphs . Many generalizations are possible ; the triangle could be replaced by other graphs . Is it true that every G((4h+2)n), every induced subgraph",c17,International Conference on Enterprise Information Systems,cp17,accepted,f1400,2008,2008-12-03
s1743,p1743,Topics in Intersection Graph Theory,"Preface 1. Intersection Graphs. Basic Concepts Intersection Classes Parsimonious Set Representations Clique Graphs Line Graphs Hypergraphs 2. Chordal Graphs. Chordal Graphs as Intersection Graphs Other Characterizations Tree Hypergraphs Some Applications of Chordal Graphs Split Graphs 3. Interval Graphs. Definitions and Characterizations Interval Hypergraphs Proper Interval Graphs Some Applications of Interval Graphs 4. Competition Graphs. Neighborhood Graphs Competition Graphs Interval Competition Graphs Upper Bound Graphs 5. Threshold Graphs. Definitions and Characterizations Threshold Graphs as Intersection Graphs Difference Graphs and Ferrers Digraphs Some Applications of Threshold Graphs 6. Other Kinds of Intersection. p-Intersection Graphs Intersection Multigraphs and Pseudographs Tolerance Intersection Graphs 7. Guide to Related Topics. Assorted Geometric Intersection Graphs Bipartite Intersection Graphs, Intersection Digraphs, and Catch (Di)Graphs Chordal Bipartite and Weakly Chordal Graphs Circle Graphs and Permutation Graphs Clique Graphs of Chordal Graphs and Clique-Helly Graphs Containment, Comparability, Cocomparability, and Asteroidal Triple-Free Graphs Infinite Intersection Graphs Miscellaneous Topics P4-Free Chordal Graphs and Cographs Powers of Intersection Graphs Sphere-of-Influence Graphs Strongly Chordal Graphs Bibliography Index.",c94,Vision,cp94,accepted,f1401,2020,2020-05-23
s1744,p1744,Selected Topics in Graph Theory 2,Abstract content goes here ...,c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f1402,2010,2010-06-01
s1745,p1745,A ring in graph theory,"We call a point set in a complex K a 0-cell if it contains just one point of K, and a 1-cell if it is an open arc. A set L of 0-cells and 1-cells of K is called a linear graph on K if (i) no two members of L intersect, (ii) the union of all the members of L is K, (iii) each end-point of a 1-cell of L is a 0-cell of L and (iv) the number of 0-cells and 1-cells of L is finite and not 0.",c41,Software Product Lines Conference,cp41,accepted,f1403,2002,2002-05-05
s1746,p1746,On some extremal problems in graph theory,"In this paper we are concerned with various graph invariants (girth, diameter, expansion constants, eigenvalues of the Laplacian, tree number) and their analogs for weighted graphs -- weighing the graph changes a combinatorial problem to one in analysis. We study both weighted and unweighted graphs which are extremal for these invariants. In the unweighted case we concentrate on finding extrema among all (usually) regular graphs with the same number of vertices; we also study the relationships between such graphs.",c29,International Conference on Software Engineering,cp29,accepted,f1404,2015,2015-09-02
s1747,p1747,"Introduction to Chordal Graphs and Clique Trees, in Graph Theory and Sparse Matrix Computation","Kjjrull U., Triangulation of graph-algorithms giving small total state space. 19 in the number of minimal separators. This possible amendment will resolve a theoretical problem raised by KBMK93, KBMK94] and further addressed by PS95] but will hardly aaect the running time of our algorithm. 18 The entry Frag in Table 3 measures the number of fragments produced for the dynamic programming phase. The entries R and R k measure, respectively, the number of minimal separators and the number of minimal separators of size less than k where k = 7. Note again that R k is smaller than R; Many minimal separators are generated in the rst phase of QuickTree but are not needed for the dynamic programming phase. A second reason for the high values of T 1 is that the algorithm runs over almost all pairs of vertices and for each pair fa; bg produces all minimal a; b-separators. However, after a few pairs, the algorithm usually nds most of the minimal sepa-rators and the remaining run time is just used to verify that indeed all minimal separators have been generated. Table 4 shows the number of Good-Pairs|pairs that generated at least one new minimal separator. All-Pairs denote the number of pairs we used which guarantee that all minimal separators have been generated. For Table 4, 30% of the edges were dropped. Table 4 suggests that if there is no needed guarantee of optimal triangulation, then Phase 1 can be run on a fraction of the possible pairs of vertices and then the dynamic programming phase can be applied. For example, on 3 graphs with 75 nodes and treewidth 7, we selected the top 20% of pairs that had a maximal mutual distance and got close to optimal triangulations (in two instances we got the optimal treewidth 7 and once we got 9 instead of 7). The average running time was reduced from 675 to 373 seconds. 6 Discussion In many applications the treewidth of a triangulation is just an approximation to the real optimization problem. For example, for the updating problem in Bayesian networks, one needs to nd a triangulation that minimizes the sum P i 2 w(Ci) where w is a positive additive weight function on the vertices of G and C i are the cliques of the triangulation Kj90, BG96]. The triangulation algorithm presented herein can be modiied to accommodate such variants. Currently, the algorithm …",c76,International Conference on Artificial Neural Networks,cp76,accepted,f1405,2013,2013-11-17
s1748,p1748,Lectures on Spectral Graph Theory,Contents Chapter 1. Eigenvalues and the Laplacian of a graph 1 1.1. Introduction 1 1.2. The Laplacian and eigenvalues 2 1.3. Basic facts about the spectrum of a graph 6,c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f1406,2020,2020-07-02
s1749,p1749,Graph theory with applications to algorithms and computer science,"Partial table of contents: Finite Figures Consisting Of Regular Polygons (J. Akiyama, et al.). Eigenvalues, Geometric Expanders and Sorting in Rounds (N. Alon). Long Path Enumeration Algorithms for Timing Verification on Large Digital Systems (T. Asano and S. Sato). On Upsets in Bipartite Tournaments (K. Bagga). Some Results on Binary Matrices Obtained via Bipartite Tournaments (K. Bagga and L. Beineke). Partitioning the Nodes of a Graph (E. Barnes). A Graph Theoretical Characterization of Minimal Deadlocks in Petri Nets (J. Bermond and G. Memmi). On Graceful Directed Graphs that Are Computational Models of Some Algebraic Systems (G. Bloom and D. Hsu). The Cut Frequency Vector (F. Boesch). Diameter Vulnerability in Networks (J. Bond and C. Peyrat). Generalized Colorings of Outerplanar and Planar Graphs (I. Broere and C. Mynhardt). The Ramsey Number for the Pair Complete Bipartite Graph-Graph of Limited Degree (S. Burr, et al.). Embedding Graphs in Books: A Layout Problem with Applications to VLSI Design (F. Chung). Hamilton Cycles and Quotients of Bipartite Graphs (I. Dejter). Problems and Results on Chromatic Numbers in Finite and Infinite Graphs (P. Erdos). Supraconvergence and Functions that Sum to Zero on Cycles (V. Faber and A. White, Jr.). Edge-Disjoint Hamiltonian Cycles (R. Faudree, et al.). Studies Related to the Ramsey Number r(K d5 u - e) (R. Faudree, et al). The Structural Complexity of Flowgraphs (N. Fenton). n-Domination in Graphs (J. Fink and M. Jacobson).",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f1407,2011,2011-03-10
s1751,p1751,Some Topics in Graph Theory,1. Basic terminology 2. Edge-colourings of graphs 3. Symmetries in graphs 4. Packing of graphs 5. Computational complexity of graph properties.,c95,IEEE International Conference on Computer Vision,cp95,accepted,f1408,2017,2017-01-08
s1752,p1752,Compactness results in extremal graph theory,Abstract content goes here ...,c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1409,2015,2015-10-10
s1753,p1753,A Friendly Introduction to Graph Theory,1. Introductory Concepts. 2. Introduction to Graphs and their Uses. 3. Trees and Bipartite Graphs. 4. Distance and Connectivity. 5. Eularian and Hamiltonian Graphs. 6. Graph Coloring. 7. Matrices. 8. Graph Algorithms. 9. Planar Graphs. 10. Digraphs and Networks. 11. Special Topics. Answers/Solutions to Selected Exercises. Index.,c107,British Machine Vision Conference,cp107,accepted,f1410,2012,2012-09-17
s1754,p1754,Use of Graph Theory to Support Map Generalization,"In the generalization of a concept, we seek to preserve the essential characteristics and behavior of objects. In map generalization, the appropriate selection and application of procedures (such as merging, exaggeration, and selection) require information at the geometric, attribute, and topological levels. This article highlights the potential of graph theoretic representations in providing the topological information necessary for the efficient and effective application of specific generalization procedures. Besides ease of algebraic manipulation, the principal benefit of a graph theoretic approach is the ability to detect and thus preserve topological characteristics of map objects such as isolation, adjacency, and connectivity. While it is true that topologically based systems have been developed for consistency checking and error detection during editing, this article emphasizes the benefits from a map-generalization perspective. Examples are given with respect to specific generalization procedures ...",c61,Jahrestagung der Gesellschaft für Informatik,cp61,accepted,f1411,2017,2017-07-24
s1755,p1755,Graph theory for image analysis: an approach based on the shortest spanning tree,"The paper describes methods of image segmentation and edge detection based on graph-theoretic representations of images. The image is mapped onto a weighted graph and a spanning tree of this graph is used to describe regions or edges in the image. Edge detection is shown to be a dual problem to segmentation. A number of methods are developed, each providing a different segmentation or edge detection technique. The simplest of these uses the shortest spanning tree (SST), a notion that forms the basis of the other improved methods. These further methods make use of global pictorial information, removing many of the problems of the SST segmentation in its simple form and of other pixel linking algorithms. An important feature in all of the proposed methods is that regions may be described in a hierarchical way.",c12,International Conference on Statistical and Scientific Database Management,cp12,accepted,f1412,2002,2002-04-01
s1756,p1756,Extremal problems in graph theory,The aim of this note is to give an account of some recent results and state a number of conjectures concerning extremal properties of graphs.,j310,Journal of Graph Theory,jv310,accepted,f1413,2016,2016-06-14
s1757,p1757,"Graph Theory: Flows, Matrices","STRUCTURE OF THE GRAPH MODEL The abstract graph Geometrical realization of graphs Components Leaves Blocks The strongly connected components of directed graphs Problems OPTIMAL FLOWS Two basic problems Maximal set of independent paths The optimal assignment problem The Hungarian method Max flow-min cut Dynamic flow The mobilization problem The synthesis of flow problems Optical planning The role of the critical path Minimal cost transportation Minimal cost flows Problems GRAPHS AND MATRICES The adjacency matrix The incidence matrix The circuit matrix Interrelations between the matrices of graphs The spectrum of graphs, the complexity Linear electrical networks Further matrices associated with graphs Problems and solutions",c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f1414,2006,2006-02-21
s1758,p1758,Graph Theory and Probability. II,"Define f(k, l) as the least integer so t h a t every graph having f(k, l) vertices contains either a complete graph of order k or a set of l independent vertices (a complete graph of order k is a graph of k vertices every two of which are connected by an edge, a set of I vertices is called independent if no two are connected by an edge). Throughout this paper c1, c2, … will denote positive absolute constants. It is known (1, 2) that (1) and in a previous paper (3) I stated that I can prove that for every ∈ > 0 and l > l(∈), f (3, l) > l2-∈ . In the present paper I am going to prove that (2)",j313,Canadian Journal of Mathematics - Journal Canadien de Mathematiques,jv313,accepted,f1415,2006,2006-03-14
s1759,p1759,Parallel computations in graph theory,"In parallel computation two approaches are common; namely unbounded parallelism and bounded parallelism. In this paper both approaches will be considered. The problem of unbounded parallelism is studied in section II and some lower and upper bounds on different connectivity problems for directed and undirected graphs are presented. In section III we mention bounded parallelism and three different k-parallel graph search techniques, namely k-depth search, breadth depth search, and breadth-first search. Each algorithm is analyzed with respect to the optimal serial algorithm. It is shown that for sufficiently dense graphs the parallel breadth first search technique is very close to the optimal bound. Techniques for searching sparse graphs are also discussed.",c97,Interspeech,cp97,accepted,f1416,2004,2004-06-25
s1760,p1760,An extremal problem in graph theory,"G(n;l) will denote a graph of n vertices and l edges. Let f0(n, k) be the smallest integer such that there is a G (n;f0(n, k)) in which for every set of k vertices there is a vertex joined to each of these. Thus for example fo = 3 since in a triangle each pair of vertices is joined to a third. It can readily be checked that fo = 5 (the extremal graph consists of a complete 4-gon with one edge removed). In general we will prove: Let n > k, andthen f0(n, k) = f(n, k).",j315,Journal of the Australian Mathematical Society,jv315,accepted,f1417,2009,2009-05-04
s1761,p1761,Chemical applications of graph theory,Abstract content goes here ...,c81,IEEE Annual Symposium on Foundations of Computer Science,cp81,accepted,f1418,2004,2004-08-26
s1763,p1763,Graph theory and its engineering applications,Basic theory foundations of electrical network theory directed-graph solutions of linear algebraic equations topological analysis of linear systems trees and their generation the realizability of directed graphs with prescribed degrees state equations of networks.,c22,International Conference on Data Technologies and Applications,cp22,accepted,f1419,2020,2020-02-21
s1764,p1764,Matrices in Combinatorics and Graph Theory,Abstract content goes here ...,c44,International Workshop on Green and Sustainable Software,cp44,accepted,f1420,2008,2008-05-14
s1765,p1765,A Beginner's Guide to Graph Theory,"Graphs.- Walks, Paths and Cycles.- Connectivity.- Trees.- Linear Spaces Associated with Graphs.- Factorizations.- Graph Colorings.- Planarity.- Labeling.- Ramsey Theory.- Digraphs.- Critical Paths.- Flows in Networks.- Computational Considerations.- Communications Networks and Small-Worlds.",c76,International Conference on Artificial Neural Networks,cp76,accepted,f1421,2013,2013-06-10
s1766,p1766,On finite 0-simple semigroups and graph theory,Abstract content goes here ...,j316,Mathematical Systems Theory,jv316,accepted,f1422,2007,2007-04-10
s1767,p1767,Fundamentals of Graph Theory,Abstract content goes here ...,c92,Advances in Soft Computing,cp92,accepted,f1423,2009,2009-02-04
s1768,p1768,On-Line Coloring and Recursive Graph Theory,"An on-line vertex coloring algorithm receives the vertices ofa graph in some externally determined order, and, whenever a new vertex is presented, the algorithm also learns to which of the previously presented vertices the new vertex is adjacent. As each vertex is received, the algorithm must make an irrevocable choice of a color to assign the new vertex, and it makes this choice without knowledge of future vertices. A class of graphs $r$ is said to be on-line $\chi$-bounded if there exists an on-line algorithm $A$ and a function $f$ such that $A$ uses at most $f(\omega(G))$ colors to properly color any graph $G$ in \Gamma. If $H$ is a graph, let Forb($H$) denote the class of graphs that do not induce $H$. The goal of this paper is to establish that Forb($T$) is on-line $\chi$-bounded for every radius-2 tree $T$. As a corollary, the authors answer a question of Schmerl's; the authors show that every recursive cocomparability graph can be recursively colored with a number of colors that depends only on its clique number.",j317,SIAM Journal on Discrete Mathematics,jv317,accepted,f1424,2016,2016-08-05
s1769,p1769,Graph Theory,Abstract content goes here ...,c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1425,2015,2015-03-07
s1770,p1770,A First Look at Graph Theory,"This book is intended to be an introductory text for mathematics and computer science students at the second and third year levels in universities. It gives an introduction to the subject with sufficient theory for students at those levels, with emphasis on algorithms and applications.",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f1426,2018,2018-03-07
s1771,p1771,Contentment in graph theory: Covering graphs with cliques,Abstract content goes here ...,c36,Conference on Software Engineering Education and Training,cp36,accepted,f1427,2015,2015-10-05
s1772,p1772,Topological organic chemistry. 1. Graph theory and topological indices of alkanes,"coding reaction transforms and can be made more or less REFERENCES AND NOTES discriminating by using an appropriate number of eigenvalues. However, the method is not reversible in that the index cannot (1) Wiswesser, W. J. J . Chem. Inf, Comput. Sci. 1982, 22, 8 8 . (2) Wiswesser, W. J. J . Chem. I f . Comput. Sci. 1985, 25, 258. (3) RandiE, M. J . Chem. In$ Compur. Sci. 1984, 24, 164. (4) Ash, J. E.; Chubb, P. A.; Ward, S. E.; Welford, S. M.; Willett, P. be used to derive the original structure. On the other hand, it can be used to derive a unique numbering for substructures by ordering the eigenvalues of the defined &nnectivity matrix. Another useful benefit is associated with the eigenvectors, which can be used to determine the attribution of each atom to substructures upon disconnection of the main structure into distinct fragments' This be when the disconnections of a main structure into reacting substructures Communication, Storage and Retrieval of Chemical Information; Ellis H o r w d Ltd.: Chichester, U.K., 1985. (5) Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; Vetterling, W. T. Numerical Recipes: The Art of Scientific Computing; Cambridge University Press: Cambridge, U.K., 1986. (6) Henze, H. R.; Blair, C. M. J . Am. Chem. SOC. 1931, 53, 3077. (7) Lederberg, J.; Sutherland, G. L.; Buchanan, B. G.; Feigenbaum, E. A.; Robertson, A. V.; Duffield, A. M.; Djerassi, C. J. Am. Chem. Soc. 1969, are considered. 91. 2973.",j214,Journal of chemical information and computer sciences,jv214,accepted,f1428,2022,2022-04-14
s1773,p1773,Geometric Graph Theory,"Note: Professor Pach's number: [172]; 2nd edition Reference DCG-CHAPTER-2008-027 Record created on 2008-11-18, modified on 2017-05-12",c62,International Conference on Software Reuse,cp62,accepted,f1429,2006,2006-10-25
s1774,p1774,Applied Graph Theory,Abstract content goes here ...,c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f1430,2014,2014-08-11
s1775,p1775,Algebraic Graph Theory: COLOURING PROBLEMS,Abstract content goes here ...,c35,EUROMICRO Conference on Software Engineering and Advanced Applications,cp35,accepted,f1431,2005,2005-07-13
s1777,p1777,Graph Decompositions: A Study in Infinite Graph Theory,Note to the reader Introduction Fundamental facts and concepts Separating simplices and the existence of prime decompositions Simplicial minors and the existence of prime decompositions The uniqueness of prime decompositions Decompositions into small factors Applications of simplicial decompositions Appendix: Some notes on set theory References Subject index Index of symbols.,c2,International Symposium on Intelligent Data Analysis,cp2,accepted,f1432,2014,2014-09-03
s1779,p1779,GRAPH THEORY IN PRACTICE : PART I,"This reprint is provided for personal and noncommercial use. For any other use, please send a request to Permissions,",c49,International Symposium on Search Based Software Engineering,cp49,accepted,f1433,2012,2012-01-21
s1781,p1781,Some Applications of Graph Theory to the Structural Analysis of Mechanisms,Abstract content goes here ...,c36,Conference on Software Engineering Education and Training,cp36,accepted,f1434,2015,2015-10-15
s1783,p1783,"Spectral Graph Theory, Regional Conference Series in Math.",Abstract content goes here ...,c107,British Machine Vision Conference,cp107,accepted,f1435,2012,2012-09-24
s1786,p1786,Graph Theory Applications,1: Basic Ideas. 2: Connectivity. 3: Trees. 4: Traversability. 4: Planarity. 6: Matrices. 7: Digraphs. 8: Coverings and Colourings. 9: Algorithms. 10: Matroids. 11: Miscellaneous Applications. 12: Operations Research. 13: Electrical Engineering. 14: Industrial Engineering. 15: Science. 16: Civil Engineering.,c85,International Conference on Graph Transformation,cp85,accepted,f1436,2007,2007-01-07
s1787,p1787,Graph Theory and Feynman Integrals,Abstract content goes here ...,c105,Biometrics and Identity Management,cp105,accepted,f1437,2006,2006-12-01
s1788,p1788,Introductory Graph Theory,Abstract content goes here ...,c84,The Web Conference,cp84,accepted,f1438,2006,2006-08-10
s1791,p1791,Graph theory and Feynman integrals,Abstract content goes here ...,c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f1439,2018,2018-04-09
s1792,p1792,Introductory Graph Theory,Abstract content goes here ...,c94,Vision,cp94,accepted,f1440,2020,2020-03-29
s1793,p1793,Graph theory and combinatorics,"WORKING PAPERS q C. Borgs, J. Chayes, N. Immorlica, A. Kalai, V. S. Mirrokni and C. Papadimitriou, The Myth of the Folk Theorem, submitted to STOC. q U. Feige, N. Immorlica, V.S.Mirrokni and H. Nazerzadeh, Functional Approximations: A new approach for analyzing Heuristics, submitted to STOC. q B. Awerbuch, Y. Azar, and A. Epstein, V. S. Mirrokni, A. Skopalik, Fast Convergence to Nearly Optimal Solutions in Potential Games, submitted to STOC. q J. Hartline, V. S. Mirrokni, and M. Sundararajan, Marketing Strategies over Social Networks, submitted to WWW. q U. Feige, N. Immorlica, V.S. Mirrokni and H. Nazerzadeh, Combinatorial Allocation Mechanisms with Penalties for Banner Advertisement, submitted to WWW. q R. Andersen, C. Borgs, J. Chayes, U. Feige, A. Flaxman, A. Kalai, V. S. Mirrokni, M. Tennenholtz, Trust-based Recommendation Systems: An axiomatic Approach, submitted to WWW. q V. S. Mirrokni, M. Schapira, J. Vondrak, Tight Information-Theoretic Lower Bounds for Maximizing Social-Welfare in Combinatorial Auctions, submitted to IPCO. q M. Goemans, N. Harvey, R. Kleinberg, V. S. Mirrokni, On Learning submodular functions, to be submitted to ICALP. q H. Ackermann, P. Goldberg, V. S. Mirrokni, H. Roeglin, and B. Voecking, Uncoordinated Twosided Markets, to be submitted to ACM EC. q M. Ghodsi, M. Mahini, V. S. Mirrokni, and M. ZadiMoghaddam Singleton Betting for Permutation Betting Markets. q R. Andersen, C. Borgs, J. Chayes, K. Jain, J. Hopcroft, V. S. Mirrokni and S. Teng, Locally Computable Link Spam Features. q V.S. Mirrokni, A. Skopalik, On the Complexity of Nash Dynamics and Sink Equilibria. q R. Andersen, V. S. Mirrokni, Overlapping Clustering for Distributed Computation.",c111,International Society for Music Information Retrieval Conference,cp111,accepted,f1441,2001,2001-02-18
s1794,p1794,Graph Theory in Practice: Part II,"This reprint is provided for personal and noncommercial use. For any other use, please send a request to Permissions,",j319,American Scientist,jv319,accepted,f1442,2012,2012-07-21
s1795,p1795,Graph theory in network analysis,Abstract content goes here ...,c43,ACM Symposium on Applied Computing,cp43,accepted,f1443,2001,2001-02-15
s1797,p1797,An Application of Graph Theory to Additive Number Theory,Abstract content goes here ...,c107,British Machine Vision Conference,cp107,accepted,f1444,2012,2012-07-02
s1798,p1798,Fractional Graph Theory,Abstract content goes here ...,c74,IEEE International Conference on Tools with Artificial Intelligence,cp74,accepted,f1445,2003,2003-12-26
s1800,p1800,GRAPH THEORY IN PRACTICE : PART I,"This reprint is provided for personal and noncommercial use. For any other use, please send a request to Permissions,",c95,IEEE International Conference on Computer Vision,cp95,accepted,f1446,2017,2017-12-20
s1801,p1801,"Affordances. Motivations, and the World Graph Theory","O'Keefe and Nadel (1978) distinguish two paradigms for navigation, the ""locale system"" for map-based navigation and the ""taxon (behavioral orientation) system"" for route navigation. This article models the taxon system, the map-based system, and their interaction, and argues that the map-based system involves the interaction of hippocampus and other systems. We relate taxes to the notion of an affordance. Just as a rat may have basic taxes for approaching food or avoiding a bright light, so does it have a wider repertoire of affordances for possible actions associated with immediate sensing of its environment. We propose that affordances are extracted by the rat posterior parietal cortex, which guides action selection by the premotor cortex and is influenced also by hypothalamic drive information. The taxon-affordances model (TAM) for taxon-based determination of movement direction is based on models of frog detour behavior, with expectations of future reward implemented using reinforcement learning. The specification of the direction of movement is refined by current affordances and motivational information to yield an appropriate course of action. The world graph (WG) theory expands the idea of a map by developing the hypothesis that cognitive and motivational states interact. This article describes an implementation of this theory, the WG model. The integrated TAM-WG model then allows us to explain data on the behavior of rats with and without fornix lesions, which disconnect the hippocampus from other neural systems.",j320,Adaptive Behavior,jv320,accepted,f1447,2021,2021-09-11
s1802,p1802,Graph theory and applications,Abstract content goes here ...,c9,Pacific Symposium on Biocomputing,cp9,accepted,f1448,2009,2009-10-21
s1803,p1803,Three short proofs in graph theory,Abstract content goes here ...,c74,IEEE International Conference on Tools with Artificial Intelligence,cp74,accepted,f1449,2003,2003-01-13
s1804,p1804,Graph theory in network analysis,Abstract content goes here ...,c49,International Symposium on Search Based Software Engineering,cp49,accepted,f1450,2012,2012-03-13
s1805,p1805,Graph theory and Feynman integrals,Abstract content goes here ...,c43,ACM Symposium on Applied Computing,cp43,accepted,f1451,2001,2001-11-10
s1807,p1807,From time series to complex networks: The visibility graph,"In this work we present a simple and fast computational method, the visibility algorithm, that converts a time series into a graph. The constructed graph inherits several properties of the series in its structure. Thereby, periodic series convert into regular graphs, and random series do so into random graphs. Moreover, fractal series convert into scale-free networks, enhancing the fact that power law degree distributions are related to fractality, something highly discussed recently. Some remarkable examples and analytical tools are outlined to test the method's reliability. Many different measures, recently developed in the complex network theory, could by means of this new approach characterize time series from a new point of view.",j20,Proceedings of the National Academy of Sciences of the United States of America,jv20,accepted,f1452,2006,2006-06-16
s1808,p1808,On a Connection of Number Theory with Graph Theory,Abstract content goes here ...,c91,Workshop on Algorithms and Models for the Web-Graph,cp91,accepted,f1453,2022,2022-10-20
s1809,p1809,Discrete Mathematics With Graph Theory,"From the Publisher: 
Adopting a user-friendly, conversationaland at times humorousstyle, these authors make the principles and practices of discrete mathematics as stimulating as possible while presenting comprehensive, rigorous coverage. Examples and exercises integrated throughout each chapter serve to pique reader interest and bring clarity to even the most complex concepts. Above all, the book is designed to engage today's readers in the interesting, applicable facets of modern mathematics. More than 200 worked examples and problems, as well as over 2500 exercises are included. Full solutions are provided in the back of the book. More than 150 Pausesshort questions inserted at strategic pointsare included. Full solutions to Pauses are included at the end of each section. For educators in area of discrete mathematics.",c6,Americas Conference on Information Systems,cp6,accepted,f1454,2007,2007-11-22
s1810,p1810,On a Problem in Graph Theory,Suppose there are n towns every pair of which are connected by a single one-way road (roads meet only at towns). Is it possible to choose the direction of the traffic on all the roads so that if any two towns are named there is always a third from which the two named can be reached directly by road?,j321,Mathematical Gazette,jv321,accepted,f1455,2019,2019-01-17
s1813,p1813,The Many Facets of Graph Theory,Abstract content goes here ...,c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,cp79,accepted,f1456,2020,2020-12-12
s1814,p1814,Maximizing the total number of spanning trees in a graph: Two related problems in graph theory and optimum design theory,Abstract content goes here ...,j318,Journal of combinatorial theory. Series B (Print),jv318,accepted,f1457,2017,2017-09-28
s1815,p1815,An Extremal Problem for Sets with Applications to Graph Theory,Abstract content goes here ...,j322,Journal of combinatorial theory. Series A,jv322,accepted,f1458,2018,2018-10-03
s1816,p1816,Graph Theory As A Mathematical Model In Social Science,Abstract content goes here ...,c26,PS,cp26,accepted,f1459,2010,2010-03-29
s1818,p1818,Applied graph theory: Graphs and electrical networks,Abstract content goes here ...,j168,Proceedings of the IEEE,jv168,accepted,f1460,2019,2019-05-30
s1819,p1819,Graph theory and Gaussian elimination.,Abstract content goes here ...,c94,Vision,cp94,accepted,f1461,2020,2020-01-02
s1821,p1821,Graph Theory and Algorithms,Abstract content goes here ...,j75,Lecture Notes in Computer Science,jv75,accepted,f1462,2015,2015-03-09
s1822,p1822,On a valence problem in extremal graph theory,Abstract content goes here ...,j312,Discrete Mathematics,jv312,accepted,f1463,2013,2013-03-05
s1823,p1823,Extremal graph theory with emphasis on probabilistic methods,Subdivisions Contractions Small graphs of large girth Large graphs of small diameter Cycles in dense graphs The evolution of random graphs The size Ramsey number of a path Weakly saturated graphs List colourings.,c41,Software Product Lines Conference,cp41,accepted,f1464,2002,2002-12-11
s1824,p1824,Graph theory and related topics,Abstract content goes here ...,c111,International Society for Music Information Retrieval Conference,cp111,accepted,f1465,2001,2001-11-28
s1825,p1825,Computational chemical graph theory,Abstract content goes here ...,c50,International Conference on Automated Software Engineering,cp50,accepted,f1466,2008,2008-09-29
s1826,p1826,Extremal problems in graph theory,Abstract content goes here ...,c113,International Conference on Image Analysis and Processing,cp113,accepted,f1467,2002,2002-12-15
s1827,p1827,Graph theory and computing,Abstract content goes here ...,c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1468,2007,2007-08-22
s1829,p1829,"Graph theory, coding theory, and block designs","Introduction 1. A brief introduction to design theory 2. Strongly regular graphs 3, Quasi-symmetric designs 4. Strongly regular graphs with no triangles 5. Polarities of designs 6. Extension of graphs 7. Codes 8. Cyclic codes 9. Threshold decoding 10. Reed-Muller codes 11. Self-orthogonal codes and designs 12. Quadratic residue codes 13. Symmetry codes over GF(3) 14. Nearly perfect binary codes and uniformly packed codes 15. Association schemes References Index.",c98,North American Chapter of the Association for Computational Linguistics,cp98,accepted,f1469,2009,2009-02-21
s1830,p1830,Applications of graph theory,Abstract content goes here ...,c76,International Conference on Artificial Neural Networks,cp76,accepted,f1470,2013,2013-04-25
s1831,p1831,Proof Techniques in Graph Theory,Abstract content goes here ...,c14,International Conference on Exploring Services Science,cp14,accepted,f1471,2016,2016-12-04
s1832,p1832,Graph theory and molecular orbitals,Abstract content goes here ...,c9,Pacific Symposium on Biocomputing,cp9,accepted,f1472,2009,2009-07-15
s1833,p1833,Graph Theory and Topology in Chemistry,Abstract content goes here ...,c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f1473,2006,2006-07-22
s1834,p1834,Progress in Graph Theory,Abstract content goes here ...,c15,International Conference on Conceptual Structures,cp15,accepted,f1474,2011,2011-10-31
s1835,p1835,Algebraic methods in graph theory,Abstract content goes here ...,c97,Interspeech,cp97,accepted,f1475,2004,2004-08-13
s1836,p1836,Graph theory in modern engineering,Abstract content goes here ...,c74,IEEE International Conference on Tools with Artificial Intelligence,cp74,accepted,f1476,2003,2003-02-13
s1837,p1837,Topics in graph theory,Abstract content goes here ...,c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1477,2007,2007-05-29
s1838,p1838,Foundations of Chemical Reaction Network Theory,Abstract content goes here ...,j323,Applied Mathematical Sciences,jv323,accepted,f1478,2013,2013-03-05
s1839,p1839,LANDSCAPE CONNECTIVITY: A GRAPH‐THEORETIC PERSPECTIVE,"Ecologists are familiar with two data structures commonly used to represent landscapes. Vector-based maps delineate land cover types as polygons, while raster lattices represent the landscape as a grid. Here we adopt a third lattice data structure, the graph. A graph represents a landscape as a set of nodes (e.g., habitat patches) connected to some degree by edges that join pairs of nodes functionally (e.g., via dispersal). Graph theory is well developed in other fields, including geography (transportation networks, routing ap- plications, siting problems) and computer science (circuitry and network optimization). We present an overview of basic elements of graph theory as it might be applied to issues of connectivity in heterogeneous landscapes, focusing especially on applications of metapo- pulation theory in conservation biology. We develop a general set of analyses using a hypothetical landscape mosaic of habitat patches in a nonhabitat matrix. Our results suggest that a simple graph construct, the minimum spanning tree, can serve as a powerful guide to decisions about the relative importance of individual patches to overall landscape con- nectivity. We then apply this approach to an actual conservation scenario involving the",c105,Biometrics and Identity Management,cp105,accepted,f1479,2006,2006-04-14
s1840,p1840,Graph theory,3. Colorings of graphs 7 3.,c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f1480,2004,2004-10-22
s1841,p1841,An Introduction to the Theory of Graph Spectra: References,Preface 1. Introduction 2. Graph operations and modifications 3. Spectrum and structure 4. Characterizations by spectra 5. Structure and one eigenvalue 6. Spectral techniques 7. Laplacians 8. Additional topics 9. Applications Appendix Bibliography Index of symbols Index.,c105,Biometrics and Identity Management,cp105,accepted,f1481,2006,2006-03-10
s1842,p1842,Graph Structure and Monadic Second-Order Logic - A Language-Theoretic Approach,"The study of graph structure has advanced in recent years with great strides: finite graphs can be described algebraically, enabling them to be constructed out of more basic elements. Separately the properties of graphs can be studied in a logical language called monadic second-order logic. In this book, these two features of graph structure are brought together for the first time in a presentation that unifies and synthesizes research over the last 25 years. The author not only provides a thorough description of the theory, but also details its applications, on the one hand to the construction of graph algorithms, and, on the other to the extension of formal language theory to finite graphs. Consequently the book will be of interest to graduate students and researchers in graph theory, finite model theory, formal language theory, and complexity theory.",c24,Decision Support Systems,cp24,accepted,f1482,2013,2013-08-08
s1844,p1844,Supereulerian graphs and the Petersen graph,"A graph G is supereulerian if G has a spanning eulerian subgraph. Boesch et al. [J. Graph Theory, 1, 79–84 (1977)] proposed the problem of characterizing supereulerian graphs. In this paper, we prove that any 3-edge-connected graph with at most 11 edge-cuts of size 3 is supereulerian if and only if it cannot be contractible to the Petersen graph. This extends a former result of Catlin and Lai [J. Combin. Theory, Ser. B, 66, 123–139 (1996)].",j318,Journal of combinatorial theory. Series B (Print),jv318,accepted,f1483,2017,2017-11-17
s1845,p1845,Guidelines for a graph-theoretic implementation of structural equation modeling,"Structural equation modeling (SEM) is increasingly being chosen by researchers as a framework for gaining scientific insights from the quantitative analyses of data. New ideas and methods emerging from the study of causality, influences from the field of graphical modeling, and advances in statistics are expanding the rigor, capability, and even purpose of SEM. Guidelines for implementing the expanded capabilities of SEM are currently lacking. In this paper we describe new developments in SEM that we believe constitute a third-generation of the methodology. Most characteristic of this new approach is the generalization of the structural equation model as a causal graph. In this generalization, analyses are based on graph theoretic principles rather than analyses of matrices. Also, new devices such as metamodels and causal diagrams, as well as an increased emphasis on queries and probabilistic reasoning, are now included. Estimation under a graph theory framework permits the use of Bayesian or likelihood methods. The guidelines presented start from a declaration of the goals of the analysis. We then discuss how theory frames the modeling process, requirements for causal interpretation, model specification choices, selection of estimation method, model evaluation options, and use of queries, both to summarize retrospective results and for prospective analyses. 
 
The illustrative example presented involves monitoring data from wetlands on Mount Desert Island, home of Acadia National Park. Our presentation walks through the decision process involved in developing and evaluating models, as well as drawing inferences from the resulting prediction equations. In addition to evaluating hypotheses about the connections between human activities and biotic responses, we illustrate how the structural equation (SE) model can be queried to understand how interventions might take advantage of an environmental threshold to limit Typha invasions. 
 
The guidelines presented provide for an updated definition of the SEM process that subsumes the historical matrix approach under a graph-theory implementation. The implementation is also designed to permit complex specifications and to be compatible with various estimation methods. Finally, they are meant to foster the use of probabilistic reasoning in both retrospective and prospective considerations of the quantitative implications of the results.",j325,Ecosphere,jv325,accepted,f1484,2021,2021-10-16
s1846,p1846,Graph theoretical analysis of magnetoencephalographic functional connectivity in Alzheimer's disease.,"In this study we examined changes in the large-scale structure of resting-state brain networks in patients with Alzheimer's disease compared with non-demented controls, using concepts from graph theory. Magneto-encephalograms (MEG) were recorded in 18 Alzheimer's disease patients and 18 non-demented control subjects in a no-task, eyes-closed condition. For the main frequency bands, synchronization between all pairs of MEG channels was assessed using a phase lag index (PLI, a synchronization measure insensitive to volume conduction). PLI-weighted connectivity networks were calculated, and characterized by a mean clustering coefficient and path length. Alzheimer's disease patients showed a decrease of mean PLI in the lower alpha and beta band. In the lower alpha band, the clustering coefficient and path length were both decreased in Alzheimer's disease patients. Network changes in the lower alpha band were better explained by a 'Targeted Attack' model than by a 'Random Failure' model. Thus, Alzheimer's disease patients display a loss of resting-state functional connectivity in lower alpha and beta bands even when a measure insensitive to volume conduction effects is used. Moreover, the large-scale structure of lower alpha band functional networks in Alzheimer's disease is more random. The modelling results suggest that highly connected neural network 'hubs' may be especially at risk in Alzheimer's disease.",c83,International Conference on Computer Graphics and Interactive Techniques,cp83,accepted,f1485,2015,2015-05-27
s1847,p1847,Theory and Applications,"Infinitary rewriting generalises usual finitary rewriting by providing infinite reduction sequences with a notion of convergence. The idea of – at least conceptually – assigning a meaning to infinite derivations is well-known, for example, from lazy functional programming or from process calculi. Infinitary rewriting makes it possible to apply rewriting in order to obtain a formal model for such infinite derivations. The goal of this thesis is to comprehensively survey the field of infinitary term rewriting, to point out its shortcomings, and to try to overcome some of these shortcomings. The most significant problem that arises in infinitary rewriting is the inherent difficulty to finitely represent and, hence, to implement it. To this end, we consider term graph rewriting, which is able to finitely represent restricted forms of infinitary term rewriting. Moreover, we study different models that are used to formalise infinite reduction sequences: The well-established metric approach as well as an alternative approach using partial orders. Both methods together with the consequent infinitary versions of confluence and termination properties are analysed on an abstract level. Based on this, we argue that the partial order model has more advantageous properties and represents the intuition of convergence in a more natural way. This assessment is also backed up by the results that we obtain for infinitary term rewriting: Unlike the metric approach, the partial order approach admits to generalise some results known from finitary orthogonal term rewriting – most importantly, confluence. It is also shown that so-called Böhm trees, usually constructed rather intricately, naturally arise as normal forms in the partial order model. Finally, we devise a complete ultrametric and a complete semilattice on term graphs both of which are used to introduce infinitary term graph rewriting. This is supposed to serve as a tool in order to investigate the limitations of term graph rewriting for implementing infinitary term rewriting.",c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f1486,2020,2020-04-27
s1850,p1850,Graph Spectra for Complex Networks,"Analyzing the behavior of complex networks is an important element in the design of new man-made structures such as communication systems and biologically engineered molecules. Because any complex network can be represented by a graph, and therefore in turn by a matrix, graph theory has become a powerful tool in the investigation of network performance. This self-contained book provides a concise introduction to the theory of graph spectra and its applications to the study of complex networks. Covering a range of types of graphs and topics important to the analysis of complex systems, this guide provides the mathematical foundation needed to understand and apply spectral insight to real-world systems. In particular, the general properties of both the adjacency and Laplacian spectrum of graphs are derived and applied to complex networks. An ideal resource for researchers and students in communications networking as well as in physics and mathematics.",c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,cp99,accepted,f1487,2015,2015-04-23
s1851,p1851,Riemann–Roch and Abel–Jacobi theory on a finite graph,Abstract content goes here ...,c76,International Conference on Artificial Neural Networks,cp76,accepted,f1488,2013,2013-08-15
s1852,p1852,Connectedness Index of uncertain Graph,"In practical applications of graph theory, non-deterministic factors are frequently encountered. This paper employs uncertainty theory to deal with non-deterministic factors in problems of graph connectivity. The concepts of uncertain graph and connectedness index of uncertain graph are proposed in this paper. It presents two algorithms to calculate connectedness index of an uncertain graph.",c64,Experimental Software Engineering Network,cp64,accepted,f1489,2014,2014-12-27
s1853,p1853,Topological Graph Polynomials in Colored Group Field Theory,Abstract content goes here ...,c64,Experimental Software Engineering Network,cp64,accepted,f1490,2014,2014-09-01
s1855,p1855,Spectra of Graphs: Theory and Applications,Introduction. Basic Concepts of the Spectrum of a Graph. Operations on Graphs and the Resulting Spectra. Relations Between Spectral and Structural Properties of Graphs. The Divisor of a Graph. The Spectrum and the Group of Automorphisms. Characterization of Graphs by Means of Spectra. Spectra Techniques in Graph Theory and Combinatories. Applications in Chemistry an Physics. Some Additional Results. Appendix. Tables of Graph Spectra Biblgraphy. Index of Symbols. Index of Names. Subject Index.,c2,International Symposium on Intelligent Data Analysis,cp2,accepted,f1491,2014,2014-04-24
s1856,p1856,Theory of didactical situations in mathematics,"ion is not inevitable. In fact, shopkeepers have never abstracted the structures of modules which regulate their financial exchanges because they haven’t the motivation to do so. Schematization and formulation The schematization and the formulation of the structure follows the process of identification and of updating. Diénès does not plan any general situations specific to this stage (would something which is well conceived of spell itself out clearly?) but the representation by a graph is often envisaged as a simplified but natural and direct expression of the thought of a child. Representing objects by points and operators by arrows is learned by the use of imitation, like a language. Symbolization Symbolization is the transcription in a new language of properties represented in the preceding stage.",c22,International Conference on Data Technologies and Applications,cp22,accepted,f1492,2020,2020-06-02
s1857,p1857,New Integrable 4D Quantum Field Theories from Strongly Deformed Planar N=4 Supersymmetric Yang-Mills Theory.,"We introduce a family of new integrable quantum field theories in four dimensions by considering the γ-deformed N=4 supersymmetric Yang-Mills (SYM) theory in the double scaling limit of large imaginary twists and small coupling. This limit discards the gauge fields and retains only certain Yukawa and scalar interactions with three arbitrary effective couplings. In the 't Hooft limit, these 4D theories are integrable, and contain a wealth of conformal correlators such that the whole arsenal of AdS/CFT integrability remains applicable. As a special case of these models, we obtain a quantum field theory of two complex scalars with a chiral, quartic interaction. The Berenstein-Maldacena-Nastase vacuum anomalous dimension is dominated in each loop order by a single ""wheel"" graph, whose bulk represents an integrable ""fishnet"" graph. This explicitly demonstrates the all-loop integrability of gamma-deformed planar N=4 SYM theory, at least in our limit. Using this feature and integrability results we provide an explicit conjecture for the periods of double-wheel graphs with an arbitrary number of spokes in terms of multiple zeta values of limited depth.",j91,Physical Review Letters,jv91,accepted,f1493,2006,2006-07-15
s1859,p1859,Graph Algorithms in the Language of Linear Algebra,"The thesis presents usefulness of duality between graph and his adjacency matrix. The teoretical part provides the basis of graph theory and matrix algebra mainly focusing on sparse matrices and options of their presentation witch takes into account the number of nonzero elements in the matrix. The thesis includes presentation of possible operations on sparse matrices and algorithms that basically work on graphs, but with help of duality between graph and his adjacency matrix can be presented with sequence of operations on matrices. 
Practical part presents implementation of some algorithms that can work both with graphs or their adjacency matrices in programming language Java and testing algorithms that work with matrices. 
It focuses on comparison in efficiency of algorithm working with matrix written in standard mode and with matrix written in format for sparse matrices. It also studies witch presentation of matrices works beter for witch algorithm.",c105,Biometrics and Identity Management,cp105,accepted,f1494,2006,2006-10-09
s1860,p1860,Open problems in the spectral theory of signed graphs,"Signed graphs are graphs whose edges get a sign $+1$ or $-1$ (the signature). Signed graphs can be studied by means of graph matrices extended to signed graphs in a natural way. Recently, the spectra of signed graphs have attracted much attention from graph spectra specialists. One motivation is that the spectral theory of signed graphs elegantly generalizes the spectral theories of unsigned graphs. On the other hand, unsigned graphs do not disappear completely, since their role can be taken by the special case of balanced signed graphs. 
Therefore, spectral problems defined and studied for unsigned graphs can be considered in terms of signed graphs, and sometimes such generalization shows nice properties which cannot be appreciated in terms of (unsigned) graphs. Here, we survey some general results on the adjacency spectra of signed graphs, and we consider some spectral problems which are inspired from the spectral theory of (unsigned) graphs.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f1495,2011,2011-12-21
s1861,p1861,TOPICS IN GEOMETRIC GROUP THEORY,"We present a brief overview of methods and results in geometric group theory, with the goal of introducing the reader to both topological and metric perspectives. Prerequisites are kept to a minimum: we require only basic algebra, graph theory, and metric space topology.",c23,International Conference on Open and Big Data,cp23,accepted,f1496,2012,2012-04-22
s1862,p1862,Introduction to Coding Theory,"Definition 1 (distance amplified code G(C)) Let G = (L,R,E) be a bipartite graph with L = [n], R = [m], which is D-left-regular and d-right-regular. Let C be a binary linear code of block length n = |L|. For c ∈ {0, 1}n, define G(c) ∈ ({0, 1}d)m by G(c)j = (cΓ1(j), cΓ2(j), · · · , cΓd(j)), for j ∈ [m], where Γi(j) ∈ L denotes the i-th neighbor of j ∈ R. Now define the code G(C) as G(C) = {G(c)|c ∈ C}. Since each bit of a codeword c ∈ C is repeated D times in the associated codeword G(c) ∈ G(C), we have",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f1497,2002,2002-08-06
s1863,p1863,Image Processing and Analysis With Graphs: theory and Practice,"Graph Theory Concepts and Definitions Used in Image Processing and Analysis, O. Lezoray and L. Grady Introduction Basic Graph Theory Graph Representation Paths, Trees, and Connectivity Graph Models in Image Processing and Analysis Graph Cuts-Combinatorial Optimization in Vision, H. Ishikawa Introduction Markov Random Field Basic Graph Cuts: Binary Labels Multi-Label Minimization Examples Higher-Order Models in Computer Vision, P. Kohli and C. Rother Introduction Higher-Order Random Fields Patch and Region-Based Potentials Relating Appearance Models and Region-Based Potentials Global Potentials Maximum a Posteriori Inference A Parametric Maximum Flow Approach for Discrete Total Variation Regularization, A. Chambolle and J. Darbon Introduction Idea of the approach Numerical Computations Applications Targeted Image Segmentation Using Graph Methods, L. Grady The Regularization of Targeted Image Segmentation Target Specification Conclusion A Short Tour of Mathematical Morphology on Edge and Vertex Weighted Graphs, L. Najman and F. Meyer Introduction Graphs and lattices Neighborhood Operations on Graphs Filters Connected Operators and Filtering with the Component Tree Watershed Cuts MSF Cut Hierarchy and Saliency Maps Optimization and the Power Watershed Partial Difference Equations on Graphs for Local and Nonlocal Image Processing, A. Elmoataz, O. Lezoray, V.-T. Ta, and S. Bougleux Introduction Difference Operators on Weighted Graphs Construction of Weighted Graphs p-Laplacian Regularization on Graphs Examples Image Denoising with Nonlocal Spectral Graph Wavelets, D.K. Hammond, L. Jacques, and P. Vandergheynst Introduction Spectral Graph Wavelet Transform Nonlocal Image Graph Hybrid Local/Nonlocal Image Graph Scaled Laplacian Model Applications to Image Denoising Conclusions Acknowledgments Image and Video Matting, J. Wang Introduction Graph Construction for Image Matting Solving Image Matting Graphs Data Set Video Matting Optimal Simultaneous Multisurface and Multiobject Image Segmentation, X. Wu, M.K. Garvin, and M. Sonka Introduction Motivation and Problem Description Methods for Graph-Based Image Segmentation Case Studies Conclusion Acknowledgments Hierarchical Graph Encodings, L. Brun and W. Kropatsch Introduction Regular Pyramids Irregular Pyramids Parallel construction schemes Irregular Pyramids and Image properties Graph-Based Dimensionality Reduction, J.A. Lee and M. Verleysen Summary Introduction Classical methods Nonlinearity through Graphs Graph-Based Distances Graph-Based Similarities Graph embedding Examples and comparisons Graph Edit Distance-Theory, Algorithms, and Applications, M. Ferrer and H. Bunke Introduction Definitions and Graph Matching Theoretical Aspects of GED GED Computation Applications of GED The Role of Graphs in Matching Shapes and in Categorization, B. Kimia Introduction Using Shock Graphs for Shape Matching Using Proximity Graphs for Categorization Conclusion Acknowledgment 3D Shape Registration Using Spectral Graph Embedding and Probabilistic Matching, A. Sharma, R. Horaud, and D. Mateus Introduction Graph Matrices Spectral Graph Isomorphism Graph Embedding and Dimensionality Reduction Spectral Shape Matching Experiments and Results Discussion Appendix: Permutation and Doubly- stochastic Matrices Appendix: The Frobenius Norm Appendix: Spectral Properties of the Normalized Laplacian Modeling Images with Undirected Graphical Models, M.F. Tappen Introduction Background Graphical Models for Modeling Image Patches Pixel-Based Graphical Models Inference in Graphical Models Learning in Undirected Graphical Models Tree-Walk Kernels for Computer Vision, Z. Harchaoui and F. Bach Introduction Tree-Walk Kernels as Graph Kernels The Region Adjacency Graph Kernel as a Tree-Walk Kernel The Point Cloud Kernel as a Tree-Walk Kernel Experimental Results Conclusion Acknowledgments",c42,Joint Conference of International Workshop on Software Measurement and International Conference on Software Process and Product Measurement,cp42,accepted,f1498,2019,2019-12-01
s1864,p1864,Nonstable K-theory for Graph Algebras,Abstract content goes here ...,c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f1499,2015,2015-01-06
s1865,p1865,Some Recent Progress and Applications in Graph Minor Theory,Abstract content goes here ...,c74,IEEE International Conference on Tools with Artificial Intelligence,cp74,accepted,f1500,2003,2003-09-03
s1866,p1866,A theory of graph comprehension.,Abstract content goes here ...,c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f1501,2005,2005-11-11
s1867,p1867,Introduction to the Algebraic Theory of Graph Grammars (A Survey),Abstract content goes here ...,c37,Asia-Pacific Software Engineering Conference,cp37,accepted,f1502,2008,2008-07-27
s1868,p1868,Coalitional game theory for communication networks,"In this tutorial, we provided a comprehensive overview of coalitional game theory, and its usage in wireless and communication networks. For this purpose, we introduced a novel classification of coalitional games by grouping the sparse literature into three distinct classes of games: canonical coalitional games, coalition formation games, and coalitional graph games. For each class, we explained in details the fundamental properties, discussed the main solution concepts, and provided an in-depth analysis of the methodologies and approaches for using these games in both game theory and communication applications. The presented applications have been carefully selected from a broad range of areas spanning a diverse number of research problems. The tutorial also sheds light on future opportunities for using the strong analytical tool of coalitional games in a number of applications. In a nutshell, this article fills a void in existing communications literature, by providing a novel tutorial on applying coalitional game theory in communication networks through comprehensive theory and technical details as well as through practical examples drawn from both game theory and communication application.",j327,IEEE Signal Processing Magazine,jv327,accepted,f1503,2011,2011-12-05
s1869,p1869,"Algorithmic graph minor theory: Decomposition, approximation, and coloring","At the core of the seminal graph minor theory of Robertson and Seymour is a powerful structural theorem capturing the structure of graphs excluding a fixed minor. This result is used throughout graph theory and graph algorithms, but is existential. We develop a polynomial-time algorithm using topological graph theory to decompose a graph into the structure guaranteed by the theorem: a clique-sum of pieces almost-embeddable into bounded-genus surfaces. This result has many applications. In particular we show applications to developing many approximation algorithms, including a 2-approximation to graph coloring, constant-factor approximations to treewidth and the largest grid minor, combinatorial polylogarithmic approximation to half-integral multicommodity flow, subexponential fixed-parameter algorithms, and PTASs for many minimization and maximization problems, on graphs excluding a fixed minor.",c81,IEEE Annual Symposium on Foundations of Computer Science,cp81,accepted,f1504,2004,2004-07-12
s1870,p1870,Graph-theoretic connectivity control of mobile robot networks,"In this paper, we provide a theoretical framework for controlling graph connectivity in mobile robot networks. We discuss proximity-based communication models composed of disk-based or uniformly-fading-signal-strength communication links. A graph-theoretic definition of connectivity is provided, as well as an equivalent definition based on algebraic graph theory, which employs the adjacency and Laplacian matrices of the graph and their spectral properties. Based on these results, we discuss centralized and distributed algorithms to maintain, increase, and control connectivity in mobile robot networks. The various approaches discussed in this paper range from convex optimization and subgradient-descent algorithms, for the maximization of the algebraic connectivity of the network, to potential fields and hybrid systems that maintain communication links or control the network topology in a least restrictive manner. Common to these approaches is the use of mobility to control the topology of the underlying communication network. We discuss applications of connectivity control to multirobot rendezvous, flocking and formation control, where so far, network connectivity has been considered an assumption.",j168,Proceedings of the IEEE,jv168,accepted,f1505,2019,2019-05-15
s1871,p1871,Graph Minor Theory,"A monumental project in graph theory was recently completed. The project, started by Robertson and Seymour, and later joined by Thomas, led to entirely new concepts and a new way of looking at graph theory. The motivating problem was Kuratowski’s characterization of planar graphs, and a far-reaching generalization of this, conjectured by Wagner: If a class of graphs is minor-closed (i.e., it is closed under deleting and contracting edges), then it can be characterized by a finite number of excluded minors. The proof of this conjecture is based on a very general theorem about the structure of large graphs: If a minor-closed class of graphs does not contain all graphs, then every graph in it is glued together in a tree-like fashion from graphs that can almost be embedded in a fixed surface. We describe the precise formulation of the main results and survey some of its applications to algorithmic and structural problems in graph theory.",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f1506,2018,2018-05-19
s1872,p1872,Fundamental Theory for Typed Attributed Graph Transformation,Abstract content goes here ...,c85,International Conference on Graph Transformation,cp85,accepted,f1507,2007,2007-02-20
s1873,p1873,Estimating and understanding exponential random graph models,"We introduce a method for the theoretical analysis of exponential random graph models. The method is based on a large-deviations approximation to the normalizing constant shown to be consistent using theory developed by Chatterjee and Varadhan [European J. Combin. 32 (2011) 1000-1017]. The theory explains a host of difficulties encountered by applied workers: many distinct models have essentially the same MLE, rendering the problems ``practically'' ill-posed. We give the first rigorous proofs of ``degeneracy'' observed in these models. Here, almost all graphs have essentially no edges or are essentially complete. We supplement recent work of Bhamidi, Bresler and Sly [2008 IEEE 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS) (2008) 803-812 IEEE] showing that for many models, the extra sufficient statistics are useless: most realizations look like the results of a simple Erd\H{o}s-R\'{e}nyi model. We also find classes of models where the limiting graphs differ from Erd\H{o}s-R\'{e}nyi graphs. A limitation of our approach, inherited from the limitation of graph limit theory, is that it works only for dense graphs.",c32,International Conference on Software Technology: Methods and Tools,cp32,accepted,f1508,2010,2010-04-15
s1875,p1875,"Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods",Abstract The subject of graphical methods for data analysis and for data presentation needs a scientific foundation. In this article we take a few steps in the direction of establishing such a foundation. Our approach is based on graphical perception—the visual decoding of information encoded on graphs—and it includes both theory and experimentation to test the theory. The theory deals with a small but important piece of the whole process of graphical perception. The first part is an identification of a set of elementary perceptual tasks that are carried out when people extract quantitative information from graphs. The second part is an ordering of the tasks on the basis of how accurately people perform them. Elements of the theory are tested by experimentation in which subjects record their judgments of the quantitative information on graphs. The experiments validate these elements but also suggest that the set of elementary tasks should be expanded. The theory provides a guideline for graph construction...,c58,Australian Software Engineering Conference,cp58,accepted,f1509,2021,2021-05-16
s1876,p1876,Graph-theoretic methods in database theory,"As in many areas of computer science and other disciplines, graph theoretic tools play an important role also in databases. Many concepts are best captured in terms of graphs or hypergraphs, and problems can then be formulated and solved using graph theoretic algorithms. There is a great number of such examples from schema design, dependency theory, transaction processing, query optimization, data distribution, and a host of other areas. We will not attempt to touch on the wide range of all these applications. Rather, we will concentrate on a particular, basic type of problems that has attracted a great deal of attention in the database literature over the last few years and has come to play a central role: techniques for searching graphs and computing transitive closure, and some of the applications and related problems in query processing. There is an extensive literature on these types of problems, which we cannot reasonably hope to cover in this space, but we shall give a flavour of the issues that arise in solving these problems in various frameworks.",c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,cp86,accepted,f1510,2012,2012-08-02
s1877,p1877,An algebraic theory of graph reduction,"We show how membership in classes of graphs definable in monadic second order logic and of bounded treewidth can be decided by finite sets of terminating reduction rules. The method is constructive in the sense that we describe an algorithm which will produce, from a formula in monadic second order logic and an integer k such that the class defined by the formula is of treewidth ≤ k, a set of rewrite rules that reduces any member of the class to one of finitely many graphs, in a number of steps bounded by the size of the graph. This reduction system corresponds to an algorithm that runs in time linear in the size of the graph.",c105,Biometrics and Identity Management,cp105,accepted,f1511,2006,2006-07-21
s1879,p1879,The Structure of Complex Networks: Theory and Applications,"This book deals with the analysis of the structure of complex networks by combining results from graph theory, physics, and pattern recognition. The book is divided into two parts. 11 chapters are dedicated to the development of theoretical tools for the structural analysis of networks, and 7 chapters are illustrating, in a critical way, applications of these tools to real-world scenarios. The first chapters provide detailed coverage of adjacency and metric and topological properties of networks, followed by chapters devoted to the analysis of individual fragments and fragment-based global invariants in complex networks. Chapters that analyse the concepts of communicability, centrality, bipartivity, expansibility and communities in networks follow. The second part of this book is devoted to the analysis of genetic, protein residue, protein-protein interaction, intercellular, ecological and socio-economic networks, including important breakthroughs as well as examples of the misuse of structural concepts.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1512,2007,2007-10-22
s1880,p1880,Graph Degree Linkage: Agglomerative Clustering on a Directed Graph,Abstract content goes here ...,c87,European Conference on Computer Vision,cp87,accepted,f1513,2014,2014-02-05
s1881,p1881,The Gewirtz Graph: An Exercise in the Theory of Graph Spectra,"We prove that there is a unique graph (on 56 vertices) with spectrum 101235(-4)20 and examine its structure. It turns out that, e.g., the Coxeter graph (on 28 vertices) and the Sylvester graph (on 36 vertices) are induced subgraphs. We give descriptions of this graph.",c12,International Conference on Statistical and Scientific Database Management,cp12,accepted,f1514,2002,2002-10-12
s1882,p1882,Graph- Theoretical Approaches to the Theory of Voting*,"In this article, language, concepts, and theorems from the theory of directed graphs are used to characterize and analyze the structure of majority preference. A number of results are then derived concerning ""sincere,"" ""sophisticated,"" and ""cooperative"" voting decisions under two common majority voting procedures. These results supplement the work of Black and Farquharson. Perhaps contrary to ""common-sense"" thinking, general strategic manipulation of voting processes has beneficial consequences. It is widely recognized-and not only by political scientists-that the decisions of a voting body may be affected not only by such obviously relevant matters as the preferences of its members and their participation in or absence from particular votes, but also by such ""technical"" matters as the nature of the voting procedure and the order in which proposals are voted on. It is also recognized that voting may have ""gamelike"" characteristics offering strategic opportunities both to voters as individuals and to voters in coalitions. Finally, most political scientists-though probably few politicians or citizens-are by now aware of the ""paradox of voting"" and may have some sense of its connection with these questions of decision, procedure, and strategy. Over the past decade or so a somewhat technical literature on the theory of voting has developed in the ""public choice"" area. The present article adds to this literature by presenting a number of new propositions concerning majority voting under two common voting procedures. These propositions pertain to the questions alluded to in the first paragraph. These new results, together with some more familiar ones, are obtained by employing language, concepts, and theorems from the mathematical theory of directed graphs. In these respects, the article will be of interest primarily to specialists in the area *This article is in part a combination and revision of two unpublished papers:",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f1515,2005,2005-01-09
s1883,p1883,Graph models of habitat mosaics.,"Graph theory is a body of mathematics dealing with problems of connectivity, flow, and routing in networks ranging from social groups to computer networks. Recently, network applications have erupted in many fields, and graph models are now being applied in landscape ecology and conservation biology, particularly for applications couched in metapopulation theory. In these applications, graph nodes represent habitat patches or local populations and links indicate functional connections among populations (i.e. via dispersal). Graphs are models of more complicated real systems, and so it is appropriate to review these applications from the perspective of modelling in general. Here we review recent applications of network theory to habitat patches in landscape mosaics. We consider (1) the conceptual model underlying these applications; (2) formalization and implementation of the graph model; (3) model parameterization; (4) model testing, insights, and predictions available through graph analyses; and (5) potential implications for conservation biology and related applications. In general, and for a variety of ecological systems, we find the graph model a remarkably robust framework for applications concerned with habitat connectivity. We close with suggestions for further work on the parameterization and validation of graph models, and point to some promising analytic insights.",j328,Ecology Letters,jv328,accepted,f1516,2005,2005-02-22
s1885,p1885,"MaxQuant enables high peptide identification rates, individualized p.p.b.-range mass accuracies and proteome-wide protein quantification",Abstract content goes here ...,j0,Nature Biotechnology,jv0,accepted,f1517,2006,2006-04-21
s1886,p1886,Graph Ramsey theory and the polynomial hierarchy,"Summary form only given, as follows. In the Ramsey theory of graphs F/spl rarr/(G, H) means that for every way of coloring the edges of F red and blue F will contain either a red G or a blue H as a subgraph. The problem ARROWING of deciding whether F/spl rarr/(G, H) lies in /spl Pi//sub 2//sup P/=coNP/sup NP/ and it was shown to be coNP-hard by S.A. Burr (1990). We prove that ARROWING is actually /spl Pi//sub 2//sup P/-complete, simultaneously settling a conjecture of Burr and providing a natural example of a problem complete for a higher level of the polynomial hierarchy. We also consider several specific variants of ARROWING, where G and H are restricted to particular families of graphs. We have a general completeness result for this case under the assumption that certain graphs are constructible in polynomial time. Furthermore we show that STRONG ARROWING, the version of ARROWING for induced subgraphs, is /spl Pi//sub 2//sup P/-complete.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1518,2007,2007-03-25
s1887,p1887,A Spectral Graph Uncertainty Principle,"The spectral theory of graphs provides a bridge between classical signal processing and the nascent field of graph signal processing. In this paper, a spectral graph analogy to Heisenberg's celebrated uncertainty principle is developed. Just as the classical result provides a tradeoff between signal localization in time and frequency, this result provides a fundamental tradeoff between a signal's localization on a graph and in its spectral domain. Using the eigenvectors of the graph Laplacian as a surrogate Fourier basis, quantitative definitions of graph and spectral “spreads” are given, and a complete characterization of the feasibility region of these two quantities is developed. In particular, the lower boundary of the region, referred to as the uncertainty curve, is shown to be achieved by eigenvectors associated with the smallest eigenvalues of an affine family of matrices. The convexity of the uncertainty curve allows it to be found to within ε by a fast approximation algorithm requiring O(ε-1/2) typically sparse eigenvalue evaluations. Closed-form expressions for the uncertainty curves for some special classes of graphs are derived, and an accurate analytical approximation for the expected uncertainty curve of Erd-s-Rényi random graphs is developed. These theoretical results are validated by numerical experiments, which also reveal an intriguing connection between diffusion processes on graphs and the uncertainty bounds.",j291,IEEE Transactions on Information Theory,jv291,accepted,f1519,2010,2010-05-16
s1888,p1888,Multibond graph elements in physical systems theory,Abstract content goes here ...,c50,International Conference on Automated Software Engineering,cp50,accepted,f1520,2008,2008-08-15
s1889,p1889,Statistical mechanics of complex networks,"The emergence of order in natural systems is a constant source of inspiration for both physical and biological sciences. While the spatial order characterizing for example the crystals has been the basis of many advances in contemporary physics, most complex systems in nature do not offer such high degree of order. Many of these systems form complex networks whose nodes are the elements of the system and edges represent the interactions between them. 
Traditionally complex networks have been described by the random graph theory founded in 1959 by Paul Erdohs and Alfred Renyi. One of the defining features of random graphs is that they are statistically homogeneous, and their degree distribution (characterizing the spread in the number of edges starting from a node) is a Poisson distribution. In contrast, recent empirical studies, including the work of our group, indicate that the topology of real networks is much richer than that of random graphs. In particular, the degree distribution of real networks is a power-law, indicating a heterogeneous topology in which the majority of the nodes have a small degree, but there is a significant fraction of highly connected nodes that play an important role in the connectivity of the network. 
The scale-free topology of real networks has very important consequences on their functioning. For example, we have discovered that scale-free networks are extremely resilient to the random disruption of their nodes. On the other hand, the selective removal of the nodes with highest degree induces a rapid breakdown of the network to isolated subparts that cannot communicate with each other. 
The non-trivial scaling of the degree distribution of real networks is also an indication of their assembly and evolution. Indeed, our modeling studies have shown us that there are general principles governing the evolution of networks. Most networks start from a small seed and grow by the addition of new nodes which attach to the nodes already in the system. This process obeys preferential attachment: the new nodes are more likely to connect to nodes with already high degree. We have proposed a simple model based on these two principles wich was able to reproduce the power-law degree distribution of real networks. Perhaps even more importantly, this model paved the way to a new paradigm of network modeling, trying to capture the evolution of networks, not just their static topology.",c112,Very Large Data Bases Conference,cp112,accepted,f1521,2018,2018-06-20
s1890,p1890,Stochastic blockmodel approximation of a graphon: Theory and consistent estimation,"Non-parametric approaches for analyzing network data based on exchangeable graph models (ExGM) have recently gained interest. The key object that defines an ExGM is often referred to as a graphon. This non-parametric perspective on network modeling poses challenging questions on how to make inference on the graphon underlying observed network data. In this paper, we propose a computationally efficient procedure to estimate a graphon from a set of observed networks generated from it. This procedure is based on a stochastic blockmodel approximation (SBA) of the graphon. We show that, by approximating the graphon with a stochastic block model, the graphon can be consistently estimated, that is, the estimation error vanishes as the size of the graph approaches infinity.",c109,International Conference on Mobile Data Management,cp109,accepted,f1522,2014,2014-11-12
s1891,p1891,Consensus and Cooperation in Networked Multi-Agent Systems,"This paper provides a theoretical framework for analysis of consensus algorithms for multi-agent networked systems with an emphasis on the role of directed information flow, robustness to changes in network topology due to link/node failures, time-delays, and performance guarantees. An overview of basic concepts of information consensus in networks and methods of convergence and performance analysis for the algorithms are provided. Our analysis framework is based on tools from matrix theory, algebraic graph theory, and control theory. We discuss the connections between consensus problems in networked dynamic systems and diverse applications including synchronization of coupled oscillators, flocking, formation control, fast consensus in small-world networks, Markov processes and gossip-based algorithms, load balancing in networks, rendezvous in space, distributed sensor fusion in sensor networks, and belief propagation. We establish direct connections between spectral and structural properties of complex networks and the speed of information diffusion of consensus algorithms. A brief introduction is provided on networked systems with nonlocal information flow that are considerably faster than distributed systems with lattice-type nearest neighbor interactions. Simulation results are presented that demonstrate the role of small-world effects on the speed of consensus algorithms and cooperative control of multivehicle formations",j168,Proceedings of the IEEE,jv168,accepted,f1523,2019,2019-12-21
s1892,p1892,On the algebraic theory of graph colorings,Abstract content goes here ...,c33,International Conference on Agile Software Development,cp33,accepted,f1524,2022,2022-01-21
s1894,p1894,Parallel and Distributed Computation: Numerical Methods,"gineering, computer science, operations research, and applied mathematics. It is essentially a self-contained work, with the development of the material occurring in the main body of the text and excellent appendices on linear algebra and analysis, graph theory, duality theory, and probability theory and Markov chains supporting it. The introduction discusses parallel and distributed architectures, complexity measures, and communication and synchronization issues, and it presents both Jacobi and Gauss-Seidel iterations, which serve as algorithms of reference for many of the computational approaches addressed later. After the introduction, the text is organized in two parts: synchronous algorithms and asynchronous algorithms. The discussion of synchronous algorithms comprises four chapters, with Chapter 2 presenting both direct methods (converging to the exact solution within a finite number of steps) and iterative methods for linear",c40,IEEE International Conference on Software Maintenance and Evolution,cp40,accepted,f1525,2013,2013-02-22
s1895,p1895,Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions,"An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks.",c75,International Conference on Machine Learning,cp75,accepted,f1526,2005,2005-10-08
s1896,p1896,A homology theory for spanning tress of a graph,Abstract content goes here ...,c98,North American Chapter of the Association for Computational Linguistics,cp98,accepted,f1527,2009,2009-02-16
s1897,p1897,Spectral sparsification of graphs: theory and algorithms,"Graph sparsification is the approximation of an arbitrary graph by a sparse graph.
 We explain what it means for one graph to be a spectral approximation of another and review the development of algorithms for spectral sparsification. In addition to being an interesting concept, spectral sparsification has been an important tool in the design of nearly linear-time algorithms for solving systems of linear equations in symmetric, diagonally dominant matrices. The fast solution of these linear systems has already led to breakthrough results in combinatorial optimization, including a faster algorithm for finding approximate maximum flows and minimum cuts in an undirected network.",c91,Workshop on Algorithms and Models for the Web-Graph,cp91,accepted,f1528,2022,2022-01-22
s1898,p1898,Graph-based Natural Language Processing and Information Retrieval,"Graph theory and the fields of natural language processing and information retrieval are well-studied disciplines. Traditionally, these areas have been perceived as distinct, with different algorithms, different applications, and different potential end-users. However, recent research has shown that these disciplines are intimately connected, with a large variety of natural language processing and information retrieval applications finding efficient solutions within graph-theoretical frameworks. This book extensively covers the use of graph-based algorithms for natural language processing and information retrieval. It brings together topics as diverse as lexical semantics, text summarization, text mining, ontology construction, text classification, and information retrieval, which are connected by the common underlying theme of the use of graph-theoretical methods for text and information processing tasks. Readers will come away with a firm understanding of the major methods and applications in natural language processing and information retrieval that rely on graph-based representations and algorithms.",c69,International Conference on Parallel Processing,cp69,accepted,f1529,2010,2010-10-19
s1899,p1899,The theory of the monetary circuit,"The present paper reviews the pre-history, the process of formation and the possible directions of future development of the theory of monetary circuit. The author reveals the main theoretical constructions of Graziany and the other leading representatives of the circuitist school. The principles of derivation of transaction and balance sheet matrices reflecting the main ideas of the theory are discussed. The dynamic variants of the theory as well as the connection between the circuitist approach and the input-output model are subject to examination. The paper studies the possibility the circuitist approach to be further broadened on the basis of the mathematical graphs theory. The author emphasizes that the theory of monetary circuit denies the neoclassical dichotomy and rejects the postulate of the neutrality of money. The opportunity is also offered to upgrade the monetary circuit theory by using the mathematical graph theory. The paper includes also a critical evaluation of the presented theory.",c27,ACM-SIAM Symposium on Discrete Algorithms,cp27,accepted,f1530,2022,2022-08-01
s1900,p1900,Information flow and cooperative control of vehicle formations,"We consider the problem of cooperation among a collection of vehicles performing a shared task using intervehicle communication to coordinate their actions. Tools from algebraic graph theory prove useful in modeling the communication network and relating its topology to formation stability. We prove a Nyquist criterion that uses the eigenvalues of the graph Laplacian matrix to determine the effect of the communication topology on formation stability. We also propose a method for decentralized information exchange between vehicles. This approach realizes a dynamical system that supplies each vehicle with a common reference to be used for cooperative motion. We prove a separation principle that decomposes formation stability into two components: Stability of this is achieved information flow for the given graph and stability of an individual vehicle for the given controller. The information flow can thus be rendered highly robust to changes in the graph, enabling tight formation control despite limitations in intervehicle communication capability.",j329,IEEE Transactions on Automatic Control,jv329,accepted,f1531,2002,2002-01-13
s1901,p1901,Spectra of graphs : theory and application,Introduction. Basic Concepts of the Spectrum of a Graph. Operations on Graphs and the Resulting Spectra. Relations Between Spectral and Structural Properties of Graphs. The Divisor of a Graph. The Spectrum and the Group of Automorphisms. Characterization of Graphs by Means of Spectra. Spectra Techniques in Graph Theory and Combinatories. Applications in Chemistry an Physics. Some Additional Results. Appendix. Tables of Graph Spectra Biblgraphy. Index of Symbols. Index of Names. Subject Index.,c1,Technical Symposium on Computer Science Education,cp1,accepted,f1532,2002,2002-09-03
s1902,p1902,BrainNet Viewer: A Network Visualization Tool for Human Brain Connectomics,"The human brain is a complex system whose topological organization can be represented using connectomics. Recent studies have shown that human connectomes can be constructed using various neuroimaging technologies and further characterized using sophisticated analytic strategies, such as graph theory. These methods reveal the intriguing topological architectures of human brain networks in healthy populations and explore the changes throughout normal development and aging and under various pathological conditions. However, given the huge complexity of this methodology, toolboxes for graph-based network visualization are still lacking. Here, using MATLAB with a graphical user interface (GUI), we developed a graph-theoretical network visualization toolbox, called BrainNet Viewer, to illustrate human connectomes as ball-and-stick models. Within this toolbox, several combinations of defined files with connectome information can be loaded to display different combinations of brain surface, nodes and edges. In addition, display properties, such as the color and size of network elements or the layout of the figure, can be adjusted within a comprehensive but easy-to-use settings panel. Moreover, BrainNet Viewer draws the brain surface, nodes and edges in sequence and displays brain networks in multiple views, as required by the user. The figure can be manipulated with certain interaction functions to display more detailed information. Furthermore, the figures can be exported as commonly used image file formats or demonstration video for further use. BrainNet Viewer helps researchers to visualize brain networks in an easy, flexible and quick manner, and this software is freely available on the NITRC website (www.nitrc.org/projects/bnv/).",j108,PLoS ONE,jv108,accepted,f1533,2006,2006-09-25
s1903,p1903,Automated planning - theory and practice,1 Introduction and Overview I Classical Planning 2 Representations for Classical Planning*3 Complexity of Classical Planning*4 State-Space Planning*5 Plan-Space Planning II Neoclassical Planning 6 Planning-Graph Techniques*7 Propositional Satisfiability Techniques*8 Constraint Satisfaction Techniques III Heuristics and Control Strategies 9 Heuristics in Planning*10 Control Rules in Planning*11 Hierarchical Task Network Planning*12 Control Strategies in Deductive Planning IV Planning with Time and Resources 13 Time for Planning*14 Temporal Planning*15 Planning and Resource Scheduling V Planning under Uncertainty 16 Planning based on Markov Decision Processes*17 Planning based on Model Checking*18 Uncertainty with Neo-Classical Techniques VI Case Studies and Applications 19 Space Applications*20 Planning in Robotics*21 Planning for Manufacturability Analysis*22 Emergency Evacuation Planning *23 Planning in the Game of Bridge VII Conclusion 24 Conclusion and Other Topics VIII Appendices A Search Procedures and Computational Complexity*B First Order Logic*C Model Checking,c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f1534,2014,2014-10-02
s1905,p1905,Applications of Hyperstructure Theory,Abstract content goes here ...,c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1535,2015,2015-01-21
s1906,p1906,Computing topological parameters of biological networks,"UNLABELLED
Rapidly increasing amounts of molecular interaction data are being produced by various experimental techniques and computational prediction methods. In order to gain insight into the organization and structure of the resultant large complex networks formed by the interacting molecules, we have developed the versatile Cytoscape plugin NetworkAnalyzer. It computes and displays a comprehensive set of topological parameters, which includes the number of nodes, edges, and connected components, the network diameter, radius, density, centralization, heterogeneity, and clustering coefficient, the characteristic path length, and the distributions of node degrees, neighborhood connectivities, average clustering coefficients, and shortest path lengths. NetworkAnalyzer can be applied to both directed and undirected networks and also contains extra functionality to construct the intersection or union of two networks. It is an interactive and highly customizable application that requires no expert knowledge in graph theory from the user.


AVAILABILITY
NetworkAnalyzer can be downloaded via the Cytoscape web site: http://www.cytoscape.org",c75,International Conference on Machine Learning,cp75,accepted,f1536,2005,2005-04-12
s1907,p1907,On maximizing the second smallest eigenvalue of a state-dependent graph Laplacian,"We consider the set G consisting of graphs of fixed order and weighted edges. The vertex set of graphs in G will correspond to point masses and the weight for an edge between two vertices is a functional of the distance between them. We pose the problem of finding the best vertex positional configuration in the presence of an additional proximity constraint, in the sense that, the second smallest eigenvalue of the corresponding graph Laplacian is maximized. In many recent applications of algebraic graph theory in systems and control, the second smallest eigenvalue of Laplacian has emerged as a critical parameter that influences the stability and robustness properties of dynamic systems that operate over an information network. Our motivation in the present work is to ""assign"" this Laplacian eigenvalue when relative positions of various elements dictate the interconnection of the underlying weighted graph. In this venue, one would then be able to ""synthesize"" information graphs that have desirable system theoretic properties.",j329,IEEE Transactions on Automatic Control,jv329,accepted,f1537,2002,2002-04-26
s1908,p1908,Evolutionary Dynamics: Exploring the Equations of Life,Preface 1. Introduction 2. What Evolution Is 3. Fitness Landscapes and Sequence Spaces 4. Evolutionary Games 5. Prisoners of the Dilemma 6. Finite Populations 7. Games in Finite Populations 8. Evolutionary Graph Theory 9. Spatial Games 10. HIV Infection 11. The Evolution of Virulence 12. The Evolutionary Dynamics of Cancer 13. Language Evolution 14. Conclusion Further Reading References Index,c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1538,2015,2015-04-11
s1910,p1910,Large Networks and Graph Limits,"The book Large Networks and Graph Limits, xiv + 475 pp., published in late 2012, comprises five parts, the first an illuminating introduction and the last a tantalizing taste of how the scope of the theory developed in its pages might be extended to other combinatorial structures than graphs. The three central parts treat in depth the topics of graph algebras, limits for sequences of dense graphs (this constitutes the most substantial part, occupying nearly half the book) and limits for sequences of bounded degree graphs. Primarily the book is aimed at graduate students and research mathematicians interested in graph theory and its application to networks (for example, the internet and networks in social science, biology, statistical physics and engineering). There are 23 chapters and an appendix, the latter conveniently giving necessary background from areas of mathematics outside mainstream graph theory. A bibliography collects together the extensive research in this area up to 2012, and a subject, author and notation index facilitate navigation of the book. The author maintains a webpage for corrections and supplementary material. Indeed, via the author’s homepage the reader can freely access the many papers he has written with collaborators on the topic of graph homomorphisms and graph limits. The book synthesizes much of the material in these papers, with some revision in",c22,International Conference on Data Technologies and Applications,cp22,accepted,f1539,2020,2020-05-01
s1912,p1912,Spectra of Graphs,"This book gives an elementary treatment of the basic material about graph spectra, both for ordinary, and Laplace and Seidel spectra. The text progresses systematically, by covering standard topics before presenting some new material on trees, strongly regular graphs, two-graphs, association schemes, p-ranks of configurations and similar topics. Exercises at the end of each chapter provide practice and vary from easy yet interesting applications of the treated theory, to little excursions into related topics. Tables, references at the end of the book, an author and subject index enrich the text. Spectra of Graphs is written for researchers, teachers and graduate students interested in graph spectra. The reader is assumed to be familiar with basic linear algebra and eigenvalues, although some more advanced topics in linear algebra, like the Perron-Frobenius theorem and eigenvalue interlacing are included.",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f1540,2017,2017-07-19
s1913,p1913,A Formal Basis for the Heuristic Determination of Minimum Cost Paths,"Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies.",j331,IEEE Transactions on Systems Science and Cybernetics,jv331,accepted,f1541,2002,2002-12-28
s1914,p1914,Social Network Analysis,"This paper reports on the development of social network analysis, tracing its origins in classical sociology and its more recent formulation in social scientific and mathematical work. It is argued that the concept of social network provides a powerful model for social structure, and that a number of important formal methods of social network analysis can be discerned. Social network analysis has been used in studies of kinship structure, social mobility, science citations, contacts among members of deviant groups, corporate power, international trade exploitation, class structure, and many other areas. A review of the formal models proposed in graph theory, multidimensional scaling, and algebraic topology is followed by extended illustrations of social network analysis in the study of community structure and interlocking directorships.",c41,Software Product Lines Conference,cp41,accepted,f1542,2002,2002-11-23
s1916,p1916,Random graphs with arbitrary degree distributions and their applications.,"Recent work on the structure of social networks and the internet has focused attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph.",c57,IEEE International Conference on Engineering of Complex Computer Systems,cp57,accepted,f1543,2003,2003-04-27
s1918,p1918,Wiener Index of Trees: Theory and Applications,Abstract content goes here ...,c98,North American Chapter of the Association for Computational Linguistics,cp98,accepted,f1544,2009,2009-10-17
s1919,p1919,The knowledge complexity of interactive proof-systems,"Usually, a proof of a theorem contains more knowledge than the mere fact that the theorem is true. For instance, to prove that a graph is Hamiltonian it suffices to exhibit a Hamiltonian tour in it; however, this seems to contain more knowledge than the single bit Hamiltonian/non-Hamiltonian.In this paper a computational complexity theory of the “knowledge” contained in a proof is developed. Zero-knowledge proofs are defined as those proofs that convey no additional knowledge other than the correctness of the proposition in question. Examples of zero-knowledge proof systems are given for the languages of quadratic residuosity and 'quadratic nonresiduosity. These are the first examples of zero-knowledge proofs for languages not known to be efficiently recognizable.",c88,Symposium on the Theory of Computing,cp88,accepted,f1545,2014,2014-05-08
s1920,p1920,"The Dynamics of Message Passing on Dense Graphs, with Applications to Compressed Sensing","“Approximate message passing” (AMP) algorithms have proved to be effective in reconstructing sparse signals from a small number of incoherent linear measurements. Extensive numerical experiments further showed that their dynamics is accurately tracked by a simple one-dimensional iteration termed state evolution. In this paper, we provide rigorous foundation to state evolution. We prove that indeed it holds asymptotically in the large system limit for sensing matrices with independent and identically distributed Gaussian entries. While our focus is on message passing algorithms for compressed sensing, the analysis extends beyond this setting, to a general class of algorithms on dense graphs. In this context, state evolution plays the role that density evolution has for sparse graphs. The proof technique is fundamentally different from the standard approach to density evolution, in that it copes with a large number of short cycles in the underlying factor graph. It relies instead on a conditioning technique recently developed by Erwin Bolthausen in the context of spin glass theory.",j291,IEEE Transactions on Information Theory,jv291,accepted,f1546,2010,2010-10-26
s1921,p1921,Geometry of cuts and metrics,Abstract content goes here ...,c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f1547,2015,2015-06-21
s1922,p1922,Geometric Algorithms and Combinatorial Optimization,Abstract content goes here ...,c2,International Symposium on Intelligent Data Analysis,cp2,accepted,f1548,2014,2014-01-27
s1923,p1923,Brain graphs: graphical models of the human brain connectome.,"Brain graphs provide a relatively simple and increasingly popular way of modeling the human brain connectome, using graph theory to abstractly define a nervous system as a set of nodes (denoting anatomical regions or recording electrodes) and interconnecting edges (denoting structural or functional connections). Topological and geometrical properties of these graphs can be measured and compared to random graphs and to graphs derived from other neuroscience data or other (nonneural) complex systems. Both structural and functional human brain graphs have consistently demonstrated key topological properties such as small-worldness, modularity, and heterogeneous degree distributions. Brain graphs are also physically embedded so as to nearly minimize wiring cost, a key geometric property. Here we offer a conceptual review and methodological guide to graphical analysis of human neuroimaging data, with an emphasis on some of the key assumptions, issues, and trade-offs facing the investigator.",j332,Annual Review of Clinical Psychology,jv332,accepted,f1549,2022,2022-07-06
s1924,p1924,The small world of the cerebral cortex,Abstract content goes here ...,j333,Neuroinformatics,jv333,accepted,f1550,2011,2011-07-11
s1926,p1926,Challenges with graph interpretation: a review of the literature,"With the growing emphasis on the development of scientific inquiry skills, the display and interpretation of data are becoming increasingly important. Graph interpretation competence is, in fact, essential to understanding today’s world and to be scientifically literate. However, graph interpretation is a complex and challenging activity. Graph interpretation competence is affected by many factors, including aspects of graph characteristics, the content of the graph and viewers’ prior knowledge. For instance, the prior theory and expectations that students have may lead to biases and misinterpretation of graphs. One basic controversy that remains unanswered, for example, is what should we teach first in order to make students scientific literate, how to graph or how to interpret a graph? If it is the case that the ability to interpret a graph be developed prior to the ability to create, then it is important to understand what graph interpretation entails. This paper reviews current literature on graph interpretation competence and argues that it should be explicitly taught given its importance and its complexity.",c63,IEEE International Software Metrics Symposium,cp63,accepted,f1551,2008,2008-09-04
s1927,p1927,Stability of multiagent systems with time-dependent communication links,"We study a simple but compelling model of network of agents interacting via time-dependent communication links. The model finds application in a variety of fields including synchronization, swarming and distributed decision making. In the model, each agent updates his current state based upon the current information received from neighboring agents. Necessary and/or sufficient conditions for the convergence of the individual agents' states to a common value are presented, thereby extending recent results reported in the literature. The stability analysis is based upon a blend of graph-theoretic and system-theoretic tools with the notion of convexity playing a central role. The analysis is integrated within a formal framework of set-valued Lyapunov theory, which may be of independent interest. Among others, it is observed that more communication does not necessarily lead to faster convergence and may eventually even lead to a loss of convergence, even for the simple models discussed in the present paper.",j329,IEEE Transactions on Automatic Control,jv329,accepted,f1552,2002,2002-08-04
s1928,p1928,Rigid graph control architectures for autonomous formations,"This article sets out the rudiments of a theory for analyzing and creating architectures appropriate to the control of formations of autonomous vehicles. The theory rests on ideas of rigid graph theory, some but not all of which are old. The theory, however, has some gaps in it, and their elimination would help in applications. Some of the gaps in the relevant graph theory are as follows. First, there is as yet no analogue for three-dimensional graphs of Laman's theorem, which provides a combinatorial criterion for rigidity in two-dimensional graphs. Second, for three-dimensional graphs there is no analogue of the two-dimensional Henneberg construction for growing or deconstructing minimally rigid graphs although there are conjectures. Third, global rigidity can easily be characterized for two-dimensional graphs, but not for three-dimensional graphs.",j334,IEEE Control Systems,jv334,accepted,f1553,2017,2017-06-29
s1930,p1930,Frequency assignment: Theory and applications,"In this paper we introduce the minimum-order approach to frequency assignment and present a theory which relates this approach to the traditional one. This new approach is potentially more desirable than the traditional one. We model assignment problems as both frequency-distance constrained and frequency constrained optimization problems. The frequency constrained approach should be avoided if distance separation is employed to mitigate interference. A restricted class of graphs, called disk graphs, plays a central role in frequency-distance constrained problems. We introduce two generalizations of chromatic number and show that many frequency assignment problems are equivalent to generalized graph coloring problems. Using these equivalences and recent results concerning the complexity of graph coloring, we classify many frequency assignment problems according to the ""execution time efficiency"" of algorithms that may be devised for their solution. We discuss applications to important real world problems and identify areas for further work.",j168,Proceedings of the IEEE,jv168,accepted,f1554,2019,2019-11-23
s1932,p1932,Second-Order Consensus for Multiagent Systems With Directed Topologies and Nonlinear Dynamics,"This paper considers a second-order consensus problem for multiagent systems with nonlinear dynamics and directed topologies where each agent is governed by both position and velocity consensus terms with a time-varying asymptotic velocity. To describe the system's ability for reaching consensus, a new concept about the generalized algebraic connectivity is defined for strongly connected networks and then extended to the strongly connected components of the directed network containing a spanning tree. Some sufficient conditions are derived for reaching second-order consensus in multiagent systems with nonlinear dynamics based on algebraic graph theory, matrix theory, and Lyapunov control approach. Finally, simulation examples are given to verify the theoretical analysis.",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1555,2015,2015-01-20
s1933,p1933,Shock Waves on the Highway,A simple theory of traffic flow is developed by replacing individual vehicles with a continuous “fluid” density and applying an empirical relation between speed and density. Characteristic features of the resulting theory are a simple “graph-shearing” process for following the development of traffic waves in time and the frequent appearance of shock waves. The effect of a traffic signal on traffic streams is studied and found to exhibit a threshold effect wherein the disturbances are minor for light traffic but suddenly build to large values when a critical density is exceeded.,c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f1556,2014,2014-01-25
s1934,p1934,The rainbow connection of a graph is (at most) reciprocal to its minimum degree,"An edge‐colored graph Gis rainbow edge‐connected if any two vertices are connected by a path whose edges have distinct colors. The rainbow connection of a connected graph G, denoted by rc(G), is the smallest number of colors that are needed in order to make Grainbow edge‐connected. We prove that if Ghas nvertices and minimum degree δ then rc(G)<20n/δ. This solves open problems from Y. Caro, A. Lev, Y. Roditty, Z. Tuza, and R. Yuster (Electron J Combin 15 (2008), #R57) and S. Chakrborty, E. Fischer, A. Matsliah, and R. Yuster (Hardness and algorithms for rainbow connectivity, Freiburg (2009), pp. 243–254). A vertex‐colored graph Gis rainbow vertex‐connected if any two vertices are connected by a path whose internal vertices have distinct colors. The rainbow vertex‐connection of a connected graph G, denoted by rvc(G), is the smallest number of colors that are needed in order to make Grainbow vertex‐connected. One cannot upper‐bound one of these parameters in terms of the other. Nevertheless, we prove that if Ghas nvertices and minimum degree δ then rvc(G)<11n/δ. We note that the proof in this case is different from the proof for the edge‐colored case, and we cannot deduce one from the other. © 2009 Wiley Periodicals, Inc. J Graph Theory 63: 185–191, 2010",j310,Journal of Graph Theory,jv310,accepted,f1557,2016,2016-08-04
s1935,p1935,Topological quantum chemistry,Abstract content goes here ...,j62,Nature,jv62,accepted,f1558,2017,2017-09-30
s1936,p1936,A Theory of Graphs,Abstract content goes here ...,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f1559,2016,2016-05-14
s1937,p1937,Modeling and control of formations of nonholonomic mobile robots,"This paper addresses the control of a team of nonholonomic mobile robots navigating in a terrain with obstacles while maintaining a desired formation and changing formations when required, using graph theory. We model the team as a triple, (g, r, H), consisting of a group element g that describes the gross position of the lead robot, a set of shape variables r that describe the relative positions of robots, and a control graph H that describes the behaviors of the robots in the formation. Our framework enables the representation and enumeration of possible control graphs and the coordination of transitions between any two formations.",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f1560,2005,2005-10-23
s1939,p1939,"ergm: A Package to Fit, Simulate and Diagnose Exponential-Family Models for Networks.","We describe some of the capabilities of the ergm package and the statistical theory underlying it. This package contains tools for accomplishing three important, and interrelated, tasks involving exponential-family random graph models (ERGMs): estimation, simulation, and goodness of fit. More precisely, ergm has the capability of approximating a maximum likelihood estimator for an ERGM given a network data set; simulating new network data sets from a fitted ERGM using Markov chain Monte Carlo; and assessing how well a fitted ERGM does at capturing characteristics of a particular network data set.",j335,Journal of Statistical Software,jv335,accepted,f1561,2017,2017-02-11
s1940,p1940,Big Data Analysis with Signal Processing on Graphs: Representation and processing of massive data sets with irregular structure,"Analysis and processing of very large data sets, or big data, poses a significant challenge. Massive data sets are collected and studied in numerous domains, from engineering sciences to social networks, biomolecular research, commerce, and security. Extracting valuable information from big data requires innovative approaches that efficiently process large amounts of data as well as handle and, moreover, utilize their structure. This article discusses a paradigm for large-scale data analysis based on the discrete signal processing (DSP) on graphs (DSPG). DSPG extends signal processing concepts and methodologies from the classical signal processing theory to data indexed by general graphs. Big data analysis presents several challenges to DSPG, in particular, in filtering and frequency analysis of very large data sets. We review fundamental concepts of DSPG, including graph signals and graph filters, graph Fourier transform, graph frequency, and spectrum ordering, and compare them with their counterparts from the classical signal processing theory. We then consider product graphs as a graph model that helps extend the application of DSPG methods to large data sets through efficient implementation based on parallelization and vectorization. We relate the presented framework to existing methods for large-scale data processing and illustrate it with an application to data compression.",j327,IEEE Signal Processing Magazine,jv327,accepted,f1562,2011,2011-10-16
s1941,p1941,KEGGgraph: a graph approach to KEGG PATHWAY in R and bioconductor,"Motivation: KEGG PATHWAY is a service of Kyoto Encyclopedia of Genes and Genomes (KEGG), constructing manually curated pathway maps that represent current knowledge on biological networks in graph models. While valuable graph tools have been implemented in R/Bioconductor, to our knowledge there is currently no software package to parse and analyze KEGG pathways with graph theory. Results: We introduce the software package KEGGgraph in R and Bioconductor, an interface between KEGG pathways and graph models as well as a collection of tools for these graphs. Superior to existing approaches, KEGGgraph captures the pathway topology and allows further analysis or dissection of pathway graphs. We demonstrate the use of the package by the case study of analyzing human pancreatic cancer pathway. Availability:KEGGgraph is freely available at the Bioconductor web site (http://www.bioconductor.org). KGML files can be downloaded from KEGG FTP site (ftp://ftp.genome.jp/pub/kegg/xml). Contact: j.zhang@dkfz-heidelberg.de Supplementary information: Supplementary data are available at Bioinformatics online.",c67,Enterprise Application Integration,cp67,accepted,f1563,2002,2002-03-17
s1942,p1942,Introduction to Quantum Graphs,"A ""quantum graph"" is a graph considered as a one-dimensional complex and equipped with a differential operator (""Hamiltonian""). Quantum graphs arise naturally as simplified models in mathematics, physics, chemistry, and engineering when one considers propagation of waves of various nature through a quasi-one-dimensional (e.g., ""meso-"" or ""nano-scale"") system that looks like a thin neighborhood of a graph. Works that currently would be classified as discussing quantum graphs have been appearing since at least the 1930s, and since then, quantum graphs techniques have been applied successfully in various areas of mathematical physics, mathematics in general and its applications. One can mention, for instance, dynamical systems theory, control theory, quantum chaos, Anderson localization, microelectronics, photonic crystals, physical chemistry, nano-sciences, superconductivity theory, etc. Quantum graphs present many non-trivial mathematical challenges, which makes them dear to a mathematician's heart. Work on quantum graphs has brought together tools and intuition coming from graph theory, combinatorics, mathematical physics, PDEs, and spectral theory. This book provides a comprehensive introduction to the topic, collecting the main notions and techniques. It also contains a survey of the current state of the quantum graph research and applications.",c78,Neural Information Processing Systems,cp78,accepted,f1564,2012,2012-11-04
s1943,p1943,Kron Reduction of Graphs With Applications to Electrical Networks,"Consider a weighted undirected graph and its corresponding Laplacian matrix, possibly augmented with additional diagonal elements corresponding to self-loops. The Kron reduction of this graph is again a graph whose Laplacian matrix is obtained by the Schur complement of the original Laplacian matrix with respect to a specified subset of nodes. The Kron reduction process is ubiquitous in classic circuit theory and in related disciplines such as electrical impedance tomography, smart grid monitoring, transient stability assessment, and analysis of power electronics. Kron reduction is also relevant in other physical domains, in computational applications, and in the reduction of Markov chains. Related concepts have also been studied as purely theoretic problems in the literature on linear algebra. In this paper we analyze the Kron reduction process from the viewpoint of algebraic graph theory. Specifically, we provide a comprehensive and detailed graph-theoretic analysis of Kron reduction encompassing topological, algebraic, spectral, resistive, and sensitivity analyses. Throughout our theoretic elaborations we especially emphasize the practical applicability of our results to various problem setups arising in engineering, computation, and linear algebra. Our analysis of Kron reduction leads to novel insights both on the mathematical and the physical side.",j336,IEEE Transactions on Circuits and Systems Part 1: Regular Papers,jv336,accepted,f1565,2002,2002-06-12
s1945,p1945,GRAPH LIMITS AND EXCHANGEABLE RANDOM GRAPHS,"We develop a clear connection between de Finetti’s theorem for exchangeable arrays (work of Aldous–Hoover–Kallenberg) and the emerging area of graph limits (work of Lovász and many coauthors). Along the way, we translate the graph theory into more classical prob-",c87,European Conference on Computer Vision,cp87,accepted,f1566,2014,2014-05-04
s1946,p1946,Graph-based Knowledge Representation - Computational Foundations of Conceptual Graphs,Abstract content goes here ...,c71,IEEE International Conference on Information Reuse and Integration,cp71,accepted,f1567,2022,2022-02-20
s1947,p1947,"Towards a Theory of Scale-Free Graphs: Definition, Properties, and Implications","There is a large, popular, and growing literature on ""scale-free"" networks with the Internet along with metabolic networks representing perhaps the canonical examples. While this has in many ways reinvigorated graph theory, there is unfortunately no consistent, precise definition of scale-free graphs and few rigorous proofs of many of their claimed properties. In fact, it is easily shown that the existing theory has many inherent contradictions and that the most celebrated claims regarding the Internet and biology are verifiably false. In this paper, we introduce a structural metric that allows us to differentiate between all simple, connected graphs having an identical degree sequence, which is of particular interest when that sequence satisfies a power law relationship. We demonstrate that the proposed structural metric yields considerable insight into the claimed properties of SF graphs and provides one possible measure of the extent to which a graph is scale-free. This structural view can be related to previously studied graph properties such as the various notions of self-similarity, likelihood, betweenness and assortativity. Our approach clarifies much of the confusion surrounding the sensational qualitative claims in the current literature, and offers a rigorous and quantitative alternative, while suggesting the potential for a rich and interesting theory. This paper is aimed at readers familiar with the basics of Internet technology and comfortable with a theorem-proof style of exposition, but who may be unfamiliar with the existing literature on scale-free networks.",j338,Internet Mathematics,jv338,accepted,f1568,2020,2020-03-23
s1948,p1948,"Sharp thresholds of graph properties, and the -sat problem","Consider G(n, p) to be the probability space of random graphs on n vertices with edge probability p. We will be considering subsets of this space defined by monotone graph properties. A monotone graph property P is a property of graphs such that a) P is invariant under graph automorphisims. b) If graph H has property P , then so does any graph G having H as a subgraph. A monotone symmetric family of graphs is a family defined by such a property. One of the first observations made about random graphs by Erdos and Renyi in their seminal work on random graph theory [12] was the existence of threshold phenomena, the fact that for many interesting properties P , the probability of P appearing in G(n, p) exhibits a sharp increase at a certain critical value of the parameter p. Bollobas and Thomason proved the existence of threshold functions for all monotone set properties ([6]), and in [14] it is shown that this behavior is quite general, and that all monotone graph properties exhibit threshold behavior, i.e. the probability of their appearance increases from values very close to 0 to values close to 1 in a very small interval. More precise analysis of the size of the threshold interval is done in [7]. This threshold behavior which occurs in various settings which arise in combinatorics and computer science is an instance of the phenomenon of phase transitions which is the subject of much interest in statistical physics. One of the main questions that arises in studying phase transitions is: how “sharp” is the transition? For example, one of the motivations for this paper arose from the question of the sharpness of the phase transition for the property of satisfiability of a random kCNF Boolean formula. Nati Linial, who introduced me to this problem, suggested that although much concrete analysis was being performed on this problem the best approach would be to find general conditions for sharpness of the phase transition, answering the question posed in [14] as to the relation between the length of the threshold interval and the value of the critical probability. In this paper we indeed introduce a simple condition and prove it is sufficient. Stated roughly, in the setting of random graphs, the main theorem states that if a property has a coarse threshold, then it can be approximated by the property of having certain given graphs as a subgraph. This condition can be applied in a more",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f1569,2002,2002-06-22
s1952,p1952,Topological properties of hypercubes,"The n-dimensional hypercube is a highly concurrent loosely coupled multiprocessor based on the binary n-cube topology. Machines based on the hypercube topology have been advocated as ideal parallel architectures for their powerful interconnection features. The authors examine the hypercube from the graph-theory point of view and consider those features that make its connectivity so appealing. Among other things, they propose a theoretical characterization of the n-cube as a graph and and show how to map various other topologies into a hypercube. >",c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f1570,2020,2020-06-19
s1954,p1954,Resistance distance,Abstract content goes here ...,c100,ACM SIGMOD Conference,cp100,accepted,f1571,2010,2010-09-18
s1955,p1955,Renormalization in Quantum Field Theory and the Riemann–Hilbert Problem I: The Hopf Algebra Structure of Graphs and the Main Theorem,Abstract content goes here ...,c32,International Conference on Software Technology: Methods and Tools,cp32,accepted,f1572,2010,2010-07-20
s1956,p1956,The average distances in random graphs with given expected degrees,"Random graph theory is used to examine the “small-world phenomenon”; any two strangers are connected through a short chain of mutual acquaintances. We will show that for certain families of random graphs with given expected degrees the average distance is almost surely of order log n/log d̃, where d̃ is the weighted average of the sum of squares of the expected degrees. Of particular interest are power law random graphs in which the number of vertices of degree k is proportional to 1/kβ for some fixed exponent β. For the case of β > 3, we prove that the average distance of the power law graphs is almost surely of order log n/log d̃. However, many Internet, social, and citation networks are power law graphs with exponents in the range 2 < β < 3 for which the power law random graphs have average distance almost surely of order log log n, but have diameter of order log n (provided having some mild constraints for the average distance and maximum degree). In particular, these graphs contain a dense subgraph, which we call the core, having nc/log log n vertices. Almost all vertices are within distance log log n of the core although there are vertices at distance log n from the core.",j20,Proceedings of the National Academy of Sciences of the United States of America,jv20,accepted,f1573,2006,2006-06-10
s1957,p1957,Complex Graphs and Networks,Graph theory in the information age Old and new concentration inequalities A generative model--the preferential attachment scheme Duplication models for biological networks Random graphs with given expected degrees The rise of the giant component Average distance and the diameter Eigenvalues of the adjacency matrix of $G(\mathbf{w})$ The semi-circle law for $G(\mathbf{w})$ Coupling on-line and off-line analyses of random graphs The configuration model for power law graphs The small world phenomenon in hybrid graphs Bibliography Index.,c27,ACM-SIAM Symposium on Discrete Algorithms,cp27,accepted,f1574,2022,2022-09-30
s1958,p1958,An Improved Spectral Graph Partitioning Algorithm for Mapping Parallel Computations,Efficient use of a distributed memory parallel computer requires that the computational load be balanced across processors in a way that minimizes interprocessor communication. A new domain mapping algorithm is presented that extends recent work in which ideas from spectral graph theory have been applied to this problem. The generalization of spectral graph bisection involves a novel use of multiple eigenvectors to allow for division of a computation into four or eight parts at each stage of a recursive decomposition. The resulting method is suitable for scientific computations like irregular finite elements or differences performed on hypercube or mesh architecture machines. Experimental results confirm that the new method provides better decompositions arrived at more economically and robustly than with previous spectral methods. This algorithm allows for arbitrary nonnegative weights on both vertices and edges to model inhomogeneous computation and communication. A new spectral lower bound for graph bi...,j340,SIAM Journal on Scientific Computing,jv340,accepted,f1575,2002,2002-03-27
s1959,p1959,A New Theory of Deadlock-Free Adaptive Routing in Wormhole Networks,"The theoretical background for the design of deadlock-free adaptive routing algorithms for wormhole networks is developed. The author proposes some basic definitions and two theorems. These create the conditions to verify that an adaptive algorithm is deadlock-free, even when there are cycles in the channel dependency graph. Two design methodologies are also proposed. The first supplies algorithms with a high degree of freedom, without increasing the number of physical channels. The second methodology is intended for the design of fault-tolerant algorithms. Some examples are given to show the application of the methodologies. Simulations show the performance improvement that can be achieved by designing the routing algorithms with the new theory. >",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f1576,2018,2018-03-24
s1960,p1960,Handbook of Combinatorics,"Part 1 Structures: graphs - basic graph theory - paths and circuits, J.A. Bondy, connectivity and network flows, A. Frank, matchings and extensions, W.R. Pulleyblank, colouring, stable sets and perfect graphs, B. Toft, embeddings and minors, C. Thomassen, random graphs, M. Karonski finite sets and relations - hypergraphs, P. Duchet, partially ordered sets, W.T. Trotter matroids - matroids - fundamental concepts, D.J.A. Welsh, matroid minors, P.D. Seymour, matroid optimization and algorithms, R.E. Bixby and W.H. Cunningham symmetric structures - permutation groups, P.J. Cameron, finite geometries, P.J. Cameron, block designs, A.E. Brouwer, association schemes, A.E. Brouwer and W. Haemers, codes, J.H. van Lint combinatorial structures in geometry and number theory - extremal problems in combinatorial geometry, P. Erdos and G. Purdy, convex polytopes and related complexes, V. Klee and P. Kleinschmidt, point lattices, J.C. Lagarias, combinatorial number theory, C. Pomerance and A. Sarkozy. Part 2 Aspects: algebraic enumeration, I.M. Gessel and R.P. Stanley asymptotic enumeration methods, A.M. Odlyzko extremal graph theory, B. Bollobas extremal set systems, P. Frankl Ramsey theory, J. Nesetril discrepancy theory, J. Beck and V.T. Sos automorphism groups, isomorphism, reconstruction, L. Babai optimization, M. Grotschel and L. Lovasz computational complexity, D.B. Shmoys and E. Tardos. Part 3 Methods: polyhedral combinatorics, A. Schrijver tools from linear algebra, C.D. Godsil tools from higher algebra, N. Alon probabilistic methods, J. Spencer topological methods, A. Bjorner. Part 4 Applications: combinatorics in operations research, A. Kolen and J.K. Lenstra combinatorics in electrical engineering and statics, A. Recski combinatorics in statistical mechanics, C.D. Godsil et al combinatorics in chemistry, D.H. Rouvray applications of combinatorics to molecular biology, M.S. Waterman combinatorics in computer science, L. Lovasz et al combinatorics in pure mathematics, L. Lovasz et al. Part 5 Horizons: infinite combinatorics, A. Hajnal combinatorial games, R.K. Guy the history of combinatorics, N.L. Biggs et al.",c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,cp99,accepted,f1577,2015,2015-05-02
s1961,p1961,Shock Graphs and Shape Matching,Abstract content goes here ...,c71,IEEE International Conference on Information Reuse and Integration,cp71,accepted,f1578,2022,2022-04-27
s1962,p1962,Topological index based on the ratios of geometrical and arithmetical means of end-vertex degrees of edges,Abstract content goes here ...,c6,Americas Conference on Information Systems,cp6,accepted,f1579,2007,2007-05-02
s1963,p1963,Algebraic Approaches to Graph Transformation - Part I: Basic Concepts and Double Pushout Approach,"The algebraic approaches to graph transformation are based on the concept of gluing of graphs, modelled by pushouts in suitable categories of graphs and graph morphisms. This allows one not only to give an explicit algebraic or set theoretical description of the constructions, but also to use concepts and results from category theory in order to build up a rich theory and to give elegant proofs even in complex situations. In this chapter we start with an overwiev of the basic notions common to the two algebraic approaches, the ""double-pushout (DPO) approach"" and the ""single-pushout (SPO) approach""; next we present the classical theory and some recent development of the double-pushout approach. The next chapter is devoted instead to the single-pushout approach, and it is closed by a comparison between the two approaches. -- This document will appear as a chapter of the ""The Handbook of Graph Grammars. Volume I: Foundations"", G. Rozenberg (Ed.), World Scientific.",c9,Pacific Symposium on Biocomputing,cp9,accepted,f1580,2009,2009-02-13
s1964,p1964,Consensus Conditions of Multi-Agent Systems With Time-Varying Topologies and Stochastic Communication Noises,"This paper investigates the average-consensus problem of first-order discrete-time multi-agent networks in uncertain communication environments. Each agent can only use its own and neighbors' information to design its control input. To attenuate the communication noises, a distributed stochastic approximation type protocol is used. By using probability limit theory and algebraic graph theory, consensus conditions for this kind of protocols are obtained: (A) For the case of fixed topologies, a necessary and sufficient condition for mean square average-consensus is given, which is also sufficient for almost sure consensus. (B) For the case of time-varying topologies, sufficient conditions for mean square average-consensus and almost sure consensus are given, respectively. Especially, if the network switches between jointly-containing-spanning-tree, instantaneously balanced graphs, then the designed protocol can guarantee that each individual state converges, both almost surely and in mean square, to a common random variable, whose expectation is right the average of the initial states of the whole system, and whose variance describes the static maximum mean square error between each individual state and the average of the initial states of the whole system.",j329,IEEE Transactions on Automatic Control,jv329,accepted,f1581,2002,2002-08-15
s1966,p1966,Mathematics of networks,"An introduction to the mathematical tools used in the study of networks. Topics discussed include: the adjacency matrix; weighted, directed, acyclic, and bipartite networks; multilayer and dynamic networks; trees; planar networks. Some basic properties of networks are then discussed, including degrees, density and sparsity, paths on networks, component structure, and connectivity and cut sets. The final part of the chapter focuses on the graph Laplacian and its applications to network visualization, graph partitioning, the theory of random walks, and other problems.",c85,International Conference on Graph Transformation,cp85,accepted,f1582,2007,2007-06-08
s1967,p1967,"The image foresting transform: theory, algorithms, and applications","The image foresting transform (IFT) is a graph-based approach to the design of image processing operators based on connectivity. It naturally leads to correct and efficient implementations and to a better understanding of how different operators relate to each other. We give here a precise definition of the IFT, and a procedure to compute it-a generalization of Dijkstra's algorithm-with a proof of correctness. We also discuss implementation issues and illustrate the use of the IFT in a few applications.",j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,jv299,accepted,f1583,2020,2020-07-17
s1968,p1968,Applications of Combinatorial Matrix Theory to Laplacian Matrices of Graphs,"Matrix Theory Preliminaries Vector Norms, Matrix Norms, and the Spectral Radius of a Matrix Location of Eigenvalues Perron-Frobenius Theory M-Matrices Doubly Stochastic Matrices Generalized Inverses Graph Theory Preliminaries Introduction to Graphs Operations of Graphs and Special Classes of Graphs Trees Connectivity of Graphs Degree Sequences and Maximal Graphs Planar Graphs and Graphs of Higher Genus Introduction to Laplacian Matrices Matrix Representations of Graphs The Matrix Tree Theorem The Continuous Version of the Laplacian Graph Representations and Energy Laplacian Matrices and Networks The Spectra of Laplacian Matrices The Spectra of Laplacian Matrices Under Certain Graph Operations Upper Bounds on the Set of Laplacian Eigenvalues The Distribution of Eigenvalues Less than One and Greater than One The Grone-Merris Conjecture Maximal (Threshold) Graphs and Integer Spectra Graphs with Distinct Integer Spectra The Algebraic Connectivity Introduction to the Algebraic Connectivity of Graphs The Algebraic Connectivity as a Function of Edge Weight The Algebraic Connectivity with Regard to Distances and Diameters The Algebraic Connectivity in Terms of Edge Density and the Isoperimetric Number The Algebraic Connectivity of Planar Graphs The Algebraic Connectivity as a Function Genus k where k is greater than 1 The Fiedler Vector and Bottleneck Matrices for Trees The Characteristic Valuation of Vertices Bottleneck Matrices for Trees Excursion: Nonisomorphic Branches in Type I Trees Perturbation Results Applied to Extremizing the Algebraic Connectivity of Trees Application: Joining Two Trees by an Edge of Infinite Weight The Characteristic Elements of a Tree The Spectral Radius of Submatrices of Laplacian Matrices for Trees Bottleneck Matrices for Graphs Constructing Bottleneck Matrices for Graphs Perron Components of Graphs Minimizing the Algebraic Connectivity of Graphs with Fixed Girth Maximizing the Algebraic Connectivity of Unicyclic Graphs with Fixed Girth Application: The Algebraic Connectivity and the Number of Cut Vertices The Spectral Radius of Submatrices of Laplacian Matrices for Graphs The Group Inverse of the Laplacian Matrix Constructing the Group Inverse for a Laplacian Matrix of a Weighted Tree The Zenger Function as a Lower Bound on the Algebraic Connectivity The Case of the Zenger Equalling the Algebraic Connectivity in Trees Application: The Second Derivative of the Algebraic Connectivity as a Function of Edge Weight",c100,ACM SIGMOD Conference,cp100,accepted,f1584,2010,2010-08-10
s1969,p1969,Strategic Interaction and Networks,"This paper brings a general network analysis to a wide class of economic games. A network, or interaction matrix, tells who directly interacts with whom. A major challenge is determining how network structure shapes overall outcomes. We have a striking result. Equilibrium conditions depend on a single number: the lowest eigenvalue of a network matrix. Combining tools from potential games, optimization, and spectral graph theory, we study games with linear best replies and characterize the Nash and stable equilibria for any graph and for any impact of players’ actions. When the graph is sufficiently absorptive (as measured by this eigenvalue), there is a unique equilibrium. When it is less absorptive, stable equilibria always involve extreme play where some agents take no actions at all. This paper is the first to show the importance of this measure to social and economic outcomes, and we relate it to different network link patterns.",c26,PS,cp26,accepted,f1585,2010,2010-12-06
s1970,p1970,"Two’s company, three (or more) is a simplex",Abstract content goes here ...,j341,Journal of Computational Neuroscience,jv341,accepted,f1586,2005,2005-08-10
s1971,p1971,On Communicating Finite-State Machines,"A model of commumcations protocols based on finite-state machines is investigated. The problem addressed is how to ensure certain generally desirable properties, which make protocols ""wellformed,"" that is, specify a response to those and only those events that can actually occur. It is determined to what extent the problem is solvable, and one approach to solving it ts described. Categories and SubJect Descriptors' C 2 2 [Computer-Conununication Networks]: Network Protocols-protocol verification; F 1 1 [Computation by Abstract Devices] Models of Computation--automata; G.2.2 [Discrete Mathematics] Graph Theory--graph algoruhms; trees General Terms: Reliability, Verification Additional",c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f1587,2019,2019-01-03
s1972,p1972,Graph embedding: a general framework for dimensionality reduction,"In the last decades, a large family of algorithms - supervised or unsupervised; stemming from statistic or geometry theory - have been proposed to provide different solutions to the problem of dimensionality reduction. In this paper, beyond the different motivations of these algorithms, we propose a general framework, graph embedding along with its linearization and kernelization, which in theory reveals the underlying objective shared by most previous algorithms. It presents a unified perspective to understand these algorithms; that is, each algorithm can be considered as the direct graph embedding or its linear/kernel extension of some specific graph characterizing certain statistic or geometry property of a data set. Furthermore, this framework is a general platform to develop new algorithm for dimensionality reduction. To this end, we propose a new supervised algorithm, Marginal Fisher Analysis (MFA), for dimensionality reduction by designing two graphs that characterize the intra-class compactness and inter-class separability, respectively. MFA measures the intra-class compactness with the distance between each data point and its neighboring points of the same class, and measures the inter-class separability with the class margins; thus it overcomes the limitations of traditional Linear Discriminant Analysis algorithm in terms of data distribution assumptions and available projection directions. The toy problem on artificial data and the real face recognition experiments both show the superiority of our proposed MFA in comparison to LDA.",c90,Computer Vision and Pattern Recognition,cp90,accepted,f1588,2008,2008-12-03
s1973,p1973,Entanglement in Graph States and its Applications,"Graph states form a rich class of entangled states that exhibit important aspects of multi-partite entanglement. At the same time, they can be described by a number of parameters that grows only moderately with the system size. They have a variety of applications in quantum information theory, most prominently as algorithmic resources in the context of the one-way quantum computer, but also in other fields such as quantum error correction and multi-partite quantum communication, as well as in the study of foundational issues such as non-locality and decoherence. In this review, we give a tutorial introduction into the theory of graph states. We introduce various equivalent ways how to define graph states, and discuss the basic notions and properties of these states. The focus of this review is on their entanglement properties. These include aspects of non-locality, bi-partite and multi-partite entanglement and its classification in terms of the Schmidt measure, the distillability properties of mixed entangled states close to a pure graph state, as well as the robustness of their entanglement under decoherence. We review some of the known applications of graph states, as well as proposals for their experimental implementation.",c16,Knowledge Discovery and Data Mining,cp16,accepted,f1589,2003,2003-07-12
s1974,p1974,Combinatorial Algebraic Topology,Abstract content goes here ...,c19,ACM Conference on Economics and Computation,cp19,accepted,f1590,2002,2002-03-12
s1976,p1976,"Towards a spectral theory of graphs based on the signless Laplacian, I","A spectral graph theory is a theory in which graphs are studied by means of eigenvalues of a matrix �� which is in a prescribed way defined for any graph. This theory is called �� -theory. We outline a spectral theory of graphs based on the signless Laplacians �� and compare it with other spectral theories, in particular with those based on the adjacency matrix �� and the Laplacian �� . The �� -theory can be composed using various connections to other theories: equivalency with �� -theory and �� -theory for regular graphs, or with �� -theory for bipartite graphs, general analogies with �� -theory and analogies with �� -theory via line graphs and subdivision graphs. We present results on graph operations, inequalities for eigenvalues and reconstruction problems.",c59,British Computer Society Conference on Human-Computer Interaction,cp59,accepted,f1591,2019,2019-03-13
s1977,p1977,Semi-supervised learning with graphs,"In traditional machine learning approaches to classification, one uses only a labeled set to train the classifier. Labeled instances however are often difficult, expensive, or time consuming to obtain, as they require the efforts of experienced human annotators. Meanwhile unlabeled data may be relatively easy to collect, but there has been few ways to use them. Semi-supervised learning addresses this problem by using large amount of unlabeled data, together with the labeled data, to build better classifiers. Because semi-supervised learning requires less human effort and gives higher accuracy, it is of great interest both in theory and in practice. 
We present a series of novel semi-supervised learning approaches arising from a graph representation, where labeled and unlabeled instances are represented as vertices, and edges encode the similarity between instances. They address the following questions: How to use unlabeled data? (label propagation); What is the probabilistic interpretation? (Gaussian fields and harmonic functions); What if we can choose labeled data? (active learning); How to construct good graphs? (hyperparameter learning); How to work with kernel machines like SVM? (graph kernels); How to handle complex data like sequences? (kernel conditional random fields); How to handle scalability and induction? (harmonic mixtures). An extensive literature review is included at the end.",c71,IEEE International Conference on Information Reuse and Integration,cp71,accepted,f1592,2022,2022-03-12
s1978,p1978,"Protein-to-protein interactions: Technologies, databases, and algorithms","Studying proteins and their structures has an important role for understanding protein functionalities. Recently, due to important results obtained with proteomics, a great interest has been given to interactomics, that is, the study of protein-to-protein interactions, called PPI, or more generally, interactions among macromolecules, particularly within cells. Interactomics means studying, modeling, storing, and retrieving protein-to-protein interactions as well as algorithms for manipulating, simulating, and predicting interactions. PPI data can be obtained from biological experiments studying interactions. Modeling and storing PPIs can be realized by using graph theory and graph data management, thus graph databases can be queried for further experiments. PPI graphs can be used as input for data-mining algorithms, where raw data are binary interactions forming interaction graphs, and analysis algorithms retrieve biological interactions among proteins (i.e., PPI biological meanings). For instance, predicting the interactions between two or more proteins can be obtained by mining interaction networks stored in databases. In this article we survey modeling, storing, analyzing, and manipulating PPI data. After describing the main PPI models, mostly based on graphs, the article reviews PPI data representation and storage, as well as PPI databases. Algorithms and software tools for analyzing and managing PPI networks are discussed in depth. The article concludes by discussing the main challenges and research directions in PPI networks.",c15,International Conference on Conceptual Structures,cp15,accepted,f1593,2011,2011-12-11
s1979,p1979,Ramanujan Graphs,"In the last two decades, the theory of Ramanujan graphs has gained prominence primarily for two reasons. First, from a practical viewpoint, these graphs resolve an extremal problem in communication network theory (see for example [2]). Second, from a more aesthetic viewpoint, they fuse diverse branches of pure mathematics, namely, number theory, representation theory and algebraic geometry. The purpose of this survey is to unify some of the recent developments and expose certain open problems in the area. This survey is by no means an exhaustive one and demonstrates a highly number-theoretic bias. For more comprehensive surveys, we refer the reader to [27], [9] or [13]. For a more up-to-date survey highlighting the connection between graph theory and automorphic representations, we refer the reader to Winnie Li’s recent survey article [11]. A is a triple consisting of avertex (X), an and a map that associates to each edge two vertices (not necessarily distinct) called itsendpoints. A is an edge whose endpoints are equal. Multiple edges are edges having the same pair of endpoints. A is one having no loops or multiple edges. If a graph has loops or multiple edges, we will call it a multigraph. When two verticesu andv are endpoints of an edge, we say they are and (sometimes) write to indicate this. To any graph, we may associate the which is ann matrix (wheren |) with rows and columns indexed by the elements of the vertex set and the y)-th entry is the number of edges connecting and y. Since our graphs are undirected, the matrix is symmetric. Consequently, all of its eigenvalues are real. The convention regarding terminology is not clear in the literature. Most use the term ‘graph’ to mean a simple graph as we have defined it above. Thus, the",c11,Hawaii International Conference on System Sciences,cp11,accepted,f1594,2006,2006-03-14
s1981,p1981,The topology of multidimensional potential energy surfaces: Theory and application to peptide structure and kinetics,"Topological characteristics of multidimensional potential energy surfaces are explored and the full conformation space is mapped on the set of local minima. This map partitions conformation space into energy-dependent or temperature-dependent “attraction basins’’ and generates a “disconnectivity’’ graph that reflects the basin connectivity and characterizes the shape of the multidimensional surface. The partitioning of the conformation space is used to express the temporal behavior of the system in terms of basin-to-basin kinetics instead of the usual state-to-state transitions. For this purpose the transition matrix of the system is expressed in terms of basin-to-basin transitions and the corresponding master equation is solved. As an example, the approach is applied to the tetrapeptide, isobutyryl-(ala)3-NH-methyl (IAN), which is the shortest peptide that can form a full helical turn. A nearly complete list of minima and barriers is available for this system from the work of Czerminiski and Elber. The m...",c74,IEEE International Conference on Tools with Artificial Intelligence,cp74,accepted,f1595,2003,2003-02-23
s1982,p1982,Incidence matrices and interval graphs,"Abstract : According to present genetic theory, the fine structure of genes consists of linearly ordered elements. A mutant gene is obtained by alteration of some connected portion of this structure. By examining data obtained from suitable experiments, it can be determined whether or not the blemished portions of two mutant genes intersect or not, and thus intersection data for a large number of mutants can be represented as an undirected graph. If this graph is an interval graph, then the observed data is consistent with a linear model of the gene. The problem of determining when a graph is an interval graph is a special case of the following problem concerning (0, 1)-matrices: When can the rows of such a matrix be permuted so as to make the 1's in each colum appear consecutively. A complete theory is obtained for this latter problem, culminating in a decomposition theorem which leads to a rapid algorithm for deciding the question, and for constructing the desired permutation when one exists.",c36,Conference on Software Engineering Education and Training,cp36,accepted,f1596,2015,2015-06-23
s1983,p1983,Workshop on Algorithms and Models for the Web Graph,Abstract content goes here ...,c91,Workshop on Algorithms and Models for the Web-Graph,cp91,accepted,f1597,2022,2022-03-18
s1985,p1985,Thermodynamic perturbation theory of polymerization,"We derive several extensions of a previously given first‐order perturbation theory (TPT 1) for fluids in which chain and ring polymers can be formed, due to the presence of two singly bondable molecular attraction sites. We retain graphs which contain a single chain of attraction bonds, and evaluate second‐order perturbation theory (TPT 2) within this framework. The previous formulation with sites fixed in the molecule is generalized to allow movable sites, thus permitting calculations for flexible bead polymers. The TPT 1 result for the equation of state of an equilibrium mixture of chain lengths is in good agreement with simulations of flexible bead polymers of fixed bead number N, when the mean number ν of beads is equated to the fixed N of the simulations. TPT 2 differs from TPT 1 by a rather small term, with improved agreement. If the distribution of movable sites includes configurations such that bonding of one site blocks bonding of the other, then graph resummation must be used. Resummed TPT yield...",c95,IEEE International Conference on Computer Vision,cp95,accepted,f1598,2017,2017-03-16
s1986,p1986,Every monotone graph property has a sharp threshold,"In their seminal work which initiated random graph theory Erdos and Renyi discovered that many graph properties have sharp thresholds as the number of vertices tends to infinity. We prove a conjecture of Linial that every monotone graph property has a sharp threshold. This follows from the following theorem. Let Vn(p) = {0, 1}n denote the Hamming space endowed with the probability measure μp defined by μp( 1, 2, . . . , n) = pk · (1 − p)n−k, where k = 1 + 2 + · · · + n. Let A be a monotone subset of Vn. We say that A is symmetric if there is a transitive permutation group Γ on {1, 2, . . . , n} such that A is invariant under Γ. Theorem. For every symmetric monotone A, if μp(A) > then μq(A) > 1− for q = p+ c1 log(1/2 )/ logn. (c1 is an absolute constant.)",c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f1599,2006,2006-03-17
s1987,p1987,Geometric Group Theory,"The aim of this talk is to define the space of ends of a f.g. (finitely generated) group. First, define the space of ends Ends(X) for a metric space X. Define what quasi-isometries are, briefly sketch the concept of proper and geodesic spaces and show that quasi-isometries between such spaces induce homeomorphisms between their spaces of ends. This can be found in [3], p.144/45. Define the Cayley graph for a group presentation and give the main ideas of Theorem 1.5 in [10]. Conclude.",c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,cp99,accepted,f1600,2015,2015-10-12
s1989,p1989,A Best Possible Heuristic for the k-Center Problem,"In this paper we present a 2-approximation algorithm for the k-center problem with triangle inequality. This result is “best possible” since for any δ < 2 the existence of δ-approximation algorithm would imply that P = NP. It should be noted that no δ-approximation algorithm, for any constant δ, has been reported to date. Linear programming duality theory provides interesting insight to the problem and enables us to derive, in O|E| log |E| time, a solution with value no more than twice the k-center optimal value. 
 
A by-product of the analysis is an O|E| algorithm that identifies a dominating set in G2, the square of a graph G, the size of which is no larger than the size of the minimum dominating set in the graph G. The key combinatorial object used is called a strong stable set, and we prove the NP-completeness of the corresponding decision problem.",j343,Mathematics of Operations Research,jv343,accepted,f1601,2001,2001-07-23
s1991,p1991,"Instantons on ALE spaces, quiver varieties, and Kac-Moody algebras","To Professor Shoshichi Kobayashi on his 60th birthday 1. Introduction. In this paper we shall introduce a new family of varieties, which we call quiver varieties, and study their geometric structures. They have close relation to the singularity theory and the representation theory of the Kac-Moody algebras. Our original motivation was to study solutions of the anti-self-dual Yang-Mills equations on a particular class of 4-dimensional noncompact complete manifolds, the so-called ALE spaces (or the ALE gravitational instantons), which were constructed by Kronheimer [Krl]. In [KN] we gave a description of the framed moduli space of all solutions in terms of solutions of a system of quadratic equations (called the ADHM equations) for representations of a quiver on an affine, simply laced Dynkin graph. It is an analogue of the description, given by Atiyah, Drinfeld, Hitchin, and Manin [ADHM], of the moduli space for IR 4 (or S4) in terms of solutions of a quadratic equation for certain finite-dimensional matrices. Once we set aside their gauge-theoretic origin, there is no longer reason to restrict ourselves to affine Dynkin graphs. Definitions can be generalized to arbitrary finite graphs. We get what we call quiver varieties. We study geometric structures of quiver varieties in this paper. In [Nal] it was noticed that the moduli space of anti-self-dual connections on ALE spaces has a hyper-K/ihler structure, namely a Riemannian metric equipped with three endo-morphisms I, J, K of the tangent bundle which satisfy the relations of quaternion algebra and are covariant constant with respect to the Levi-Civita connection: The same holds for general quiver varieties. In particular, quiver varieties have holomorphic symplectic forms. We study further properties of the quiver variety, such as a natural *-action, symplectic geometry, topology, and so on. As ALE spaces closely related to simple singularities, quiver varieties have very special kinds of singularities that enjoy very nice properties. Surprisingly, the ADHM equation appears in a very different context. In [L3] Lusztig used it to construct ""canonical bases"" of the part U-of the quantized enveloping algebra U associated by Drinfeld and Jimbo to the graph. Motivated by his results, we give a geometric construction of irreducible highest-weight integrable representations of the Kac-Moody algebra associated to the graph (Theorem 10.14). The weight space of the representation space will be given as a vector space consisting of constructible functions on a Lagrangian subvariety of a quiver variety. The action of the Kac-Moody …",c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f1602,2020,2020-05-17
s1993,p1993,Computational Invariant Theory,Abstract content goes here ...,c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f1603,2019,2019-02-27
s1994,p1994,The Average Distance in a Random Graph with Given Expected Degrees,"Random graph theory is used to examine the ""small-world phenomenon""– any two strangers are connected through a short chain of mutual acquaintances. We will show that for certain families of random graphs with given expected degrees, the average distance is almost surely of order log n/ logd̃ where d̃ is the weighted average of the sum of squares of the expected degrees. Of particular interest are power law random graphs in which the number of vertices of degree k is proportional to 1/k β for some fixed exponent β. For the case of β > 3, we prove that the average distance of the power law graphs is almost surely of order log n/ log d̃. However, many Internet, social, and citation networks are power law graphs with exponents in the range 2 < β < 3 for which the power law random graphs have average distance almost surely of order log log n, but have diameter of order log n (provided having some mild constraints for the average distance and maximum degree). In particular, these graphs contain a dense subgraph, that we call the core, having n c/ log log n vertices. Almost all vertices are within distance log log n of the core although there are vertices at distance log n from the core.",j338,Internet Mathematics,jv338,accepted,f1604,2020,2020-04-10
s1996,p1996,I-path analysis,"A circuit at the register transfer level is denoted as an RTL circuit. The paper describes a method for extracting the RTL circuit structure from the circuit formal description, using the I-path concept. The way of representing the RTL circuit structure by a labelled directed graph where nodes represent components and arcs represent connections between them, is presented. Labels identifying the component type are attached to the nodes, and other labels are attached to arcs to identify attributes of connections. It is shown, how the graph theory algorithms can be used to derive the information about the accessibility of circuit components, i.e., the existence of I-paths between them, and the sequences of control and clock signals which must be generated to transfer the information along the existing I-paths.<<ETX>>",c17,International Conference on Enterprise Information Systems,cp17,accepted,f1605,2008,2008-11-10
s1997,p1997,"Sparsity - Graphs, Structures, and Algorithms",Abstract content goes here ...,c0,International Conference on Human Factors in Computing Systems,cp0,accepted,f1606,2019,2019-07-11
s1998,p1998,A course on the Web graph,"A Course on the Web Graph provides a comprehensive introduction to state-of-the-art research on the applications of graph theory to real-world networks such as the web graph. It is the first mathematically rigorous textbook discussing both models of the web graph and algorithms for searching the web. After introducing key tools required for the study of web graph mathematics, an overview is given of the most widely studied models for the web graph. A discussion of popular web search algorithms, e.g. PageRank, is followed by additional topics, such as applications of infinite graph theory to the web graph, spectral properties of power law graphs, domination in the web graph, and the spread of viruses in networks. The book is based on a graduate course taught at the AARMS 2006 Summer School at Dalhousie University. As such it is self-contained and includes over 100 exercises. The reader of the book will gain a working knowledge of current research in graph theory and its modern applications. In addition, the reader will learn first-hand about models of the web, and the mathematics underlying modern search engines.",c28,International Conference on Collaboration Technologies and Systems,cp28,accepted,f1607,2009,2009-06-19
s1999,p1999,Network theory of microscopic and macroscopic behavior of master equation systems,"A general microscopic and macroscopic theory is developed for systems which are governed by a (linear) master equation. The theory is based on a network representation of the master equation, and the results are obtained mostly by application of some basic theorems of mathematical graph theory. In the microscopic part of the theory, the construction of a steady state solution of the master equation in terms of graph theoretical elements is described (Kirchhoff's theorem), and it is shown that the master equation satisfies a global asymptotic Liapunov stability criterion with respect to this state. The Glansdorff-Prigogine criterion turns out to be the differential version and thus a special case of the global criterion. In the macroscopic part of the theory, a general prescription is given describing macrostates of the systems arbitrarily far from equilibrium in the language of generalized forces and fluxes of nonlinear irreversible thermodynamics. As a particular result, Onsager's reciprocity relations for the phenomenological coefficients are obtained as coinciding with the reciprocity relations of a near-to-equilibrium network.",c30,IEEE Aerospace Conference,cp30,accepted,f1608,2006,2006-08-06
s2000,p2000,The Cambridge Structural Database,"This paper is the definitive article describing the creation, maintenance, information content and availability of the Cambridge Structural Database (CSD), the world’s repository of small molecule crystal structures.",c2,International Symposium on Intelligent Data Analysis,cp2,accepted,f1609,2014,2014-03-24
s2001,p2001,The PRIDE database and related tools and resources in 2019: improving support for quantification data,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world’s largest data repository of mass spectrometry-based proteomics data, and is one of the founding members of the global ProteomeXchange (PX) consortium. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2016. In the last 3 years, public data sharing through PRIDE (as part of PX) has definitely become the norm in the field. In parallel, data re-use of public proteomics data has increased enormously, with multiple applications. We first describe the new architecture of PRIDE Archive, the archival component of PRIDE. PRIDE Archive and the related data submission framework have been further developed to support the increase in submitted data volumes and additional data types. A new scalable and fault tolerant storage backend, Application Programming Interface and web interface have been implemented, as a part of an ongoing process. Additionally, we emphasize the improved support for quantitative proteomics data through the mzTab format. At last, we outline key statistics on the current data contents and volume of downloads, and how PRIDE data are starting to be disseminated to added-value resources including Ensembl, UniProt and Expression Atlas.",c74,IEEE International Conference on Tools with Artificial Intelligence,cp74,accepted,f1610,2003,2003-05-31
s2002,p2002,"MIMIC-III, a freely accessible critical care database",Abstract content goes here ...,j14,Scientific Data,jv14,accepted,f1611,2020,2020-01-05
s2005,p2005,"The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible","A system-wide understanding of cellular function requires knowledge of all functional interactions between the expressed proteins. The STRING database aims to collect and integrate this information, by consolidating known and predicted protein–protein association data for a large number of organisms. The associations in STRING include direct (physical) interactions, as well as indirect (functional) interactions, as long as both are specific and biologically meaningful. Apart from collecting and reassessing available experimental data on protein–protein interactions, and importing known pathways and protein complexes from curated databases, interaction predictions are derived from the following sources: (i) systematic co-expression analysis, (ii) detection of shared selective signals across genomes, (iii) automated text-mining of the scientific literature and (iv) computational transfer of interaction knowledge between organisms based on gene orthology. In the latest version 10.5 of STRING, the biggest changes are concerned with data dissemination: the web frontend has been completely redesigned to reduce dependency on outdated browser technologies, and the database can now also be queried from inside the popular Cytoscape software framework. Further improvements include automated background analysis of user inputs for functional enrichments, and streamlined download options. The STRING resource is available online, at http://string-db.org/.",c105,Biometrics and Identity Management,cp105,accepted,f1612,2006,2006-05-15
s2006,p2006,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/.",j240,International Journal of Systematic and Evolutionary Microbiology,jv240,accepted,f1613,2016,2016-04-27
s2008,p2008,Pfam: the protein families database,"Pfam, available via servers in the UK (http://pfam.sanger.ac.uk/) and the USA (http://pfam.janelia.org/), is a widely used database of protein families, containing 14 831 manually curated entries in the current release, version 27.0. Since the last update article 2 years ago, we have generated 1182 new families and maintained sequence coverage of the UniProt Knowledgebase (UniProtKB) at nearly 80%, despite a 50% increase in the size of the underlying sequence database. Since our 2012 article describing Pfam, we have also undertaken a comprehensive review of the features that are provided by Pfam over and above the basic family data. For each feature, we determined the relevance, computational burden, usage statistics and the functionality of the feature in a website context. As a consequence of this review, we have removed some features, enhanced others and developed new ones to meet the changing demands of computational biology. Here, we describe the changes to Pfam content. Notably, we now provide family alignments based on four different representative proteome sequence data sets and a new interactive DNA search interface. We also discuss the mapping between Pfam and known 3D structures.",c78,Neural Information Processing Systems,cp78,accepted,f1614,2012,2012-11-03
s2009,p2009,The SILVA ribosomal RNA gene database project: improved data processing and web-based tools,"SILVA (from Latin silva, forest, http://www.arb-silva.de) is a comprehensive web resource for up to date, quality-controlled databases of aligned ribosomal RNA (rRNA) gene sequences from the Bacteria, Archaea and Eukaryota domains and supplementary online services. The referred database release 111 (July 2012) contains 3 194 778 small subunit and 288 717 large subunit rRNA gene sequences. Since the initial description of the project, substantial new features have been introduced, including advanced quality control procedures, an improved rRNA gene aligner, online tools for probe and primer evaluation and optimized browsing, searching and downloading on the website. Furthermore, the extensively curated SILVA taxonomy and the new non-redundant SILVA datasets provide an ideal reference for high-throughput classification of data from next-generation sequencing approaches.",c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f1615,2011,2011-09-21
s2010,p2010,The Molecular Signatures Database Hallmark Gene Set Collection,Abstract content goes here ...,c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f1616,2002,2002-12-29
s2011,p2011,The Pfam protein families database in 2019,"Abstract The last few years have witnessed significant changes in Pfam (https://pfam.xfam.org). The number of families has grown substantially to a total of 17,929 in release 32.0. New additions have been coupled with efforts to improve existing families, including refinement of domain boundaries, their classification into Pfam clans, as well as their functional annotation. We recently began to collaborate with the RepeatsDB resource to improve the definition of tandem repeat families within Pfam. We carried out a significant comparison to the structural classification database, namely the Evolutionary Classification of Protein Domains (ECOD) that led to the creation of 825 new families based on their set of uncharacterized families (EUFs). Furthermore, we also connected Pfam entries to the Sequence Ontology (SO) through mapping of the Pfam type definitions to SO terms. Since Pfam has many community contributors, we recently enabled the linking between authorship of all Pfam entries with the corresponding authors’ ORCID identifiers. This effectively permits authors to claim credit for their Pfam curation and link them to their ORCID record.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1617,2007,2007-07-11
s2012,p2012,ImageNet: A large-scale hierarchical image database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",c91,Workshop on Algorithms and Models for the Web-Graph,cp91,accepted,f1618,2022,2022-02-11
s2014,p2014,The carbohydrate-active enzymes database (CAZy) in 2013,"The Carbohydrate-Active Enzymes database (CAZy; http://www.cazy.org) provides online and continuously updated access to a sequence-based family classification linking the sequence to the specificity and 3D structure of the enzymes that assemble, modify and breakdown oligo- and polysaccharides. Functional and 3D structural information is added and curated on a regular basis based on the available literature. In addition to the use of the database by enzymologists seeking curated information on CAZymes, the dissemination of a stable nomenclature for these enzymes is probably a major contribution of CAZy. The past few years have seen the expansion of the CAZy classification scheme to new families, the development of subfamilies in several families and the power of CAZy for the analysis of genomes and metagenomes. This article outlines the changes that have occurred in CAZy during the past 5 years and presents our novel effort to display the resolution and the carbohydrate ligands in crystallographic complexes of CAZymes.",c64,Experimental Software Engineering Network,cp64,accepted,f1619,2014,2014-11-28
s2015,p2015,Places: A 10 Million Image Database for Scene Recognition,"The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.",j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,jv299,accepted,f1620,2020,2020-05-27
s2016,p2016,Introducing EzTaxon-e: a prokaryotic 16S rRNA gene sequence database with phylotypes that represent uncultured species.,"Despite recent advances in commercially optimized identification systems, bacterial identification remains a challenging task in many routine microbiological laboratories, especially in situations where taxonomically novel isolates are involved. The 16S rRNA gene has been used extensively for this task when coupled with a well-curated database, such as EzTaxon, containing sequences of type strains of prokaryotic species with validly published names. Although the EzTaxon database has been widely used for routine identification of prokaryotic isolates, sequences from uncultured prokaryotes have not been considered. Here, the next generation database, named EzTaxon-e, is formally introduced. This new database covers not only species within the formal nomenclatural system but also phylotypes that may represent species in nature. In addition to an identification function based on Basic Local Alignment Search Tool (blast) searches and pairwise global sequence alignments, a new objective method of assessing the degree of completeness in sequencing is proposed. All sequences that are held in the EzTaxon-e database have been subjected to phylogenetic analysis and this has resulted in a complete hierarchical classification system. It is concluded that the EzTaxon-e database provides a useful taxonomic backbone for the identification of cultured and uncultured prokaryotes and offers a valuable means of communication among microbiologists who routinely encounter taxonomically novel isolates. The database and its analytical functions can be found at http://eztaxon-e.ezbiocloud.net/.",j240,International Journal of Systematic and Evolutionary Microbiology,jv240,accepted,f1621,2016,2016-04-17
s2017,p2017,Pfam: The protein families database in 2021,"Abstract The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.",c0,International Conference on Human Factors in Computing Systems,cp0,accepted,f1622,2019,2019-06-25
s2018,p2018,"Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation","The RefSeq project at the National Center for Biotechnology Information (NCBI) maintains and curates a publicly available database of annotated genomic, transcript, and protein sequence records (http://www.ncbi.nlm.nih.gov/refseq/). The RefSeq project leverages the data submitted to the International Nucleotide Sequence Database Collaboration (INSDC) against a combination of computation, manual curation, and collaboration to produce a standard set of stable, non-redundant reference sequences. The RefSeq project augments these reference sequences with current knowledge including publications, functional features and informative nomenclature. The database currently represents sequences from more than 55 000 organisms (>4800 viruses, >40 000 prokaryotes and >10 000 eukaryotes; RefSeq release 71), ranging from a single record to complete genomes. This paper summarizes the current status of the viral, prokaryotic, and eukaryotic branches of the RefSeq project, reports on improvements to data access and details efforts to further expand the taxonomic representation of the collection. We also highlight diverse functional curation initiatives that support multiple uses of RefSeq data including taxonomic validation, genome annotation, comparative genomics, and clinical testing. We summarize our approach to utilizing available RNA-Seq and other data types in our manual curation process for vertebrate, plant, and other species, and describe a new direction for prokaryotic genomes and protein name management.",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1623,2015,2015-09-15
s2019,p2019,WordNet : an electronic lexical database,"Part 1 The lexical database: nouns in WordNet, George A. Miller modifiers in WordNet, Katherine J. Miller a semantic network of English verbs, Christiane Fellbaum design and implementation of the WordNet lexical database and searching software, Randee I. Tengi. Part 2: automated discovery of WordNet relations, Marti A. Hearst representing verb alterations in WordNet, Karen T. Kohl et al the formalization of WordNet by methods of relational concept analysis, Uta E. Priss. Part 3 Applications of WordNet: building semantic concordances, Shari Landes et al performance and confidence in a semantic annotation task, Christiane Fellbaum et al WordNet and class-based probabilities, Philip Resnik combining local context and WordNet similarity for word sense identification, Claudia Leacock and Martin Chodorow using WordNet for text retrieval, Ellen M. Voorhees lexical chains as representations of context for the detection and correction of malapropisms, Graeme Hirst and David St-Onge temporal indexing through lexical chaining, Reem Al-Halimi and Rick Kazman COLOR-X - using knowledge from WordNet for conceptual modelling, J.F.M. Burg and R.P. van de Riet knowledge processing on an extended WordNet, Sanda M. Harabagiu and Dan I Moldovan appendix - obtaining and using WordNet.",c8,The Compass,cp8,accepted,f1624,2016,2016-10-01
s2020,p2020,WordNet: A Lexical Database for English,"Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].",c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f1625,2005,2005-07-11
s2021,p2021,GTDB-Tk: a toolkit to classify genomes with the Genome Taxonomy Database,Abstract Summary The Genome Taxonomy Database Toolkit (GTDB-Tk) provides objective taxonomic assignments for bacterial and archaeal genomes based on the GTDB. GTDB-Tk is computationally efficient and able to classify thousands of draft genomes in parallel. Here we demonstrate the accuracy of the GTDB-Tk taxonomic assignments by evaluating its performance on a phylogenetically diverse set of 10 156 bacterial and archaeal metagenome-assembled genomes. Availability and implementation GTDB-Tk is implemented in Python and licenced under the GNU General Public Licence v3.0. Source code and documentation are available at: https://github.com/ecogenomics/gtdbtk. Supplementary information Supplementary data are available at Bioinformatics online.,c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f1626,2006,2006-11-19
s2022,p2022,HMDB 4.0: the human metabolome database for 2018,"Abstract The Human Metabolome Database or HMDB (www.hmdb.ca) is a web-enabled metabolomic database containing comprehensive information about human metabolites along with their biological roles, physiological concentrations, disease associations, chemical reactions, metabolic pathways, and reference spectra. First described in 2007, the HMDB is now considered the standard metabolomic resource for human metabolic studies. Over the past decade the HMDB has continued to grow and evolve in response to emerging needs for metabolomics researchers and continuing changes in web standards. This year's update, HMDB 4.0, represents the most significant upgrade to the database in its history. For instance, the number of fully annotated metabolites has increased by nearly threefold, the number of experimental spectra has grown by almost fourfold and the number of illustrated metabolic pathways has grown by a factor of almost 60. Significant improvements have also been made to the HMDB’s chemical taxonomy, chemical ontology, spectral viewing, and spectral/text searching tools. A great deal of brand new data has also been added to HMDB 4.0. This includes large quantities of predicted MS/MS and GC–MS reference spectral data as well as predicted (physiologically feasible) metabolite structures to facilitate novel metabolite identification. Additional information on metabolite-SNP interactions and the influence of drugs on metabolite levels (pharmacometabolomics) has also been added. Many other important improvements in the content, the interface, and the performance of the HMDB website have been made and these should greatly enhance its ease of use and its potential applications in nutrition, biochemistry, clinical chemistry, clinical genetics, medicine, and metabolomics science.",c32,International Conference on Software Technology: Methods and Tools,cp32,accepted,f1627,2010,2010-05-10
s2023,p2023,A global database of COVID-19 vaccinations,Abstract content goes here ...,j48,Nature Human Behaviour,jv48,accepted,f1628,2019,2019-09-04
s2024,p2024,2016 update of the PRIDE database and its related tools,"The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development 'PRIDE Cluster' and 'PRIDE Proteomes', which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.",c77,Networks,cp77,accepted,f1629,2019,2019-09-22
s2025,p2025,Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments,"Most face databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, background, camera quality, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database, Labeled Faces in the Wild, is provided as an aid in studying the latter, unconstrained, recognition problem. The database contains labeled face photographs spanning the range of conditions typically encountered in everyday life. The database exhibits “natural” variability in factors such as pose, lighting, race, accessories, occlusions, and background. In addition to describing the details of the database, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. We provide baseline results, including results of a state of the art face recognition system combined with a face alignment system. To facilitate experimentation on the database, we provide several parallel databases, including an aligned version.",c31,International Conference on Evaluation & Assessment in Software Engineering,cp31,accepted,f1630,2022,2022-08-26
s2026,p2026,CARD 2020: antibiotic resistome surveillance with the comprehensive antibiotic resistance database,"Abstract The Comprehensive Antibiotic Resistance Database (CARD; https://card.mcmaster.ca) is a curated resource providing reference DNA and protein sequences, detection models and bioinformatics tools on the molecular basis of bacterial antimicrobial resistance (AMR). CARD focuses on providing high-quality reference data and molecular sequences within a controlled vocabulary, the Antibiotic Resistance Ontology (ARO), designed by the CARD biocuration team to integrate with software development efforts for resistome analysis and prediction, such as CARD’s Resistance Gene Identifier (RGI) software. Since 2017, CARD has expanded through extensive curation of reference sequences, revision of the ontological structure, curation of over 500 new AMR detection models, development of a new classification paradigm and expansion of analytical tools. Most notably, a new Resistomes & Variants module provides analysis and statistical summary of in silico predicted resistance variants from 82 pathogens and over 100 000 genomes. By adding these resistance variants to CARD, we are able to summarize predicted resistance using the information included in CARD, identify trends in AMR mobility and determine previously undescribed and novel resistance variants. Here, we describe updates and recent expansions to CARD and its biocuration process, including new resources for community biocuration of AMR molecular reference data.",c106,Chinese Conference on Biometric Recognition,cp106,accepted,f1631,2016,2016-08-28
s2027,p2027,The Cambridge Structural Database: a quarter of a million crystal structures and rising.,"The Cambridge Structural Database (CSD) now contains data for more than a quarter of a million small-molecule crystal structures. The information content of the CSD, together with methods for data acquisition, processing and validation, are summarized, with particular emphasis on the chemical information added by CSD editors. Nearly 80% of new structural data arrives electronically, mostly in CIF format, and the CCDC acts as the official crystal structure data depository for 51 major journals. The CCDC now maintains both a CIF archive (more than 73,000 CIFs dating from 1996), as well as the distributed binary CSD archive; the availability of data in both archives is discussed. A statistical survey of the CSD is also presented and projections concerning future accession rates indicate that the CSD will contain at least 500,000 crystal structures by the year 2010.",j314,Acta Crystallographica Section B Structural Science,jv314,accepted,f1632,2016,2016-01-16
s2028,p2028,"Greengenes, a Chimera-Checked 16S rRNA Gene Database and Workbench Compatible with ARB","ABSTRACT A 16S rRNA gene database (http://greengenes.lbl.gov ) addresses limitations of public repositories by providing chimera screening, standard alignment, and taxonomic classification using multiple published taxonomies. It was found that there is incongruent taxonomic nomenclature among curators even at the phylum level. Putative chimeras were identified in 3% of environmental sequences and in 0.2% of records derived from isolates. Environmental sequences were classified into 100 phylum-level lineages in the Archaea and Bacteria.",j344,Applied and Environmental Microbiology,jv344,accepted,f1633,2012,2012-11-28
s2029,p2029,CDD/SPARCLE: the conserved domain database in 2020,"As NLM's Conserved Domain Database (CDD) enters its 20th year of operations as a publicly available resource, CDD curation staff continues to develop hierarchical classifications of widely distributed protein domain families, and to record conserved sites associated with molecular function, so that they can be mapped onto user queries in support of hypothesis-driven biomolecular research. CDD offers both an archive of pre-computed domain annotations as well as live search services for both single protein or nucleotide queries and larger sets of protein query sequences. CDD staff has continued to characterize protein families via conserved domain architectures and has built up a significant corpus of curated domain architectures in support of naming bacterial proteins in RefSeq. These architecture definitions are available via SPARCLE, the Subfamily Protein Architecture Labeling Engine. CDD can be accessed at https://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",c14,International Conference on Exploring Services Science,cp14,accepted,f1634,2016,2016-06-05
s2030,p2030,Freebase: a collaboratively created graph database for structuring human knowledge,"Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1635,2007,2007-06-04
s2031,p2031,The Carbohydrate-Active EnZymes database (CAZy): an expert resource for Glycogenomics,"The Carbohydrate-Active Enzyme (CAZy) database is a knowledge-based resource specialized in the enzymes that build and breakdown complex carbohydrates and glycoconjugates. As of September 2008, the database describes the present knowledge on 113 glycoside hydrolase, 91 glycosyltransferase, 19 polysaccharide lyase, 15 carbohydrate esterase and 52 carbohydrate-binding module families. These families are created based on experimentally characterized proteins and are populated by sequences from public databases with significant similarity. Protein biochemical information is continuously curated based on the available literature and structural information. Over 6400 proteins have assigned EC numbers and 700 proteins have a PDB structure. The classification (i) reflects the structural features of these enzymes better than their sole substrate specificity, (ii) helps to reveal the evolutionary relationships between these enzymes and (iii) provides a convenient framework to understand mechanistic properties. This resource has been available for over 10 years to the scientific community, contributing to information dissemination and providing a transversal nomenclature to glycobiologists. More recently, this resource has been used to improve the quality of functional predictions of a number genome projects by providing expert annotation. The CAZy resource resides at URL: http://www.cazy.org/.",c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,cp20,accepted,f1636,2014,2014-05-25
s2033,p2033,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",Abstract content goes here ...,j5,Genome Biology,jv5,accepted,f1637,2020,2020-12-09
s2034,p2034,An improved method of constructing a database of monthly climate observations and associated high‐resolution grids,"A database of monthly climate observations from meteorological stations is constructed. The database includes six climate elements and extends over the global land surface. The database is checked for inhomogeneities in the station records using an automated method that refines previous methods by using incomplete and partially overlapping records and by detecting inhomogeneities with opposite signs in different seasons. The method includes the development of reference series using neighbouring stations. Information from different sources about a single station may be combined, even without an overlapping period, using a reference series. Thus, a longer station record may be obtained and fragmentation of records reduced. The reference series also enables 1961–90 normals to be calculated for a larger proportion of stations.",c30,IEEE Aerospace Conference,cp30,accepted,f1638,2006,2006-12-13
s2037,p2037,The InterPro protein families and domains database: 20 years on,"Abstract The InterPro database (https://www.ebi.ac.uk/interpro/) provides an integrative classification of protein sequences into families, and identifies functionally important domains and conserved sites. InterProScan is the underlying software that allows protein and nucleic acid sequences to be searched against InterPro's signatures. Signatures are predictive models which describe protein families, domains or sites, and are provided by multiple databases. InterPro combines signatures representing equivalent families, domains or sites, and provides additional information such as descriptions, literature references and Gene Ontology (GO) terms, to produce a comprehensive resource for protein classification. Founded in 1999, InterPro has become one of the most widely used resources for protein family annotation. Here, we report the status of InterPro (version 81.0) in its 20th year of operation, and its associated software, including updates to database content, the release of a new website and REST API, and performance improvements in InterProScan.",c58,Australian Software Engineering Conference,cp58,accepted,f1639,2021,2021-01-29
s2042,p2042,Learning Deep Features for Scene Recognition using Places Database,"Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.",c2,International Symposium on Intelligent Data Analysis,cp2,accepted,f1640,2014,2014-06-21
s2043,p2043,SCOP: a structural classification of proteins database for the investigation of sequences and structures.,Abstract content goes here ...,j173,Journal of Molecular Biology,jv173,accepted,f1641,2012,2012-03-20
s2045,p2045,A fast quantum mechanical algorithm for database search,"were proposed in the early 1980’s [Benioff80] and shown to be at least as powerful as classical computers an important but not surprising result, since classical computers, at the deepest level, ultimately follow the laws of quantum mechanics. The description of quantum mechanical computers was formalized in the late 80’s and early 90’s [Deutsch85][BB92] [BV93] [Yao93] and they were shown to be more powerful than classical computers on various specialized problems. In early 1994, [Shor94] demonstrated that a quantum mechanical computer could efficiently solve a well-known problem for which there was no known efficient algorithm using classical computers. This is the problem of integer factorization, i.e. testing whether or not a given integer, N, is prime, in a time which is a finite power of o (logN) . ----------------------------------------------",c88,Symposium on the Theory of Computing,cp88,accepted,f1642,2014,2014-08-31
s2046,p2046,dbSNP: the NCBI database of genetic variation,"In response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the National Center for Biotechnology Information (NCBI) has established the dbSNP database [S.T.Sherry, M.Ward and K. Sirotkin (1999) Genome Res., 9, 677-679]. Submissions to dbSNP will be integrated with other sources of information at NCBI such as GenBank, PubMed, LocusLink and the Human Genome Project data. The complete contents of dbSNP are available to the public at website: http://www.ncbi.nlm.nih.gov/SNP. The complete contents of dbSNP can also be downloaded in multiple formats via anonymous FTP at ftp://ncbi.nlm.nih.gov/snp/.",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f1643,2017,2017-02-27
s2047,p2047,The COG database: an updated version includes eukaryotes,Abstract content goes here ...,j85,BMC Bioinformatics,jv85,accepted,f1644,2007,2007-12-20
s2048,p2048,Introduction to WordNet: An On-line Lexical Database,"Standard alphabetical procedures for organizing lexical information put together words that are spelled alike and scatter words with similar or related meanings haphazardly through the list. Unfortunately, there is no obvious alternative, no other simple way for lexicographers to keep track of what has been done or for readers to find the word they are looking for. But a frequent objection to this solution is that finding things on an alphabetical list can be tedious and time-consuming. Many people who would like to refer to a dictionary decide not to bother with it because finding the information would interrupt their work and break their train of thought.",c109,International Conference on Mobile Data Management,cp109,accepted,f1645,2014,2014-12-16
s2050,p2050,The HITRAN 2008 molecular spectroscopic database,Abstract content goes here ...,c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f1646,2011,2011-10-09
s2051,p2051,"PlantCARE, a database of plant cis-acting regulatory elements and a portal to tools for in silico analysis of promoter sequences","PlantCARE is a database of plant cis-acting regulatory elements, enhancers and repressors. Regulatory elements are represented by positional matrices, consensus sequences and individual sites on particular promoter sequences. Links to the EMBL, TRANSFAC and MEDLINE databases are provided when available. Data about the transcription sites are extracted mainly from the literature, supplemented with an increasing number of in silico predicted data. Apart from a general description for specific transcription factor sites, levels of confidence for the experimental evidence, functional information and the position on the promoter are given as well. New features have been implemented to search for plant cis-acting regulatory elements in a query sequence. Furthermore, links are now provided to a new clustering and motif search method to investigate clusters of co-expressed genes. New regulatory elements can be sent automatically and will be added to the database after curation. The PlantCARE relational database is available via the World Wide Web at http://sphinx.rug.ac.be:8080/PlantCARE/.",c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f1647,2004,2004-08-06
s2052,p2052,CDD: NCBI's conserved domain database,"NCBI's CDD, the Conserved Domain Database, enters its 15th year as a public resource for the annotation of proteins with the location of conserved domain footprints. Going forward, we strive to improve the coverage and consistency of domain annotation provided by CDD. We maintain a live search system as well as an archive of pre-computed domain annotation for sequences tracked in NCBI's Entrez protein database, which can be retrieved for single sequences or in bulk. We also maintain import procedures so that CDD contains domain models and domain definitions provided by several collections available in the public domain, as well as those produced by an in-house curation effort. The curation effort aims at increasing coverage and providing finer-grained classifications of common protein domains, for which a wealth of functional and structural data has become available. CDD curation generates alignment models of representative sequence fragments, which are in agreement with domain boundaries as observed in protein 3D structure, and which model the structurally conserved cores of domain families as well as annotate conserved features. CDD can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",c107,British Machine Vision Conference,cp107,accepted,f1648,2012,2012-08-29
s2053,p2053,HMDB: A large video database for human motion recognition,"With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.",c94,Vision,cp94,accepted,f1649,2020,2020-09-02
s2054,p2054,Fundamentals of Database Systems,"From the Publisher: 
Fundamentals of Database Systems combines clear explanations of theory and design, broad coverage of models and real systems, and excellent examples with up-to-date introductions to modern database technologies. This edition is completely revised and updated, and reflects the latest trends in technological and application development. Professors Elmasri and Navathe focus on the relational model and include coverage of recent object-oriented developments. They also address advanced modeling and system enhancements in the areas of active databases, temporal and spatial databases, and multimedia information systems. This edition also surveys the latest application areas of data warehousing, data mining, web databases, digital libraries, GIS, and genome databases. New to the Third Edition 
Reorganized material on data modeling to clearly separate entity relationship modeling, extended entity relationship modeling, and object-oriented modeling Expanded coverage of the object-oriented and object/relational approach to data management, including ODMG and SQL3 Uses examples from real database systems including OracleTM and Microsoft AccessAE Includes discussion of decision support applications of data warehousing and data mining, as well as emerging technologies of web databases, multimedia, and mobile databases Covers advanced modeling in the areas of active, temporal, and spatial databases Provides coverage of issues of physical database tuning Discusses current database application areas of GIS, genome, and digital libraries",c39,International Conference on Global Software Engineering,cp39,accepted,f1650,2020,2020-03-13
s2055,p2055,The COG database: a tool for genome-scale analysis of protein functions and evolution,"Rational classification of proteins encoded in sequenced genomes is critical for making the genome sequences maximally useful for functional and evolutionary studies. The database of Clusters of Orthologous Groups of proteins (COGs) is an attempt on a phylogenetic classification of the proteins encoded in 21 complete genomes of bacteria, archaea and eukaryotes (http://www. ncbi.nlm. nih.gov/COG). The COGs were constructed by applying the criterion of consistency of genome-specific best hits to the results of an exhaustive comparison of all protein sequences from these genomes. The database comprises 2091 COGs that include 56-83% of the gene products from each of the complete bacterial and archaeal genomes and approximately 35% of those from the yeast Saccharomyces cerevisiae genome. The COG database is accompanied by the COGNITOR program that is used to fit new proteins into the COGs and can be applied to functional and phylogenetic annotation of newly sequenced genomes.",c97,Interspeech,cp97,accepted,f1651,2004,2004-01-15
s2056,p2056,TCMSP: a database of systems pharmacology for drug discovery from herbal medicines,Abstract content goes here ...,j347,Journal of Cheminformatics,jv347,accepted,f1652,2022,2022-08-19
s2057,p2057,"Repbase Update, a database of repetitive elements in eukaryotic genomes",Abstract content goes here ...,j348,Mobile DNA,jv348,accepted,f1653,2016,2016-02-06
s2058,p2058,LabelMe: A Database and Web-Based Tool for Image Annotation,Abstract content goes here ...,j349,International Journal of Computer Vision,jv349,accepted,f1654,2019,2019-07-03
s2059,p2059,"NCBI reference sequences (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins","The National Center for Biotechnology Information (NCBI) Reference Sequence (RefSeq) database (http://www.ncbi.nlm.nih.gov/RefSeq/) provides a non-redundant collection of sequences representing genomic data, transcripts and proteins. Although the goal is to provide a comprehensive dataset representing the complete sequence information for any given species, the database pragmatically includes sequence data that are currently publicly available in the archival databases. The database incorporates data from over 2400 organisms and includes over one million proteins representing significant taxonomic diversity spanning prokaryotes, eukaryotes and viruses. Nucleotide and protein sequences are explicitly linked, and the sequences are linked to other resources including the NCBI Map Viewer and Gene. Sequences are annotated to include coding regions, conserved domains, variation, references, names, database cross-references, and other features using a combined approach of collaboration and other input from the scientific community, automated annotation, propagation from GenBank and curation by NCBI staff.",c40,IEEE International Conference on Software Maintenance and Evolution,cp40,accepted,f1655,2013,2013-11-02
s2060,p2060,CARD 2017: expansion and model-centric curation of the comprehensive antibiotic resistance database,"The Comprehensive Antibiotic Resistance Database (CARD; http://arpcard.mcmaster.ca) is a manually curated resource containing high quality reference data on the molecular basis of antimicrobial resistance (AMR), with an emphasis on the genes, proteins and mutations involved in AMR. CARD is ontologically structured, model centric, and spans the breadth of AMR drug classes and resistance mechanisms, including intrinsic, mutation-driven and acquired resistance. It is built upon the Antibiotic Resistance Ontology (ARO), a custom built, interconnected and hierarchical controlled vocabulary allowing advanced data sharing and organization. Its design allows the development of novel genome analysis tools, such as the Resistance Gene Identifier (RGI) for resistome prediction from raw genome sequence. Recent improvements include extensive curation of additional reference sequences and mutations, development of a unique Model Ontology and accompanying AMR detection models to power sequence analysis, new visualization tools, and expansion of the RGI for detection of emergent AMR threats. CARD curation is updated monthly based on an interplay of manual literature curation, computational text mining, and genome analysis.",c67,Enterprise Application Integration,cp67,accepted,f1656,2002,2002-06-18
s2061,p2061,miRDB: an online database for prediction of functional microRNA targets,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that act as master regulators in many biological processes. miRNAs function mainly by downregulating the expression of their gene targets. Thus, accurate prediction of miRNA targets is critical for characterization of miRNA functions. To this end, we have developed an online database, miRDB, for miRNA target prediction and functional annotations. Recently, we have performed major updates for miRDB. Specifically, by employing an improved algorithm for miRNA target prediction, we now present updated transcriptome-wide target prediction data in miRDB, including 3.5 million predicted targets regulated by 7000 miRNAs in five species. Further, we have implemented the new prediction algorithm into a web server, allowing custom target prediction with user-provided sequences. Another new database feature is the prediction of cell-specific miRNA targets. miRDB now hosts the expression profiles of over 1000 cell lines and presents target prediction data that are tailored for specific cell models. At last, a new web query interface has been added to miRDB for prediction of miRNA functions by integrative analysis of target prediction and Gene Ontology data. All data in miRDB are freely accessible at http://mirdb.org.",c88,Symposium on the Theory of Computing,cp88,accepted,f1657,2014,2014-06-02
s2062,p2062,IPD-IMGT/HLA Database,"Abstract The IPD-IMGT/HLA Database, http://www.ebi.ac.uk/ipd/imgt/hla/, currently contains over 25 000 allele sequence for 45 genes, which are located within the Major Histocompatibility Complex (MHC) of the human genome. This region is the most polymorphic region of the human genome, and the levels of polymorphism seen exceed most other genes. Some of the genes have several thousand variants and are now termed hyperpolymorphic, rather than just simply polymorphic. The IPD-IMGT/HLA Database has provided a stable, highly accessible, user-friendly repository for this information, providing the scientific and medical community access to the many variant sequences of this gene system, that are critical for the successful outcome of transplantation. The number of currently known variants, and dramatic increase in the number of new variants being identified has necessitated a dedicated resource with custom tools for curation and publication. The challenge for the database is to continue to provide a highly curated database of sequence variants, while supporting the increased number of submissions and complexity of sequences. In order to do this, traditional methods of accessing and presenting data will be challenged, and new methods will need to be utilized to keep pace with new discoveries.",c87,European Conference on Computer Vision,cp87,accepted,f1658,2014,2014-10-14
s2063,p2063,SUN database: Large-scale scene recognition from abbey to zoo,"Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.",c69,International Conference on Parallel Processing,cp69,accepted,f1659,2010,2010-03-07
s2064,p2064,The AR face database,Abstract content goes here ...,c6,Americas Conference on Information Systems,cp6,accepted,f1660,2007,2007-03-07
s2066,p2066,Human Protein Reference Database—2009 update,"Human Protein Reference Database (HPRD—http://www.hprd.org/), initially described in 2003, is a database of curated proteomic information pertaining to human proteins. We have recently added a number of new features in HPRD. These include PhosphoMotif Finder, which allows users to find the presence of over 320 experimentally verified phosphorylation motifs in proteins of interest. Another new feature is a protein distributed annotation system—Human Proteinpedia (http://www.humanproteinpedia.org/)—through which laboratories can submit their data, which is mapped onto protein entries in HPRD. Over 75 laboratories involved in proteomics research have already participated in this effort by submitting data for over 15 000 human proteins. The submitted data includes mass spectrometry and protein microarray-derived data, among other data types. Finally, HPRD is also linked to a compendium of human signaling pathways developed by our group, NetPath (http://www.netpath.org/), which currently contains annotations for several cancer and immune signaling pathways. Since the last update, more than 5500 new protein sequences have been added, making HPRD a comprehensive resource for studying the human proteome.",c64,Experimental Software Engineering Network,cp64,accepted,f1661,2014,2014-03-01
s2067,p2067,DEAP: A Database for Emotion Analysis ;Using Physiological Signals,"We present a multimodal data set for the analysis of human affective states. The electroencephalogram (EEG) and peripheral physiological signals of 32 participants were recorded as each watched 40 one-minute long excerpts of music videos. Participants rated each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. For 22 of the 32 participants, frontal face video was also recorded. A novel method for stimuli selection is proposed using retrieval by affective tags from the last.fm website, video highlight detection, and an online assessment tool. An extensive analysis of the participants' ratings during the experiment is presented. Correlates between the EEG signal frequencies and the participants' ratings are investigated. Methods and results are presented for single-trial classification of arousal, valence, and like/dislike ratings using the modalities of EEG, peripheral physiological signals, and multimedia content analysis. Finally, decision fusion of the classification results from different modalities is performed. The data set is made publicly available and we encourage other researchers to use it for testing their own affective state estimation methods.",j350,IEEE Transactions on Affective Computing,jv350,accepted,f1662,2022,2022-03-05
s2068,p2068,CDD: a Conserved Domain Database for the functional annotation of proteins,"NCBI’s Conserved Domain Database (CDD) is a resource for the annotation of protein sequences with the location of conserved domain footprints, and functional sites inferred from these footprints. CDD includes manually curated domain models that make use of protein 3D structure to refine domain models and provide insights into sequence/structure/function relationships. Manually curated models are organized hierarchically if they describe domain families that are clearly related by common descent. As CDD also imports domain family models from a variety of external sources, it is a partially redundant collection. To simplify protein annotation, redundant models and models describing homologous families are clustered into superfamilies. By default, domain footprints are annotated with the corresponding superfamily designation, on top of which specific annotation may indicate high-confidence assignment of family membership. Pre-computed domain annotation is available for proteins in the Entrez/Protein dataset, and a novel interface, Batch CD-Search, allows the computation and download of annotation for large sets of protein queries. CDD can be accessed via http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml.",c43,ACM Symposium on Applied Computing,cp43,accepted,f1663,2001,2001-01-09
s2069,p2069,Plant cis-acting regulatory DNA elements (PLACE) database: 1999,"PLACE (http://www.dna.affrc.go.jp/htdocs/PLACE/) is a database of nucleotide sequence motifs found in plant cis-acting regulatory DNA elements. Motifs were extracted from previously published reports on genes in vascular plants. In addition to the motifs originally reported, their variations in other genes or in other plant species in later reports are also compiled. Documents for each motif in the PLACE database contains, in addition to a motif sequence, a brief definition and description of each motif, and relevant literature with PubMed ID numbers and GenBank accession numbers where available. Users can search their query sequences for cis-elements using the Signal Scan program at our web site. The results will be reported in one of the three forms. Clicking the PLACE accession numbers in the result report will open the pertinent motif document. Clicking the PubMed or GenBank accession number in the document will allow users to access to these databases, and to read the of the literature or the annotation in the DNA database. This report summarizes the present status of this database and available tools.",c50,International Conference on Automated Software Engineering,cp50,accepted,f1664,2008,2008-05-29
s2070,p2070,"Repbase Update, a database of eukaryotic repetitive elements","Repbase Update is a comprehensive database of repetitive elements from diverse eukaryotic organisms. Currently, it contains over 3600 annotated sequences representing different families and subfamilies of repeats, many of which are unreported anywhere else. Each sequence is accompanied by a short description and references to the original contributors. Repbase Update includes Repbase Reports, an electronic journal publishing newly discovered transposable elements, and the Transposon Pub, a web-based browser of selected chromosomal maps of transposable elements. Sequences from Repbase Update are used to screen and annotate repetitive elements using programs such as Censor and RepeatMasker. Repbase Update is available on the worldwide web at http://www.girinst.org/Repbase_Update.html.",j351,Cytogenetic and Genome Research,jv351,accepted,f1665,2003,2003-11-17
s2071,p2071,ChEMBL: a large-scale bioactivity database for drug discovery,"ChEMBL is an Open Data database containing binding, functional and ADMET information for a large number of drug-like bioactive compounds. These data are manually abstracted from the primary published literature on a regular basis, then further curated and standardized to maximize their quality and utility across a wide range of chemical biology and drug-discovery research problems. Currently, the database contains 5.4 million bioactivity measurements for more than 1 million compounds and 5200 protein targets. Access is available through a web-based interface, data downloads and web services at: https://www.ebi.ac.uk/chembldb.",c59,British Computer Society Conference on Human-Computer Interaction,cp59,accepted,f1666,2019,2019-12-02
s2074,p2074,The HITRAN2012 molecular spectroscopic database,Abstract content goes here ...,c36,Conference on Software Engineering Education and Training,cp36,accepted,f1667,2015,2015-02-16
s2075,p2075,Protein backbone angle restraints from searching a database for chemical shift and sequence homology,Abstract content goes here ...,j353,Journal of Biomolecular NMR,jv353,accepted,f1668,2003,2003-08-09
s2076,p2076,The Immune Epitope Database (IEDB): 2018 update,"Abstract The Immune Epitope Database (IEDB, iedb.org) captures experimental data confined in figures, text and tables of the scientific literature, making it freely available and easily searchable to the public. The scope of the IEDB extends across immune epitope data related to all species studied and includes antibody, T cell, and MHC binding contexts associated with infectious, allergic, autoimmune, and transplant related diseases. Having been publicly accessible for >10 years, the recent focus of the IEDB has been improved query and reporting functionality to meet the needs of our users to access and summarize data that continues to grow in quantity and complexity. Here we present an update on our current efforts and future goals.",c114,IEEE International Conference on Robotics and Automation,cp114,accepted,f1669,2011,2011-02-06
s2078,p2078,The BioGRID interaction database: 2019 update,"Abstract The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the curation and archival storage of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2018 (build 3.4.164), BioGRID contains records for 1 598 688 biological interactions manually annotated from 55 809 publications for 71 species, as classified by an updated set of controlled vocabularies for experimental detection methods. BioGRID also houses records for >700 000 post-translational modification sites. BioGRID now captures chemical interaction data, including chemical–protein interactions for human drug targets drawn from the DrugBank database and manually curated bioactive compounds reported in the literature. A new dedicated aspect of BioGRID annotates genome-wide CRISPR/Cas9-based screens that report gene–phenotype and gene–gene relationships. An extension of the BioGRID resource called the Open Repository for CRISPR Screens (ORCS) database (https://orcs.thebiogrid.org) currently contains over 500 genome-wide screens carried out in human or mouse cell lines. All data in BioGRID is made freely available without restriction, is directly downloadable in standard formats and can be readily incorporated into existing applications via our web service platforms. BioGRID data are also freely distributed through partner model organism databases and meta-databases.",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f1670,2002,2002-06-15
s2082,p2082,ONCOMINE: a cancer microarray database and integrated data-mining platform.,Abstract content goes here ...,c11,Hawaii International Conference on System Sciences,cp11,accepted,f1671,2006,2006-03-24
s2083,p2083,The Cambridge Structural Database in retrospect and prospect.,"The Cambridge Crystallographic Data Centre (CCDC) was established in 1965 to record numerical, chemical and bibliographic data relating to published organic and metal-organic crystal structures. The Cambridge Structural Database (CSD) now stores data for nearly 700,000 structures and is a comprehensive and fully retrospective historical archive of small-molecule crystallography. Nearly 40,000 new structures are added each year. As X-ray crystallography celebrates its centenary as a subject, and the CCDC approaches its own 50th year, this article traces the origins of the CCDC as a publicly funded organization and its onward development into a self-financing charitable institution. Principally, however, we describe the growth of the CSD and its extensive associated software system, and summarize its impact and value as a basis for research in structural chemistry, materials science and the life sciences, including drug discovery and drug development. Finally, the article considers the CCDC's funding model in relation to open access and open data paradigms.",c17,International Conference on Enterprise Information Systems,cp17,accepted,f1672,2008,2008-03-05
s2084,p2084,The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web],"In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.",j327,IEEE Signal Processing Magazine,jv327,accepted,f1673,2011,2011-08-28
s2087,p2087,IEMOCAP: interactive emotional dyadic motion capture database,Abstract content goes here ...,j354,Language Resources and Evaluation,jv354,accepted,f1674,2012,2012-05-08
s2088,p2088,HMDB 3.0—The Human Metabolome Database in 2013,"The Human Metabolome Database (HMDB) (www.hmdb.ca) is a resource dedicated to providing scientists with the most current and comprehensive coverage of the human metabolome. Since its first release in 2007, the HMDB has been used to facilitate research for nearly 1000 published studies in metabolomics, clinical biochemistry and systems biology. The most recent release of HMDB (version 3.0) has been significantly expanded and enhanced over the 2009 release (version 2.0). In particular, the number of annotated metabolite entries has grown from 6500 to more than 40 000 (a 600% increase). This enormous expansion is a result of the inclusion of both ‘detected’ metabolites (those with measured concentrations or experimental confirmation of their existence) and ‘expected’ metabolites (those for which biochemical pathways are known or human intake/exposure is frequent but the compound has yet to be detected in the body). The latest release also has greatly increased the number of metabolites with biofluid or tissue concentration data, the number of compounds with reference spectra and the number of data fields per entry. In addition to this expansion in data quantity, new database visualization tools and new data content have been added or enhanced. These include better spectral viewing tools, more powerful chemical substructure searches, an improved chemical taxonomy and better, more interactive pathway maps. This article describes these enhancements to the HMDB, which was previously featured in the 2009 NAR Database Issue. (Note to referees, HMDB 3.0 will go live on 18 September 2012.).",c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f1675,2001,2001-02-12
s2089,p2089,MODOMICS: a database of RNA modification pathways. 2017 update,"Abstract MODOMICS is a database of RNA modifications that provides comprehensive information concerning the chemical structures of modified ribonucleosides, their biosynthetic pathways, the location of modified residues in RNA sequences, and RNA-modifying enzymes. In the current database version, we included the following new features and data: extended mass spectrometry and liquid chromatography data for modified nucleosides; links between human tRNA sequences and MINTbase - a framework for the interactive exploration of mitochondrial and nuclear tRNA fragments; new, machine-friendly system of unified abbreviations for modified nucleoside names; sets of modified tRNA sequences for two bacterial species, updated collection of mammalian tRNA modifications, 19 newly identified modified ribonucleosides and 66 functionally characterized proteins involved in RNA modification. Data from MODOMICS have been linked to the RNAcentral database of RNA sequences. MODOMICS is available at http://modomics.genesilico.pl.",c4,Annual Conference on Genetic and Evolutionary Computation,cp4,accepted,f1676,2019,2019-12-26
s2090,p2090,The IPD and IMGT/HLA database: allele variant databases,"The Immuno Polymorphism Database (IPD) was developed to provide a centralized system for the study of polymorphism in genes of the immune system. Through the IPD project we have established a central platform for the curation and publication of locus-specific databases involved either directly or related to the function of the Major Histocompatibility Complex in a number of different species. We have collaborated with specialist groups or nomenclature committees that curate the individual sections before they are submitted to IPD for online publication. IPD consists of five core databases, with the IMGT/HLA Database as the primary database. Through the work of the various nomenclature committees, the HLA Informatics Group and in collaboration with the European Bioinformatics Institute we are able to provide public access to this data through the website http://www.ebi.ac.uk/ipd/. The IPD project continues to develop with new tools being added to address scientific developments, such as Next Generation Sequencing, and to address user feedback and requests. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the immunogenetics community, and the wider research and clinical communities.",c2,International Symposium on Intelligent Data Analysis,cp2,accepted,f1677,2014,2014-08-12
s2091,p2091,The Pfam protein families database,"Pfam is a large collection of protein multiple sequence alignments and profile hidden Markov models. Pfam is available on the WWW in the UK at http://www.sanger.ac.uk/Software/Pfam/, in Sweden at http://www.cgr.ki.se/Pfam/ and in the US at http://pfam.wustl.edu/. The latest version (4.3) of Pfam contains 1815 families. These Pfam families match 63% of proteins in SWISS-PROT 37 and TrEMBL 9. For complete genomes Pfam currently matches up to half of the proteins. Genomic DNA can be directly searched against the Pfam library using the Wise2 package.",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1678,2015,2015-05-19
s2092,p2092,APD3: the antimicrobial peptide database as a tool for research and education,"The antimicrobial peptide database (APD, http://aps.unmc.edu/AP/) is an original database initially online in 2003. The APD2 (2009 version) has been regularly updated and further expanded into the APD3. This database currently focuses on natural antimicrobial peptides (AMPs) with defined sequence and activity. It includes a total of 2619 AMPs with 261 bacteriocins from bacteria, 4 AMPs from archaea, 7 from protists, 13 from fungi, 321 from plants and 1972 animal host defense peptides. The APD3 contains 2169 antibacterial, 172 antiviral, 105 anti-HIV, 959 antifungal, 80 antiparasitic and 185 anticancer peptides. Newly annotated are AMPs with antibiofilm, antimalarial, anti-protist, insecticidal, spermicidal, chemotactic, wound healing, antioxidant and protease inhibiting properties. We also describe other searchable annotations, including target pathogens, molecule-binding partners, post-translational modifications and animal models. Amino acid profiles or signatures of natural AMPs are important for peptide classification, prediction and design. Finally, we summarize various database applications in research and education.",c6,Americas Conference on Information Systems,cp6,accepted,f1679,2007,2007-01-13
s2095,p2095,"The MEROPS database of proteolytic enzymes, their substrates and inhibitors in 2017 and a comparison with peptidases in the PANTHER database","Abstract The MEROPS database (http://www.ebi.ac.uk/merops/) is an integrated source of information about peptidases, their substrates and inhibitors. The hierarchical classification is: protein-species, family, clan, with an identifier at each level. The MEROPS website moved to the EMBL-EBI in 2017, requiring refactoring of the code-base and services provided. The interface to sequence searching has changed and the MEROPS protein sequence libraries can be searched at the EMBL-EBI with HMMER, FastA and BLASTP. Cross-references have been established between MEROPS and the PANTHER database at both the family and protein-species level, which will help to improve curation and coverage between the resources. Because of the increasing size of the MEROPS sequence collection, in future only sequences of characterized proteins, and from completely sequenced genomes of organisms of evolutionary, medical or commercial significance will be added. As an example, peptidase homologues in four proteomes from the Asgard superphylum of Archaea have been identified and compared to other archaean, bacterial and eukaryote proteomes. This has given insights into the origins and evolution of peptidase families, including an expansion in the number of proteasome components in Asgard archaeotes and as organisms increase in complexity. Novel structures for proteasome complexes in archaea are postulated.",c100,ACM SIGMOD Conference,cp100,accepted,f1680,2010,2010-08-27
s2097,p2097,The Gene Expression Omnibus Database,Abstract content goes here ...,c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1681,2007,2007-01-03
s2098,p2098,"Image database TID2013: Peculiarities, results and perspectives",Abstract content goes here ...,j356,Signal processing. Image communication,jv356,accepted,f1682,2014,2014-01-25
s2099,p2099,A New Database on Financial Development and Structure,"The authors introduce a new database of indicators of financial development and structure across countries and over time. This database is unique in that it unites a variety of indicators that measure the size, activity, and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, by introducing indicators of the size and activity of non bank financial institutions, and by presenting measures of the size of bond and primary equity markets. The compiled data permit the construction of financial structure indicators to measure whether, for example, a country's banks are larger, more active, and more efficient than its stock markets. These indicators can then be used to investigate the empirical link between the legal, regulatory, and policy environment and indicators of financial structure. They can also be used to analyze the implications of financial structure for economic growth. The authors describe the sources and construction of, and the intuition behind, different indicators and present descriptive statistics.",c56,European Conference on Software Process Improvement,cp56,accepted,f1683,2016,2016-01-10
s2100,p2100,Presentation and validation of the Radboud Faces Database,"Many research fields concerned with the processing of information contained in human faces would benefit from face stimulus sets in which specific facial characteristics are systematically varied while other important picture characteristics are kept constant. Specifically, a face database in which displayed expressions, gaze direction, and head orientation are parametrically varied in a complete factorial design would be highly useful in many research domains. Furthermore, these stimuli should be standardised in several important, technical aspects. The present article presents the freely available Radboud Faces Database offering such a stimulus set, containing both Caucasian adult and children images. This face database is described both procedurally and in terms of content, and a validation study concerning its most important characteristics is presented. In the validation study, all frontal images were rated with respect to the shown facial expression, intensity of expression, clarity of expression, genuineness of expression, attractiveness, and valence. The results show very high recognition of the intended facial expressions.",c113,International Conference on Image Analysis and Processing,cp113,accepted,f1684,2002,2002-10-25
s2102,p2102,The BioGRID interaction database: 2017 update,"The Biological General Repository for Interaction Datasets (BioGRID: https://thebiogrid.org) is an open access database dedicated to the annotation and archival of protein, genetic and chemical interactions for all major model organism species and humans. As of September 2016 (build 3.4.140), the BioGRID contains 1 072 173 genetic and protein interactions, and 38 559 post-translational modifications, as manually annotated from 48 114 publications. This dataset represents interaction records for 66 model organisms and represents a 30% increase compared to the previous 2015 BioGRID update. BioGRID curates the biomedical literature for major model organism species, including humans, with a recent emphasis on central biological processes and specific human diseases. To facilitate network-based approaches to drug discovery, BioGRID now incorporates 27 501 chemical–protein interactions for human drug targets, as drawn from the DrugBank database. A new dynamic interaction network viewer allows the easy navigation and filtering of all genetic and protein interaction data, as well as for bioactive compounds and their established targets. BioGRID data are directly downloadable without restriction in a variety of standardized formats and are freely distributed through partner model organism databases and meta-databases.",c62,International Conference on Software Reuse,cp62,accepted,f1685,2006,2006-01-04
s2104,p2104,"Federated database systems for managing distributed, heterogeneous, and autonomous databases","A federated database system (FDBS) is a collection of cooperating database systems that are autonomous and possibly heterogeneous. In this paper, we define a reference architecture for distributed database management systems from system and schema viewpoints and show how various FDBS architectures can be developed. We then define a methodology for developing one of the popular architectures of an FDBS. Finally, we discuss critical issues related to developing and operating an FDBS.",c114,IEEE International Conference on Robotics and Automation,cp114,accepted,f1686,2011,2011-01-08
s2105,p2105,ZINC - A Free Database of Commercially Available Compounds for Virtual Screening,"A critical barrier to entry into structure-based virtual screening is the lack of a suitable, easy to access database of purchasable compounds. We have therefore prepared a library of 727,842 molecules, each with 3D structure, using catalogs of compounds from vendors (the size of this library continues to grow). The molecules have been assigned biologically relevant protonation states and are annotated with properties such as molecular weight, calculated LogP, and number of rotatable bonds. Each molecule in the library contains vendor and purchasing information and is ready for docking using a number of popular docking programs. Within certain limits, the molecules are prepared in multiple protonation states and multiple tautomeric forms. In one format, multiple conformations are available for the molecules. This database is available for free download (http://zinc.docking.org) in several common file formats including SMILES, mol2, 3D SDF, and DOCK flexibase format. A Web-based query tool incorporating a molecular drawing interface enables the database to be searched and browsed and subsets to be created. Users can process their own molecules by uploading them to a server. Our hope is that this database will bring virtual screening libraries to a wide community of structural biologists and medicinal chemists.",j358,Journal of Chemical Information and Modeling,jv358,accepted,f1687,2006,2006-01-21
s2107,p2107,The CELEX Lexical Database (CD-ROM),Abstract content goes here ...,c24,Decision Support Systems,cp24,accepted,f1688,2013,2013-09-13
s2108,p2108,Development of a global land cover characteristics database and IGBP DISCover from 1 km AVHRR data,"Researchers from the U.S. Geological Survey, University of Nebraska-Lincoln and the European Commission's Joint Research Centre, Ispra, Italy produced a 1 km resolution global land cover characteristics database for use in a wide range of continental-to global-scale environmental studies. This database provides a unique view of the broad patterns of the biogeographical and ecoclimatic diversity of the global land surface, and presents a detailed interpretation of the extent of human development. The project was carried out as an International Geosphere-Biosphere Programme, Data and Information Systems (IGBP-DIS) initiative. The IGBP DISCover global land cover product is an integral component of the global land cover database. DISCover includes 17 general land cover classes defined to meet the needs of IGBP core science projects. A formal accuracy assessment of the DISCover data layer will be completed in 1998. The 1 km global land cover database was developed through a continent-by-continent unsupervised classification of 1 km monthly Advanced Very High Resolution Radiometer (AVHRR) Normalized Difference Vegetation Index (NDVI) composites covering 1992-1993. Extensive post-classification stratification was necessary to resolve spectral/temporal confusion between disparate land cover types. The complete global database consists of 961 seasonal land cover regions that capture patterns of land cover, seasonality and relative primary productivity. The seasonal land cover regions were aggregated to produce seven separate land cover data sets used for global environmental modelling and assessment. The data sets include IGBP DISCover, U.S. Geological Survey Anderson System, Simple Biosphere Model, Simple Biosphere Model 2, Biosphere-Atmosphere Transfer Scheme, Olson Ecosystems and Running Global Remote Sensing Land Cover. The database also includes all digital sources that were used in the classification. The complete database can be sourced from the website: http://edcwww.cr.usgs.gov/landdaac/glcc/glcc.html.",c72,Intelligent Systems in Molecular Biology,cp72,accepted,f1689,2003,2003-08-07
s2109,p2109,FaceWarehouse: A 3D Facial Expression Database for Visual Computing,"We present FaceWarehouse, a database of 3D facial expressions for visual computing applications. We use Kinect, an off-the-shelf RGBD camera, to capture 150 individuals aged 7-80 from various ethnic backgrounds. For each person, we captured the RGBD data of her different expressions, including the neutral expression and 19 other expressions such as mouth-opening, smile, kiss, etc. For every RGBD raw data record, a set of facial feature points on the color image such as eye corners, mouth contour, and the nose tip are automatically localized, and manually adjusted if better accuracy is required. We then deform a template facial mesh to fit the depth data as closely as possible while matching the feature points on the color image to their corresponding points on the mesh. Starting from these fitted face meshes, we construct a set of individual-specific expression blendshapes for each person. These meshes with consistent topology are assembled as a rank-3 tensor to build a bilinear face model with two attributes: identity and expression. Compared with previous 3D facial databases, for every person in our database, there is a much richer matching collection of expressions, enabling depiction of most human facial actions. We demonstrate the potential of FaceWarehouse for visual computing with four applications: facial image manipulation, face component transfer, real-time performance-based facial image animation, and facial animation retargeting from video to image.",j245,IEEE Transactions on Visualization and Computer Graphics,jv245,accepted,f1690,2009,2009-11-01
s2110,p2110,Mouse Genome Database (MGD) 2019,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the community model organism genetic and genome resource for the laboratory mouse. MGD is the authoritative source for biological reference data sets related to mouse genes, gene functions, phenotypes, and mouse models of human disease. MGD is the primary outlet for official gene, allele and mouse strain nomenclature based on the guidelines set by the International Committee on Standardized Nomenclature for Mice. In this report we describe significant enhancements to MGD, including two new graphical user interfaces: (i) the Multi Genome Viewer for exploring the genomes of multiple mouse strains and (ii) the Phenotype-Gene Expression matrix which was developed in collaboration with the Gene Expression Database (GXD) and allows researchers to compare gene expression and phenotype annotations for mouse genes. Other recent improvements include enhanced efficiency of our literature curation processes and the incorporation of Transcriptional Start Site (TSS) annotations from RIKEN’s FANTOM 5 initiative.",c9,Pacific Symposium on Biocomputing,cp9,accepted,f1691,2009,2009-10-06
s2112,p2112,FRED-MD: A Monthly Database for Macroeconomic Research,"This article describes a large, monthly frequency, macroeconomic database with the goal of establishing a convenient starting point for empirical analysis that requires “big data.” The dataset mimics the coverage of those already used in the literature but has three appealing features. First, it is designed to be updated monthly using the Federal Reserve Economic Data (FRED) database. Second, it will be publicly accessible, facilitating comparison of related research and replication of empirical work. Third, it will relieve researchers from having to manage data changes and revisions. We show that factors extracted from our dataset share the same predictive content as those based on various vintages of the so-called Stock–Watson dataset. In addition, we suggest that diffusion indexes constructed as the partial sum of the factor estimates can potentially be useful for the study of business cycle chronology. Supplementary materials for this article are available online.",c112,Very Large Data Bases Conference,cp112,accepted,f1692,2018,2018-12-27
s2113,p2113,Principles of Distributed Database Systems,Abstract content goes here ...,c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f1693,2011,2011-05-25
s2116,p2116,The Comparative Toxicogenomics Database: update 2017,"The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) provides information about interactions between chemicals and gene products, and their relationships to diseases. Core CTD content (chemical-gene, chemical-disease and gene-disease interactions manually curated from the literature) are integrated with each other as well as with select external datasets to generate expanded networks and predict novel associations. Today, core CTD includes more than 30.5 million toxicogenomic connections relating chemicals/drugs, genes/proteins, diseases, taxa, Gene Ontology (GO) annotations, pathways, and gene interaction modules. In this update, we report a 33% increase in our core data content since 2015, describe our new exposure module (that harmonizes exposure science information with core toxicogenomic data) and introduce a novel dataset of GO-disease inferences (that identify common molecular underpinnings for seemingly unrelated pathologies). These advancements centralize and contextualize real-world chemical exposures with molecular pathways to help scientists generate testable hypotheses in an effort to understand the etiology and mechanisms underlying environmentally influenced diseases.",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f1694,2008,2008-02-13
s2117,p2117,PhenoScanner: a database of human genotype–phenotype associations,"Abstract Summary: PhenoScanner is a curated database of publicly available results from large-scale genetic association studies. This tool aims to facilitate ‘phenome scans’, the cross-referencing of genetic variants with many phenotypes, to help aid understanding of disease pathways and biology. The database currently contains over 350 million association results and over 10 million unique genetic variants, mostly single nucleotide polymorphisms. It is accompanied by a web-based tool that queries the database for associations with user-specified variants, providing results according to the same effect and non-effect alleles for each input variant. The tool provides the option of searching for trait associations with proxies of the input variants, calculated using the European samples from 1000 Genomes and Hapmap. Availability and Implementation: PhenoScanner is available at www.phenoscanner.medschl.cam.ac.uk. Contact: jrs95@medschl.cam.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",c18,Conference on Innovative Data Systems Research,cp18,accepted,f1695,2012,2012-06-26
s2118,p2118,Rfam 12.0: updates to the RNA families database,"The Rfam database (available at http://rfam.xfam.org) is a collection of non-coding RNA families represented by manually curated sequence alignments, consensus secondary structures and annotation gathered from corresponding Wikipedia, taxonomy and ontology resources. In this article, we detail updates and improvements to the Rfam data and website for the Rfam 12.0 release. We describe the upgrade of our search pipeline to use Infernal 1.1 and demonstrate its improved homology detection ability by comparison with the previous version. The new pipeline is easier for users to apply to their own data sets, and we illustrate its ability to annotate RNAs in genomic and metagenomic data sets of various sizes. Rfam has been expanded to include 260 new families, including the well-studied large subunit ribosomal RNA family, and for the first time includes information on short sequence- and structure-based RNA motifs present within families.",c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f1696,2005,2005-01-19
s2119,p2119,The SIDER database of drugs and side effects,"Unwanted side effects of drugs are a burden on patients and a severe impediment in the development of new drugs. At the same time, adverse drug reactions (ADRs) recorded during clinical trials are an important source of human phenotypic data. It is therefore essential to combine data on drugs, targets and side effects into a more complete picture of the therapeutic mechanism of actions of drugs and the ways in which they cause adverse reactions. To this end, we have created the SIDER (‘Side Effect Resource’, http://sideeffects.embl.de) database of drugs and ADRs. The current release, SIDER 4, contains data on 1430 drugs, 5880 ADRs and 140 064 drug–ADR pairs, which is an increase of 40% compared to the previous version. For more fine-grained analyses, we extracted the frequency with which side effects occur from the package inserts. This information is available for 39% of drug–ADR pairs, 19% of which can be compared to the frequency under placebo treatment. SIDER furthermore contains a data set of drug indications, extracted from the package inserts using Natural Language Processing. These drug indications are used to reduce the rate of false positives by identifying medical terms that do not correspond to ADRs.",c19,ACM Conference on Economics and Computation,cp19,accepted,f1697,2002,2002-10-23
s2120,p2120,The BioGRID interaction database: 2015 update,"The Biological General Repository for Interaction Datasets (BioGRID: http://thebiogrid.org) is an open access database that houses genetic and protein interactions curated from the primary biomedical literature for all major model organism species and humans. As of September 2014, the BioGRID contains 749 912 interactions as drawn from 43 149 publications that represent 30 model organisms. This interaction count represents a 50% increase compared to our previous 2013 BioGRID update. BioGRID data are freely distributed through partner model organism databases and meta-databases and are directly downloadable in a variety of formats. In addition to general curation of the published literature for the major model species, BioGRID undertakes themed curation projects in areas of particular relevance for biomedical sciences, such as the ubiquitin-proteasome system and various human disease-associated interaction networks. BioGRID curation is coordinated through an Interaction Management System (IMS) that facilitates the compilation interaction records through structured evidence codes, phenotype ontologies, and gene annotation. The BioGRID architecture has been improved in order to support a broader range of interaction and post-translational modification types, to allow the representation of more complex multi-gene/protein interactions, to account for cellular phenotypes through structured ontologies, to expedite curation through semi-automated text-mining approaches, and to enhance curation quality control.",c39,International Conference on Global Software Engineering,cp39,accepted,f1698,2020,2020-10-24
s2122,p2122,The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): a completed reference database of lung nodules on CT scans.,"PURPOSE
The development of computer-aided diagnostic (CAD) methods for lung nodule detection, classification, and quantitative assessment can be facilitated through a well-characterized repository of computed tomography (CT) scans. The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) completed such a database, establishing a publicly available reference for the medical imaging research community. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process.


METHODS
Seven academic centers and eight medical imaging companies collaborated to identify, address, and resolve challenging organizational, technical, and clinical issues to provide a solid foundation for a robust database. The LIDC/IDRI Database contains 1018 cases, each of which includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories (""nodule > or =3 mm,"" ""nodule <3 mm,"" and ""non-nodule > or =3 mm""). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.


RESULTS
The Database contains 7371 lesions marked ""nodule"" by at least one radiologist. 2669 of these lesions were marked ""nodule > or =3 mm"" by at least one radiologist, of which 928 (34.7%) received such marks from all four radiologists. These 2669 lesions include nodule outlines and subjective nodule characteristic ratings.


CONCLUSIONS
The LIDC/IDRI Database is expected to provide an essential medical imaging research resource to spur CAD development, validation, and dissemination in clinical practice.",j361,Medical Physics (Lancaster),jv361,accepted,f1699,2013,2013-09-02
s2123,p2123,circBase: a database for circular RNAs,"Recently, several laboratories have reported thousands of circular RNAs (circRNAs) in animals. Numerous circRNAs are highly stable and have specific spatiotemporal expression patterns. Even though a function for circRNAs is unknown, these features make circRNAs an interesting class of RNAs as possible biomarkers and for further research. We developed a database and website, “circBase,” where merged and unified data sets of circRNAs and the evidence supporting their expression can be accessed, downloaded, and browsed within the genomic context. circBase also provides scripts to identify known and novel circRNAs in sequencing data. The database is freely accessible through the web server at http://www.circbase.org/.",j362,RNA: A publication of the RNA Society,jv362,accepted,f1700,2018,2018-08-06
s2124,p2124,The Database of Interacting Proteins: 2004 update,"The Database of Interacting Proteins (http://dip.doe-mbi.ucla.edu) aims to integrate the diverse body of experimental evidence on protein-protein interactions into a single, easily accessible online database. Because the reliability of experimental evidence varies widely, methods of quality assessment have been developed and utilized to identify the most reliable subset of the interactions. This CORE set can be used as a reference when evaluating the reliability of high-throughput protein-protein interaction data sets, for development of prediction methods, as well as in the studies of the properties of protein interaction networks.",c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f1701,2022,2022-01-22
s2125,p2125,The immune epitope database (IEDB) 3.0,"The IEDB, www.iedb.org, contains information on immune epitopes—the molecular targets of adaptive immune responses—curated from the published literature and submitted by National Institutes of Health funded epitope discovery efforts. From 2004 to 2012 the IEDB curation of journal articles published since 1960 has caught up to the present day, with >95% of relevant published literature manually curated amounting to more than 15 000 journal articles and more than 704 000 experiments to date. The revised curation target since 2012 has been to make recent research findings quickly available in the IEDB and thereby ensure that it continues to be an up-to-date resource. Having gathered a comprehensive dataset in the IEDB, a complete redesign of the query and reporting interface has been performed in the IEDB 3.0 release to improve how end users can access this information in an intuitive and biologically accurate manner. We here present this most recent release of the IEDB and describe the user testing procedures as well as the use of external ontologies that have enabled it.",c75,International Conference on Machine Learning,cp75,accepted,f1702,2005,2005-08-05
s2126,p2126,HMDB: the Human Metabolome Database,"The Human Metabolome Database (HMDB) is currently the most complete and comprehensive curated collection of human metabolite and human metabolism data in the world. It contains records for more than 2180 endogenous metabolites with information gathered from thousands of books, journal articles and electronic databases. In addition to its comprehensive literature-derived data, the HMDB also contains an extensive collection of experimental metabolite concentration data compiled from hundreds of mass spectra (MS) and Nuclear Magnetic resonance (NMR) metabolomic analyses performed on urine, blood and cerebrospinal fluid samples. This is further supplemented with thousands of NMR and MS spectra collected on purified, reference metabolites. Each metabolite entry in the HMDB contains an average of 90 separate data fields including a comprehensive compound description, names and synonyms, structural information, physico-chemical data, reference NMR and MS spectra, biofluid concentrations, disease associations, pathway information, enzyme data, gene sequence data, SNP and mutation data as well as extensive links to images, references and other public databases. Extensive searching, relational querying and data browsing tools are also provided. The HMDB is designed to address the broad needs of biochemists, clinical chemists, physicians, medical geneticists, nutritionists and members of the metabolomics community. The HMDB is available at:",c97,Interspeech,cp97,accepted,f1703,2004,2004-07-24
s2127,p2127,SYFPEITHI: database for MHC ligands and peptide motifs,Abstract content goes here ...,j363,Immunogenetics,jv363,accepted,f1704,2018,2018-09-25
s2128,p2128,Spanner: Google's Globally-Distributed Database,"Spanner is Google’s scalable, multiversion, globally distributed, and synchronously replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions. This article describes how Spanner is structured, its feature set, the rationale underlying various design decisions, and a novel time API that exposes clock uncertainty. This API and its implementation are critical to supporting external consistency and a variety of powerful features: nonblocking reads in the past, lock-free snapshot transactions, and atomic schema changes, across all of Spanner.",c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f1705,2006,2006-08-09
s2129,p2129,A Database for Handwritten Text Recognition Research,"An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, 5000 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each image was scanned from mail in a working post office at 300 pixels/in in 8-bit gray scale on a high-quality flat bed digitizer. The data were unconstrained for the writer, style, and method of preparation. These characteristics help overcome the limitations of earlier databases that contained only isolated characters or were prepared in a laboratory setting under prescribed circumstances. Also, the database is divided into explicit training and testing sets to facilitate the sharing of results among researchers as well as performance comparisons. >",j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,jv299,accepted,f1706,2020,2020-09-29
s2130,p2130,Database Resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 37 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include iCn3D, MutaBind, and the Antimicrobial Resistance Gene Reference Database; and resources that were updated in the past year include My Bibliography, SciENcv, the Pathogen Detection Project, Assembly, Genome, the Genome Data Viewer, BLAST and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",c8,The Compass,cp8,accepted,f1707,2016,2016-07-01
s2131,p2131,Systemic Banking Crises Database,Abstract content goes here ...,c62,International Conference on Software Reuse,cp62,accepted,f1708,2006,2006-01-21
s2133,p2133,Tmbase-A database of membrane spanning protein segments,Abstract content goes here ...,c83,International Conference on Computer Graphics and Interactive Techniques,cp83,accepted,f1709,2015,2015-03-22
s2134,p2134,The SWISS-PROT protein sequence database and its supplement TrEMBL in 2000,"SWISS-PROT is a curated protein sequence database which strives to provide a high level of annotation (such as the description of the function of a protein, its domains structure, post-translational modifications, variants, etc.), a minimal level of redundancy and high level of integration with other databases. Recent developments of the database include format and content enhancements, cross-references to additional databases, new documentation files and improvements to TrEMBL, a computer-annotated supplement to SWISS-PROT. TrEMBL consists of entries in SWISS-PROT-like format derived from the translation of all coding sequences (CDSs) in the EMBL Nucleotide Sequence Database, except the CDSs already included in SWISS-PROT. We also describe the Human Proteomics Initiative (HPI), a major project to annotate all known human sequences according to the quality standards of SWISS-PROT. SWISS-PROT is available at: http://www.expasy.ch/sprot/ and http://www.ebi.ac.uk/swissprot/",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1710,2015,2015-09-17
s2135,p2135,Universal database search tool for proteomics,Abstract content goes here ...,j107,Nature Communications,jv107,accepted,f1711,2019,2019-08-25
s2136,p2136,A database of German emotional speech,"The article describes a database of emotional speech. Ten actors (5 female and 5 male) simulated the emotions, producing 10 German utterances (5 short and 5 longer sentences) which could be used in everyday communication and are interpretable in all applied emotions. The recordings were taken in an anechoic chamber with high-quality recording equipment. In addition to the sound electro-glottograms were recorded. The speech material comprises about 800 sentences (seven emotions * ten actors * ten sentences + some second versions). The complete database was evaluated in a perception test regarding the recognisability of emotions and their naturalness. Utterances recognised better than 80% and judged as natural by more than 60% of the listeners were phonetically labelled in a narrow transcription with special markers for voice-quality, phonatory and articulatory settings and articulatory features. The database can be accessed by the public via the internet (http://www.expressive-speech.net/emodb/).",c97,Interspeech,cp97,accepted,f1712,2004,2004-12-24
s2137,p2137,"The CMU Pose, Illumination, and Expression Database","In the Fall of 2000, we collected a database of more than 40,000 facial images of 68 people. Using the Carnegie Mellon University 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this the CMU pose, illumination, and expression (PIE) database. We describe the imaging hardware, the collection procedure, the organization of the images, several possible uses, and how to obtain the database.",j299,IEEE Transactions on Pattern Analysis and Machine Intelligence,jv299,accepted,f1713,2020,2020-04-15
s2138,p2138,TRY – a global database of plant traits,"Plant traits – the morphological, anatomical, physiological, biochemical and phenological characteristics of plants and their organs – determine how primary producers respond to environmental factors, affect other trophic levels, influence ecosystem processes and services and provide a link from species richness to ecosystem functional diversity. Trait data thus represent the raw material for a wide range of research from evolutionary biology, community and functional ecology to biogeography. Here we present the global database initiative named TRY, which has united a wide range of the plant trait research community worldwide and gained an unprecedented buy‐in of trait data: so far 93 trait databases have been contributed. The data repository currently contains almost three million trait entries for 69 000 out of the world's 300 000 plant species, with a focus on 52 groups of traits characterizing the vegetative and regeneration stages of the plant life cycle, including growth, dispersal, establishment and persistence. A first data analysis shows that most plant traits are approximately log‐normally distributed, with widely differing ranges of variation across traits. Most trait variation is between species (interspecific), but significant intraspecific variation is also documented, up to 40% of the overall variation. Plant functional types (PFTs), as commonly used in vegetation models, capture a substantial fraction of the observed variation – but for several traits most variation occurs within PFTs, up to 75% of the overall variation. In the context of vegetation models these traits would better be represented by state variables rather than fixed parameter values. The improved availability of plant trait data in the unified global database is expected to support a paradigm shift from species to trait‐based ecology, offer new opportunities for synthetic plant trait research and enable a more realistic and empirically grounded representation of terrestrial vegetation in Earth system models.",j364,Global Change Biology,jv364,accepted,f1714,2007,2007-09-06
s2139,p2139,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfill the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. Recent developments include the following. A community annotation project has been instigated in which acknowledged experts are invited to contribute summaries for peptidases. Software has been written to provide an Internet-based data entry form. Contributors are acknowledged on the relevant web page. A new display showing the intron/exon structures of eukaryote peptidase genes and the phasing of the junctions has been implemented. It is now possible to filter the list of peptidases from a completely sequenced bacterial genome for a particular strain of the organism. The MEROPS filing pipeline has been altered to circumvent the restrictions imposed on non-interactive blastp searches, and a HMMER search using specially generated alignments to maximize the distribution of organisms returned in the search results has been added.",c59,British Computer Society Conference on Human-Computer Interaction,cp59,accepted,f1715,2019,2019-04-26
s2140,p2140,"Development and validation of a global database of lakes, reservoirs and wetlands",Abstract content goes here ...,c50,International Conference on Automated Software Engineering,cp50,accepted,f1716,2008,2008-06-04
s2141,p2141,Genevestigator V3: A Reference Expression Database for the Meta-Analysis of Transcriptomes,"The Web-based software tool Genevestigator provides powerful tools for biologists to explore gene expression across a wide variety of biological contexts. Its first releases, however, were limited by the scaling ability of the system architecture, multiorganism data storage and analysis capability, and availability of computationally intensive analysis methods. Genevestigator V3 is a novel meta-analysis system resulting from new algorithmic and software development using a client/server architecture, large-scale manual curation and quality control of microarray data for several organisms, and curation of pathway data for mouse and Arabidopsis. In addition to improved querying features, Genevestigator V3 provides new tools to analyze the expression of genes in many different contexts, to identify biomarker genes, to cluster genes into expression modules, and to model expression responses in the context of metabolic and regulatory networks. Being a reference expression database with user-friendly tools, Genevestigator V3 facilitates discovery research and hypothesis validation.",c3,Frontiers in Education Conference,cp3,accepted,f1717,2016,2016-07-29
s2142,p2142,The UCSC Genome Browser Database,"The University of California Santa Cruz (UCSC) Genome Browser Database is an up to date source for genome sequence data integrated with a large collection of related annotations. The database is optimized to support fast interactive performance with the web-based UCSC Genome Browser, a tool built on top of the database for rapid visualization and querying of the data at many levels. The annotations for a given genome are displayed in the browser as a series of tracks aligned with the genomic sequence. Sequence data and annotations may also be viewed in a text-based tabular format or downloaded as tab-delimited flat files. The Genome Browser Database, browsing tools and downloadable data files can all be found on the UCSC Genome Bioinformatics website (http://genome.ucsc.edu), which also contains links to documentation and related technical information.",c21,Grid Computing Environments,cp21,accepted,f1718,2005,2005-07-01
s2143,p2143,2016 update of the PRIDE database and its related tools,"The PRoteomics IDEntifications (PRIDE) database is one of the world-leading data repositories of mass spectrometry (MS)-based proteomics data. Since the beginning of 2014, PRIDE Archive (http://www.ebi.ac.uk/pride/archive/) is the new PRIDE archival system, replacing the original PRIDE database. Here we summarize the developments in PRIDE resources and related tools since the previous update manuscript in the Database Issue in 2013. PRIDE Archive constitutes a complete redevelopment of the original PRIDE, comprising a new storage backend, data submission system and web interface, among other components. PRIDE Archive supports the most-widely used PSI (Proteomics Standards Initiative) data standard formats (mzML and mzIdentML) and implements the data requirements and guidelines of the ProteomeXchange Consortium. The wide adoption of ProteomeXchange within the community has triggered an unprecedented increase in the number of submitted data sets (around 150 data sets per month). We outline some statistics on the current PRIDE Archive data contents. We also report on the status of the PRIDE related stand-alone tools: PRIDE Inspector, PRIDE Converter 2 and the ProteomeXchange submission tool. Finally, we will give a brief update on the resources under development ‘PRIDE Cluster’ and ‘PRIDE Proteomes’, which provide a complementary view and quality-scored information of the peptide and protein identification data available in PRIDE Archive.",j102,Nucleic Acids Research,jv102,accepted,f1719,2002,2002-02-05
s2144,p2144,The Transporter Classification Database (TCDB): recent advances,"The Transporter Classification Database (TCDB; http://www.tcdb.org) is a freely accessible reference database for transport protein research, which provides structural, functional, mechanistic, evolutionary and disease/medical information about transporters from organisms of all types. TCDB is the only transport protein classification database adopted by the International Union of Biochemistry and Molecular Biology (IUBMB). It consists of more than 10 000 non-redundant transport systems with more than 11 000 reference citations, classified into over 1000 transporter families. Transporters in TCDB can be single or multi-component systems, categorized in a functional/phylogenetic hierarchical system of classes, subclasses, families, subfamilies and transport systems. TCDB also includes updated software designed to analyze the distinctive features of transport proteins, extending its usefulness. Here we present a comprehensive update of the database contents and features and summarize recent discoveries recorded in TCDB.",c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f1720,2006,2006-03-13
s2145,p2145,Saccharomyces Genome Database: the genomics resource of budding yeast,"The Saccharomyces Genome Database (SGD, http://www.yeastgenome.org) is the community resource for the budding yeast Saccharomyces cerevisiae. The SGD project provides the highest-quality manually curated information from peer-reviewed literature. The experimental results reported in the literature are extracted and integrated within a well-developed database. These data are combined with quality high-throughput results and provided through Locus Summary pages, a powerful query engine and rich genome browser. The acquisition, integration and retrieval of these data allow SGD to facilitate experimental design and analysis by providing an encyclopedia of the yeast genome, its chromosomal features, their functions and interactions. Public access to these data is provided to researchers and educators via web pages designed for optimal ease of use.",c64,Experimental Software Engineering Network,cp64,accepted,f1721,2014,2014-08-05
s2146,p2146,The UCSC Genome Browser database: 2015 update,"Launched in 2001 to showcase the draft human genome assembly, the UCSC Genome Browser database (http://genome.ucsc.edu) and associated tools continue to grow, providing a comprehensive resource of genome assemblies and annotations to scientists and students worldwide. Highlights of the past year include the release of a browser for the first new human genome reference assembly in 4 years in December 2013 (GRCh38, UCSC hg38), a watershed comparative genomics annotation (100-species multiple alignment and conservation) and a novel distribution mechanism for the browser (GBiB: Genome Browser in a Box). We created browsers for new species (Chinese hamster, elephant shark, minke whale), ‘mined the web’ for DNA sequences and expanded the browser display with stacked color graphs and region highlighting. As our user community increasingly adopts the UCSC track hub and assembly hub representations for sharing large-scale genomic annotation data sets and genome sequencing projects, our menu of public data hubs has tripled.",c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f1722,2016,2016-10-29
s2147,p2147,An Overview of the Global Historical Climatology Network-Daily Database,"AbstractA database is described that has been designed to fulfill the need for daily climate data over global land areas. The dataset, known as Global Historical Climatology Network (GHCN)-Daily, was developed for a wide variety of potential applications, including climate analysis and monitoring studies that require data at a daily time resolution (e.g., assessments of the frequency of heavy rainfall, heat wave duration, etc.). The dataset contains records from over 80 000 stations in 180 countries and territories, and its processing system produces the official archive for U.S. daily data. Variables commonly include maximum and minimum temperature, total daily precipitation, snowfall, and snow depth; however, about two-thirds of the stations report precipitation only. Quality assurance checks are routinely applied to the full dataset, but the data are not homogenized to account for artifacts associated with the various eras in reporting practice at any particular station (i.e., for changes in systematic...",c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f1723,2015,2015-06-19
s2149,p2149,An Overview of the China Meteorological Administration Tropical Cyclone Database,"AbstractThe China Meteorological Administration (CMA)’s tropical cyclone (TC) database includes not only the best-track dataset but also TC-induced wind and precipitation data. This article summarizes the characteristics and key technical details of the CMA TC database. In addition to the best-track data, other phenomena that occurred with the TCs are also recorded in the dataset, such as the subcenters, extratropical transitions, outer-range severe winds associated with TCs over the South China Sea, and coastal severe winds associated with TCs landfalling in China. These data provide additional information for researchers. The TC-induced wind and precipitation data, which map the distribution of severe wind and rainfall, are also helpful for investigating the impacts of TCs. The study also considers the changing reliability of the various data sources used since the database was created and the potential causes of temporal and spatial inhomogeneities within the datasets. Because of the greater number of ...",c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f1724,2005,2005-04-29
s2150,p2150,Comet: An open‐source MS/MS sequence database search tool,"Proteomics research routinely involves identifying peptides and proteins via MS/MS sequence database search. Thus the database search engine is an integral tool in many proteomics research groups. Here, we introduce the Comet search engine to the existing landscape of commercial and open‐source database search tools. Comet is open source, freely available, and based on one of the original sequence database search tools that has been widely used for many years.",j235,Proteomics,jv235,accepted,f1725,2006,2006-09-19
s2151,p2151,Database resources of the National Center for Biotechnology Information,"The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. Additional NCBI resources focus on literature (PubMed Central (PMC), Bookshelf and PubReader), health (ClinVar, dbGaP, dbMHC, the Genetic Testing Registry, HIV-1/Human Protein Interaction Database and MedGen), genomes (BioProject, Assembly, Genome, BioSample, dbSNP, dbVar, Epigenomics, the Map Viewer, Nucleotide, Probe, RefSeq, Sequence Read Archive, the Taxonomy Browser and the Trace Archive), genes (Gene, Gene Expression Omnibus (GEO), HomoloGene, PopSet and UniGene), proteins (Protein, the Conserved Domain Database (CDD), COBALT, Conserved Domain Architecture Retrieval Tool (CDART), the Molecular Modeling Database (MMDB) and Protein Clusters) and chemicals (Biosystems and the PubChem suite of small molecule databases). The Entrez system provides search and retrieval operations for most of these databases. Augmenting many of the web applications are custom implementations of the BLAST program optimized to search specialized datasets. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",c23,International Conference on Open and Big Data,cp23,accepted,f1726,2012,2012-08-16
s2152,p2152,The Dartmouth Stellar Evolution Database,"The ever-expanding depth and quality of photometric and spectroscopic observations of stellar populations increase the need for theoretical models in regions of age-composition parameter space that are largely unexplored at present. Stellar evolution models that employ the most advanced physics and cover a wide range of compositions are needed to extract the most information from current observations of both resolved and unresolved stellar populations. The Dartmouth Stellar Evolution Database is a collection of stellar evolution tracks and isochrones that spans a range of [Fe/H] from –2.5 to +0.5, [α/Fe] from –0.2 to +0.8 (for [Fe/H] ⩽ 0) or +0.2 (for [Fe/H] > 0), and initial He mass fractions from Y = 0.245 to 0.40. Stellar evolution tracks were computed for masses between 0.1 and 4 M☉, allowing isochrones to be generated for ages as young as 250 Myr. For the range in masses where the core He flash occurs, separate He-burning tracks were computed starting from the zero age horizontal branch. The tracks and isochrones have been transformed to the observational plane in a variety of photometric systems including standard UBV(RI)C, Stromgren uvby, SDSS ugriz, 2MASS JHKs, and HST ACS/WFC and WFPC2. The Dartmouth Stellar Evolution Database is accessible through a Web site at http://stellar.dartmouth.edu/~models/ where all tracks, isochrones, and additional files can be downloaded.",c63,IEEE International Software Metrics Symposium,cp63,accepted,f1727,2008,2008-05-19
s2153,p2153,Notes on CEPII’s Distances Measures: The GeoDist Database,"GeoDist makes available the exhaustive set of gravity variables used in Mayer and Zignago (2005). GeoDist provides several geographical variables, in particular bilateral distances measured using citylevel data to assess the geographic distribution of population inside each nation. We have calculated different measures of bilateral distances available for most countries across the world (225 countries in the current version of the database). For most of them, different calculations of “intra-national distances” are also available. The GeoDist webpage provides two distinct files: a country-specific one (geo_cepii)and a dyadic one (dist_cepii) including a set of different distance and common dummy variables used in gravity equations to identify particular links between countries such as colonial past, common languages, contiguity. We try to improve upon the existing similar datasets in terms of geographical coverage, quality of measurement and number of variables provided.",c29,International Conference on Software Engineering,cp29,accepted,f1728,2015,2015-05-12
s2154,p2154,NGA-West2 Database,"The NGA-West2 project database expands on its predecessor to include worldwide ground motion data recorded from shallow crustal earthquakes in active tectonic regimes post-2000 and a set of small-to-moderate-magnitude earthquakes in California between 1998 and 2011. The database includes 21,336 (mostly) three-component records from 599 events. The parameter space covered by the database is M 3.0 to M 7.9, closest distance of 0.05 to 1,533 km, and site time-averaged shear-wave velocity in the top 30 m of VS30 = 94 m/s to 2,100 m/s (although data becomes sparse for distances >400 km and VS30 > 1,200 m/s or <150 m/s). The database includes uniformly processed time series and response spectral ordinates for 111 periods ranging from 0.01 s to 20 s at 11 damping ratios. Ground motions and metadata for source, path, and site conditions were subject to quality checks by ground motion prediction equation developers and topical working groups.",c84,The Web Conference,cp84,accepted,f1729,2006,2006-11-15
s2155,p2155,PPDB: The Paraphrase Database,"We present the 1.0 release of our paraphrase database, PPDB. Its English portion, PPDB:Eng, contains over 220 million paraphrase pairs, consisting of 73 million phrasal and 8 million lexical paraphrases, as well as 140 million paraphrase patterns, which capture many meaning-preserving syntactic transformations. The paraphrases are extracted from bilingual parallel corpora totaling over 100 million sentence pairs and over 2 billion English words. We also release PPDB:Spa, a collection of 196 million Spanish paraphrases. Each paraphrase pair in PPDB contains a set of associated scores, including paraphrase probabilities derived from the bitext data and a variety of monolingual distributional similarity scores computed from the Google n-grams and the Annotated Gigaword corpus. Our release includes pruning tools that allow users to determine their own precision/recall tradeoff.",c98,North American Chapter of the Association for Computational Linguistics,cp98,accepted,f1730,2009,2009-09-13
s2156,p2156,The PLANTS Database,Abstract content goes here ...,c77,Networks,cp77,accepted,f1731,2019,2019-09-08
s2157,p2157,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases,"The MetaCyc database (MetaCyc.org) is a freely accessible comprehensive database describing metabolic pathways and enzymes from all domains of life. The majority of MetaCyc pathways are small-molecule metabolic pathways that have been experimentally determined. MetaCyc contains more than 2400 pathways derived from >46 000 publications, and is the largest curated collection of metabolic pathways. BioCyc (BioCyc.org) is a collection of 5700 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems, and pathway-hole fillers. The BioCyc website offers a variety of tools for querying and analyzing PGDBs, including Omics Viewers and tools for comparative analysis. This article provides an update of new developments in MetaCyc and BioCyc during the last two years, including addition of Gibbs free energy values for compounds and reactions; redesign of the primary gene/protein page; addition of a tool for creating diagrams containing multiple linked pathways; several new search capabilities, including searching for genes based on sequence patterns, searching for databases based on an organism's phenotypes, and a cross-organism search; and a metabolite identifier translation service.",c32,International Conference on Software Technology: Methods and Tools,cp32,accepted,f1732,2010,2010-10-14
s2158,p2158,"PATRIC, the bacterial bioinformatics database and analysis resource","The Pathosystems Resource Integration Center (PATRIC) is the all-bacterial Bioinformatics Resource Center (BRC) (http://www.patricbrc.org). A joint effort by two of the original National Institute of Allergy and Infectious Diseases-funded BRCs, PATRIC provides researchers with an online resource that stores and integrates a variety of data types [e.g. genomics, transcriptomics, protein–protein interactions (PPIs), three-dimensional protein structures and sequence typing data] and associated metadata. Datatypes are summarized for individual genomes and across taxonomic levels. All genomes in PATRIC, currently more than 10 000, are consistently annotated using RAST, the Rapid Annotations using Subsystems Technology. Summaries of different data types are also provided for individual genes, where comparisons of different annotations are available, and also include available transcriptomic data. PATRIC provides a variety of ways for researchers to find data of interest and a private workspace where they can store both genomic and gene associations, and their own private data. Both private and public data can be analyzed together using a suite of tools to perform comparative genomic or transcriptomic analysis. PATRIC also includes integrated information related to disease and PPIs. All the data and integrated analysis and visualization tools are freely available. This manuscript describes updates to the PATRIC since its initial report in the 2007 NAR Database Issue.",c59,British Computer Society Conference on Human-Computer Interaction,cp59,accepted,f1733,2019,2019-07-10
s2160,p2160,New software for searching the Cambridge Structural Database and visualizing crystal structures.,"Two new programs have been developed for searching the Cambridge Structural Database (CSD) and visualizing database entries: ConQuest and Mercury. The former is a new search interface to the CSD, the latter is a high-performance crystal-structure visualizer with extensive facilities for exploring networks of intermolecular contacts. Particular emphasis has been placed on making the programs as intuitive as possible. Both ConQuest and Mercury run under Windows and various types of Unix, including Linux.",j314,Acta Crystallographica Section B Structural Science,jv314,accepted,f1734,2016,2016-08-04
s2161,p2161,FEEDBACK ON A PUBLICLY DISTRIBUTED IMAGE DATABASE: THE MESSIDOR DATABASE,"The Messidor database, which contains hundreds of eye fundus images, has been publicly distributed since 2008. It was created by the Messidor project in order to evaluate automatic lesion segmentation and diabetic retinopathy grading methods. Designing, producing and maintaining such a database entails significant costs. By publicly sharing it, one hopes to bring a valuable resource to the public research community. However, the real interest and benefit of the research community is not easy to quantify. We analyse here the feedback on the Messidor database, after more than 6 years of diffusion. This analysis should apply to other similar research databases.",c35,EUROMICRO Conference on Software Engineering and Advanced Applications,cp35,accepted,f1735,2005,2005-04-29
s2162,p2162,Gene Ontology Consortium: The Gene Ontology (GO) database and informatics resource,"The Gene Ontology (GO) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classifications that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. Many model organism databases and genome annotation groups use the GO and contribute their annotation sets to the GO resource. The GO database integrates the vocabularies and contributed annotations and provides full access to this information in several formats. Members of the GO Consortium continually work collectively, involving outside experts as needed, to expand and update the GO vocabularies. The GO Web resource also provides access to extensive documentation about the GO project and links to applications that use GO data for functional analyses.",c25,International Conference on Contemporary Computing,cp25,accepted,f1736,2014,2014-12-05
s2163,p2163,CHIANTI - an atomic database for emission lines - I. Wavelengths greater than 50 Å,"CHIANTI consists of a critically evaluated set of atomic data and transition probabilities necessary to calculate the emission line spectrum of astrophysical plasmas. The data consist of atomic energy levels, atomic radiative data such as wavelengths, weighted oscillator strengths and A values, and electron collisional excitation rates. A set of programs that use these data to calculate the spectrum in a desired wavelength range as a function of temperature and density is also provided. A suite of programs has been developed to carry out plasma diagnostics of astrophysical plasmas. The state-of-the-art contents of the CHIANTI database will be described and some of the most important results obtained from the use of the CHIANTI database will be reviewed.",c33,International Conference on Agile Software Development,cp33,accepted,f1737,2022,2022-08-08
s2164,p2164,A Multimodal Database for Affect Recognition and Implicit Tagging,"MAHNOB-HCI is a multimodal database recorded in response to affective stimuli with the goal of emotion recognition and implicit tagging research. A multimodal setup was arranged for synchronized recording of face videos, audio signals, eye gaze data, and peripheral/central nervous system physiological signals. Twenty-seven participants from both genders and different cultural backgrounds participated in two experiments. In the first experiment, they watched 20 emotional videos and self-reported their felt emotions using arousal, valence, dominance, and predictability as well as emotional keywords. In the second experiment, short videos and images were shown once without any tag and then with correct or incorrect tags. Agreement or disagreement with the displayed tags was assessed by the participants. The recorded videos and bodily responses were segmented and stored in a database. The database is made available to the academic community via a web-based system. The collected data were analyzed and single modality and modality fusion results for both emotion recognition and implicit tagging experiments are reported. These results show the potential uses of the recorded modalities and the significance of the emotion elicitation protocol.",j350,IEEE Transactions on Affective Computing,jv350,accepted,f1738,2022,2022-04-26
s2165,p2165,"The CMU Pose, Illumination, and Expression (PIE) database","Between October 2000 and December 2000, we collected a database of over 40,000 facial images of 68 people. Using the CMU (Carnegie Mellon University) 3D Room, we imaged each person across 13 different poses, under 43 different illumination conditions, and with four different expressions. We call this database the CMU Pose, Illumination and Expression (PIE) database. In this paper, we describe the imaging hardware, the collection procedure, the organization of the database, several potential uses of the database, and how to obtain the database.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1739,2007,2007-07-18
s2166,p2166,Principles of database and knowledge- base systems,Abstract content goes here ...,c69,International Conference on Parallel Processing,cp69,accepted,f1740,2010,2010-09-27
s2167,p2167,Systemic Banking Crises: A New Database,"This paper presents a new database on the timing of systemic banking crises and policy responses to resolve them. The database covers the universe of systemic banking crises for the period 1970-2007, with detailed data on crisis containment and resolution policies for 42 crisis episodes, and also includes data on the timing of currency crises and sovereign debt crises. The database extends and builds on the Caprio, Klingebiel, Laeven, and Noguera (2005) banking crisis database, and is the most complete and detailed database on banking crises to date.",j40,Social Science Research Network,jv40,accepted,f1741,2017,2017-01-14
s2168,p2168,Biological control of insect pests by insect parasitoids and predators: the BIOCAT database.,"The structure of the BIOCAT database, which contains records of the introductions of insect natural enemies for the control of insect pests worldwide, and is now available online, is explained. It is a useful summary of biological control effort and a guide to factors which may influence the success of introduction programmes, but is not detailed enough for making firm predictions.",j365,Biocontrol News and Information,jv365,accepted,f1742,2022,2022-07-28
s2169,p2169,PH2 - A dermoscopic image database for research and benchmarking,"The increasing incidence of melanoma has recently promoted the development of computer-aided diagnosis systems for the classification of dermoscopic images. Unfortunately, the performance of such systems cannot be compared since they are evaluated in different sets of images by their authors and there are no public databases available to perform a fair evaluation of multiple systems. In this paper, a dermoscopic image database, called PH2, is presented. The PH2 database includes the manual segmentation, the clinical diagnosis, and the identification of several dermoscopic structures, performed by expert dermatologists, in a set of 200 dermoscopic images. The PH2 database will be made freely available for research and benchmarking purposes.",c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,cp99,accepted,f1743,2015,2015-12-10
s2170,p2170,Development of a 2001 National land-cover database for the United States,"Multi-Resolution Land Characterization 2001 (MRLC 2001) is a second-generation Federal consortium designed to create an updated pool of nation-wide Landsat 5 and 7 imagery and derive a second-generation National Land Cover Database (NLCD 2001). The objectives of this multi-layer, multi-source database are two fold: first, to provide consistent land cover for all 50 States, and second, to provide a data framework which allows flexibility in developing and applying each independent data component to a wide variety of other applications. Components in the database include the following: (1) normalized imagery for three time periods per path/row, (2) ancillary data, including a 30 m Digital Elevation Model (DEM) derived into slope, aspect and slope position, (3) perpixel estimates of percent imperviousness and percent tree canopy (4) 29 classes of land cover data derived from the imagery, ancillary data, and derivatives, (5) classification rules, confidence estimates, and metadata from the land cover classification. This database is now being developed using a Mapping Zone approach, with 66 Zones in the continental United States and 23 Zones in Alaska. Results from three initial mapping Zones show single-pixel land cover accuracies ranging from 73 to 77 percent, imperviousness accuracies ranging from 83 to 91 percent, tree canopy accuracies ranging from 78 to 93 percent, and an estimated 50 percent increase in mapping efficiency over previous methods. The database has now entered the production phase and is being created using extensive partnering in the Federal government with planned completion by 2006.",c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f1744,2005,2005-01-09
s2171,p2171,Atlantic Hurricane Database Uncertainty and Presentation of a New Database Format,"Abstract“Best tracks” are National Hurricane Center (NHC) poststorm analyses of the intensity, central pressure, position, and size of Atlantic and eastern North Pacific basin tropical and subtropical cyclones. This paper estimates the uncertainty (average error) for Atlantic basin best track parameters through a survey of the NHC Hurricane Specialists who maintain and update the Atlantic hurricane database. A comparison is then made with a survey conducted over a decade ago to qualitatively assess changes in the uncertainties. Finally, the implications of the uncertainty estimates for NHC analysis and forecast products as well as for the prediction goals of the Hurricane Forecast Improvement Program are discussed.",c29,International Conference on Software Engineering,cp29,accepted,f1745,2015,2015-01-08
s2172,p2172,The UCSC Genome Browser database: 2014 update,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a large collection of organisms, primarily vertebrates, with an emphasis on the human and mouse genomes. The Browser’s web-based tools provide an integrated environment for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic data sets. As of September 2013, the database contained genomic sequence and a basic set of annotation ‘tracks’ for ∼90 organisms. Significant new annotations include a 60-species multiple alignment conservation track on the mouse, updated UCSC Genes tracks for human and mouse, and several new sets of variation and ENCODE data. New software tools include a Variant Annotation Integrator that returns predicted functional effects of a set of variants uploaded as a custom track, an extension to UCSC Genes that displays haplotype alleles for protein-coding genes and an expansion of data hubs that includes the capability to display remotely hosted user-provided assembly sequence in addition to annotation data. To improve European access, we have added a Genome Browser mirror (http://genome-euro.ucsc.edu) hosted at Bielefeld University in Germany.",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f1746,2008,2008-01-31
s2173,p2173,The Gene Ontology (GO) database and informatics resource,"The Gene Ontology (GO) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classiﬁcations that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. Many model organism databases and genome annotation groups use the GO and contribute their annotation sets to the GO resource. The GO database inte-grates the vocabularies and contributed annotations and provides full access to this information in several formats. Members of the GO Consortium con-tinually work collectively, involving outside experts as needed, to expand and update the GO vocabularies. The GO Web resource also provides access to extensive documentation about the GO project and links to applications that use GO data for functional analyses.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f1747,2010,2010-04-19
s2174,p2174,InterPro: the integrative protein signature database,"The InterPro database (http://www.ebi.ac.uk/interpro/) integrates together predictive models or ‘signatures’ representing protein domains, families and functional sites from multiple, diverse source databases: Gene3D, PANTHER, Pfam, PIRSF, PRINTS, ProDom, PROSITE, SMART, SUPERFAMILY and TIGRFAMs. Integration is performed manually and approximately half of the total ∼58 000 signatures available in the source databases belong to an InterPro entry. Recently, we have started to also display the remaining un-integrated signatures via our web interface. Other developments include the provision of non-signature data, such as structural data, in new XML files on our FTP site, as well as the inclusion of matchless UniProtKB proteins in the existing match XML files. The web interface has been extended and now links out to the ADAN predicted protein–protein interaction database and the SPICE and Dasty viewers. The latest public release (v18.0) covers 79.8% of UniProtKB (v14.1) and consists of 16 549 entries. InterPro data may be accessed either via the web address above, via web services, by downloading files by anonymous FTP or by using the InterProScan search software (http://www.ebi.ac.uk/Tools/InterProScan/).",c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f1748,2016,2016-09-19
s2175,p2175,JASPAR: an open-access database for eukaryotic transcription factor binding profiles,"The analysis of regulatory regions in genome sequences is strongly based on the detection of potential transcription factor binding sites. The preferred models for representation of transcription factor binding specificity have been termed position-specific scoring matrices. JASPAR is an open-access database of annotated, high-quality, matrix-based transcription factor binding site profiles for multicellular eukaryotes. The profiles were derived exclusively from sets of nucleotide sequences experimentally demonstrated to bind transcription factors. The database is complemented by a web interface for browsing, searching and subset selection, an online sequence analysis utility and a suite of programming tools for genome-wide and comparative genomic analysis of regulatory regions. JASPAR is available at http://jaspar. cgb.ki.se.",c10,Big Data,cp10,accepted,f1749,2021,2021-07-19
s2177,p2177,"DIP, the Database of Interacting Proteins: a research tool for studying cellular networks of protein interactions","The Database of Interacting Proteins (DIP: http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. It provides the scientific community with an integrated set of tools for browsing and extracting information about protein interaction networks. As of September 2001, the DIP catalogs approximately 11 000 unique interactions among 5900 proteins from >80 organisms; the vast majority from yeast, Helicobacter pylori and human. Tools have been developed that allow users to analyze, visualize and integrate their own experimental data with the information about protein-protein interactions available in the DIP database.",c88,Symposium on the Theory of Computing,cp88,accepted,f1750,2014,2014-08-27
s2178,p2178,High Resolution XPS of Organic Polymers: The Scienta ESCA300 Database,Description of the spectrometer x-ray source monochromator electron lens hemispherical analyser multichannel detector sample analysis chamber charge compensation performance on conducting samples performance on insulating samples performance on testing of the spectrometer experimental protocol sample mounting data acquisition correction of binding energy scale for sample charging curve fitting lineshapes shake-up structure valence bands impurities x-ray degradation organization of the database list of polymers and acronyms the database appendix 1 - primary C 1s shifts appendix 2 - secondary C 1s shifts appendix 3.1 - 0 1s binding energies in CHO polymers appendix 3.2 - 0 1s binding energies in other polymers appendix 4 - N 1s binding energies appendix 5 - F 1s binding energies appendix 6 - binding energies and spin-orbit constants for core-line doublets apendix 7 - binding energies of peaks appearing in the valence band region.,c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f1751,2006,2006-08-02
s2180,p2180,Storing and querying ordered XML using a relational database system,"XML is quickly becoming the de facto standard for data exchange over the Internet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to ""shred"" XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML's ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates.",c100,ACM SIGMOD Conference,cp100,accepted,f1752,2010,2010-07-18
s2181,p2181,Systematic meta-analyses of Alzheimer disease genetic association studies: the AlzGene database,Abstract content goes here ...,j274,Nature Genetics,jv274,accepted,f1753,2020,2020-03-22
s2182,p2182,A comparative analysis of methodologies for database schema integration,"One of the fundamental principles of the database approach is that a database allows a nonredundant, unified representation of all data managed in an organization. This is achieved only when methodologies are available to support integration across organizational and application boundaries.
Methodologies for database design usually perform the design activity by separately producing several schemas, representing parts of the application, which are subsequently merged. Database schema integration is the activity of integrating the schemas of existing or proposed databases into a global, unified schema.
The aim of the paper is to provide first a unifying framework for the problem of schema integration, then a comparative review of the work done thus far in this area. Such a framework, with the associated analysis of the existing approaches, provides a basis for identifying strengths and weaknesses of individual methodologies, as well as general guidelines for future improvements and extensions.",c28,International Conference on Collaboration Technologies and Systems,cp28,accepted,f1754,2009,2009-12-25
s2183,p2183,MEROPS: the peptidase database,"Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has a hierarchical classification in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The classification framework is used for attaching information at each level. An important focus of the database has become distinguishing one peptidase from another through identifying the specificity of the peptidase in terms of where it will cleave substrates and with which inhibitors it will interact. We have collected over 39 000 known cleavage sites in proteins, peptides and synthetic substrates. These allow us to display peptidase specificity and alignments of protein substrates to give an indication of how well a cleavage site is conserved, and thus its probable physiological relevance. While the number of new peptidase families and clans has only grown slowly the number of complete genomes has greatly increased. This has allowed us to add an analysis tool to the relevant species pages to show significant gains and losses of peptidase genes relative to related species.",c63,IEEE International Software Metrics Symposium,cp63,accepted,f1755,2008,2008-08-05
s2184,p2184,Basis Set Exchange: A Community Database for Computational Sciences,"Basis sets are some of the most important input data for computational models in the chemistry, materials, biology, and other science domains that utilize computational quantum mechanics methods. Providing a shared, Web-accessible environment where researchers can not only download basis sets in their required format but browse the data, contribute new basis sets, and ultimately curate and manage the data as a community will facilitate growth of this resource and encourage sharing both data and knowledge. We describe the Basis Set Exchange (BSE), a Web portal that provides advanced browsing and download capabilities, facilities for contributing basis set data, and an environment that incorporates tools to foster development and interaction of communities. The BSE leverages and enables continued development of the basis set library originally assembled at the Environmental Molecular Sciences Laboratory.",j358,Journal of Chemical Information and Modeling,jv358,accepted,f1756,2006,2006-02-10
s2185,p2185,Mouse Genome Database (MGD)-2017: community knowledge resource for the laboratory mouse,"The Mouse Genome Database (MGD: http://www.informatics.jax.org) is the primary community data resource for the laboratory mouse. It provides a highly integrated and highly curated system offering a comprehensive view of current knowledge about mouse genes, genetic markers and genomic features as well as the associations of those features with sequence, phenotypes, functional and comparative information, and their relationships to human diseases. MGD continues to enhance access to these data, to extend the scope of data content and visualizations, and to provide infrastructure and user support that ensures effective and efficient use of MGD in the advancement of scientific knowledge. Here, we report on recent enhancements made to the resource and new features.",c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,cp79,accepted,f1757,2020,2020-12-09
s2186,p2186,Measuring Financial Inclusion: The Global Findex Database,"This paper provides the first analysis of the Global Financial Inclusion (Global Findex) Database, a new set of indicators that measure how adults in 148 economies save, borrow, make payments, and manage risk. The data show that 50 percent of adults worldwide have an account at a formal financial institution, though account penetration varies widely across regions, income groups and individual characteristics. In addition, 22 percent of adults report having saved at a formal financial institution in the past 12 months, and 9 percent report having taken out a new loan from a bank, credit union or microfinance institution in the past year. Although half of adults around the world remain unbanked, at least 35 percent of them report barriers to account use that might be addressed by public policy. Among the most commonly reported barriers are high cost, physical distance, and lack of proper documentation, though there are significant differences across regions and individual characteristics.",c3,Frontiers in Education Conference,cp3,accepted,f1758,2016,2016-11-30
s2188,p2188,The UCSC Genome Browser Database: update 2006,"The University of California Santa Cruz Genome Browser Database (GBD) contains sequence and annotation data for the genomes of about a dozen vertebrate species and several major model organisms. Genome annotations typically include assembly data, sequence composition, genes and gene predictions, mRNA and expressed sequence tag evidence, comparative genomics, regulation, expression and variation data. The database is optimized to support fast interactive performance with web tools that provide powerful visualization and querying capabilities for mining the data. The Genome Browser displays a wide variety of annotations at all scales from single nucleotide level up to a full chromosome. The Table Browser provides direct access to the database tables and sequence data, enabling complex queries on genome-wide datasets. The Proteome Browser graphically displays protein properties. The Gene Sorter allows filtering and comparison of genes by several metrics including expression data and several gene properties. BLAT and In Silico PCR search for sequences in entire genomes in seconds. These tools are highly integrated and provide many hyperlinks to other databases and websites. The GBD, browsing tools, downloadable data files and links to documentation and other information can be found at .",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f1759,2018,2018-08-07
s2189,p2189,The COG database: new developments in phylogenetic classification of proteins from complete genomes,"The database of Clusters of Orthologous Groups of proteins (COGs), which represents an attempt on a phylogenetic classification of the proteins encoded in complete genomes, currently consists of 2791 COGs including 45 350 proteins from 30 genomes of bacteria, archaea and the yeast Saccharomyces cerevisiae (http://www.ncbi.nlm.nih. gov/COG). In addition, a supplement to the COGs is available, in which proteins encoded in the genomes of two multicellular eukaryotes, the nematode Caenorhabditis elegans and the fruit fly Drosophila melanogaster, and shared with bacteria and/or archaea were included. The new features added to the COG database include information pages with structural and functional details on each COG and literature references, improvements of the COGNITOR program that is used to fit new proteins into the COGs, and classification of genomes and COGs constructed by using principal component analysis.",c27,ACM-SIAM Symposium on Discrete Algorithms,cp27,accepted,f1760,2022,2022-01-28
s2190,p2190,"NCBI Reference Sequence (RefSeq): a curated non-redundant sequence database of genomes, transcripts and proteins","The National Center for Biotechnology Information (NCBI) Reference Sequence (RefSeq) database (http://www.ncbi.nlm.nih.gov/RefSeq/) provides a non-redundant collection of sequences representing genomic data, transcripts and proteins. Although the goal is to provide a comprehensive dataset representing the complete sequence information for any given species, the database pragmatically includes sequence data that are currently publicly available in the archival databases. The database incorporates data from over 2400 organisms and includes over one million proteins representing significant taxonomic diversity spanning prokaryotes, eukaryotes and viruses. Nucleotide and protein sequences are explicitly linked, and the sequences are linked to other resources including the NCBI Map Viewer and Gene. Sequences are annotated to include coding regions, conserved domains, variation, references, names, database cross-references, and other features using a combined approach of collaboration and other input from the scientific community, automated annotation, propagation from GenBank and curation by NCBI staff.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f1761,2011,2011-10-21
s2191,p2191,Access path selection in a relational database management system,"In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research Laboratory.",c100,ACM SIGMOD Conference,cp100,accepted,f1762,2010,2010-03-30
s2192,p2192,The Cochrane Database of Systematic Reviews,"A thin film resonator comprising a piezoelectric material and having a controllable or tunable resonant frequency. The resonator is formed on a substrate having a cavity formed therein below the piezoelectric film material. A bending electrode is disposed within the cavity and the application of a voltage between the bending electrode and one of the resonator electrodes, creates an electric field that causes the substrate region to bend. These stresses caused: by the bending are transferred to the thin film resonator, subjecting the piezoelectric film to stresses and thereby changing the resonant properties of the thin film resonator.",c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f1763,2021,2021-08-08
s2194,p2194,Mouse Genome Database (MGD)-2018: knowledgebase for the laboratory mouse,"Abstract The Mouse Genome Database (MGD; http://www.informatics.jax.org) is the key community mouse database which supports basic, translational and computational research by providing integrated data on the genetics, genomics, and biology of the laboratory mouse. MGD serves as the source for biological reference data sets related to mouse genes, gene functions, phenotypes and disease models with an increasing emphasis on the association of these data to human biology and disease. We report here on recent enhancements to this resource, including improved access to mouse disease model and human phenotype data and enhanced relationships of mouse models to human disease.",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f1764,2018,2018-10-25
s2195,p2195,The UCSC Genome Browser database: update 2011,"The University of California, Santa Cruz Genome Browser (http://genome.ucsc.edu) offers online access to a database of genomic sequence and annotation data for a wide variety of organisms. The Browser also has many tools for visualizing, comparing and analyzing both publicly available and user-generated genomic data sets, aligning sequences and uploading user data. Among the features released this year are a gene search tool and annotation track drag-reorder functionality as well as support for BAM and BigWig/BigBed file formats. New display enhancements include overlay of multiple wiggle tracks through use of transparent coloring, options for displaying transformed wiggle data, a ‘mean+whiskers’ windowing function for display of wiggle data at high zoom levels, and more color schemes for microarray data. New data highlights include seven new genome assemblies, a Neandertal genome data portal, phenotype and disease association data, a human RNA editing track, and a zebrafish Conservation track. We also describe updates to existing tracks.",c83,International Conference on Computer Graphics and Interactive Techniques,cp83,accepted,f1765,2015,2015-08-03
s2196,p2196,Survey of graph database models,"Graph database models can be defined as those in which data structures for the schema and instances are modeled as graphs or generalizations of them, and data manipulation is expressed by graph-oriented operations and type constructors. These models took off in the eighties and early nineties alongside object-oriented models. Their influence gradually died out with the emergence of other database models, in particular geographical, spatial, semistructured, and XML. Recently, the need to manage information with graph-like nature has reestablished the relevance of this area. The main objective of this survey is to present the work that has been conducted in the area of graph database modeling, concentrating on data structures, query languages, and integrity constraints.",c21,Grid Computing Environments,cp21,accepted,f1766,2005,2005-08-16
s2197,p2197,World Database on Protected Areas (WDPA),Abstract content goes here ...,c11,Hawaii International Conference on System Sciences,cp11,accepted,f1767,2006,2006-03-02
s2198,p2198,PID: the Pathway Interaction Database,"The Pathway Interaction Database (PID, http://pid.nci.nih.gov) is a freely available collection of curated and peer-reviewed pathways composed of human molecular signaling and regulatory events and key cellular processes. Created in a collaboration between the US National Cancer Institute and Nature Publishing Group, the database serves as a research tool for the cancer research community and others interested in cellular pathways, such as neuroscientists, developmental biologists and immunologists. PID offers a range of search features to facilitate pathway exploration. Users can browse the predefined set of pathways or create interaction network maps centered on a single molecule or cellular process of interest. In addition, the batch query tool allows users to upload long list(s) of molecules, such as those derived from microarray experiments, and either overlay these molecules onto predefined pathways or visualize the complete molecular connectivity map. Users can also download molecule lists, citation lists and complete database content in extensible markup language (XML) and Biological Pathways Exchange (BioPAX) Level 2 format. The database is updated with new pathway content every month and supplemented by specially commissioned articles on the practical uses of other relevant online tools.",c53,International Conference on Software Engineering and Knowledge Engineering,cp53,accepted,f1768,2018,2018-04-13
s2200,p2200,World Database on Protected Areas (WDPA),Abstract content goes here ...,c51,Conference of the Centre for Advanced Studies on Collaborative Research,cp51,accepted,f1769,2011,2011-05-05
s2201,p2201,PID: the Pathway Interaction Database,"The Pathway Interaction Database (PID, http://pid.nci.nih.gov) is a freely available collection of curated and peer-reviewed pathways composed of human molecular signaling and regulatory events and key cellular processes. Created in a collaboration between the US National Cancer Institute and Nature Publishing Group, the database serves as a research tool for the cancer research community and others interested in cellular pathways, such as neuroscientists, developmental biologists and immunologists. PID offers a range of search features to facilitate pathway exploration. Users can browse the predefined set of pathways or create interaction network maps centered on a single molecule or cellular process of interest. In addition, the batch query tool allows users to upload long list(s) of molecules, such as those derived from microarray experiments, and either overlay these molecules onto predefined pathways or visualize the complete molecular connectivity map. Users can also download molecule lists, citation lists and complete database content in extensible markup language (XML) and Biological Pathways Exchange (BioPAX) Level 2 format. The database is updated with new pathway content every month and supplemented by specially commissioned articles on the practical uses of other relevant online tools.",c10,Big Data,cp10,accepted,f1770,2021,2021-09-28
s2202,p2202,The BioGRID interaction database: 2013 update,"The Biological General Repository for Interaction Datasets (BioGRID: http//thebiogrid.org) is an open access archive of genetic and protein interactions that are curated from the primary biomedical literature for all major model organism species. As of September 2012, BioGRID houses more than 500 000 manually annotated interactions from more than 30 model organisms. BioGRID maintains complete curation coverage of the literature for the budding yeast Saccharomyces cerevisiae, the fission yeast Schizosaccharomyces pombe and the model plant Arabidopsis thaliana. A number of themed curation projects in areas of biomedical importance are also supported. BioGRID has established collaborations and/or shares data records for the annotation of interactions and phenotypes with most major model organism databases, including Saccharomyces Genome Database, PomBase, WormBase, FlyBase and The Arabidopsis Information Resource. BioGRID also actively engages with the text-mining community to benchmark and deploy automated tools to expedite curation workflows. BioGRID data are freely accessible through both a user-defined interactive interface and in batch downloads in a wide variety of formats, including PSI-MI2.5 and tab-delimited files. BioGRID records can also be interrogated and analyzed with a series of new bioinformatics tools, which include a post-translational modification viewer, a graphical viewer, a REST service and a Cytoscape plugin.",c106,Chinese Conference on Biometric Recognition,cp106,accepted,f1771,2016,2016-08-28
s2203,p2203,STRING: a database of predicted functional associations between proteins,"Functional links between proteins can often be inferred from genomic associations between the genes that encode them: groups of genes that are required for the same function tend to show similar species coverage, are often located in close proximity on the genome (in prokaryotes), and tend to be involved in gene-fusion events. The database STRING is a precomputed global resource for the exploration and analysis of these associations. Since the three types of evidence differ conceptually, and the number of predicted interactions is very large, it is essential to be able to assess and compare the significance of individual predictions. Thus, STRING contains a unique scoring-framework based on benchmarks of the different types of associations against a common reference set, integrated in a single confidence score per prediction. The graphical representation of the network of inferred, weighted protein interactions provides a high-level view of functional linkage, facilitating the analysis of modularity in biological processes. STRING is updated continuously, and currently contains 261 033 orthologs in 89 fully sequenced genomes. The database predicts functional interactions at an expected level of accuracy of at least 80% for more than half of the genes; it is online at http://www.bork.embl-heidelberg.de/STRING/.",c31,International Conference on Evaluation & Assessment in Software Engineering,cp31,accepted,f1772,2022,2022-10-09
s2204,p2204,A major upgrade of the VALD database,"Vienna atomic line database (VALD) is a collection of critically evaluated laboratory parameters for individual atomic transitions, complemented by theoretical calculations. VALD is actively used by astronomers for stellar spectroscopic studies—model atmosphere calculations, atmospheric parameter determinations, abundance analysis etc. The two first VALD releases contained parameters for atomic transitions only. In a major upgrade of VALD—VALD3, publically available from spring 2014, atomic data was complemented with parameters of molecular lines. The diatomic molecules C2, CH, CN, CO, OH, MgH, SiH, TiO are now included. For each transition VALD provides species name, wavelength, energy, quantum number J and Landé-factor of the lower and upper levels, radiative, Stark and van der Waals damping factors and a full description of electronic configurarion and term information of both levels. Compared to the previous versions we have revised and verify all of the existing data and added new measurements and calculations for transitions in the range between 20 Å and 200 microns. All transitions were complemented with term designations in a consistent way and electron configurations when available. All data were checked for consistency: listed wavelength versus Ritz, selection rules etc. A new bibliographic system keeps track of literature references for each parameter in a given transition throughout the merging process so that every selected data entry can be traced to the original source. The query language and the extraction tools can now handle various units, vacuum and air wavelengths. In the upgrade process we had an intensive interaction with data producers, which was very helpful for improving the quality of the VALD content.",c25,International Conference on Contemporary Computing,cp25,accepted,f1773,2014,2014-04-17
s2205,p2205,Rice Annotation Project Database (RAP-DB): An Integrative and Interactive Database for Rice Genomics,"The Rice Annotation Project Database (RAP-DB, http://rapdb.dna.affrc.go.jp/) has been providing a comprehensive set of gene annotations for the genome sequence of rice, Oryza sativa (japonica group) cv. Nipponbare. Since the first release in 2005, RAP-DB has been updated several times along with the genome assembly updates. Here, we present our newest RAP-DB based on the latest genome assembly, Os-Nipponbare-Reference-IRGSP-1.0 (IRGSP-1.0), which was released in 2011. We detected 37,869 loci by mapping transcript and protein sequences of 150 monocot species. To provide plant researchers with highly reliable and up to date rice gene annotations, we have been incorporating literature-based manually curated data, and 1,626 loci currently incorporate literature-based annotation data, including commonly used gene names or gene symbols. Transcriptional activities are shown at the nucleotide level by mapping RNA-Seq reads derived from 27 samples. We also mapped the Illumina reads of a Japanese leading japonica cultivar, Koshihikari, and a Chinese indica cultivar, Guangluai-4, to the genome and show alignments together with the single nucleotide polymorphisms (SNPs) and gene functional annotations through a newly developed browser, Short-Read Assembly Browser (S-RAB). We have developed two satellite databases, Plant Gene Family Database (PGFD) and Integrative Database of Cereal Gene Phylogeny (IDCGP), which display gene family and homologous gene relationships among diverse plant species. RAP-DB and the satellite databases offer simple and user-friendly web interfaces, enabling plant and genome researchers to access the data easily and facilitating a broad range of plant research topics.",j221,Plant and Cell Physiology,jv221,accepted,f1774,2014,2014-02-08
s2207,p2207,"HITEMP, the high-temperature molecular spectroscopic database",Abstract content goes here ...,c59,British Computer Society Conference on Human-Computer Interaction,cp59,accepted,f1775,2019,2019-10-14
s2208,p2208,MRC Psycholinguistic Database,Abstract content goes here ...,c29,International Conference on Software Engineering,cp29,accepted,f1776,2015,2015-02-18
s2209,p2209,METLIN: A Metabolite Mass Spectral Database,"Endogenous metabolites have gained increasing interest over the past 5 years largely for their implications in diagnostic and pharmaceutical biomarker discovery. METLIN (http://metlin.scripps.edu), a freely accessible web-based data repository, has been developed to assist in a broad array of metabolite research and to facilitate metabolite identification through mass analysis. METLIN includes an annotated list of known metabolite structural information that is easily cross-correlated with its catalogue of high-resolution Fourier transform mass spectrometry (FTMS) spectra, tandem mass spectrometry (MS/MS) spectra, and LC/MS data.",j367,Therapeutic Drug Monitoring,jv367,accepted,f1777,2005,2005-03-29
s2210,p2210,Introduction to Database Systems,Preface About the Authors 1 What's in a Database? 2 Relational Model 3 Relational Calculus 4 Relational Algebra 5 SQL 6 SQL and Programming Languages 7 Entity-Relationship Model 8 Normalisation 9 Conclusion References Index,c6,Americas Conference on Information Systems,cp6,accepted,f1778,2007,2007-06-22
s2211,p2211,IT’IS Database for Thermal and Electromagnetic Parameters of Biological Tissues,Abstract content goes here ...,c64,Experimental Software Engineering Network,cp64,accepted,f1779,2014,2014-06-28
s2212,p2212,"Human genetic variation database, a reference database of genetic variations in the Japanese population",Abstract content goes here ...,j368,Journal of Human Genetics,jv368,accepted,f1780,2013,2013-08-01
s2213,p2213,OPM database and PPM web server: resources for positioning of proteins in membranes,"The Orientations of Proteins in Membranes (OPM) database is a curated web resource that provides spatial positions of membrane-bound peptides and proteins of known three-dimensional structure in the lipid bilayer, together with their structural classification, topology and intracellular localization. OPM currently contains more than 1200 transmembrane and peripheral proteins and peptides from approximately 350 organisms that represent approximately 3800 Protein Data Bank entries. Proteins are classified into classes, superfamilies and families and assigned to 21 distinct membrane types. Spatial positions of proteins with respect to the lipid bilayer are optimized by the PPM 2.0 method that accounts for the hydrophobic, hydrogen bonding and electrostatic interactions of the proteins with the anisotropic water-lipid environment described by the dielectric constant and hydrogen-bonding profiles. The OPM database is freely accessible at http://opm.phar.umich.edu. Data can be sorted, searched or retrieved using the hierarchical classification, source organism, localization in different types of membranes. The database offers downloadable coordinates of proteins and peptides with membrane boundaries. A gallery of protein images and several visualization tools are provided. The database is supplemented by the PPM server (http://opm.phar.umich.edu/server.php) which can be used for calculating spatial positions in membranes of newly determined proteins structures or theoretical models.",c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f1781,2006,2006-05-30
s2214,p2214,AVA: A large-scale database for aesthetic visual analysis,"With the ever-expanding volume of visual content available, the ability to organize and navigate such content by aesthetic preference is becoming increasingly important. While still in its nascent stage, research into computational models of aesthetic preference already shows great potential. However, to advance research, realistic, diverse and challenging databases are needed. To this end, we introduce a new large-scale database for conducting Aesthetic Visual Analysis: AVA. It contains over 250,000 images along with a rich variety of meta-data including a large number of aesthetic scores for each image, semantic labels for over 60 categories as well as labels related to photographic style. We show the advantages of AVA with respect to existing databases in terms of scale, diversity, and heterogeneity of annotations. We then describe several key insights into aesthetic preference afforded by AVA. Finally, we demonstrate, through three applications, how the large scale of AVA can be leveraged to improve performance on existing preference tasks.",c65,Formal Concept Analysis,cp65,accepted,f1782,2008,2008-07-16
s2216,p2216,Database: The Journal of Biological Databases and Curation,"Evolution provides the unifying framework with which to understand biology. The coherent investigation of genic and genomic data often requires comparative genomics analyses based on whole-genome alignments, sets of homologous genes and other relevant datasets in order to evaluate and answer evolutionary-related questions. However, the complexity and computational requirements of producing such data are substantial: this has led to only a small number of reference resources that are used for most comparative analyses. The Ensembl comparative genomics resources are one such reference set that facilitates comprehensive and reproducible analysis of chordate genome data. Ensembl computes pairwise and multiple whole-genome alignments from which large-scale synteny, per-base conservation scores and constrained elements are obtained. Gene alignments are used to define Ensembl Protein Families, GeneTrees and homologies for both protein-coding and non-coding RNA genes. These resources are updated frequently and have a consistent informatics infrastructure and data presentation across all supported species. Specialized web-based visualizations are also available including synteny displays, collapsible gene tree plots, a gene family locator and different alignment views. The Ensembl comparative genomics infrastructure is extensively reused for the analysis of non-vertebrate species by other projects including Ensembl Genomes and Gramene and much of the information here is relevant to these projects. The consistency of the annotation across species and the focus on vertebrates makes Ensembl an ideal system to perform and support vertebrate comparative genomic analyses. We use robust software and pipelines to produce reference comparative data and make it freely available.Database URL: http://www.ensembl.org.",c58,Australian Software Engineering Conference,cp58,accepted,f1783,2021,2021-06-24
s2217,p2217,MODOMICS: a database of RNA modification pathways—2013 update,"MODOMICS is a database of RNA modifications that provides comprehensive information concerning the chemical structures of modified ribonucleosides, their biosynthetic pathways, RNA-modifying enzymes and location of modified residues in RNA sequences. In the current database version, accessible at http://modomics.genesilico.pl, we included new features: a census of human and yeast snoRNAs involved in RNA-guided RNA modification, a new section covering the 5′-end capping process, and a catalogue of ‘building blocks’ for chemical synthesis of a large variety of modified nucleosides. The MODOMICS collections of RNA modifications, RNA-modifying enzymes and modified RNAs have been also updated. A number of newly identified modified ribonucleosides and more than one hundred functionally and structurally characterized proteins from various organisms have been added. In the RNA sequences section, snRNAs and snoRNAs with experimentally mapped modified nucleosides have been added and the current collection of rRNA and tRNA sequences has been substantially enlarged. To facilitate literature searches, each record in MODOMICS has been cross-referenced to other databases and to selected key publications. New options for database searching and querying have been implemented, including a BLAST search of protein sequences and a PARALIGN search of the collected nucleic acid sequences.",c112,Very Large Data Bases Conference,cp112,accepted,f1784,2018,2018-04-02
s2218,p2218,The IntAct molecular interaction database in 2012,"IntAct is an open-source, open data molecular interaction database populated by data either curated from the literature or from direct data depositions. Two levels of curation are now available within the database, with both IMEx-level annotation and less detailed MIMIx-compatible entries currently supported. As from September 2011, IntAct contains approximately 275 000 curated binary interaction evidences from over 5000 publications. The IntAct website has been improved to enhance the search process and in particular the graphical display of the results. New data download formats are also available, which will facilitate the inclusion of IntAct's data in the Semantic Web. IntAct is an active contributor to the IMEx consortium (http://www.imexconsortium.org). IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",c22,International Conference on Data Technologies and Applications,cp22,accepted,f1785,2020,2020-03-23
s2219,p2219,Phenol-Explorer 3.0: a major update of the Phenol-Explorer database to incorporate data on the effects of food processing on polyphenol content,"Polyphenols are a major class of bioactive phytochemicals whose consumption may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, type II diabetes and cancers. Phenol-Explorer, launched in 2009, is the only freely available web-based database on the content of polyphenols in food and their in vivo metabolism and pharmacokinetics. Here we report the third release of the database (Phenol-Explorer 3.0), which adds data on the effects of food processing on polyphenol contents in foods. Data on >100 foods, covering 161 polyphenols or groups of polyphenols before and after processing, were collected from 129 peer-reviewed publications and entered into new tables linked to the existing relational design. The effect of processing on polyphenol content is expressed in the form of retention factor coefficients, or the proportion of a given polyphenol retained after processing, adjusted for change in water content. The result is the first database on the effects of food processing on polyphenol content and, following the model initially defined for Phenol-Explorer, all data may be traced back to original sources. The new update will allow polyphenol scientists to more accurately estimate polyphenol exposure from dietary surveys. Database URL: http://www.phenol-explorer.eu",c22,International Conference on Data Technologies and Applications,cp22,accepted,f1786,2020,2020-03-30
s2220,p2220,A 3D facial expression database for facial behavior research,"Traditionally, human facial expressions have been studied using either 2D static images or 2D video sequences. The 2D-based analysis is incapable of handing large pose variations. Although 3D modeling techniques have been extensively used for 3D face recognition and 3D face animation, barely any research on 3D facial expression recognition using 3D range data has been reported. A primary factor for preventing such research is the lack of a publicly available 3D facial expression database. In this paper, we present a newly developed 3D facial expression database, which includes both prototypical 3D facial expression shapes and 2D facial textures of 2,500 models from 100 subjects. This is the first attempt at making a 3D facial expression database available for the research community, with the ultimate goal of fostering the research on affective computing and increasing the general understanding of facial behavior and the fine 3D structure inherent in human facial expressions. The new database can be a valuable resource for algorithm assessment, comparison and evaluation",c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f1787,2006,2006-07-09
s2221,p2221,Identification of protein coding regions by database similarity search,Abstract content goes here ...,j274,Nature Genetics,jv274,accepted,f1788,2020,2020-06-13
s2222,p2222,The Ensembl genome database project,"The Ensembl (http://www.ensembl.org/) database project provides a bioinformatics framework to organise biology around the sequences of large genomes. It is a comprehensive source of stable automatic annotation of the human genome sequence, with confirmed gene predictions that have been integrated with external data sources, and is available as either an interactive web site or as flat files. It is also an open source software engineering project to develop a portable system able to handle very large genomes and associated requirements from sequence analysis to data storage and visualisation. The Ensembl site is one of the leading sources of human genome sequence annotation and provided much of the analysis for publication by the international human genome project of the draft genome. The Ensembl system is being installed around the world in both companies and academic sites on machines ranging from supercomputers to laptops.",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f1789,2017,2017-07-11
s2223,p2223,XM2VTSDB: The Extended M2VTS Database,"Keywords: vision Reference EPFL-CONF-82502 URL: ftp://ftp.idiap.ch/pub/papers/vision/avbpa99.pdf Record created on 2006-03-10, modified on 2017-05-10",c92,Advances in Soft Computing,cp92,accepted,f1790,2009,2009-05-31
s2224,p2224,Human Gene Mutation Database (HGMD®): 2003 update,"The Human Gene Mutation Database (HGMD) constitutes a comprehensive core collection of data on germ‐line mutations in nuclear genes underlying or associated with human inherited disease (www.hgmd.org). Data catalogued includes: single base‐pair substitutions in coding, regulatory and splicing‐relevant regions; micro‐deletions and micro‐insertions; indels; triplet repeat expansions as well as gross deletions; insertions; duplications; and complex rearrangements. Each mutation is entered into HGMD only once in order to avoid confusion between recurrent and identical‐by‐descent lesions. By March 2003, the database contained in excess of 39,415 different lesions detected in 1,516 different nuclear genes, with new entries currently accumulating at a rate exceeding 5,000 per annum. Since its inception, HGMD has been expanded to include cDNA reference sequences for more than 87% of listed genes, splice junction sequences, disease‐associated and functional polymorphisms, as well as links to data present in publicly available online locus‐specific mutation databases. Although HGMD has recently entered into a licensing agreement with Celera Genomics (Rockville, MD), mutation data will continue to be made freely available via the Internet. Hum Mutat 21:577–581, 2003. © 2003 Wiley‐Liss, Inc.",j209,Human Mutation,jv209,accepted,f1791,2005,2005-12-01
s2225,p2225,ATtRACT—a database of RNA-binding proteins and associated motifs,"RNA-binding proteins (RBPs) play a crucial role in key cellular processes, including RNA transport, splicing, polyadenylation and stability. Understanding the interaction between RBPs and RNA is key to improve our knowledge of RNA processing, localization and regulation in a global manner. Despite advances in recent years, a unified non-redundant resource that includes information on experimentally validated motifs, RBPs and integrated tools to exploit this information is lacking. Here, we developed a database named ATtRACT (available at http://attract.cnic.es) that compiles information on 370 RBPs and 1583 RBP consensus binding motifs, 192 of which are not present in any other database. To populate ATtRACT we (i) extracted and hand-curated experimentally validated data from CISBP-RNA, SpliceAid–F, RBPDB databases, (ii) integrated and updated the unavailable ASD database and (iii) extracted information from Protein-RNA complexes present in Protein Data Bank database through computational analyses. ATtRACT provides also efficient algorithms to search a specific motif and scan one or more RNA sequences at a time. It also allows discovering de novo motifs enriched in a set of related sequences and compare them with the motifs included in the database. Database URL: http:// attract. cnic. es",c63,IEEE International Software Metrics Symposium,cp63,accepted,f1792,2008,2008-03-07
s2226,p2226,HPIDB 2.0: a curated database for host–pathogen interactions,"Identification and analysis of host–pathogen interactions (HPI) is essential to study infectious diseases. However, HPI data are sparse in existing molecular interaction databases, especially for agricultural host–pathogen systems. Therefore, resources that annotate, predict and display the HPI that underpin infectious diseases are critical for developing novel intervention strategies. HPIDB 2.0 (http://www.agbase.msstate.edu/hpi/main.html) is a resource for HPI data, and contains 45, 238 manually curated entries in the current release. Since the first description of the database in 2010, multiple enhancements to HPIDB data and interface services were made that are described here. Notably, HPIDB 2.0 now provides targeted biocuration of molecular interaction data. As a member of the International Molecular Exchange consortium, annotations provided by HPIDB 2.0 curators meet community standards to provide detailed contextual experimental information and facilitate data sharing. Moreover, HPIDB 2.0 provides access to rapidly available community annotations that capture minimum molecular interaction information to address immediate researcher needs for HPI network analysis. In addition to curation, HPIDB 2.0 integrates HPI from existing external sources and contains tools to infer additional HPI where annotated data are scarce. Compared to other interaction databases, our data collection approach ensures HPIDB 2.0 users access the most comprehensive HPI data from a wide range of pathogens and their hosts (594 pathogen and 70 host species, as of February 2016). Improvements also include enhanced search capacity, addition of Gene Ontology functional information, and implementation of network visualization. The changes made to HPIDB 2.0 content and interface ensure that users, especially agricultural researchers, are able to easily access and analyse high quality, comprehensive HPI data. All HPIDB 2.0 data are updated regularly, are publically available for direct download, and are disseminated to other molecular interaction resources. Database URL: http://www.agbase.msstate.edu/hpi/main.html",c25,International Conference on Contemporary Computing,cp25,accepted,f1793,2014,2014-01-11
s2228,p2228,The UCSC Genome Browser database: extensions and updates 2013,"The University of California Santa Cruz (UCSC) Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a wide variety of organisms. The Browser is an integrated tool set for visualizing, comparing, analysing and sharing both publicly available and user-generated genomic datasets. As of September 2012, genomic sequence and a basic set of annotation ‘tracks’ are provided for 63 organisms, including 26 mammals, 13 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms, yeast and sea hare. In the past year 19 new genome assemblies have been added, and we anticipate releasing another 28 in early 2013. Further, a large number of annotation tracks have been either added, updated by contributors or remapped to the latest human reference genome. Among these are an updated UCSC Genes track for human and mouse assemblies. We have also introduced several features to improve usability, including new navigation menus. This article provides an update to the UCSC Genome Browser database, which has been previously featured in the Database issue of this journal.",c102,International Conference on Biometrics,cp102,accepted,f1794,2022,2022-05-12
s2232,p2232,Cloud-Screening and Quality Control Algorithms for the AERONET Database,Abstract content goes here ...,c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f1795,2016,2016-07-05
s2234,p2234,NCBI GEO: mining tens of millions of expression profiles—database and tools update,"The Gene Expression Omnibus (GEO) repository at the National Center for Biotechnology Information (NCBI) archives and freely disseminates microarray and other forms of high-throughput data generated by the scientific community. The database has a minimum information about a microarray experiment (MIAME)-compliant infrastructure that captures fully annotated raw and processed data. Several data deposit options and formats are supported, including web forms, spreadsheets, XML and Simple Omnibus Format in Text (SOFT). In addition to data storage, a collection of user-friendly web-based interfaces and applications are available to help users effectively explore, visualize and download the thousands of experiments and tens of millions of gene expression patterns stored in GEO. This paper provides a summary of the GEO database structure and user facilities, and describes recent enhancements to database design, performance, submission format options, data query and retrieval utilities. GEO is accessible at",c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f1796,2012,2012-06-28
s2235,p2235,The ConsensusPathDB interaction database: 2013 update,"Knowledge of the various interactions between molecules in the cell is crucial for understanding cellular processes in health and disease. Currently available interaction databases, being largely complementary to each other, must be integrated to obtain a comprehensive global map of the different types of interactions. We have previously reported the development of an integrative interaction database called ConsensusPathDB (http://ConsensusPathDB.org) that aims to fulfill this task. In this update article, we report its significant progress in terms of interaction content and web interface tools. ConsensusPathDB has grown mainly due to the integration of 12 further databases; it now contains 215 541 unique interactions and 4601 pathways from overall 30 databases. Binary protein interactions are scored with our confidence assessment tool, IntScore. The ConsensusPathDB web interface allows users to take advantage of these integrated interaction and pathway data in different contexts. Recent developments include pathway analysis of metabolite lists, visualization of functional gene/metabolite sets as overlap graphs, gene set analysis based on protein complexes and induced network modules analysis that connects a list of genes through various interaction types. To facilitate the interactive, visual interpretation of interaction and pathway data, we have re-implemented the graph visualization feature of ConsensusPathDB using the Cytoscape.js library.",c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f1797,2005,2005-05-10
s2236,p2236,A face antispoofing database with diverse attacks,"Face antispoofing has now attracted intensive attention, aiming to assure the reliability of face biometrics. We notice that currently most of face antispoofing databases focus on data with little variations, which may limit the generalization performance of trained models since potential attacks in real world are probably more complex. In this paper we release a face antispoofing database which covers a diverse range of potential attack variations. Specifically, the database contains 50 genuine subjects, and fake faces are made from the high quality records of the genuine faces. Three imaging qualities are considered, namely the low quality, normal quality and high quality. Three fake face attacks are implemented, which include warped photo attack, cut photo attack and video attack. Therefore each subject contains 12 videos (3 genuine and 9 fake), and the final database contains 600 video clips. Test protocol is provided, which consists of 7 scenarios for a thorough evaluation from all possible aspects. A baseline algorithm is also given for comparison, which explores the high frequency information in the facial region to determine the liveness. We hope such a database can serve as an evaluation platform for future researches in the literature.",c102,International Conference on Biometrics,cp102,accepted,f1798,2022,2022-04-10
s2237,p2237,PROGgeneV2: enhancements on the existing database,Abstract content goes here ...,j370,BMC Cancer,jv370,accepted,f1799,2017,2017-10-02
s2238,p2238,Systemic Banking Crises Database II,Abstract content goes here ...,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f1800,2021,2021-05-12
s2239,p2239,Rfam: an RNA family database,"Rfam is a collection of multiple sequence alignments and covariance models representing non-coding RNA families. Rfam is available on the web in the UK at http://www.sanger.ac.uk/Software/Rfam/ and in the US at http://rfam.wustl.edu/. These websites allow the user to search a query sequence against a library of covariance models, and view multiple sequence alignments and family annotation. The database can also be downloaded in flatfile form and searched locally using the INFERNAL package (http://infernal.wustl.edu/). The first release of Rfam (1.0) contains 25 families, which annotate over 50 000 non-coding RNA genes in the taxonomic divisions of the EMBL nucleotide database.",c14,International Conference on Exploring Services Science,cp14,accepted,f1801,2016,2016-03-06
s2240,p2240,Validation of the national health insurance research database with ischemic stroke cases in Taiwan,The National Health Insurance Research Database (NHIRD) is commonly used for pharmacoepidemiological research in Taiwan. This study evaluated the validity of the database for patients with a principal diagnosis of ischemic stroke.,j371,Pharmacoepidemiology and Drug Safety,jv371,accepted,f1802,2020,2020-09-15
s2241,p2241,"NoSQL Database: New Era of Databases for Big data Analytics - Classification, Characteristics and Comparison","Digital world is growing very fast and become more complex in the volume (terabyte to petabyte), variety (structured and un-structured and hybrid), velocity (high speed in growth) in nature. This refers to as ‘Big Data’ that is a global phenomenon. This is typically considered to be a data collection that has grown so large it can’t be effectively managed or exploited using conventional data management tools: e.g., classic relational database management systems (RDBMS) or conventional search engines. To handle this problem, traditional RDBMS are complemented by specifically designed a rich set of alternative DBMS; such as - NoSQL, NewSQL and Search-based systems. This paper motivation is to provide - classification, characteristics and evaluation of NoSQL databases in Big Data Analytics. This report is intended to help users, especially to the organizations to obtain an independent understanding of the strengths and weaknesses of various NoSQL database approaches to supporting applications that process huge volumes of data.",c23,International Conference on Open and Big Data,cp23,accepted,f1803,2012,2012-08-29
s2242,p2242,Completion of the 2001 National Land Cover Database for the conterminous United States,"Introduction Appropriate and relevant land cover information is increasingly required by a broad spectrum of scientific, economic and governmental applications as essential input to analyze such issues as assessing ecosystem status and health, understanding spatial patterns of biodiversity and developing land management policy. The publication of the first National Land Cover Dataset (NLCD 1992) (Vogelmann et al. 2001) created a 30-meter resolution land cover data layer over the conterminous United States from circa 1992 Landsat Thematic Mapper (TM) imagery. Information from this original NLCD 1992 has been used in thousands of applications in the private, public, and academic sectors ranging from assisting in placing cell phone towers to tracking how diseases spread. The national consistency of this information has provided critical analysis for many national applications such as the Heinz Center’s State of the Nation’s Ecosystems (Heinz Center 2002), the Environmental Protection Agency’s Draft Report on the Environment (USEPA 2003) and the U.S. Geological Survey National Water Quality Assessment program. Starting in 1999, new research was undertaken to expand and update NLCD 1992 into a full scale land cover database (with multiple instead of single products), and to produce it across all 50 states and Puerto Rico (Homer et al. 2004). This new database is called the National Land Cover Database 2001 (the 2001 refers to the nominal year from which most of the Landsat 5 and Landsat 7 imagery was acquired) and has been under production for 6 years. This article announces the completion of NLCD 2001 for the conterminous United States, with products that can identify one of 16 classes of land cover, the percent tree canopy, and the percent urban imperviousness for every 30-meter cell in the conterminous 48 states (approximately 27 billion cells). Both NLCD 1992 and NLCD 2001 have been produced and funded through an umbrella organization called the Multi-Resolution Land Characteristics Consortium (MRLC). This Consortium consists of 13 programs across 10 Federal agencies that require landcover data for addressing their agency needs (www.mrlc.gov). MRLC provided the umbrella to coordinate multi-agency NLCD mapping production and funding contributions. In addition to NLCD data, MRLC also offers approximately 6,200 terrain corrected Landsat 5 TM and Landsat 7 Enhanced Thematic Mapper (ETM+) scenes spanning growing season dates from 1982-2006 which are available for public web-enabled download from www.mrlc. gov. MRLC represents an excellent example of Federal government collaboration across many agencies to synergistically develop important geo-spatial data for the Nation.",c67,Enterprise Application Integration,cp67,accepted,f1804,2002,2002-03-17
s2243,p2243,"The PROSITE database, its status in 1997","The PROSITE database (http://www.expasy.ch/sprot/prosite.htm l) consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",cp38,accepted,f1805,2015,2015-11-25
s2245,p2245,The notions of consistency and predicate locks in a database system,"In database systems, users access shared data under the assumption that the data satisfies certain consistency constraints. This paper defines the concepts of transaction, consistency and schedule and shows that consistency requires that a transaction cannot request new locks after releasing a lock. Then it is argued that a transaction needs to lock a logical rather than a physical subset of the database. These subsets may be specified by predicates. An implementation of predicate locks which satisfies the consistency condition is suggested.",c78,Neural Information Processing Systems,cp78,accepted,f1806,2012,2012-01-27
s2246,p2246,EpiFactors: a comprehensive database of human epigenetic factors and complexes,"Epigenetics refers to stable and long-term alterations of cellular traits that are not caused by changes in the DNA sequence per se. Rather, covalent modifications of DNA and histones affect gene expression and genome stability via proteins that recognize and act upon such modifications. Many enzymes that catalyse epigenetic modifications or are critical for enzymatic complexes have been discovered, and this is encouraging investigators to study the role of these proteins in diverse normal and pathological processes. Rapidly growing knowledge in the area has resulted in the need for a resource that compiles, organizes and presents curated information to the researchers in an easily accessible and user-friendly form. Here we present EpiFactors, a manually curated database providing information about epigenetic regulators, their complexes, targets and products. EpiFactors contains information on 815 proteins, including 95 histones and protamines. For 789 of these genes, we include expressions values across several samples, in particular a collection of 458 human primary cell samples (for approximately 200 cell types, in many cases from three individual donors), covering most mammalian cell steady states, 255 different cancer cell lines (representing approximately 150 cancer subtypes) and 134 human postmortem tissues. Expression values were obtained by the FANTOM5 consortium using Cap Analysis of Gene Expression technique. EpiFactors also contains information on 69 protein complexes that are involved in epigenetic regulation. The resource is practical for a wide range of users, including biologists, pharmacologists and clinicians. Database URL: http://epifactors.autosome.ru",c74,IEEE International Conference on Tools with Artificial Intelligence,cp74,accepted,f1807,2003,2003-05-04
s2247,p2247,Database Systems: The Complete Book,"From the Publisher: 
This introduction to database systems offers a readable comprehensive approach with engaging, real-world examplesusers will learn how to successfully plan a database application before building it. The first half of the book provides in-depth coverage of databases from the point of view of the database designer, user, and application programmer, while the second half of the book provides in-depth coverage of databases from the point of view of the DBMS implementor. The first half of the book focuses on database design, database use, and implementation of database applications and database management systemsit covers the latest database standards SQL:1999, SQL/PSM, SQL/CLI, JDBC, ODL, and XML, with broader coverage of SQL than most other books. The second half of the book focuses on storage structures, query processing, and transaction managementit covers the main techniques in these areas with broader coverage of query optimization than most other books, along with advanced topics including multidimensional and bitmap indexes, distributed transactions, and information integration techniques. A professional reference for database designers, users, and application programmers.",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f1808,2005,2005-04-30
s2248,p2248,A New Database on the Structure and Development of the Financial Sector,"This article introduces a new database of indicators of financial structure and financial development across countries and over time. The database is unique in that it combines a wide variety of indicators that measure the size, activity, and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, introducing indicators of the size and activity of nonbank financial institutions, and constructing measures of the size of bond and primary equity markets. This article introduces a new database, the first to provide comprehensive measures of the development, structure, and performance of the financial sector. This database is the first to define and construct indicators of the size and activity of nonbank financial intermediaries, such as insurance companies, pension funds, and non-deposit money banks. It is also the first to include indicators of the size of primary equity markets and primary and secondary bond markets. In constructing the database, authors carefully deflate measures and match stock and flow variables.",c64,Experimental Software Engineering Network,cp64,accepted,f1809,2014,2014-12-30
s2250,p2250,BindingDB: a web-accessible database of experimentally determined protein–ligand binding affinities,"BindingDB () is a publicly accessible database currently containing ∼20 000 experimentally determined binding affinities of protein–ligand complexes, for 110 protein targets including isoforms and mutational variants, and ∼11 000 small molecule ligands. The data are extracted from the scientific literature, data collection focusing on proteins that are drug-targets or candidate drug-targets and for which structural data are present in the Protein Data Bank. The BindingDB website supports a range of query types, including searches by chemical structure, substructure and similarity; protein sequence; ligand and protein names; affinity ranges and molecular weight. Data sets generated by BindingDB queries can be downloaded in the form of annotated SDfiles for further analysis, or used as the basis for virtual screening of a compound database uploaded by the user. The data in BindingDB are linked both to structural data in the PDB via PDB IDs and chemical and sequence searches, and to the literature in PubMed via PubMed IDs.",c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,cp99,accepted,f1810,2015,2015-08-15
s2251,p2251,The Transporter Classification Database,"The Transporter Classification Database (TCDB; http://www.tcdb.org) serves as a common reference point for transport protein research. The database contains more than 10 000 non-redundant proteins that represent all currently recognized families of transmembrane molecular transport systems. Proteins in TCDB are organized in a five level hierarchical system, where the first two levels are the class and subclass, the second two are the family and subfamily, and the last one is the transport system. Superfamilies that contain multiple families are included as hyperlinks to the five tier TC hierarchy. TCDB includes proteins from all types of living organisms and is the only transporter classification system that is both universal and recognized by the International Union of Biochemistry and Molecular Biology. It has been expanded by manual curation, contains extensive text descriptions providing structural, functional, mechanistic and evolutionary information, is supported by unique software and is interconnected to many other relevant databases. TCDB is of increasing usefulness to the international scientific community and can serve as a model for the expansion of database technologies. This manuscript describes an update of the database descriptions previously featured in NAR database issues.",c20,Research Conference of the South African Institute of Computer Scientists and Information Technologists,cp20,accepted,f1811,2014,2014-04-17
s2252,p2252,The World Ocean Database,"The World Ocean Database (WOD) is the most comprehensive global ocean profile-plankton database available internationally without restriction. All data are in one well-documented format and are available both on DVDs for a minimal charge and on-line without charge. The latest DVD version of the WOD is the World Ocean Database 2009 (WOD09). All data in the WOD are associated with as much metadata as possible, and every ocean data value has a quality control flag associated with it. The WOD is a product of the U.S. National Oceanographic Data Center and its co-located World Data Center for Oceanography. However, the WOD exists because of the international oceanographic data exchange that has occurred under the auspices of the Intergovernmental Oceanographic Commission (IOC) and the International Council of Science (ICSU) World Data Center (WDC) system. World Data Centers are part of the ICSU World Data System.",j66,Data Science Journal,jv66,accepted,f1812,2015,2015-07-26
s2253,p2253,"MINT, the molecular interaction database: 2012 update","The Molecular INTeraction Database (MINT, http://mint.bio.uniroma2.it/mint/) is a public repository for protein–protein interactions (PPI) reported in peer-reviewed journals. The database grows steadily over the years and at September 2011 contains approximately 235 000 binary interactions captured from over 4750 publications. The web interface allows the users to search, visualize and download interactions data. MINT is one of the members of the International Molecular Exchange consortium (IMEx) and adopts the Molecular Interaction Ontology of the Proteomics Standard Initiative (PSI-MI) standards for curation and data exchange. MINT data are freely accessible and downloadable at http://mint.bio.uniroma2.it/mint/download.do. We report here the growth of the database, the major changes in curation policy and a new algorithm to assign a confidence to each interaction.",c92,Advances in Soft Computing,cp92,accepted,f1813,2009,2009-08-03
s2254,p2254,The UCSC Genome Browser database: extensions and updates 2011,"The University of California Santa Cruz Genome Browser (http://genome.ucsc.edu) offers online public access to a growing database of genomic sequence and annotations for a wide variety of organisms. The Browser is an integrated tool set for visualizing, comparing, analyzing and sharing both publicly available and user-generated genomic data sets. In the past year, the local database has been updated with four new species assemblies, and we anticipate another four will be released by the end of 2011. Further, a large number of annotation tracks have been either added, updated by contributors, or remapped to the latest human reference genome. Among these are new phenotype and disease annotations, UCSC genes, and a major dbSNP update, which required new visualization methods. Growing beyond the local database, this year we have introduced ‘track data hubs’, which allow the Genome Browser to provide access to remotely located sets of annotations. This feature is designed to significantly extend the number and variety of annotation tracks that are publicly available for visualization and analysis from within our site. We have also introduced several usability features including track search and a context-sensitive menu of options available with a right-click anywhere on the Browser's image.",c6,Americas Conference on Information Systems,cp6,accepted,f1814,2007,2007-04-04
s2255,p2255,"The PROSITE database, its status in 2002","PROSITE [Bairoch and Bucher (1994) Nucleic Acids Res., 22, 3583-3589; Hofmann et al. (1999) Nucleic Acids Res., 27, 215-219] is a method of identifying the functions of uncharacterized proteins translated from genomic or cDNA sequences. The PROSITE database (http://www.expasy.org/prosite/) consists of biologically significant patterns and profiles designed in such a way that with appropriate computational tools it can rapidly and reliably help to determine to which known family of proteins (if any) a new sequence belongs, or which known domain(s) it contains.",c114,IEEE International Conference on Robotics and Automation,cp114,accepted,f1815,2011,2011-12-18
s2256,p2256,NCBI’s Database of Genotypes and Phenotypes: dbGaP,"The Database of Genotypes and Phenotypes (dbGap, http://www.ncbi.nlm.nih.gov/gap) is a National Institutes of Health-sponsored repository charged to archive, curate and distribute information produced by studies investigating the interaction of genotype and phenotype. Information in dbGaP is organized as a hierarchical structure and includes the accessioned objects, phenotypes (as variables and datasets), various molecular assay data (SNP and Expression Array data, Sequence and Epigenomic marks), analyses and documents. Publicly accessible metadata about submitted studies, summary level data, and documents related to studies can be accessed freely on the dbGaP website. Individual-level data are accessible via Controlled Access application to scientists across the globe.",c77,Networks,cp77,accepted,f1816,2019,2019-02-16
s2257,p2257,Database of homology‐derived protein structures and the structural meaning of sequence alignment,"The database of known protein three‐dimensional structures can be significantly increased by the use of sequence homology, based on the following observations. (1) The database of known sequences, currently at more than 12,000 proteins, is two orders of magnitude larger than the database of known structures. (2) The currently most powerful method of predicting protein structures is model building by homology. (3) Structural homology can be inferred from the level of sequence similarity. (4) The threshold of sequence similarity sufficient for structural homology depends strongly on the length of the alignment. Here, we first quantify the relation between sequence similarity, structure similarity, and alignment length by an exhaustive survey of alignments between proteins of known structure and report a homology threshold curve as a function of alignment length. We then produce a database of homology‐derived secondary structure of proteins (HSSP) by aligning to each protein of known structure all sequences deemed homologous on the basis of the threshold curve. For each known protein structure, the derived database contains the aligned sequences, secondary structure, sequence variability, and sequence profile. Tertiary structures of the aligned sequences are implied, but not modeled explicity. The database effectively increases the number of known protein structures by a factor of five to more than 1800. The results may be useful in assessing the structural significance of matches in sequence database searches, in deriving preferences and patterns for structure prediction, in elucidating the structural role of conserved residues, and in modeling three‐dimensional detail by homology.",j306,"Proteins: Structure, Function, and Bioinformatics",jv306,accepted,f1817,2022,2022-12-10
s2258,p2258,Database,Abstract content goes here ...,c27,ACM-SIAM Symposium on Discrete Algorithms,cp27,accepted,f1818,2022,2022-06-05
s2259,p2259,A New Database of Financial Reforms,Abstract content goes here ...,c17,International Conference on Enterprise Information Systems,cp17,accepted,f1819,2008,2008-11-24
s2262,p2262,A gene expression database for the molecular pharmacology of cancer,Abstract content goes here ...,j274,Nature Genetics,jv274,accepted,f1820,2020,2020-12-11
s2263,p2263,MINT: the Molecular INTeraction database,Abstract content goes here ...,j102,Nucleic Acids Research,jv102,accepted,f1821,2002,2002-09-21
s2265,p2265,Harmonized World Soil Database (version 1.2),"METIS-ID: 167825 The Harmonized World Soil Database is a 30 arc-second raster database with over 15000 different soil mapping units that combines existing regional and national updates of soil information worldwide (SOTER, ESD, Soil Map of China, ISRIC-WISE) with the information contained within the 1:5 000 000 scale FAO-UNESCO Soil Map of the World (FAO, 1971-1981). The resulting raster database consists of ... 21600 rows and 43200 columns, which are linked to harmonized soil property data. The use of a standardized structure allows for the linkage of the attribute data with the raster map to display or query the composition in terms of soil units and the characterization of selected soil parameters (organic Carbon, pH, water storage capacity, soil depth, cation exchange capacity of the soil and the clay fraction, total exchangeable nutrients, lime and gypsum contents, sodium exchange percentage, salinity, textural class and granulometry). Reliability of the information contained in the database is variable: the parts of the database that still make use of the Soil Map of the World such as North America, Australia, West Africa and South Asia are considered less reliable, while most of the areas covered by SOTER databases are considered to have the highest reliability (Southern Africa, Latin America and the Caribbean, Central and Eastern Europe). Further expansion and update of the HWSD is foreseen for the near future, notably with the excellent databases held in the USA (Natural Resources Conservation Service US General Soil Map, STATSGO), Canada (Agriculture and AgriFood Canada: The National Soil Database NSDB), and Australia (CSIRO, ACLEP, Nnatural Heritage Trust and National Land and Water Resources Audit: ASRIS), and with the recently released SOTER database for Central Africa (FAO/ISRIC/Univ. Gent, 2007)",c65,Formal Concept Analysis,cp65,accepted,f1822,2008,2008-05-06
s2267,p2267,"ExoCarta 2012: database of exosomal proteins, RNA and lipids","Exosomes are membraneous nanovesicles of endocytic origin released by most cell types from diverse organisms; they play a critical role in cell–cell communication. ExoCarta (http://www.exocarta.org) is a manually curated database of exosomal proteins, RNA and lipids. The database catalogs information from both published and unpublished exosomal studies. The mode of exosomal purification and characterization, the biophysical and molecular properties are listed in the database aiding biomedical scientists in assessing the quality of the exosomal preparation and the corresponding data obtained. Currently, ExoCarta (Version 3.1) contains information on 11 261 protein entries, 2375 mRNA entries and 764 miRNA entries that were obtained from 134 exosomal studies. In addition to the data update, as a new feature, lipids identified in exosomes are added to ExoCarta. We believe that this free web-based community resource will aid researchers in identifying molecular signatures (proteins/RNA/lipids) that are specific to certain tissue/cell type derived exosomes and trigger new exosomal studies.",c28,International Conference on Collaboration Technologies and Systems,cp28,accepted,f1823,2009,2009-02-27
s2268,p2268,The IARC TP53 database: New online mutation analysis and recommendations to users,"Mutations in the tumor suppressor gene TP53 are frequent in most human cancers. Comparison of the mutation patterns in different cancers may reveal clues on the natural history of the disease. Over the past 10 years, several databases of TP53 mutations have been developed. The most extensive of these databases is maintained and developed at the International Agency for Research on Cancer. The database compiles all mutations (somatic and inherited), as well as polymorphisms, that have been reported in the published literature since 1989. The IARC TP53 mutation dataset is the largest dataset available on the variations of any human gene. The database is available at www.iarc.fr/P53/. In this paper, we describe recent developments of the database. These developments include restructuring of the database, which is now patient‐centered, with more detailed annotations on the patient (carcinogen exposure, virus infection, genetic background). In addition, a new on‐line application to retrieve somatic mutation data and analyze mutation patterns is now available. We also discuss limitations on the use of the database and provide recommendations to users. Hum Mutat 19:607–614, 2002. © 2002 Wiley‐Liss, Inc.",j209,Human Mutation,jv209,accepted,f1824,2005,2005-12-17
s2270,p2270,Standardizing the World Income Inequality Database,"Cross-national research on the causes and consequences of income inequality has been hindered by the limitations of existing inequality datasets: greater coverage across countries and over time is available from these sources only at the cost of signicantly reduced comparability across observations. This article presents the Standardized World Income Inequality Database (SWIID), which standardizes the United Nations University database (UNU-WIDER 2008) while minimizing reliance on problematic assumptions by using as much information as possible from proximate years within the same country. The resulting series of gross and net income inequality data maximize comparability for the largest possible sample of countries and years and so are better suited to broadly cross-national research than other sources.",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f1825,2008,2008-02-16
s2272,p2272,SAbDab: the structural antibody database,"Structural antibody database (SAbDab; http://opig.stats.ox.ac.uk/webapps/sabdab) is an online resource containing all the publicly available antibody structures annotated and presented in a consistent fashion. The data are annotated with several properties including experimental information, gene details, correct heavy and light chain pairings, antigen details and, where available, antibody–antigen binding affinity. The user can select structures, according to these attributes as well as structural properties such as complementarity determining region loop conformation and variable domain orientation. Individual structures, datasets and the complete database can be downloaded.",c60,IEEE International Conference on Software Engineering and Formal Methods,cp60,accepted,f1826,2011,2011-04-11
s2274,p2274,The Reptile Database,Abstract content goes here ...,c83,International Conference on Computer Graphics and Interactive Techniques,cp83,accepted,f1827,2015,2015-12-23
s2275,p2275,CDD: a Conserved Domain Database for protein classification,"The Conserved Domain Database (CDD) is the protein classification component of NCBI's Entrez query and retrieval system. CDD is linked to other Entrez databases such as Proteins, Taxonomy and PubMed®, and can be accessed at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. CD-Search, which is available at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi, is a fast, interactive tool to identify conserved domains in new protein sequences. CD-Search results for protein sequences in Entrez are pre-computed to provide links between proteins and domain models, and computational annotation visible upon request. Protein–protein queries submitted to NCBI's BLAST search service at http://www.ncbi.nlm.nih.gov/BLAST are scanned for the presence of conserved domains by default. While CDD started out as essentially a mirror of publicly available domain alignment collections, such as SMART, Pfam and COG, we have continued an effort to update, and in some cases replace these models with domain hierarchies curated at the NCBI. Here, we report on the progress of the curation effort and associated improvements in the functionality of the CDD information retrieval system.",c88,Symposium on the Theory of Computing,cp88,accepted,f1828,2014,2014-10-09
s2276,p2276,A lifespan database of adult facial stimuli,Abstract content goes here ...,j372,"Behavoir research methods, instruments & computers",jv372,accepted,f1829,2016,2016-12-28
s2277,p2277,Survey on NoSQL database,"With the development of the Internet and cloud computing, there need databases to be able to store and process big data effectively, demand for high-performance when reading and writing, so the traditional relational database is facing many new challenges. Especially in large scale and high-concurrency applications, such as search engines and SNS, using the relational database to store and query dynamic user data has appeared to be inadequate. In this case, NoSQL database created. This paper describes the background, basic characteristics, data model of NoSQL. In addition, this paper classifies NoSQL databases according to the CAP theorem. Finally, the mainstream NoSQL databases are separately described in detail, and extract some properties to help enterprises to choose NoSQL.",c41,Software Product Lines Conference,cp41,accepted,f1830,2002,2002-12-22
s2278,p2278,Database resources of the National Center for Biotechnology,"In addition to maintaining the GenBank(R) nucleic acid sequence database, the National Center for Biotechnology Information (NCBI) provides data analysis and retrieval resources for the data in GenBank and other biological data made available through NCBI's Web site. NCBI resources include Entrez, PubMed, PubMed Central (PMC), LocusLink, the NCBITaxonomy Browser, BLAST, BLAST Link (BLink), Electronic PCR (e-PCR), Open Reading Frame (ORF) Finder, References Sequence (RefSeq), UniGene, HomoloGene, ProtEST, Database of Single Nucleotide Polymorphisms (dbSNP), Human/Mouse Homology Map, Cancer Chromosome Aberration Project (CCAP), Entrez Genomes and related tools, the Map Viewer, Model Maker (MM), Evidence Viewer (EV), Clusters of Orthologous Groups (COGs) database, Retroviral Genotyping Tools, SAGEmap, Gene Expression Omnibus (GEO), Online Mendelian Inheritance in Man (OMIM), the Molecular Modeling Database (MMDB), the Conserved Domain Database (CDD), and the Conserved Domain Architecture Retrieval Tool (CDART). Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. All of the resources can be accessed through the NCBI home page at: http://www.ncbi.nlm.nih.gov.",c17,International Conference on Enterprise Information Systems,cp17,accepted,f1831,2008,2008-07-21
s2279,p2279,"MEROPS: the database of proteolytic enzymes, their substrates and inhibitors","Peptidases, their substrates and inhibitors are of great relevance to biology, medicine and biotechnology. The MEROPS database (http://merops.sanger.ac.uk) aims to fulfil the need for an integrated source of information about these. The database has hierarchical classifications in which homologous sets of peptidases and protein inhibitors are grouped into protein species, which are grouped into families, which are in turn grouped into clans. The database has been expanded to include proteolytic enzymes other than peptidases. Special identifiers for peptidases from a variety of model organisms have been established so that orthologues can be detected in other species. A table of predicted active-site residue and metal ligand positions and the residue ranges of the peptidase domains in orthologues has been added to each peptidase summary. New displays of tertiary structures, which can be rotated or have the surfaces displayed, have been added to the structure pages. New indexes for gene names and peptidase substrates have been made available. Among the enhancements to existing features are the inclusion of small-molecule inhibitors in the tables of peptidase–inhibitor interactions, a table of known cleavage sites for each protein substrate, and tables showing the substrate-binding preferences of peptidases derived from combinatorial peptide substrate libraries.",c107,British Machine Vision Conference,cp107,accepted,f1832,2012,2012-09-03
s2280,p2280,The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of Pathway/Genome Databases,"The MetaCyc database (MetaCyc.org) is a comprehensive and freely accessible database describing metabolic pathways and enzymes from all domains of life. MetaCyc pathways are experimentally determined, mostly small-molecule metabolic pathways and are curated from the primary scientific literature. MetaCyc contains >2100 pathways derived from >37 000 publications, and is the largest curated collection of metabolic pathways currently available. BioCyc (BioCyc.org) is a collection of >3000 organism-specific Pathway/Genome Databases (PGDBs), each containing the full genome and predicted metabolic network of one organism, including metabolites, enzymes, reactions, metabolic pathways, predicted operons, transport systems and pathway-hole fillers. Additions to BioCyc over the past 2 years include YeastCyc, a PGDB for Saccharomyces cerevisiae, and 891 new genomes from the Human Microbiome Project. The BioCyc Web site offers a variety of tools for querying and analysis of PGDBs, including Omics Viewers and tools for comparative analysis. New developments include atom mappings in reactions, a new representation of glycan degradation pathways, improved compound structure display, better coverage of enzyme kinetic data, enhancements of the Web Groups functionality, improvements to the Omics viewers, a new representation of the Enzyme Commission system and, for the desktop version of the software, the ability to save display states.",c63,IEEE International Software Metrics Symposium,cp63,accepted,f1833,2008,2008-10-13
s2281,p2281,DBatVir: the database of bat-associated viruses,"Emerging infectious diseases remain a significant threat to public health. Most emerging infectious disease agents in humans are of zoonotic origin. Bats are important reservoir hosts of many highly lethal zoonotic viruses and have been implicated in numerous emerging infectious disease events in recent years. It is essential to enhance our knowledge and understanding of the genetic diversity of the bat-associated viruses to prevent future outbreaks. To facilitate further research, we constructed the database of bat-associated viruses (DBatVir). Known viral sequences detected in bat samples were manually collected and curated, along with the related metadata, such as the sampling time, location, bat species and specimen type. Additional information concerning the bats, including common names, diet type, geographic distribution and phylogeny were integrated into the database to bridge the gap between virologists and zoologists. The database currently covers >4100 bat-associated animal viruses of 23 viral families detected from 196 bat species in 69 countries worldwide. It provides an overview and snapshot of the current research regarding bat-associated viruses, which is essential now that the field is rapidly expanding. With a user-friendly interface and integrated online bioinformatics tools, DBatVir provides a convenient and powerful platform for virologists and zoologists to analyze the virome diversity of bats, as well as for epidemiologists and public health researchers to monitor and track current and future bat-related infectious diseases. Database URL: http://www.mgc.ac.cn/DBatVir/",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f1834,2007,2007-09-21
s2283,p2283,PlasmoDB: a functional genomic database for malaria parasites,"PlasmoDB (http://PlasmoDB.org) is a functional genomic database for Plasmodium spp. that provides a resource for data analysis and visualization in a gene-by-gene or genome-wide scale. PlasmoDB belongs to a family of genomic resources that are housed under the EuPathDB (http://EuPathDB.org) Bioinformatics Resource Center (BRC) umbrella. The latest release, PlasmoDB 5.5, contains numerous new data types from several broad categories—annotated genomes, evidence of transcription, proteomics evidence, protein function evidence, population biology and evolution. Data in PlasmoDB can be queried by selecting the data of interest from a query grid or drop down menus. Various results can then be combined with each other on the query history page. Search results can be downloaded with associated functional data and registered users can store their query history for future retrieval or analysis.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f1835,2010,2010-10-24
s2284,p2284,Phenol-Explorer: an online comprehensive database on polyphenol contents in foods,"A number of databases on the plant metabolome describe the chemistry and biosynthesis of plant chemicals. However, no such database is specifically focused on foods and more precisely on polyphenols, one of the major classes of phytochemicals. As antoxidants, polyphenols influence human health and may play a role in the prevention of a number of chronic diseases such as cardiovascular diseases, some cancers or type 2 diabetes. To determine polyphenol intake in populations and study their association with health, it is essential to have detailed information on their content in foods. However this information is not easily collected due to the variety of their chemical structures and the variability of their content in a given food. Phenol-Explorer is the first comprehensive web-based database on polyphenol content in foods. It contains more than 37 000 original data points collected from 638 scientific articles published in peer-reviewed journals. The quality of these data has been evaluated before they were aggregated to produce final representative mean content values for 502 polyphenols in 452 foods. The web interface allows making various queries on the aggregated data to identify foods containing a given polyphenol or polyphenols present in a given food. For each mean content value, it is possible to trace all original content values and their literature sources. Phenol-Explorer is a major step forward in the development of databases on food constituents and the food metabolome. It should help researchers to better understand the role of phytochemicals in the technical and nutritional quality of food, and food manufacturers to develop tailor-made healthy foods. Database URL: http://www.phenol-explorer.eu",c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f1836,2006,2006-08-02
s2285,p2285,Database System Concepts,"From the Publisher: 
This acclaimed revision of a classic database systems text offers a complete background in the basics of database design, languages, and system implementation. It provides the latest information combined with real-world examples to help readers master concepts. All concepts are presented in a technically complete yet easy-to-understand style with notations kept to a minimum. A running example of a bank enterprise illustrates concepts at work. To further optimize comprehension, figures and examples, rather than proofs, portray concepts and anticipate results.",c6,Americas Conference on Information Systems,cp6,accepted,f1837,2007,2007-05-31
s2286,p2286,Principles Of Database And Knowledge-Base Systems,"This book goes into the details of database conception and use, it tells you everything on relational databases. from theory to the actual used algorithms.",c0,International Conference on Human Factors in Computing Systems,cp0,accepted,f1838,2019,2019-04-22
s2287,p2287,The Human Oral Microbiome Database: a web accessible resource for investigating oral microbe taxonomic and genomic information,"The human oral microbiome is the most studied human microflora, but 53% of the species have not yet been validly named and 35% remain uncultivated. The uncultivated taxa are known primarily from 16S rRNA sequence information. Sequence information tied solely to obscure isolate or clone numbers, and usually lacking accurate phylogenetic placement, is a major impediment to working with human oral microbiome data. The goal of creating the Human Oral Microbiome Database (HOMD) is to provide the scientific community with a body site-specific comprehensive database for the more than 600 prokaryote species that are present in the human oral cavity based on a curated 16S rRNA gene-based provisional naming scheme. Currently, two primary types of information are provided in HOMD—taxonomic and genomic. Named oral species and taxa identified from 16S rRNA gene sequence analysis of oral isolates and cloning studies were placed into defined 16S rRNA phylotypes and each given unique Human Oral Taxon (HOT) number. The HOT interlinks phenotypic, phylogenetic, genomic, clinical and bibliographic information for each taxon. A BLAST search tool is provided to match user 16S rRNA gene sequences to a curated, full length, 16S rRNA gene reference data set. For genomic analysis, HOMD provides comprehensive set of analysis tools and maintains frequently updated annotations for all the human oral microbial genomes that have been sequenced and publicly released. Oral bacterial genome sequences, determined as part of the Human Microbiome Project, are being added to the HOMD as they become available. We provide HOMD as a conceptual model for the presentation of microbiome data for other human body sites. Database URL: http://www.homd.org",c96,USENIX Symposium on Operating Systems Design and Implementation,cp96,accepted,f1839,2006,2006-03-19
s2289,p2289,"Principles of Database and Knowledge-Base Systems, Volume II",Abstract content goes here ...,c28,International Conference on Collaboration Technologies and Systems,cp28,accepted,f1840,2009,2009-04-27
s2290,p2290,Extending the database relational model to capture more meaning,"During the last three or four years several investigators have been exploring “semantic models” for formatted databases. The intent is to capture (in a more or less formal way) more of the meaning of the data so that database design can become more systematic and the database system itself can behave more intelligently. Two major thrusts are clear. (1) the search for meaningful units that are as small as possible—atomic semantics; (2) the search for meaningful units that are larger than the usual n-ary relation—molecular semantics. In this paper we propose extensions to the relational model to support certain atomic and molecular semantics. These extensions represent a synthesis of many ideas from the published work in semantic modeling plus the introduction of new rules for insertion, update, and deletion, as well as new algebraic operators.",j373,ACM Transactions on Database Systems,jv373,accepted,f1841,2022,2022-02-23
s2292,p2292,XCOM : Photon Cross Sections Database,Abstract content goes here ...,c5,IEEE Symposium on Visual Languages / Human-Centric Computing Languages and Environments,cp5,accepted,f1842,2001,2001-06-19
s2293,p2293,Epidemic algorithms for replicated database maintenance,"Whru a dilt~lhSC is replicated at, many sites2 maintaining mutual consistrnry among t,he sites iu the fac:e of updat,es is a signitirant problem. This paper descrikrs several randomized algorit,hms for dist,rihut.ing updates and driving t,he replicas toward consist,c>nc,y. The algorit Inns are very simple and require few guarant,ees from the underlying conllllunicat.ioll system, yc+ they rnsutc t.hat. the off(~c~t, of (‘very update is evcnt,uwlly rf+irt-ted in a11 rq1ica.s. The cost, and parformancc of t,hr algorithms arc tuned I>? c%oosing appropriat,c dist,rilMions in t,hc randoinizat,ioii step. TIN> idgoritlmls ilr(’ c*los~*ly analogoIls t,o epidemics, and t,he epidcWliolog)litc\ratiirc, ilitlh iii Illld~~rsti4lldill~ tlicir bc*liavior. One of tlW i$,oritlims 11&S brc>n implrmcWrd in the Clraringhousr sprv(brs of thr Xerox C’orporat~c~ Iiitcrnc4, solviiig long-standing prol>lf~lns of high traffic and tlatirl>ilsr inconsistcllcp.",c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f1843,2021,2021-06-03
s2294,p2294,The UCSC Genome Browser database: update 2010,"The University of California, Santa Cruz (UCSC) Genome Browser website (http://genome.ucsc.edu/) provides a large database of publicly available sequence and annotation data along with an integrated tool set for examining and comparing the genomes of organisms, aligning sequence to genomes, and displaying and sharing users’ own annotation data. As of September 2009, genomic sequence and a basic set of annotation ‘tracks’ are provided for 47 organisms, including 14 mammals, 10 non-mammal vertebrates, 3 invertebrate deuterostomes, 13 insects, 6 worms and a yeast. New data highlights this year include an updated human genome browser, a 44-species multiple sequence alignment track, improved variation and phenotype tracks and 16 new genome-wide ENCODE tracks. New features include drag-and-zoom navigation, a Wiki track for user-added annotations, new custom track formats for large datasets (bigBed and bigWig), a new multiple alignment output tool, links to variation and protein structure tools, in silico PCR utility enhancements, and improved track configuration tools.",c71,IEEE International Conference on Information Reuse and Integration,cp71,accepted,f1844,2022,2022-11-13
s2295,p2295,The Forest Inventory and Analysis Database: Database Description and Users Manual Version 4.0 for Phase 2,"This document is based on previous documentation of the nationally standardized Forest Inventory and Analysis database (Hansen and others 1992; Woudenberg and Farrenkopf 1995; Miles and others 2001). Documentation of the structure of the Forest Inventory and Analysis database (FIADB) for Phase 2 data, as well as codes and definitions, is provided. Examples for producing population level estimates are also presented. This database provides a consistent framework for storing forest inventory data across all ownerships for the entire United States. These data are available to the public.",c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,cp79,accepted,f1845,2020,2020-06-29
s2296,p2296,D2P2: database of disordered protein predictions,"We present the Database of Disordered Protein Prediction (D2P2), available at http://d2p2.pro (including website source code). A battery of disorder predictors and their variants, VL-XT, VSL2b, PrDOS, PV2, Espritz and IUPred, were run on all protein sequences from 1765 complete proteomes (to be updated as more genomes are completed). Integrated with these results are all of the predicted (mostly structured) SCOP domains using the SUPERFAMILY predictor. These disorder/structure annotations together enable comparison of the disorder predictors with each other and examination of the overlap between disordered predictions and SCOP domains on a large scale. D2P2 will increase our understanding of the interplay between disorder and structure, the genomic distribution of disorder, and its evolutionary history. The parsed data are made available in a unified format for download as flat files or SQL tables either by genome, by predictor, or for the complete set. An interactive website provides a graphical view of each protein annotated with the SCOP domains and disordered regions from all predictors overlaid (or shown as a consensus). There are statistics and tools for browsing and comparing genomes and their disorder within the context of their position on the tree of life.",c94,Vision,cp94,accepted,f1846,2020,2020-08-28
s2297,p2297,The ribosomal database project,"The Ribosomal Database Project (RDP) is a curated database that offers ribosome data along with related programs and services. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams and various software packages for handling, analyzing and displaying alignments and trees. The data are available via ftp and electronic mail. Certain analytic services are also provided by the electronic mail server.",c28,International Conference on Collaboration Technologies and Systems,cp28,accepted,f1847,2009,2009-12-26
s2298,p2298,CDD: specific functional annotation with the Conserved Domain Database,"NCBI's Conserved Domain Database (CDD) is a collection of multiple sequence alignments and derived database search models, which represent protein domains conserved in molecular evolution. The collection can be accessed at http://www.ncbi.nlm.nih.gov/Structure/cdd/cdd.shtml, and is also part of NCBI's Entrez query and retrieval system, cross-linked to numerous other resources. CDD provides annotation of domain footprints and conserved functional sites on protein sequences. Precalculated domain annotation can be retrieved for protein sequences tracked in NCBI's Entrez system, and CDD's collection of models can be queried with novel protein sequences via the CD-Search service at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. Starting with the latest version of CDD, v2.14, information from redundant and homologous domain models is summarized at a superfamily level, and domain annotation on proteins is flagged as either ‘specific’ (identifying molecular function with high confidence) or as ‘non-specific’ (identifying superfamily membership only).",c112,Very Large Data Bases Conference,cp112,accepted,f1848,2018,2018-12-23
s2299,p2299,Systemic Banking Crises Database; An Update,"We update the widely used banking crises database by Laeven and Valencia (2008, 2010) with new information on recent and ongoing crises, including updated information on policy responses and outcomes (i.e. fiscal costs, output losses, and increases in public debt). We also update our dating of sovereign debt and currency crises. The database includes all systemic banking, currency, and sovereign debt crises during the period 1970-2011. The data show some striking differences in policy responses between advanced and emerging economies as well as many similarities between past and ongoing crises.",c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f1849,2018,2018-04-16
s2300,p2300,Database indexing for production MegaBLAST searches,"Motivation: The BLAST software package for sequence comparison speeds up homology search by preprocessing a query sequence into a lookup table. Numerous research studies have suggested that preprocessing the database instead would give better performance. However, production usage of sequence comparison methods that preprocess the database has been limited to programs such as BLAT and SSAHA that are designed to find matches when query and database subsequences are highly similar. Results: We developed a new version of the MegaBLAST module of BLAST that does the initial phase of finding short seeds for matches by searching a database index. We also developed a program makembindex that preprocesses the database into a data structure for rapid seed searching. We show that the new ‘indexed MegaBLAST’ is faster than the ‘non-indexed’ version for most practical uses. We show that indexed MegaBLAST is faster than miBLAST, another implementation of BLAST nucleotide searching with a preprocessed database, for most of the 200 queries we tested. To deploy indexed MegaBLAST as part of NCBI'sWeb BLAST service, the storage of databases and the queueing mechanism were modified, so that some machines are now dedicated to serving queries for a specific database. The response time for such Web queries is now faster than it was when each computer handled queries for multiple databases. Availability: The code for indexed MegaBLAST is part of the blastn program in the NCBI C++ toolkit. The preprocessor program makembindex is also in the toolkit. Indexed MegaBLAST has been used in production on NCBI's Web BLAST service to search one version of the human and mouse genomes since October 2007. The Linux command-line executables for blastn and makembindex, documentation, and some query sets used to carry out the tests described below are available in the directory: ftp://ftp.ncbi.nlm.nih.gov/pub/agarwala/indexed_megablast Contact: schaffer@helix.nih.gov Supplementary information: Supplementary data are available at Bioinformatics online.",c62,International Conference on Software Reuse,cp62,accepted,f1850,2006,2006-01-22
s2303,p2303,The IMGT/HLA database,"It is 10 years since the IMGT/HLA database was released, providing the HLA community with a searchable repository of highly curated HLA sequences. The HLA complex is located within the 6p21.3 region of human chromosome 6 and contains more than 220 genes of diverse function. Many of the genes encode proteins of the immune system and are highly polymorphic. The naming of these HLA genes and alleles, and their quality control is the responsibility of the WHO Nomenclature Committee for Factors of the HLA System. Through the work of the HLA Informatics Group and in collaboration with the European Bioinformatics Institute, we are able to provide public access to this data through the website http://www.ebi.ac.uk/imgt/hla/. The first release contained 964 sequences, the most recent release 3300 sequences, with around 450 new sequences been added each year. The tools provided on the website have been updated to allow more complex alignments, which include genomic sequence data, as well as the development of tools for probe and primer design and the inclusion of data from the HLA Dictionary. Regular updates to the website ensure that new and confirmatory sequences are dispersed to the HLA community, and the wider research and clinical communities.",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f1851,2002,2002-10-28
s2304,p2304,The Human Gene Mutation Database: 2008 update,Abstract content goes here ...,j242,Genome Medicine,jv242,accepted,f1852,2022,2022-05-30
s2305,p2305,The COSMIC (Catalogue of Somatic Mutations in Cancer) database and website,Abstract content goes here ...,j375,British Journal of Cancer,jv375,accepted,f1853,2013,2013-03-25
s2306,p2306,Web-based database for facial expression analysis,"In the last decade, the research topic of automatic analysis of facial expressions has become a central topic in machine vision research. Nonetheless, there is a glaring lack of a comprehensive, readily accessible reference set of face images that could be used as a basis for benchmarks for efforts in the field. This lack of easily accessible, suitable, common testing resource forms the major impediment to comparing and extending the issues concerned with automatic facial expression analysis. In this paper, we discuss a number of issues that make the problem of creating a benchmark facial expression database difficult. We then present the MMI facial expression database, which includes more than 1500 samples of both static images and image sequences of faces in frontal and in profile view displaying various expressions of emotion, single and multiple facial muscle activation. It has been built as a Web-based direct-manipulation application, allowing easy access and easy search of the available images. This database represents the most comprehensive reference set of images for studies on facial expression analysis to date.",c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f1854,2006,2006-11-07
s2307,p2307,UCID: an uncompressed color image database,"Standardised image databases or rather the lack of them are one of the main weaknesses in the field of content based image retrieval (CBIR). Authors often use their own images or do not specify the source of their datasets. Naturally this makes comparison of results somewhat difficult. While a first approach towards a common colour image set has been taken by the MPEG 7 committee their database does not cater for all strands of research in the CBIR community. In particular as the MPEG-7 images only exist in compressed form it does not allow for an objective evaluation of image retrieval algorithms that operate in the compressed domain or to judge the influence image compression has on the performance of CBIR algorithms. In this paper we introduce a new dataset, UCID (pronounced ""use it"") - an Uncompressed Colour Image Dataset which tries to bridge this gap. The UCID dataset currently consists of 1338 uncompressed images together with a ground truth of a series of query images with corresponding models that an ideal CBIR algorithm would retrieve. While its initial intention was to provide a dataset for the evaluation of compressed domain algorithms, the UCID database also represents a good benchmark set for the evaluation of any kind of CBIR method as well as an image set that can be used to evaluate image compression and colour quantisation algorithms.",c54,International Workshop on Agent-Oriented Software Engineering,cp54,accepted,f1855,2015,2015-04-09
s2309,p2309,GMD@CSB.DB: the Golm Metabolome Database,"UNLABELLED
Metabolomics, in particular gas chromatography-mass spectrometry (GC-MS) based metabolite profiling of biological extracts, is rapidly becoming one of the cornerstones of functional genomics and systems biology. Metabolite profiling has profound applications in discovering the mode of action of drugs or herbicides, and in unravelling the effect of altered gene expression on metabolism and organism performance in biotechnological applications. As such the technology needs to be available to many laboratories. For this, an open exchange of information is required, like that already achieved for transcript and protein data. One of the key-steps in metabolite profiling is the unambiguous identification of metabolites in highly complex metabolite preparations from biological samples. Collections of mass spectra, which comprise frequently observed metabolites of either known or unknown exact chemical structure, represent the most effective means to pool the identification efforts currently performed in many laboratories around the world. Here we present GMD, The Golm Metabolome Database, an open access metabolome database, which should enable these processes. GMD provides public access to custom mass spectral libraries, metabolite profiling experiments as well as additional information and tools, e.g. with regard to methods, spectral information or compounds. The main goal will be the representation of an exchange platform for experimental research activities and bioinformatics to develop and improve metabolomics by multidisciplinary cooperation.


AVAILABILITY
http://csbdb.mpimp-golm.mpg.de/gmd.html


CONTACT
Steinhauser@mpimp-golm.mpg.de


SUPPLEMENTARY INFORMATION
http://csbdb.mpimp-golm.mpg.de/",c15,International Conference on Conceptual Structures,cp15,accepted,f1856,2011,2011-06-23
s2310,p2310,"Output, Input and Productivity Measures at the Industry Level: The EU KLEMS Database","This article describes the contents and the construction of the EU KLEMS Growth and Productivity Accounts. This database contains industry-level measures of output, inputs and productivity for 25 European countries, Japan and the US for the period from 1970 onwards. The article considers the methodology employed in constructing the database and shows how it can be useful in comparing productivity trends. Although growth accounts are the organising principle, it is argued that the database is useful for a wider range of applications. We give some guidance to prudent use and indicate possible extensions. Copyright © The Author(s). Journal compilation © Royal Economic Society 2009.",c109,International Conference on Mobile Data Management,cp109,accepted,f1857,2014,2014-12-23
s2311,p2311,dbEST — database for “expressed sequence tags”,Abstract content goes here ...,j274,Nature Genetics,jv274,accepted,f1858,2020,2020-12-14
s2312,p2312,The KEGG database.,"KEGG (http://www.genome.ad.jp/kegg/) is a suite of databases and associated software for understanding and simulating higher-order functional behaviours of the cell or the organism from its genome information. First, KEGG computerizes data and knowledge on protein interaction networks (PATHWAY database) and chemical reactions (LIGAND database) that are responsible for various cellular processes. Second, KEGG attempts to reconstruct protein interaction networks for all organisms whose genomes are completely sequenced (GENES and SSDB databases). Third, KEGG can be utilized as reference knowledge for functional genomics (EXPRESSION database) and proteomics (BRITE database) experiments. I will review the current status of KEGG and report on new developments in graph representation and graph computations.",j376,Novartis Foundation symposium,jv376,accepted,f1859,2007,2007-08-10
s2313,p2313,Parallel database systems: the future of high performance database systems,"The success of these systems refutes a 1983 paper predicting the demise of database machines [3]. Ten years ago the future of highly parallel database machines seemed gloomy, even to their staunchest advocates. Most database machine research had focused on specialized, often trendy, hardware such as CCD memories, bubble memories, head-per-track disks, and optical disks. None of these technologies fulfilled their promises; so there was a sense that conventional CPUs , electronic RAM, and mcving-head magnetic disks would dominate the scene for many years to come. At that time, disk throughput was predicted to double while processor speeds were predicted to increase by much larger factors. Consequently , critics predicted that multiprocessor systems would scxm be I/O limited unless a solution to the I/O bottleneck was found. Whiie these predictions were fairly accurate about the future of hardware, the critics were certainly wrong about the overall future of parallel database systems. Over the last decade 'Eradata, Tandem, and a host of startup companies have successfully developed and marketed highly parallel machines.",c40,IEEE International Conference on Software Maintenance and Evolution,cp40,accepted,f1860,2013,2013-05-20
s2315,p2315,Immune epitope database analysis resource,"The immune epitope database analysis resource (IEDB-AR: http://tools.iedb.org) is a collection of tools for prediction and analysis of molecular targets of T- and B-cell immune responses (i.e. epitopes). Since its last publication in the NAR webserver issue in 2008, a new generation of peptide:MHC binding and T-cell epitope predictive tools have been added. As validated by different labs and in the first international competition for predicting peptide:MHC-I binding, their predictive performances have improved considerably. In addition, a new B-cell epitope prediction tool was added, and the homology mapping tool was updated to enable mapping of discontinuous epitopes onto 3D structures. Furthermore, to serve a wider range of users, the number of ways in which IEDB-AR can be accessed has been expanded. Specifically, the predictive tools can be programmatically accessed using a web interface and can also be downloaded as software packages.",c29,International Conference on Software Engineering,cp29,accepted,f1861,2015,2015-09-28
s2316,p2316,The Comparative Toxicogenomics Database: update 2013,"The Comparative Toxicogenomics Database (CTD; http://ctdbase.org/) provides information about interactions between environmental chemicals and gene products and their relationships to diseases. Chemical–gene, chemical–disease and gene–disease interactions manually curated from the literature are integrated to generate expanded networks and predict many novel associations between different data types. CTD now contains over 15 million toxicogenomic relationships. To navigate this sea of data, we added several new features, including DiseaseComps (which finds comparable diseases that share toxicogenomic profiles), statistical scoring for inferred gene–disease and pathway–chemical relationships, filtering options for several tools to refine user analysis and our new Gene Set Enricher (which provides biological annotations that are enriched for gene sets). To improve data visualization, we added a Cytoscape Web view to our ChemComps feature, included color-coded interactions and created a ‘slim list’ for our MEDIC disease vocabulary (allowing diseases to be grouped for meta-analysis, visualization and better data management). CTD continues to promote interoperability with external databases by providing content and cross-links to their sites. Together, this wealth of expanded chemical–gene–disease data, combined with novel ways to analyze and view content, continues to help users generate testable hypotheses about the molecular mechanisms of environmental diseases.",c56,European Conference on Software Process Improvement,cp56,accepted,f1862,2016,2016-08-01
s2317,p2317,Encyclopedia of Database Systems,Abstract content goes here ...,c10,Big Data,cp10,accepted,f1863,2021,2021-03-23
s2318,p2318,The UMIST database for astrochemistry 2012,"We present the fifth release of the UMIST Database for Astrochemistry (UDfA). The new reaction network contains 6173 gas-phase reactions, involving 467 species, 47 of which are new to this release. We have updated rate coefficients across all reaction types. We have included 1171 new anion reactions and updated and reviewed all photorates. In addition to the usual reaction network, we also now include, for download, state-specific deuterated rate coefficients, deuterium exchange reactions and a list of surface binding energies for many neutral species. Where possible, we have referenced the original source of all new and existing data. We have tested the main reaction network using a dark cloud model and a carbon-rich circumstellar envelope model. We present and briefly discuss the results of these models.",c26,PS,cp26,accepted,f1864,2010,2010-05-26
s2319,p2319,The MRC Psycholinguistic Database,"This paper describes a computerised database of psycholinguistic information. Semantic, syntactic, phonological and orthographic information about some or all of the 98,538 words in the database is accessible, by using a specially-written and very simple programming language. Word-association data are also included in the database. Some examples are given of the use of the database for selection of stimuli to be used in psycholinguistic experimentation or linguistic research.",c7,European Conference on Modelling and Simulation,cp7,accepted,f1865,2015,2015-04-21
s2320,p2320,Crystallography Open Database – an open-access collection of crystal structures,"The Crystallography Open Database (COD) is an ongoing initiative by crystallographers to gather all published inorganic, metal–organic and small organic molecule structures in one database, providing a straightforward search and retrieval interface. The COD adopts an open-access model for its >80 000 structure files.",j377,Journal of Applied Crystallography,jv377,accepted,f1866,2003,2003-03-17
s2321,p2321,The Object Database Standard: ODMG 2.0,"This book is the first of its kind and is produced as a result of the efforts by a consortium of database companies called the Object Database Management Group (ODMG). With this book, standards are defined for object management systems and this will be the foundational book for object-oriented database product.",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1867,2015,2015-03-01
s2322,p2322,"PROSITE, a protein domain database for functional characterization and annotation","PROSITE consists of documentation entries describing protein domains, families and functional sites, as well as associated patterns and profiles to identify them. It is complemented by ProRule, a collection of rules based on profiles and patterns, which increases the discriminatory power of these profiles and patterns by providing additional information about functionally and/or structurally critical amino acids. PROSITE is largely used for the annotation of domain features of UniProtKB/Swiss-Prot entries. Among the 983 (DNA-binding) domains, repeats and zinc fingers present in Swiss-Prot (release 57.8 of 22 September 2009), 696 (∼70%) are annotated with PROSITE descriptors using information from ProRule. In order to allow better functional characterization of domains, PROSITE developments focus on subfamily specific profiles and a new profile building method giving more weight to functionally important residues. Here, we describe AMSA, an annotated multiple sequence alignment format used to build a new generation of generalized profiles, the migration of ScanProsite to Vital-IT, a cluster of 633 CPUs, and the adoption of the Distributed Annotation System (DAS) to facilitate PROSITE data integration and interchange with other sources. The latest version of PROSITE (release 20.54, of 22 September 2009) contains 1308 patterns, 863 profiles and 869 ProRules. PROSITE is accessible at: http://www.expasy.org/prosite/.",c34,IEEE Working Conference on Mining Software Repositories,cp34,accepted,f1868,2012,2012-01-25
s2323,p2323,Principles of transaction-oriented database recovery,"In this paper, a terminological framework is provided for describing different transactionoriented recovery schemes for database systems in a conceptual rather than an implementation-dependent way. By introducing the terms materialized database, propagation strategy, and checkpoint, we obtain a means for classifying arbitrary implementations from a unified viewpoint. This is complemented by a classification scheme for logging techniques, which are precisely defined by using the other terms. It is shown that these criteria are related to all relevant questions such as speed and scope of recovery and amount of redundant information required. The primary purpose of this paper, however, is to establish an adequate and precise terminology for a topic in which the confusion of concepts and implementational aspects still imposes a lot of problems.",c91,Workshop on Algorithms and Models for the Web-Graph,cp91,accepted,f1869,2022,2022-08-29
s2324,p2324,The Geneva affective picture database (GAPED): a new 730-picture database focusing on valence and normative significance,Abstract content goes here ...,j105,Behavior Research Methods,jv105,accepted,f1870,2008,2008-12-31
s2325,p2325,MODOMICS: a database of RNA modification pathways,"MODOMICS is the first comprehensive database resource for systems biology of RNA modification. It integrates information about the chemical structure of modified nucleosides, their localization in RNA sequences, pathways of their biosynthesis and enzymes that carry out the respective reactions. MODOMICS also provides literature information, and links to other databases, including the available protein sequence and structure data. The current list of modifications and pathways is comprehensive, while the dataset of enzymes is limited to Escherichia coli and Saccharomyces cerevisiae and sequence alignments are presented only for tRNAs from these organisms. RNAs and enzymes from other organisms will be included in the near future. MODOMICS can be queried by the type of nucleoside (e.g. A, G, C, U, I, m1A, nm5s2U, etc.), type of RNA, position of a particular nucleoside, type of reaction (e.g. methylation, thiolation, deamination, etc.) and name or sequence of an enzyme of interest. Options for data presentation include graphs of pathways involving the query nucleoside, multiple sequence alignments of RNA sequences and tabular forms with enzyme and literature data. The contents of MODOMICS can be accessed through the World Wide Web at .",c107,British Machine Vision Conference,cp107,accepted,f1871,2012,2012-04-10
s2327,p2327,Rfam: updates to the RNA families database,"Rfam is a collection of RNA sequence families, represented by multiple sequence alignments and covariance models (CMs). The primary aim of Rfam is to annotate new members of known RNA families on nucleotide sequences, particularly complete genomes, using sensitive BLAST filters in combination with CMs. A minority of families with a very broad taxonomic range (e.g. tRNA and rRNA) provide the majority of the sequence annotations, whilst the majority of Rfam families (e.g. snoRNAs and miRNAs) have a limited taxonomic range and provide a limited number of annotations. Recent improvements to the website, methodologies and data used by Rfam are discussed. Rfam is freely available on the Web at http://rfam.sanger.ac.uk/and http://rfam.janelia.org/.",c7,European Conference on Modelling and Simulation,cp7,accepted,f1872,2015,2015-10-24
s2330,p2330,Method to correlate tandem mass spectra of modified peptides to amino acid sequences in the protein database.,"A method to correlate uninterpreted tandem mass spectra of modified peptides, produced under low-energy (10-50 eV) collision conditions, with amino acid sequences in a protein database has been developed. The fragmentation patterns observed in the tandem mass spectra of peptides containing covalent modifications is used to directly search and fit linear amino acid sequences in the database. Specific information relevant to sites of modification is not contained in the character-based sequence information of the databases. The search method considers each putative modification site as both modified and unmodified in one pass through the database and simultaneously considers up to three different sites of modification. The search method will identify the correct sequence if the tandem mass spectrum did not represent a modified peptide. This approach is demonstrated with peptides containing modifications such as S-carboxymethylated cysteine, oxidized methionine, phosphoserine, phosphothreonine, or phosphotyrosine. In addition, a scanning approach is used in which neutral loss scans are used to initiate the acquisition of product ion MS/MS spectra of doubly charged phosphorylated peptides during a single chromatographic run for data analysis with the database-searching algorithm. The approach described in this paper provides a convenient method to match the nascent tandem mass spectra of modified peptides to sequences in a protein database and thereby identify previously unknown sites of modification.",j206,Analytical Chemistry,jv206,accepted,f1873,2014,2014-01-29
s2331,p2331,A KINETIC DATABASE FOR ASTROCHEMISTRY (KIDA),"We present a novel chemical database for gas-phase astrochemistry. Named the KInetic Database for Astrochemistry (KIDA), this database consists of gas-phase reactions with rate coefficients and uncertainties that will be vetted to the greatest extent possible. Submissions of measured and calculated rate coefficients are welcome, and will be studied by experts before inclusion into the database. Besides providing kinetic information for the interstellar medium, KIDA is planned to contain such data for planetary atmospheres and for circumstellar envelopes. Each year, a subset of the reactions in the database (kida.uva) will be provided as a network for the simulation of the chemistry of dense interstellar clouds with temperatures between 10 K and 300 K. We also provide a code, named Nahoon, to study the time-dependent gas-phase chemistry of zero-dimensional and one-dimensional interstellar sources.",c56,European Conference on Software Process Improvement,cp56,accepted,f1874,2016,2016-02-06
s2332,p2332,ChEBI: a database and ontology for chemical entities of biological interest,"Chemical Entities of Biological Interest (ChEBI) is a freely available dictionary of molecular entities focused on ‘small’ chemical compounds. The molecular entities in question are either natural products or synthetic products used to intervene in the processes of living organisms. Genome-encoded macromolecules (nucleic acids, proteins and peptides derived from proteins by cleavage) are not as a rule included in ChEBI. In addition to molecular entities, ChEBI contains groups (parts of molecular entities) and classes of entities. ChEBI includes an ontological classification, whereby the relationships between molecular entities or classes of entities and their parents and/or children are specified. ChEBI is available online at http://www.ebi.ac.uk/chebi/",c58,Australian Software Engineering Conference,cp58,accepted,f1875,2021,2021-11-29
s2333,p2333,"DAVID: Database for Annotation, Visualization, and Integrated Discovery",Abstract content goes here ...,j5,Genome Biology,jv5,accepted,f1876,2020,2020-06-02
s2334,p2334,Principles of Database Systems,"A large part is a description of relations, their algebra and calculus, and the query languages that have been designed using these concepts. There are explanations of how the theory can be used to design good systems. A description of the optimization of queries in relation-based query languages is provided, and a chapter is devoted to the recently developed protocols for guaranteeing consistency in databases that are operated on by many processes concurrently",c100,ACM SIGMOD Conference,cp100,accepted,f1877,2010,2010-08-17
s2336,p2336,"The RNA modification database, RNAMDB: 2011 update","Since its inception in 1994, The RNA Modification Database (RNAMDB, http://rna-mdb.cas.albany.edu/RNAmods/) has served as a focal point for information pertaining to naturally occurring RNA modifications. In its current state, the database employs an easy-to-use, searchable interface for obtaining detailed data on the 109 currently known RNA modifications. Each entry provides the chemical structure, common name and symbol, elemental composition and mass, CA registry numbers and index name, phylogenetic source, type of RNA species in which it is found, and references to the first reported structure determination and synthesis. Though newly transferred in its entirety to The RNA Institute, the RNAMDB continues to grow with two notable additions, agmatidine and 8-methyladenosine, appended in the last year. The RNA Modification Database is staying up-to-date with significant improvements being prepared for inclusion within the next year and the following year. The expanded future role of The RNA Modification Database will be to serve as a primary information portal for researchers across the entire spectrum of RNA-related research.",c25,International Conference on Contemporary Computing,cp25,accepted,f1878,2014,2014-08-02
s2338,p2338,TCM Database@Taiwan: The World's Largest Traditional Chinese Medicine Database for Drug Screening In Silico,"Rapid advancing computational technologies have greatly speeded up the development of computer-aided drug design (CADD). Recently, pharmaceutical companies have increasingly shifted their attentions toward traditional Chinese medicine (TCM) for novel lead compounds. Despite the growing number of studies on TCM, there is no free 3D small molecular structure database of TCM available for virtual screening or molecular simulation. To address this shortcoming, we have constructed TCM Database@Taiwan (http://tcm.cmu.edu.tw/) based on information collected from Chinese medical texts and scientific publications. TCM Database@Taiwan is currently the world's largest non-commercial TCM database. This web-based database contains more than 20,000 pure compounds isolated from 453 TCM ingredients. Both cdx (2D) and Tripos mol2 (3D) formats of each pure compound in the database are available for download and virtual screening. The TCM database includes both simple and advanced web-based query options that can specify search clauses, such as molecular properties, substructures, TCM ingredients, and TCM classification, based on intended drug actions. The TCM database can be easily accessed by all researchers conducting CADD. Over the last eight years, numerous volunteers have devoted their time to analyze TCM ingredients from Chinese medical texts as well as to construct structure files for each isolated compound. We believe that TCM Database@Taiwan will be a milestone on the path towards modernizing traditional Chinese medicine.",j108,PLoS ONE,jv108,accepted,f1879,2006,2006-03-16
s2340,p2340,The UCSC genome browser database: update 2007,"The University of California, Santa Cruz Genome Browser Database contains, as of September 2006, sequence and annotation data for the genomes of 13 vertebrate and 19 invertebrate species. The Genome Browser displays a wide variety of annotations at all scales from the single nucleotide level up to a full chromosome and includes assembly data, genes and gene predictions, mRNA and EST alignments, and comparative genomics, regulation, expression and variation data. The database is optimized for fast interactive performance with web tools that provide powerful visualization and querying capabilities for mining the data. In the past year, 22 new assemblies and several new sets of human variation annotation have been released. New features include VisiGene, a fully integrated in situ hybridization image browser; phyloGif, for drawing evolutionary tree diagrams; a redesigned Custom Track feature; an expanded SNP annotation track; and many new display options. The Genome Browser, other tools, downloadable data files and links to documentation and other information can be found at .",c25,International Conference on Contemporary Computing,cp25,accepted,f1880,2014,2014-03-24
s2341,p2341,SCface – surveillance cameras face database,Abstract content goes here ...,j378,Multimedia tools and applications,jv378,accepted,f1881,2018,2018-12-21
s2342,p2342,Constructing a research database of social and environmental reporting by UK companies,"Responds to the widely‐reported methodological problems which have arisen in research into corporate social and environmental reporting. Reports on an attempt to build a database of UK company social and environmental disclosure. The motivation behind the database is an attempt to provide, first, a data set which both refines and develops earlier attempts to capture and interpret such disclosures; second, a data set covering several years to permit longitudinal analysis; and third, a public database for accounting researchers who wish to pursue, in a systematic and comparable way, more focused hypotheses about social and environmental reporting behaviour. Explains the motivation for, the background to, and process of establishing such a database and attempts to expose the difficulties met and the assumptions made in establishing the structure of the data capture. The resultant database has already proved useful to other UK researchers. Aims to help researchers in other countries to develop their own metho...",c1,Technical Symposium on Computer Science Education,cp1,accepted,f1882,2002,2002-01-13
s2343,p2343,CHIANTI—AN ATOMIC DATABASE FOR EMISSION LINES. XII. VERSION 7 OF THE DATABASE,"The CHIANTI spectral code consists of an atomic database and a suite of computer programs to calculate the optically thin spectrum of astrophysical objects and carry out spectroscopic plasma diagnostics. The database includes atomic energy levels, wavelengths, radiative transition probabilities, collision excitation rate coefficients, and ionization and recombination rate coefficients, as well as data to calculate free–free, free–bound, and two-photon continuum emission. Version 7 has been released, which includes several new ions, significant updates to existing ions, as well as Chianti-Py, the implementation of CHIANTI software in the Python programming language. All data and programs are freely available at http://www.chiantidatabase.org, while the Python interface to CHIANTI can be found at http://chiantipy.sourceforge.net.",c8,The Compass,cp8,accepted,f1883,2016,2016-12-23
s2344,p2344,"Semantic database modeling: survey, applications, and research issues","Most common database management systems represent information in a simple record-based format. Semantic modeling provides richer data structuring capabilities for database applications. In particular, research in this area has articulated a number of constructs that provide mechanisms for representing structurally complex interrelations among data typically arising in commercial applications. In general terms, semantic modeling complements work on knowledge representation (in artificial intelligence) and on the new generation of database models based on the object-oriented paradigm of programming languages.
This paper presents an in-depth discussion of semantic data modeling. It reviews the philosophical motivations of semantic models, including the need for high-level modeling abstractions and the reduction of semantic overloading of data type constructors. It then provides a tutorial introduction to the primary components of semantic models, which are the explicit representation of objects, attributes of and relationships among objects, type constructors for building complex types, ISA relationships, and derived schema components. Next, a survey of the prominent semantic models in the literature is presented. Further, since a broad area of research has developed around semantic modeling, a number of related topics based on these models are discussed, including data languages, graphical interfaces, theoretical investigations, and physical implementation strategies.",c23,International Conference on Open and Big Data,cp23,accepted,f1884,2012,2012-12-14
s2345,p2345,Reference database of Raman spectra of biological molecules,"Raman spectra of biological materials are very complex, because they consist of signals from all molecules present in cells. In order to obtain chemical information from these spectra, it is necessary to know the Raman patterns of the possible components of a cell. In this paper, we present a collection of Raman spectra of biomolecules that can serve as references for the interpretation of Raman spectra of biological materials. We included the most important components present in a cell: (1) DNA and RNA bases (adenine, cytosine, guanine, thymine and uracil), (2) amino acids (glycine, L-alanine, L-valine, L-serine, L-glutamic acid, L-arginine, L-phenylalanine, L-tyrosine, L-tryptophan, L-histidine, L-proline), (3) fatty acids and fats (lauric acid, myristic acid, palmitic acid, stearic acid, 12-methyltetradecanoic acid, 13-methylmyristic acid, 14-methylpentadecanoic acid, 14-methylhexadecanoic acid, 15-methylpalmitic acid, oleic acid, vaccenic acid, glycerol, triolein, trilinolein, trilinolenin), (4) saccharides (β-D-glucose, lactose, cellulose, D-(+)-dextrose, D-(+)-trehalose, amylose, amylopectine, D-(+)-mannose, D-(+)-fucose, D-(−)-arabinose, D-(+)-xylose, D-(−)-fructose, D-(+)-galactosamine, N-acetyl-D-glucosamine, chitin), (5) primary metabolites (citric acid, succinic acid, fumarate, malic acid, pyruvate, phosphoenolpyruvate, coenzyme A, acetyl coenzyme A, acetoacetate, D-fructose-6-phosphate) and (6) others (β-carotene, ascorbic acid, riboflavin, glutathione). Examples of Raman spectra of bacteria and fungal spores are shown, together with band assignments to the reference products. Copyright © 2007 John Wiley & Sons, Ltd.",c62,International Conference on Software Reuse,cp62,accepted,f1885,2006,2006-06-06
s2346,p2346,Progressive skyline computation in database systems,"The skyline of a d-dimensional dataset contains the points that are not dominated by any other point on all dimensions. Skyline computation has recently received considerable attention in the database community, especially for progressive methods that can quickly return the initial results without reading the entire database. All the existing algorithms, however, have some serious shortcomings which limit their applicability in practice. In this article we develop branch-and-bound skyline (BBS), an algorithm based on nearest-neighbor search, which is I/O optimal, that is, it performs a single access only to those nodes that may contain skyline points. BBS is simple to implement and supports all types of progressive processing (e.g., user preferences, arbitrary dimensionality, etc). Furthermore, we propose several interesting variations of skyline computation, and show how BBS can be applied for their efficient processing.",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f1886,2008,2008-04-30
s2347,p2347,IPD—the Immuno Polymorphism Database,"The Immuno Polymorphism Database (IPD), http://www.ebi.ac.uk/ipd/ is a set of specialist databases related to the study of polymorphic genes in the immune system. The IPD project works with specialist groups or nomenclature committees who provide and curate individual sections before they are submitted to IPD for online publication. The IPD project stores all the data in a set of related databases. IPD currently consists of four databases: IPD-KIR, contains the allelic sequences of killer-cell immunoglobulin-like receptors, IPD-MHC, a database of sequences of the major histocompatibility complex of different species; IPD-HPA, alloantigens expressed only on platelets; and IPD-ESTDAB, which provides access to the European Searchable Tumour Cell-Line Database, a cell bank of immunologically characterized melanoma cell lines. The data is currently available online from the website and FTP directory. This article describes the latest updates and additional tools added to the IPD project.",c64,Experimental Software Engineering Network,cp64,accepted,f1887,2014,2014-12-23
s2348,p2348,Physical Properties of Ionic Liquids: Database and Evaluation,"A comprehensive database on physical properties of ionic liquids (ILs), which was collected from 109 kinds of literature sources in the period from 1984 through 2004, has been presented. There are 1680 pieces of data on the physical properties for 588 available ILs, from which 276 kinds of cations and 55 kinds of anions were extracted. In terms of the collected database, the structure-property relationship was evaluated. The correlation of melting points of two most common systems, disubstituted imidazolium tetrafluoroborate and disubstituted imidazolium hexafluorophosphate, was carried out using a quantitative structure-property relationship method.",c94,Vision,cp94,accepted,f1888,2020,2020-02-12
s2349,p2349,DIP: the Database of Interacting Proteins,"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla.edu) is a database that documents experimentally determined protein-protein interactions. This database is intended to provide the scientific community with a comprehensive and integrated tool for browsing and efficiently extracting information about protein interactions and interaction networks in biological processes. Beyond cataloging details of protein-protein interactions, the DIP is useful for understanding protein function and protein-protein relationships, studying the properties of networks of interacting proteins, benchmarking predictions of protein-protein interactions, and studying the evolution of protein-protein interactions.",c69,International Conference on Parallel Processing,cp69,accepted,f1889,2010,2010-07-03
s2350,p2350,The SIMBAD astronomical database. The CDS reference database for astronomical objects,"Simbad is the reference database for identification and bibliography of astronomical objects. It contains identifications, “basic data”, bibliography, and selected observational measurements for several million astronomical objects.  Simbad is developed and maintained by CDS, Strasbourg. Building the database contents is achieved with the help of several contributing institutes. Scanning the bibliography is the result of the collaboration of CDS with bibliographers in Observatoire de Paris (DASGAL), Institut d'Astrophysique de Paris, and Observatoire de Bordeaux. When selecting catalogues and tables for inclusion, priority is given to optimal multi-wavelength coverage of the database, and to support of research developments linked to large projects. In parallel, the systematic scanning of the bibliography reflects the diversity and general trends of astronomical research. A WWW interface to Simbad is available at: http://simbad.u-strasbg.fr/Simbad.",c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",cp38,accepted,f1890,2015,2015-02-25
s2351,p2351,The NCBI dbGaP database of genotypes and phenotypes,Abstract content goes here ...,j274,Nature Genetics,jv274,accepted,f1891,2020,2020-04-21
s2352,p2352,The NCBI BioSystems database,"The NCBI BioSystems database, found at http://www.ncbi.nlm.nih.gov/biosystems/, centralizes and cross-links existing biological systems databases, increasing their utility and target audience by integrating their pathways and systems into NCBI resources. This integration allows users of NCBI’s Entrez databases to quickly categorize proteins, genes and small molecules by metabolic pathway, disease state or other BioSystem type, without requiring time-consuming inference of biological relationships from the literature or multiple experimental datasets.",c35,EUROMICRO Conference on Software Engineering and Advanced Applications,cp35,accepted,f1892,2005,2005-01-22
s2354,p2354,The PROSITE database,"The PROSITE database consists of a large collection of biologically meaningful signatures that are described as patterns or profiles. Each signature is linked to a documentation that provides useful biological information on the protein family, domain or functional site identified by the signature. The PROSITE database is now complemented by a series of rules that can give more precise information about specific residues. During the last 2 years, the documentation and the ScanProsite web pages were redesigned to add more functionalities. The latest version of PROSITE (release 19.11 of September 27, 2005) contains 1329 patterns and 552 profile entries. Over the past 2 years more than 200 domains have been added, and now 52% of UniProtKB/Swiss-Prot entries (release 48.1 of September 27, 2005) have a cross-reference to a PROSITE entry. The database is accessible at .",c101,International Conference on Automatic Face and Gesture Recognition,cp101,accepted,f1893,2006,2006-04-03
s2355,p2355,The CIPIC HRTF database,"This paper describes a public-domain database of high-spatial-resolution head-related transfer functions measured at the UC Davis CIPIC Interface Laboratory and the methods used to collect the data.. Release 1.0 (see http://interface.cipic.ucdavis.edu) includes head-related impulse responses for 45 subjects at 25 different azimuths and 50 different elevations (1250 directions) at approximately 5/spl deg/ angular increments. In addition, the database contains anthropometric measurements for each subject. Statistics of anthropometric parameters and correlations between anthropometry and some temporal and spectral features of the HRTFs are reported.",c8,The Compass,cp8,accepted,f1894,2016,2016-02-22
s2356,p2356,Defining and cataloging exoplanets: the exoplanet.eu database,"We describe an online database for extrasolar planetary-mass candidates, which is updated regularly as new data are available. We first discuss criteria for inclusion of objects in the catalog: “definition” of a planet and several aspects of the confidence level of planet candidates. We are led to point out the contradiction between the sharpness of criteria for belonging to a catalog and the fuzziness of the confidence level for an object to be a planet. We then describe the different tables of extrasolar planetary systems, including unconfirmed candidates (which will ultimately be confirmed, or not, by direct imaging). It also provides online tools: histograms of planet and host star data, cross-correlations between these parameters, and some Virtual Observatory services. Future evolutions of the database are presented.",c30,IEEE Aerospace Conference,cp30,accepted,f1895,2006,2006-04-28
s2358,p2358,SGD: Saccharomyces Genome Database,"The Saccharomyces Genome Database (SGD) provides Internet access to the complete Saccharomyces cerevisiae genomic sequence, its genes and their products, the phenotypes of its mutants, and the literature supporting these data. The amount of information and the number of features provided by SGD have increased greatly following the release of the S.cerevisiae genomic sequence, which is currently the only complete sequence of a eukaryotic genome. SGD aids researchers by providing not only basic information, but also tools such as sequence similarity searching that lead to detailed information about features of the genome and relationships between genes. SGD presents information using a variety of user-friendly, dynamically created graphical displays illustrating physical, genetic and sequence feature maps. SGD can be accessed via the World Wide Web at http://genome-www.stanford.edu/Saccharomyces/",c17,International Conference on Enterprise Information Systems,cp17,accepted,f1896,2008,2008-05-24
s2359,p2359,An atomic and molecular database for analysis of submillimetre line observations,"Atomic and molecular data for the transitions of a number of astrophysically interesting species are summarized, in- cluding energy levels, statistical weights, Einstein A-coefficients and collisional rate coefficients. Available collisional data from quantum chemical calculations and experiments are extrapolated to higher energies (up to E/k ∼ 1000 K). These data, which are made publically available through the WWW at http://www.strw.leidenuniv.nl/∼moldata, are essential input for non-LTE line radiative transfer programs. An online version of a computer program for performing statistical equilibrium calcu- lations is also made available as part of the database. Comparisons of calculated emission lines using different sets of collisional rate coefficients are presented. This database should form an important tool in analyzing observations from current and future (sub)millimetre and infrared telescopes.",c1,Technical Symposium on Computer Science Education,cp1,accepted,f1897,2002,2002-02-10
s2360,p2360,Schism: a Workload-Driven Approach to Database Replication and Partitioning,"We present Schism, a novel workload-aware approach for database partitioning and replication designed to improve scalability of shared-nothing distributed databases. Because distributed transactions are expensive in OLTP settings (a fact we demonstrate through a series of experiments), our partitioner attempts to minimize the number of distributed transactions, while producing balanced partitions. Schism consists of two phases: i) a workload-driven, graph-based replication/partitioning phase and ii) an explanation and validation phase. The first phase creates a graph with a node per tuple (or group of tuples) and edges between nodes accessed by the same transaction, and then uses a graph partitioner to split the graph into k balanced partitions that minimize the number of cross-partition transactions. The second phase exploits machine learning techniques to find a predicate-based explanation of the partitioning strategy (i.e., a set of range predicates that represent the same replication/partitioning scheme produced by the partitioner). 
 
The strengths of Schism are: i) independence from the schema layout, ii) effectiveness on n-to-n relations, typical in social network databases, iii) a unified and fine-grained approach to replication and partitioning. We implemented and tested a prototype of Schism on a wide spectrum of test cases, ranging from classical OLTP workloads (e.g., TPC-C and TPC-E), to more complex scenarios derived from social network websites (e.g., Epinions.com), whose schema contains multiple n-to-n relationships, which are known to be hard to partition. Schism consistently outperforms simple partitioning schemes, and in some cases proves superior to the best known manual partitioning, reducing the cost of distributed transactions up to 30%.",j34,Proceedings of the VLDB Endowment,jv34,accepted,f1898,2001,2001-03-06
s2361,p2361,Database abstractions: aggregation and generalization,"Two kinds of abstraction that are fundamentally important in database design and usage are defined. Aggregation is an abstraction which turns a relationship between objects into an aggregate object. Generalization is an abstraction which turns a class of objects into a generic object. It is suggested that all objects (individual, aggregate, generic) should be given uniform treatment in models of the real world. A new data type, called generic, is developed as a primitive for defining such models. Models defined with this primitive are structured as a set of aggregation hierarchies intersecting with a set of generalization hierarchies. Abstract objects occur at the points of intersection. This high level structure provides a discipline for the organization of relational databases. In particular this discipline allows: (i) an important class of views to be integrated and maintained; (ii) stability of data and programs under certain evolutionary changes; (iii) easier understanding of complex models and more natural query formulation; (iv) a more systematic approach to database design; (v) more optimization to be performed at lower implementation levels. The generic type is formalized by a set of invariant properties. These properties should be satisfied by all relations in a database if abstractions are to be preserved. A triggering mechanism for automatically maintaining these invariants during update operations is proposed. A simple mapping of aggregation/generalization hierarchies onto owner-coupled set structures is given.",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f1899,2017,2017-06-10
s2362,p2362,PubChem's BioAssay Database,"PubChem (http://pubchem.ncbi.nlm.nih.gov) is a public repository for biological activity data of small molecules and RNAi reagents. The mission of PubChem is to deliver free and easy access to all deposited data, and to provide intuitive data analysis tools. The PubChem BioAssay database currently contains 500 000 descriptions of assay protocols, covering 5000 protein targets, 30 000 gene targets and providing over 130 million bioactivity outcomes. PubChem's bioassay data are integrated into the NCBI Entrez information retrieval system, thus making PubChem data searchable and accessible by Entrez queries. Also, as a repository, PubChem constantly optimizes and develops its deposition system answering many demands of both high- and low-volume depositors. The PubChem information platform allows users to search, review and download bioassay description and data. The PubChem platform also enables researchers to collect, compare and analyze biological test results through web-based and programmatic tools. In this work, we provide an update for the PubChem BioAssay resource, including information content growth, data model extension and new developments of data submission, retrieval, analysis and download tools.",c93,Human Language Technology - The Baltic Perspectiv,cp93,accepted,f1900,2005,2005-11-18
s2363,p2363,The National Land Cover Database,Abstract content goes here ...,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f1901,2018,2018-08-27
s2364,p2364,"The PROSITE database, its status in 1999","The PROSITE database consists of biologically significant patterns and profiles formulated in such a way that with appropriate computational tools it can help to determine to which known family of protein (if any) a new sequence belongs, or which known domain(s) it contains.",c63,IEEE International Software Metrics Symposium,cp63,accepted,f1902,2008,2008-08-23
s2365,p2365,The Object-Oriented Database System Manifesto,Abstract content goes here ...,c97,Interspeech,cp97,accepted,f1903,2004,2004-07-19
s2366,p2366,TRANSFAC: a database on transcription factors and their DNA binding sites,TRANSFAC is a database about eukaryotic transcription regulating DNA sequence elements and the transcription factors binding to and acting through them. This report summarizes the present status of this database and accompanying retrieval tools.,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f1904,2018,2018-08-13
s2367,p2367,The 'Dresden Image Database' for benchmarking digital image forensics,"This paper introduces and documents a novel image database specifically built for the purpose of development and bench-marking of camera-based digital forensic techniques. More than 14,000 images of various indoor and outdoor scenes have been acquired under controlled and thus widely comparable conditions from altogether 73 digital cameras. The cameras were drawn from only 25 different models to ensure that device-specific and model-specific characteristics can be disentangled and studied separately, as validated with results in this paper. In addition, auxiliary images for the estimation of device-specific sensor noise pattern were collected for each camera. Another subset of images to study model-specific JPEG compression algorithms has been compiled for each model. The 'Dresden Image Database' will be made freely available for scientific purposes when this accompanying paper is presented. The database is intended to become a useful resource for researchers and forensic investigators. Using a standard database as a benchmark not only makes results more comparable and reproducible, but it is also more economical and avoids potential copyright and privacy issues that go along with self-sampled benchmark sets from public photo communities on the Internet.",c43,ACM Symposium on Applied Computing,cp43,accepted,f1905,2001,2001-05-27
s2369,p2369,Using the KEGG Database Resource,"KEGG (Kyoto Encyclopedia of Genes and Genomes) is a bioinformatics resource for understanding the functions and utilities of cells and organisms from both high‐level and genomic perspectives. It is a self‐sufficient, integrated resource consisting of genomic, chemical, and network information, with cross‐references to numerous outside databases. The genomic and chemical information is a complete set of building blocks (genes and molecules) and the network information includes molecular wiring diagrams (interaction/reaction networks) and hierarchical classifications (relation networks) to represent high‐level functions. This unit describes protocols for using KEGG, focusing on molecular network information in KEGG PATHWAY, KEGG BRITE, and KEGG MODULE, perturbed molecular networks in KEGG DISEASE and KEGG DRUG, molecular building block information in KEGG GENES and KEGG LIGAND, and a mechanism for linking genomes to molecular networks in KEGG ORTHOLOGY (KO). All of these many protocols enable the user to take advantage of the full breadth of the functionality provided by KEGG. Curr. Protoc. Bioinform. 38:1.12.1‐1.12.43. © 2012 by John Wiley & Sons, Inc.",c41,Software Product Lines Conference,cp41,accepted,f1906,2002,2002-07-12
s2370,p2370,IntAct: an open source molecular interaction database,"IntAct provides an open source database and toolkit for the storage, presentation and analysis of protein interactions. The web interface provides both textual and graphical representations of protein interactions, and allows exploring interaction networks in the context of the GO annotations of the interacting proteins. A web service allows direct computational access to retrieve interaction networks in XML format. IntAct currently contains approximately 2200 binary and complex interactions imported from the literature and curated in collaboration with the Swiss-Prot team, making intensive use of controlled vocabularies to ensure data consistency. All IntAct software, data and controlled vocabularies are available at http://www.ebi.ac.uk/intact.",c35,EUROMICRO Conference on Software Engineering and Advanced Applications,cp35,accepted,f1907,2005,2005-06-09
s2371,p2371,rrndb: the Ribosomal RNA Operon Copy Number Database,"The Ribosomal RNA Operon Copy Number Database (rrndb) is an Internet-accessible database containing annotated information on rRNA operon copy number among prokaryotes. Gene redundancy is uncommon in prokaryotic genomes, yet the rRNA genes can vary from one to as many as 15 copies. Despite the widespread use of 16S rRNA gene sequences for identification of prokaryotes, information on the number and sequence of individual rRNA genes in a genome is not readily accessible. In an attempt to understand the evolutionary implications of rRNA operon redundancy, we have created a phylogenetically arranged report on rRNA gene copy number for a diverse collection of prokaryotic microorganisms. Each entry (organism) in the rrndb contains detailed information linked directly to external websites including the Ribosomal Database Project, GenBank, PubMed and several culture collections. Data contained in the rrndb will be valuable to researchers investigating microbial ecology and evolution using 16S rRNA gene sequences. The rrndb web site is directly accessible on the WWW at http://rrndb.cme. msu.edu.",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f1908,2018,2018-02-15
s2372,p2372,"The InterPro database, an integrated documentation resource for protein families, domains and functional sites","Signature databases are vital tools for identifying distant relationships in novel sequences and hence for inferring protein function. InterPro is an integrated documentation resource for protein families, domains and functional sites, which amalgamates the efforts of the PROSITE, PRINTS, Pfam and ProDom database projects. Each InterPro entry includes a functional description, annotation, literature references and links back to the relevant member database(s). Release 2.0 of InterPro (October 2000) contains over 3000 entries, representing families, domains, repeats and sites of post-translational modification encoded by a total of 6804 different regular expressions, profiles, fingerprints and Hidden Markov Models. Each InterPro entry lists all the matches against SWISS-PROT and TrEMBL (more than 1,000,000 hits from 462,500 proteins in SWISS-PROT and TrEMBL). The database is accessible for text- and sequence-based searches at http://www.ebi.ac.uk/interpro/. Questions can be emailed to interhelp@ebi.ac.uk.",c70,International Conference on Intelligent Robotics and Applications,cp70,accepted,f1909,2020,2020-11-08
s2374,p2374,BIND: the Biomolecular Interaction Network Database,"The Biomolecular Interaction Network Database (BIND: http://bind.ca) archives biomolecular interaction, complex and pathway information. A web-based system is available to query, view and submit records. BIND continues to grow with the addition of individual submissions as well as interaction data from the PDB and a number of large-scale interaction and complex mapping experiments using yeast two hybrid, mass spectrometry, genetic interactions and phage display. We have developed a new graphical analysis tool that provides users with a view of the domain composition of proteins in interaction and complex records to help relate functional domains to protein interactions. An interaction network clustering tool has also been developed to help focus on regions of interest. Continued input from users has helped further mature the BIND data specification, which now includes the ability to store detailed information about genetic interactions. The BIND data specification is available as ASN.1 and XML DTD.",c8,The Compass,cp8,accepted,f1910,2016,2016-07-29
s2375,p2375,Database-friendly random projections,"A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space where k is logarithmic in n and independent of d so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a random k-dimensional hyperplane. We give a novel construction of the embedding, suitable for database applications, which amounts to computing a simple aggregate over k random attribute partitions.",c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,cp86,accepted,f1911,2012,2012-11-03
s2376,p2376,Concurrency Control in Distributed Database Systems,"In this paper we survey, consolidate, and present the state of the art in distributed database concurrency control. The heart of our analysts is a decomposition of the concurrency control problem into two major subproblems: read-write and write-write synchronization. We describe a series of synchromzation techniques for solving each subproblem and show how to combine these techniques into algorithms for solving the entire concurrency control problem. Such algorithms are called ""concurrency control methods."" We describe 48 principal methods, including all practical algorithms that have appeared m the literature plus several new ones. We concentrate on the structure and correctness of concurrency control algorithms. Issues of performance are given only secondary treatment.",c10,Big Data,cp10,accepted,f1912,2021,2021-01-16
s2377,p2377,The International Nucleotide Sequence Database Collaboration,"Under the International Nucleotide Sequence Database Collaboration (INSDC; http://www.insdc.org), globally comprehensive public domain nucleotide sequence is captured, preserved and presented. The partners of this long-standing collaboration work closely together to provide data formats and conventions that enable consistent data submission to their databases and support regular data exchange around the globe. Clearly defined policy and governance in relation to free access to data and relationships with journal publishers have positioned INSDC databases as a key provider of the scientific record and a core foundation for the global bioinformatics data infrastructure. While growth in sequence data volumes comes no longer as a surprise to INSDC partners, the uptake of next-generation sequencing technology by mainstream science that we have witnessed in recent years brings a step-change to growth, necessarily making a clear mark on INSDC strategy. In this article, we introduce the INSDC, outline data growth patterns and comment on the challenges of increased growth.",j102,Nucleic Acids Research,jv102,accepted,f1913,2002,2002-11-17
s2378,p2378,LMSD: LIPID MAPS structure database,"The LIPID MAPS Structure Database (LMSD) is a relational database encompassing structures and annotations of biologically relevant lipids. Structures of lipids in the database come from four sources: (i) LIPID MAPS Consortium's core laboratories and partners; (ii) lipids identified by LIPID MAPS experiments; (iii) computationally generated structures for appropriate lipid classes; (iv) biologically relevant lipids manually curated from LIPID BANK, LIPIDAT and other public sources. All the lipid structures in LMSD are drawn in a consistent fashion. In addition to a classification-based retrieval of lipids, users can search LMSD using either text-based or structure-based search options. The text-based search implementation supports data retrieval by any combination of these data fields: LIPID MAPS ID, systematic or common name, mass, formula, category, main class, and subclass data fields. The structure-based search, in conjunction with optional data fields, provides the capability to perform a substructure search or exact match for the structure drawn by the user. Search results, in addition to structure and annotations, also include relevant links to external databases. The LMSD is publicly available at",c26,PS,cp26,accepted,f1914,2010,2010-06-12
s2379,p2379,SDUMLA-HMT: A Multimodal Biometric Database,Abstract content goes here ...,c106,Chinese Conference on Biometric Recognition,cp106,accepted,f1915,2016,2016-01-15
s2380,p2380,The Immune Epitope Database 2.0,"The Immune Epitope Database (IEDB, www.iedb.org) provides a catalog of experimentally characterized B and T cell epitopes, as well as data on Major Histocompatibility Complex (MHC) binding and MHC ligand elution experiments. The database represents the molecular structures recognized by adaptive immune receptors and the experimental contexts in which these molecules were determined to be immune epitopes. Epitopes recognized in humans, nonhuman primates, rodents, pigs, cats and all other tested species are included. Both positive and negative experimental results are captured. Over the course of 4 years, the data from 180 978 experiments were curated manually from the literature, which covers ∼99% of all publicly available information on peptide epitopes mapped in infectious agents (excluding HIV) and 93% of those mapped in allergens. In addition, data that would otherwise be unavailable to the public from 129 186 experiments were submitted directly by investigators. The curation of epitopes related to autoimmunity is expected to be completed by the end of 2010. The database can be queried by epitope structure, source organism, MHC restriction, assay type or host organism, among other criteria. The database structure, as well as its querying, browsing and reporting interfaces, was completely redesigned for the IEDB 2.0 release, which became publicly available in early 2009.",c14,International Conference on Exploring Services Science,cp14,accepted,f1916,2016,2016-02-15
s2381,p2381,UCI Repository of Machine Learning Database,Abstract content goes here ...,c50,International Conference on Automated Software Engineering,cp50,accepted,f1917,2008,2008-09-24
s2382,p2382,The RDP (Ribosomal Database Project),"The Ribosomal Database Project (RDP) is a curated database that offers ribosome-related data, analysis services and associated computer programs. The offerings include phylogenetically ordered alignments of ribosomal RNA (rRNA) sequences, derived phylogenetic trees, rRNA secondary structure diagrams, and various software for handling, analyzing and displaying alignments and trees. The data are available via anonymous FTP (rdp.life.uiuc.edu), electronic mail (server@rdp.life.uiuc.edu), gopher (rdpgopher.life.uiuc.edu) and WWW (http://rdpwww.life.uiuc.edu/ ). The electronic mail and WWW servers provide ribosomal probe checking, approximate phylogenetic placement of user-submitted sequences, screening for possible chimeric rRNA sequences, automated alignment, and a suggested placement of an unknown sequence on an existing phylogenetic tree.",c86,ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems,cp86,accepted,f1918,2012,2012-05-30
s2384,p2384,The DIARETDB1 Diabetic Retinopathy Database and Evaluation Protocol,"Automatic diagnosis of diabetic retinopathy from digital fundus images has been an active research topic in the medical image processing community. The research interest is justified by the excellent potential for new products in the medical industry and significant reductions in health care costs. However, the maturity of proposed algorithms cannot be judged due to the lack of commonly accepted and representative image database with a verified ground truth and strict evaluation protocol. In this study, an evaluation methodology is proposed and an image database with ground truth is described. The database is publicly available for benchmarking diagnosis algorithms. With the proposed database and protocol, it is possible to compare different algorithms, and correspondingly, analyse their maturity for technology transfer from the research laboratories to the medical practice.",c107,British Machine Vision Conference,cp107,accepted,f1919,2012,2012-11-01
s2386,p2386,An Overview of the Global Historical Climatology Network Temperature Database,"Abstract The Global Historical Climatology Network version 2 temperature database was released in May 1997. This century-scale dataset consists of monthly surface observations from ∼7000 stations from around the world. This archive breaks considerable new ground in the field of global climate databases. The enhancements include 1) data for additional stations to improve regional-scale analyses, particularly in previously data-sparse areas; 2) the addition of maximum–minimum temperature data to provide climate information not available in mean temperature data alone; 3) detailed assessments of data quality to increase the confidence in research results; 4) rigorous and objective homogeneity adjustments to decrease the effect of nonclimatic factors on the time series; 5) detailed metadata (e.g., population, vegetation, topography) that allow more detailed analyses to be conducted; and 6) an infrastructure for updating the archive at regular intervals so that current climatic conditions can constantly be put...",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f1920,2005,2005-10-16
s2387,p2387,The VizieR database of astronomical catalogues,"VizieR is a database grouping in an homoge- neous way thousands of astronomical catalogues gath- ered for decades by the Centre de Donn ees de Strasbourg (CDS) and participating institutes. The history and cur- rent status of this large collection is briefly presented, and the way these catalogues are being standardized to t in the VizieR system is described. The architecture of the database is then presented, with emphasis on the man- agement of links and of accesses to very large catalogues. Several query interfaces are currently available, making use of the ASU protocol, for browsing purposes or for use by other data processing systems such as visualisa- tion tools.",c73,Workshop on Algorithms in Bioinformatics,cp73,accepted,f1921,2018,2018-01-20
s2388,p2388,Development and use of a database of hydraulic properties of European soils,Abstract content goes here ...,c7,European Conference on Modelling and Simulation,cp7,accepted,f1922,2015,2015-01-21
s2389,p2389,The EMBL Nucleotide Sequence Database,"The EMBL Nucleotide Sequence Database (http://www.ebi.ac.uk/embl), maintained at the European Bioinformatics Institute (EBI) near Cambridge, UK, is a comprehensive collection of nucleotide sequences and annotation from available public sources. The database is part of an international collaboration with DDBJ (Japan) and GenBank (USA). Data are exchanged daily between the collaborating institutes to achieve swift synchrony. Webin is the preferred tool for individual submissions of nucleotide sequences, including Third Party Annotation (TPA) and alignments. Automated procedures are provided for submissions from large-scale sequencing projects and data from the European Patent Office. New and updated data records are distributed daily and the whole EMBL Nucleotide Sequence Database is released four times a year. Access to the sequence data is provided via ftp and several WWW interfaces. With the web-based Sequence Retrieval System (SRS) it is also possible to link nucleotide data to other specialist molecular biology databases maintained at the EBI. Other tools are available for sequence similarity searching (e.g. FASTA and BLAST). Changes over the past year include the removal of the sequence length limit, the launch of the EMBLCDSs dataset, extension of the Sequence Version Archive functionality and the revision of quality rules for TPA data.",j102,Nucleic Acids Research,jv102,accepted,f1923,2002,2002-12-02
s2390,p2390,The ecoinvent Database: Overview and Methodological Framework (7 pp),Abstract content goes here ...,c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f1924,2005,2005-03-14
s2391,p2391,A global database of soil respiration data,"Abstract. Soil respiration – RS, the flux of CO2 from the soil to the atmosphere – is probably the least well constrained component of the terrestrial carbon cycle. Here we introduce the SRDB database, a near-universal compendium of published RS data, and make it available to the scientific community both as a traditional static archive and as a dynamic community database that may be updated over time by interested users. The database encompasses all published studies that report one of the following data measured in the field (not laboratory): annual RS, mean seasonal RS, a seasonal or annual partitioning of RS into its sources fluxes, RS temperature response (Q10), or RS at 10 °C. Its orientation is thus to seasonal and annual fluxes, not shorter-term or chamber-specific measurements. To date, data from 818 studies have been entered into the database, constituting 3379 records. The data span the measurement years 1961–2007 and are dominated by temperate, well-drained forests. We briefly examine some aspects of the SRDB data – its climate space coverage, mean annual RS fluxes and their correlation with other carbon fluxes, RS variability, temperature sensitivities, and the partitioning of RS source flux – and suggest some potential lines of research that could be explored using these data. The SRDB database is available online in a permanent archive as well as via a project-hosting repository; the latter source leverages open-source software technologies to encourage wider participation in the database's future development. Ultimately, we hope that the updating of, and corrections to, the SRDB will become a shared project, managed by the users of these data in the scientific community.",c67,Enterprise Application Integration,cp67,accepted,f1925,2002,2002-04-21
s2392,p2392,Entity-relationship modeling - foundations of database technology,"From the Publisher: 
Database technology and entity-relationship (ER) modeling have meanwhile reached the level of an established technology. This book presents the achievements of research in this field in a comprehensive survey. It deals with the ER model and its extensions with regard to an integrated development and modeling of database applications and, consequently, the specification of structures, behavior, and interaction."" ""Apart from research on the ER model and the syntax, semantics, and pragmatics of database modeling the book also presents techniques for the translation of the ER model into classical database models and languages such as relational, hierarchical, and network models and languages, and also into object-oriented models."" ""The book is of interest for all database theoreticians as well as practitioners who are provided with the relevant fundamentals of database modeling.",c88,Symposium on the Theory of Computing,cp88,accepted,f1926,2014,2014-12-25
s2393,p2393,On supporting containment queries in relational database management systems,"Virtually all proposals for querying XML include a class of query we term “containment queries”. It is also clear that in the foreseeable future, a substantial amount of XML data will be stored in relational database systems. This raises the question of how to support these containment queries. The inverted list technology that underlies much of Information Retrieval is well-suited to these queries, but should we implement this technology (a) in a separate loosely-coupled IR engine, or (b) using the native tables and query execution machinery of the RDBMS? With option (b), more than twenty years of work on RDBMS query optimization, query execution, scalability, and concurrency control and recovery immediately extend to the queries and structures that implement these new operations. But all this will be irrelevant if the performance of option (b) lags that of (a) by too much. In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.",c100,ACM SIGMOD Conference,cp100,accepted,f1927,2010,2010-06-05
s2394,p2394,The ENZYME database in 2000,The ENZYME database is a repository of information related to the nomenclature of enzymes. In recent years it has became an indispensable resource for the development of metabolic databases. The current version contains information on 3705 enzymes. It is available through the ExPASy WWW server (http://www.expasy.ch/enzyme/ ).,c48,"Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication",cp48,accepted,f1928,2018,2018-10-20
s2395,p2395,ELM—the database of eukaryotic linear motifs,"Linear motifs are short, evolutionarily plastic components of regulatory proteins and provide low-affinity interaction interfaces. These compact modules play central roles in mediating every aspect of the regulatory functionality of the cell. They are particularly prominent in mediating cell signaling, controlling protein turnover and directing protein localization. Given their importance, our understanding of motifs is surprisingly limited, largely as a result of the difficulty of discovery, both experimentally and computationally. The Eukaryotic Linear Motif (ELM) resource at http://elm.eu.org provides the biological community with a comprehensive database of known experimentally validated motifs, and an exploratory tool to discover putative linear motifs in user-submitted protein sequences. The current update of the ELM database comprises 1800 annotated motif instances representing 170 distinct functional classes, including approximately 500 novel instances and 24 novel classes. Several older motif class entries have been also revisited, improving annotation and adding novel instances. Furthermore, addition of full-text search capabilities, an enhanced interface and simplified batch download has improved the overall accessibility of the ELM data. The motif discovery portion of the ELM resource has added conservation, and structural attributes have been incorporated to aid users to discriminate biologically relevant motifs from stochastically occurring non-functional instances.",c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f1929,2014,2014-11-09
s2397,p2397,The Cochrane Database of Systematic Reviews,"This paper reminds readers of this journal of an important source of summaries of the evidence for effectiveness of healthcare interventions, namely the Cochrane Database of Systematic Reviews.",c24,Decision Support Systems,cp24,accepted,f1930,2013,2013-07-07
s2398,p2398,CPPsite: a curated database of cell penetrating peptides,"Delivering drug molecules into the cell is one of the major challenges in the process of drug development. In past, cell penetrating peptides have been successfully used for delivering a wide variety of therapeutic molecules into various types of cells for the treatment of multiple diseases. These peptides have unique ability to gain access to the interior of almost any type of cell. Due to the huge therapeutic applications of CPPs, we have built a comprehensive database ‘CPPsite’, of cell penetrating peptides, where information is compiled from the literature and patents. CPPsite is a manually curated database of experimentally validated 843 CPPs. Each entry provides information of a peptide that includes ID, PubMed ID, peptide name, peptide sequence, chirality, origin, nature of peptide, sub-cellular localization, uptake efficiency, uptake mechanism, hydrophobicity, amino acid frequency and composition, etc. A wide range of user-friendly tools have been incorporated in this database like searching, browsing, analyzing, mapping tools. In addition, we have derived various types of information from these peptide sequences that include secondary/tertiary structure, amino acid composition and physicochemical properties of peptides. This database will be very useful for developing models for predicting effective cell penetrating peptides. Database URL: http://crdd.osdd.net/raghava/cppsite/.",c100,ACM SIGMOD Conference,cp100,accepted,f1931,2010,2010-09-26
s2399,p2399,Conceptual Database Design: An Entity-Relationship Approach,I. CONCEPTUAL DATABASE DESIGN. 1. An Introduction to Database Design. 2. Data Modeling Concepts. 3. Methodologies for Conceptual Design. 4. View Design. 5. View Integration. 6. Improving the Quality of a Database Schema. 7. Schema Documentation and Maintenance. II. FUNCTIONAL ANALYSIS FOR DATABASE DESIGN. 1. Functional Analysis Using the Dataflow Model. 2. Joint Data and Functional Analysis. 3. Case Study. III. LOGICAL DESIGN AND DESIGN TOOLS. 1. High-Level Logical Design Using the Entity-Relationship Model. 2. Logical Design for the Relational Model. 3. Logical Design for the Network Model. 4. Logical Design for the Hierarchical Model. 5. Database Design Tools. Index. 0805302441T04062001,c17,International Conference on Enterprise Information Systems,cp17,accepted,f1932,2008,2008-05-10
s2400,p2400,RBPDB: a database of RNA-binding specificities,"The RNA-Binding Protein DataBase (RBPDB) is a collection of experimental observations of RNA-binding sites, both in vitro and in vivo, manually curated from primary literature. To build RBPDB, we performed a literature search for experimental binding data for all RNA-binding proteins (RBPs) with known RNA-binding domains in four metazoan species (human, mouse, fly and worm). In total, RPBDB contains binding data on 272 RBPs, including 71 that have motifs in position weight matrix format, and 36 sets of sequences of in vivo-bound transcripts from immunoprecipitation experiments. The database is accessible by a web interface which allows browsing by domain or by organism, searching and export of records, and bulk data downloads. Users can also use RBPDB to scan sequences for RBP-binding sites. RBPDB is freely available, without registration at http://rbpdb.ccbr.utoronto.ca/.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f1933,2011,2011-06-09
s2402,p2402,MIPS: a database for genomes and protein sequences,"The Munich Information Center for Protein Sequences (MIPS-GSF), Martinsried, near Munich, Germany, continues its longstanding tradition to develop and maintain high quality curated genome databases. In addition, efforts have been intensified to cover the wealth of complete genome sequences in a systematic, comprehensive form. Bioinformatics, supporting national as well as European sequencing and functional analysis projects, has resulted in several up-to-date genome-oriented databases. This report describes growing databases reflecting the progress of sequencing the Arabidopsis thaliana (MATDB) and Neurospora crassa genomes (MNCDB), the yeast genome database (MYGD) extended by functional analysis data, the database of annotated human EST-clusters (HIB) and the database of the complete cDNA sequences from the DHGP (German Human Genome Project). It also contains information on the up-to-date database of complete genomes (PEDANT), the classification of protein sequences (ProtFam) and the collection of protein sequence data within the framework of the PIR-International Protein Sequence Database. These databases can be accessed through the MIPS WWW server (http://www. mips.biochem.mpg.de).",c27,ACM-SIAM Symposium on Discrete Algorithms,cp27,accepted,f1934,2022,2022-05-06
s2403,p2403,The BioGRID Interaction Database: 2008 update,"The Biological General Repository for Interaction Datasets (BioGRID) database (http://www.thebiogrid.org) was developed to house and distribute collections of protein and genetic interactions from major model organism species. BioGRID currently contains over 198 000 interactions from six different species, as derived from both high-throughput studies and conventional focused studies. Through comprehensive curation efforts, BioGRID now includes a virtually complete set of interactions reported to date in the primary literature for both the budding yeast Saccharomyces cerevisiae and the fission yeast Schizosaccharomyces pombe. A number of new features have been added to the BioGRID including an improved user interface to display interactions based on different attributes, a mirror site and a dedicated interaction management system to coordinate curation across different locations. The BioGRID provides interaction data with monthly updates to Saccharomyces Genome Database, Flybase and Entrez Gene. Source code for the BioGRID and the linked Osprey network visualization system is now freely available without restriction.",c77,Networks,cp77,accepted,f1935,2019,2019-11-28
s2404,p2404,"JASPAR, the open access database of transcription factor-binding profiles: new content and tools in the 2008 update","JASPAR is a popular open-access database for matrix models describing DNA-binding preferences for transcription factors and other DNA patterns. With its third major release, JASPAR has been expanded and equipped with additional functions aimed at both casual and power users. The heart of the JASPAR database—the JASPAR CORE sub-database—has increased by 12% in size, and three new specialized sub-databases have been added. New functions include clustering of matrix models by similarity, generation of random matrices by sampling from selected sets of existing models and a language-independent Web Service applications programming interface for matrix retrieval. JASPAR is available at http://jaspar.genereg.net.",c90,Computer Vision and Pattern Recognition,cp90,accepted,f1936,2008,2008-10-30
s2406,p2406,Towards Sensor Database Systems,Abstract content goes here ...,c109,International Conference on Mobile Data Management,cp109,accepted,f1937,2014,2014-10-05
s2407,p2407,NGA Project Strong-Motion Database,"A key component of the NGA research project was the development of a strong-motion database with improved quality and content that could be used for ground-motion research as well as for engineering practice. Development of the NGA database was executed through the Lifelines program of the PEER Center with contributions from several research organizations and many individuals in the engineering and seismological communities. Currently, the data set consists of 3551 publicly available multi-component records from 173 shallow crustal earthquakes, ranging in magnitude from 4.2 to 7.9. Each acceleration time series has been corrected and filtered, and pseudo absolute spectral acceleration at multiple damping levels has been computed for each of the 3 components of the acceleration time series. The lowest limit of usable spectral frequency was determined based on the type of filter and the filter corner frequency. For NGA model development, the two horizontal acceleration components were further rotated to form the orientation-independent measure of horizontal ground motion (GMRotI50). In addition to the ground-motion parameters, a large and comprehensive list of metadata characterizing the recording conditions of each record was also developed. NGA data have been systematically checked and reviewed by experts and NGA developers.",c16,Knowledge Discovery and Data Mining,cp16,accepted,f1938,2003,2003-07-07
s2408,p2408,The asteroid lightcurve database,Abstract content goes here ...,c105,Biometrics and Identity Management,cp105,accepted,f1939,2006,2006-03-11
s2409,p2409,SCOP: a structural classification of proteins database,"The Structural Classification of Proteins (SCOP) database provides a detailed and comprehensive description of the relationships of known protein structures. The classification is on hierarchical levels: the first two levels, family and superfamily, describe near and distant evolutionary relationships; the third, fold, describes geometrical relationships. The distinction between evolutionary relationships and those that arise from the physics and chemistry of proteins is a feature that is unique to this database so far. The sequences of proteins in SCOP provide the basis of the ASTRAL sequence libraries that can be used as a source of data to calibrate sequence search algorithms and for the generation of statistics on, or selections of, protein structures. Links can be made from SCOP to PDB-ISL: a library containing sequences homologous to proteins of known structure. Sequences of proteins of unknown structure can be matched to distantly related proteins of known structure by using pairwise sequence comparison methods to find homologues in PDB-ISL. The database and its associated files are freely accessible from a number of WWW sites mirrored from URL http://scop.mrc-lmb.cam.ac.uk/scop/",c82,Workshop on Interdisciplinary Software Engineering Research,cp82,accepted,f1940,2004,2004-08-10
s2410,p2410,The MDM2 gene amplification database.,"The p53 tumor suppressor gene is inactivated in human tumors by several distinct mechanisms. The best characterized inactivation mechanisms are: (i) gene mutation; (ii) p53 protein association with viral proteins; (iii) p53 protein association with the MDM2 cellular oncoprotein. The MDM2 gene has been shown to be abnormally up-regulated in human tumors and tumor cell lines by gene amplification, increased transcript levels and enhanced translation. This communication presents a brief review of the spectrum of MDM2 abnormalities in human tumors and compares the tissue distribution of MDM2 amplification and p53 mutation frequencies. In this study, 3889 samples from tumors or xenografts from 28 tumor types were examined for MDM2 amplification from previously published sources. The overall frequency of MDM2 amplification in these human tumors was 7%. Gene amplification was observed in 19 tumor types, with the highest frequency observed in soft tissue tumors (20%), osteosarcomas (16%) and esophageal carcinomas (13%). Tumors which showed a higher incidence of MDM2 amplification than p53 mutation were soft tissue tumors, testicular germ cell cancers and neuro-blastomas. Data from studies where both MDM2 amplification and p53 mutations were analyzed within the same samples showed that mutations in these two genes do not generally occur within the same tumor. In these studies, 29 out of a total of 33 MDM2 amplification-positive tumors had wild-type p53. We hypothesize that heretofore uncharacterized carcinogens favor MDM2 amplification over p53 mutations in certain tumor types. A database listing the MDM2 gene amplifications is available on the World Wide Web at http://www. infosci.coh.org/mdm2 . Charts of MDM2 amplification frequencies and comparisons with p53 genetic alterations are also available at this Web site.",j102,Nucleic Acids Research,jv102,accepted,f1941,2002,2002-01-26
s2411,p2411,Database Repairing and Consistent Query Answering,"Integrity constraints are semantic conditions that a database should satisfy in order to be an appropriate model of external reality. In practice, and for many reasons, a database may not satisfy those integrity constraints, and for that reason it is said to be inconsistent. However, and most likely, a large portion of the database is still semantically correct, in a sense that has to be made precise. After having provided a formal characterization of consistent data in an inconsistent database, the natural problem emerges of extracting that semantically correct data, as query answers. The consistent data in an inconsistent database is usually characterized as the data that persists across all the database instances that are consistent and minimally differ from the inconsistent instance. Those are the so-called repairs of the database. In particular, the consistent answers to a query posed to the inconsistent database are those answers that can be simultaneously obtained from all the database repairs. As expected, the notion of repair requires an adequate notion of distance that allows for the comparison of databases with respect to how much they differ from the inconsistent instance. On this basis, the minimality condition on repairs can be properly formulated. In this monograph we present and discuss these fundamental concepts, different repair semantics, algorithms for computing consistent answers to queries, and also complexity-theoretic results related to the computation of repairs and doing consistent query answering. Table of Contents: Introduction / The Notions of Repair and Consistent Answer / Tractable CQA and Query Rewriting / Logically Specifying Repairs / Decision Problems in CQA: Complexity and Algorithms / Repairs and Data Cleaning",c79,International Conference on Medical Image Computing and Computer-Assisted Intervention,cp79,accepted,f1942,2020,2020-09-07
s2414,p2414,NIST Atomic Spectra Database (version 2.0),Abstract content goes here ...,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f1943,2021,2021-04-03
s2415,p2415,LandScan: A Global Population Database for Estimating Populations at Risk,"The LandScan Global Population Project produced a world-wide 1998 population database at a 30-by 30-second resolution for estimating ambient populations at risk. Best available census counts were distributed to cells based on probability coefficients which, in turn, were based on road proximity, slope, land cover, and nighttime lights, LandScan 1998 has been completed for the entire world. Verification and validation (V&V) studies were conducted routinely for all regions and more extensively for Israel, Germany, and the southwestern United States. Geographic information systems (GIS) were essential for conflation of diverse input variables, computation of probability coefficients, allocation of population to cells, and reconciliation of cell totals with aggregate (usually province) control totals. Remote sensing was an essential source of two input variables-land cover and nighttime lights-and one ancillary database-high-resolution panchromatic imagery-used in V&V of the population model and resulting LandScan database.",c56,European Conference on Software Process Improvement,cp56,accepted,f1944,2016,2016-09-10
s2416,p2416,DisProt: the Database of Disordered Proteins,"The Database of Protein Disorder (DisProt) links structure and function information for intrinsically disordered proteins (IDPs). Intrinsically disordered proteins do not form a fixed three-dimensional structure under physiological conditions, either in their entireties or in segments or regions. We define IDP as a protein that contains at least one experimentally determined disordered region. Although lacking fixed structure, IDPs and regions carry out important biological functions, being typically involved in regulation, signaling and control. Such functions can involve high-specificity low-affinity interactions, the multiple binding of one protein to many partners and the multiple binding of many proteins to one partner. These three features are all enabled and enhanced by protein intrinsic disorder. One of the major hindrances in the study of IDPs has been the lack of organized information. DisProt was developed to enable IDP research by collecting and organizing knowledge regarding the experimental characterization and the functional associations of IDPs. In addition to being a unique source of biological information, DisProt opens doors for a plethora of bioinformatics studies. DisProt is openly available at .",c43,ACM Symposium on Applied Computing,cp43,accepted,f1945,2001,2001-10-10
s2417,p2417,The Cochrane Database of Systematic Reviews,"1 Interventions for cutaneous molluscum contagiosum (Review) Copyright © 2017 The Cochrane Collaboration. Published by John Wiley & Sons, Ltd. Main results We found 11 new studies for this update, resulting in 22 included studies with a total of 1650 participants. The studies examined the effects of topical (20 studies) and systemic interventions (2 studies). Among the new included studies were the full trial reports of three large unpublished studies, brought to our attention by an expert in the field. They all provided moderate-quality evidence for a lack of effect of 5% imiquimod compared to vehicle (placebo) on shortterm clinical cure (4 studies, 850 participants, 12 weeks after start of treatment, risk ratio (RR) 1.33, 95% confidence interval (CI) 0.92 to 1.93), medium-term clinical cure (2 studies, 702 participants, 18 weeks after start of treatment, RR 0.88, 95% CI 0.67 to 1.14), and long-term clinical cure (2 studies, 702 participants, 28 weeks after start of treatment, RR 0.97, 95% CI 0.79 to 1.17). We found similar but more certain results for short-term improvement (4 studies, 850 participants, 12 weeks after start of treatment, RR 1.14, 95% CI 0.89 to 1.47; high-quality evidence). For the outcome ’any adverse effect’, we found high-quality evidence for little or no difference between topical 5% imiquimod and vehicle (3 studies, 827 participants, RR 0.97, 95% CI 0.88 to 1.07), but application site reactions were more frequent in the groups treated with imiquimod (moderate-quality evidence): any application site reaction (3 studies, 827 participants, RR 1.41, 95% CI 1.13 to 1.77, the number needed to treat for an additional harmful outcome (NNTH) was 11); severe application site reaction (3 studies, 827 participants, RR 4.33, 95% CI 1.16 to 16.19, NNTH over 40). For the following 11 comparisons, there was limited evidence to show which treatment was superior in achieving short-term clinical cure (low-quality evidence): 5% imiquimod less effective than cryospray (1 study, 74 participants, RR 0.60, 95% CI 0.46 to 0.78) and 10% potassium hydroxide (2 studies, 67 participants, RR 0.65, 95% CI 0.46 to 0.93); 10% Australian lemon myrtle oil more effective than olive oil (1 study, 31 participants, RR 17.88, 95% CI 1.13 to 282.72); 10% benzoyl peroxide cream more effective than 0.05% tretinoin (1 study, 30 participants, RR 2.20, 95% CI 1.01 to 4.79); 5% sodium nitrite co-applied with 5% salicylic acid more effective than 5% salicylic acid alone (1 study, 30 participants, RR 3.50, 95% CI 1.23 to 9.92); and iodine plus tea tree oil more effective than tea tree oil (1 study, 37 participants, RR 0.20, 95% CI 0.07 to 0.57) or iodine alone (1 study, 37 participants, RR 0.07, 95% CI 0.01 to 0.50). Although there is some uncertainty, 10% potassium hydroxide appears to be more effective than saline (1 study, 20 participants, RR 3.50, 95% CI 0.95 to 12.90); homeopathic calcarea carbonica appears to be more effective than placebo (1 study, 20 participants, RR 5.57, 95% CI 0.93 to 33.54); 2.5% appears to be less effective than 5% solution of potassium hydroxide (1 study, 25 participants, RR 0.35, 95% CI 0.12 to 1.01); and 10% povidone iodine solution plus 50% salicylic acid plaster appears to be more effective than salicylic acid plaster alone (1 study, 30 participants, RR 1.43, 95% CI 0.95 to 2.16). We found no statistically significant differences for other comparisons (most of which addressed two different topical treatments). We found no randomised controlled trial evidence for expressing lesions or topical hydrogen peroxide. Study limitations included no blinding, many dropouts, and no intention-to-treat analysis. Except for the severe application site reactions of imiquimod, none of the evaluated treatments described above were associated with serious adverse effects (low-quality evidence). Among the most common adverse events were pain during application, erythema, and itching. Included studies of the following comparisons did not report adverse effects: calcarea carbonica versus placebo, 10% povidone iodine plus 50% salicylic acid plaster versus salicylic acid plaster, and 10% benzoyl peroxide versus 0.05% tretinoin. We were unable to judge the risk of bias in most studies due to insufficient information, especially regarding concealment of allocation and possible selective reporting. We considered five studies to be at low risk of bias. Authors’ conclusions No single intervention has been shown to be convincingly effective in the treatment of molluscum contagiosum. We found moderatequality evidence that topical 5% imiquimod was no more effective than vehicle in terms of clinical cure, but led to more application site reactions, and high-quality evidence that there was no difference between the treatments in terms of short-term improvement. However, high-quality evidence showed a similar number of general side effects in both groups. As the evidence found did not favour any one treatment, the natural resolution of molluscum contagiosum remains a strong method for dealing with the condition. P L A I N L A N G U A G E S U M M A R Y Treatments for molluscum contagiosum, a common viral skin infection in children Review question 2 Interventions for cutaneous molluscum contagiosum (Review) Copyright © 2017 The Cochrane Collaboration. Published by John Wiley & Sons, Ltd. We reviewed the evidence for the effect of any treatment on the common viral skin infection molluscum contagiosum. We excluded people with a repressed immune system or sexually transmitted molluscum contagiosum. Background Molluscum contagiosum in healthy people is a self limiting, relatively harmless viral skin infection. It mainly affects children and adolescents and is rare in adults. It occurs worldwide, but seems much more frequent in geographic areas with warm climates. Molluscum contagiosum usually presents as single or multiple pimples filled with an oily substance. People may seek treatment for social and cosmetic reasons and because of concerns about spreading the disease to others. Treatment is intended to speed up the healing process. Study characteristics We searched the literature to July 2016. We included 22 trials (total of 1650 participants). Twenty of the studies evaluated topical treatment, and two studies evaluated treatment taken by mouth (oral). Comparisons included physical therapies, as well as topical and oral treatments. Most studies were set in hospital outpatient or emergency departments, and were performed in North America, the UK, Asia, or South America. Participants were of both sexes and were mainly children or young adults. Follow-up duration varied from 3 to 28 weeks after randomisation. Only five studies had longer than 3 months’ follow-up. Five studies reported commercial funding, three studies obtained medication for free from pharmaceutical companies, 12 studies did not mention the source of funding, one study reported charity funding, and one study reported they had had no financial support. Key results We found that many common treatments for molluscum, such as physical destruction, have not been adequately evaluated. Some of the included treatments are not part of standard practice. We found moderate-quality evidence that topical 5% imiquimod is probably no more effective than vehicle (i.e. the same cream but without imiquimod) in achieving short-, medium-, and long-term clinical cure. High-quality (and thus more certain) evidence showed that topical 5% imiquimod is no better than placebo at improving molluscum up to three months after the start of treatment. High-quality evidence showed that 5% imiquimod differed little or not at all in the number of side effects compared to vehicle. However, moderate-quality evidence suggests that there are probably more application site reactions when using topical 5% imiquimod compared with vehicle. Low-quality evidence, based on one or two mostly small studies, revealed the following results for the outcome short-term clinical cure: 5% imiquimod less effective than cryospray or 10% potassium hydroxide; 10% Australian lemon myrtle oil more effective than olive oil; 10% benzoyl peroxide cream more effective than 0.05% tretinoin; 5% sodium nitrite co-applied with 5% salicylic acid more effective than 5% salicylic acid alone; and iodine plus tea tree oil more effective than tea tree oil or iodine alone. We found more uncertain (low-quality) evidence to suggest that 10% potassium hydroxide is more effective than saline; homeopathic calcarea carbonica is more effective than placebo; 2.5% solution of potassium hydroxide is less effective than 5% solution of potassium hydroxide; and 10% povidone iodine solution and 50% salicylic acid plaster are more effective than salicylic acid plaster alone. Except for the severe application site reactions of imiquimod, none of these treatments led to serious adverse effects (low-quality evidence). Pain during treatment application, redness, and itching were among the most reported adverse effects. We found no differences between the treatments assessed in the other comparisons. We found no randomised trials for several commonly used treatments, such as expressing lesions with an orange stick or topical hydrogen peroxide. Since most lesions resolve within months, unless better evidence for the superiority of active treatments emerges, molluscum contagiosum can be left to heal naturally. Quality of the evidence For topical imiquimod, the quality of the evidence for clinical cure, short-term improvement, and adverse effects was moderate to high. For all other comparisons, the quality of the evidence for short-term clinical cure and adverse effects was low. Common limitations of the included studies were that the numbers of participants were small, the investigators were not blinded, and participants who did not complete the study (numerous in some studies) were not included in the analyses. 3 Interventions for cutaneous molluscum contagiosum (Rev",c66,Annual Conference on Innovation and Technology in Computer Science Education,cp66,accepted,f1946,2002,2002-10-09
s2418,p2418,A high-resolution 3D dynamic facial expression database,"Face information processing relies on the quality of data resource. From the data modality point of view, a face database can be 2D or 3D, and static or dynamic. From the task point of view, the data can be used for research of computer based automatic face recognition, face expression recognition, face detection, or cognitive and psychological investigation. With the advancement of 3D imaging technologies, 3D dynamic facial sequences (called 4D data) have been used for face information analysis. In this paper, we focus on the modality of 3D dynamic data for the task of facial expression recognition. We present a newly created high-resolution 3D dynamic facial expression database, which is made available to the scientific research community. The database contains 606 3D facial expression sequences captured from 101 subjects of various ethnic backgrounds. The database has been validated through our facial expression recognition experiment using an HMM based 3D spatio-temporal facial descriptor. It is expected that such a database shall be used to facilitate the facial expression analysis from a static 3D space to a dynamic 3D space, with a goal of scrutinizing facial behavior at a higher level of detail in a real 3D spatio-temporal domain.",c110,IEEE International Conference on Automatic Face & Gesture Recognition,cp110,accepted,f1947,2008,2008-06-16
s2420,p2420,A comparison of a graph database and a relational database: a data provenance perspective,"Relational databases have been around for many decades and are the database technology of choice for most traditional data-intensive storage and retrieval applications. Retrievals are usually accomplished using SQL, a declarative query language. Relational database systems are generally efficient unless the data contains many relationships requiring joins of large tables. Recently there has been much interest in data stores that do not use SQL exclusively, the so-called NoSQL movement. Examples are Google's BigTable and Facebook's Cassandra. This paper reports on a comparison of one such NoSQL graph database called Neo4j with a common relational database system, MySQL, for use as the underlying technology in the development of a software system to record and query data provenance information.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f1948,2010,2010-05-29
s2421,p2421,GeneDB—an annotation database for pathogens,"GeneDB (http://www.genedb.org) is a genome database for prokaryotic and eukaryotic pathogens and closely related organisms. The resource provides a portal to genome sequence and annotation data, which is primarily generated by the Pathogen Genomics group at the Wellcome Trust Sanger Institute. It combines data from completed and ongoing genome projects with curated annotation, which is readily accessible from a web based resource. The development of the database in recent years has focused on providing database-driven annotation tools and pipelines, as well as catering for increasingly frequent assembly updates. The website has been significantly redesigned to take advantage of current web technologies, and improve usability. The current release stores 41 data sets, of which 17 are manually curated and maintained by biologists, who review and incorporate data from the scientific literature, as well as other sources. GeneDB is primarily a production and annotation database for the genomes of predominantly pathogenic organisms.",c9,Pacific Symposium on Biocomputing,cp9,accepted,f1949,2009,2009-03-23
s2422,p2422,The United Kingdom Chemical Database Service,"The Chemical Database Service (CDS) is a national service, funded by the Chemistry Programme of the United Kingdom Engineering and Physical Sciences Research Council (EPSRC). It provides access for UK academics to a range of chemistry databases in the areas of crystallography, synthetic organic chemistry, spectroscopy, and physical chemistry. Three post-doctoral chemists are available to assist users with problems, run training courses, and also give advice to the community on accessing other sources of chemical data and software.",j214,Journal of chemical information and computer sciences,jv214,accepted,f1950,2022,2022-06-25
s2423,p2423,The IntAct molecular interaction database in 2010,"IntAct is an open-source, open data molecular interaction database and toolkit. Data is abstracted from the literature or from direct data depositions by expert curators following a deep annotation model providing a high level of detail. As of September 2009, IntAct contains over 200.000 curated binary interaction evidences. In response to the growing data volume and user requests, IntAct now provides a two-tiered view of the interaction data. The search interface allows the user to iteratively develop complex queries, exploiting the detailed annotation with hierarchical controlled vocabularies. Results are provided at any stage in a simplified, tabular view. Specialized views then allows ‘zooming in’ on the full annotation of interactions, interactors and their properties. IntAct source code and data are freely available at http://www.ebi.ac.uk/intact.",c0,International Conference on Human Factors in Computing Systems,cp0,accepted,f1951,2019,2019-06-14
s2425,p2425,PHOSIDA 2011: the posttranslational modification database,"The primary purpose of PHOSIDA (http://www.phosida.com) is to manage posttranslational modification sites of various species ranging from bacteria to human. Since its last report, PHOSIDA has grown significantly in size and evolved in scope. It comprises more than 80 000 phosphorylated, N-glycosylated or acetylated sites from nine different species. All sites are obtained from high-resolution mass spectrometric data using the same stringent quality criteria. One of the main distinguishing features of PHOSIDA is the provision of a wide range of analysis tools. PHOSIDA is comprised of three main components: the database environment, the prediction platform and the toolkit section. The database environment integrates and combines high-resolution proteomic data with multiple annotations. High-accuracy species-specific phosphorylation and acetylation site predictors, trained on the modification sites contained in PHOSIDA, allow the in silico determination of modified sites on any protein on the basis of the primary sequence. The toolkit section contains methods that search for sequence motif matches or identify de novo consensus, sequences from large scale data sets.",c61,Jahrestagung der Gesellschaft für Informatik,cp61,accepted,f1952,2017,2017-06-04
s2426,p2426,The TIGRFAMs database of protein families,"TIGRFAMs is a collection of manually curated protein families consisting of hidden Markov models (HMMs), multiple sequence alignments, commentary, Gene Ontology (GO) assignments, literature references and pointers to related TIGRFAMs, Pfam and InterPro models. These models are designed to support both automated and manually curated annotation of genomes. TIGRFAMs contains models of full-length proteins and shorter regions at the levels of superfamilies, subfamilies and equivalogs, where equivalogs are sets of homologous proteins conserved with respect to function since their last common ancestor. The scope of each model is set by raising or lowering cutoff scores and choosing members of the seed alignment to group proteins sharing specific function (equivalog) or more general properties. The overall goal is to provide information with maximum utility for the annotation process. TIGRFAMs is thus complementary to Pfam, whose models typically achieve broad coverage across distant homologs but end at the boundaries of conserved structural domains. The database currently contains over 1600 protein families. TIGRFAMs is available for searching or downloading at www.tigr.org/TIGRFAMs.",c80,International Conference on Learning Representations,cp80,accepted,f1953,2005,2005-08-12
s2427,p2427,XCOM: Photon Cross Section Database (version 1.2),Abstract content goes here ...,c52,Workshop on Learning from Authoritative Security Experiment Results,cp52,accepted,f1954,2022,2022-06-19
s2429,p2429,Integrating compression and execution in column-oriented database systems,"Column-oriented database system architectures invite a re-evaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads.In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in row-oriented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.",c87,European Conference on Computer Vision,cp87,accepted,f1955,2014,2014-02-27
s2430,p2430,"BioModels Database: a free, centralized database of curated, published, quantitative kinetic models of biochemical and cellular systems","BioModels Database (), part of the international initiative BioModels.net, provides access to published, peer-reviewed, quantitative models of biochemical and cellular systems. Each model is carefully curated to verify that it corresponds to the reference publication and gives the proper numerical results. Curators also annotate the components of the models with terms from controlled vocabularies and links to other relevant data resources. This allows the users to search accurately for the models they need. The models can currently be retrieved in the SBML format, and import/export facilities are being developed to extend the spectrum of formats supported by the resource.",c32,International Conference on Software Technology: Methods and Tools,cp32,accepted,f1956,2010,2010-07-24
s2431,p2431,Tuberculosis Drug Resistance Mutation Database,Andreas Sandgren and colleagues describe a new comprehensive resource on drug resistance mutations inM. tuberculosis.,j380,PLoS Medicine,jv380,accepted,f1957,2020,2020-11-08
s2433,p2433,Gigascope: a stream database for network applications,"We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.",c100,ACM SIGMOD Conference,cp100,accepted,f1958,2010,2010-05-28
s2434,p2434,EcoCyc: a comprehensive database resource for Escherichia coli,"The EcoCyc database (http://EcoCyc.org/) is a comprehensive source of information on the biology of the prototypical model organism Escherichia coli K12. The mission for EcoCyc is to contain both computable descriptions of, and detailed comments describing, all genes, proteins, pathways and molecular interactions in E.coli. Through ongoing manual curation, extensive information such as summary comments, regulatory information, literature citations and evidence types has been extracted from 8862 publications and added to Version 8.5 of the EcoCyc database. The EcoCyc database can be accessed through a World Wide Web interface, while the downloadable Pathway Tools software and data files enable computational exploration of the data and provide enhanced querying capabilities that web interfaces cannot support. For example, EcoCyc contains carefully curated information that can be used as training sets for bioinformatics prediction of entities such as promoters, operons, genetic networks, transcription factor binding sites, metabolic pathways, functionally related genes, protein complexes and protein–ligand interactions.",c89,Conference on Uncertainty in Artificial Intelligence,cp89,accepted,f1959,2015,2015-10-02
s2435,p2435,"THE COLOGNE DATABASE FOR MOLECULAR SPECTROSCOPY, CDMS",Abstract content goes here ...,c94,Vision,cp94,accepted,f1960,2020,2020-11-03
s2436,p2436,The MUG facial expression database,This paper presents a new extended collection of posed and induced facial expression image sequences. All sequences were captured in a controlled laboratory environment with high resolution and no occlusions. The collection consists of two parts: The first part depicts eighty six subjects performing the six basic expressions according to the “emotion prototypes” as defined in the Investigator's Guide in the FACS manual. The second part contains the same subjects recorded while they were watching an emotion inducing video. Most of the database recordings are available to the scientific community. Beyond the emotion related annotation the database contains also manual and automatic annotation of 80 facial landmark points for a significant number of frames. The database contains sufficient material for the development and the statistical evaluation of facial expression recognition systems using posed and induced expressions.,c27,ACM-SIAM Symposium on Discrete Algorithms,cp27,accepted,f1961,2022,2022-04-05
s2437,p2437,CDD: a curated Entrez database of conserved domain alignments,"The Conserved Domain Database (CDD) is now indexed as a separate database within the Entrez system and linked to other Entrez databases such as MEDLINE(R). This allows users to search for domain types by name, for example, or to view the domain architecture of any protein in Entrez's sequence database. CDD can be accessed on the WorldWideWeb at http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=cdd. Users may also employ the CD-Search service to identify conserved domains in new sequences, at http://www.ncbi.nlm.nih.gov/Structure/cdd/wrpsb.cgi. CD-Search results, and pre-computed links from Entrez's protein database, are calculated using the RPS-BLAST algorithm and Position Specific Score Matrices (PSSMs) derived from CDD alignments. CD-Searches are also run by default for protein-protein queries submitted to BLAST(R) at http://www.ncbi.nlm.nih.gov/BLAST. CDD mirrors the publicly available domain alignment collections SMART and PFAM, and now also contains alignment models curated at NCBI. Structure information is used to identify the core substructure likely to be present in all family members, and to produce sequence alignments consistent with structure conservation. This alignment model allows NCBI curators to annotate 'columns' corresponding to functional sites conserved among family members.",c71,IEEE International Conference on Information Reuse and Integration,cp71,accepted,f1962,2022,2022-03-17
s2438,p2438,Phospho.ELM: a database of phosphorylation sites—update 2008,"Phospho.ELM is a manually curated database of eukaryotic phosphorylation sites. The resource includes data collected from published literature as well as high-throughput data sets. The current release of Phospho.ELM (version 7.0, July 2007) contains 4078 phospho-protein sequences covering 12 025 phospho-serine, 2362 phospho-threonine and 2083 phospho-tyrosine sites. The entries provide information about the phosphorylated proteins and the exact position of known phosphorylated instances, the kinases responsible for the modification (where known) and links to bibliographic references. The database entries have hyperlinks to easily access further information from UniProt, PubMed, SMART, ELM, MSD as well as links to the protein interaction databases MINT and STRING. A new BLAST search tool, complementary to retrieval by keyword and UniProt accession number, allows users to submit a protein query (by sequence or UniProt accession) to search against the curated data set of phosphorylated peptides. Phospho.ELM is available on line at: http://phospho.elm.eu.org",c83,International Conference on Computer Graphics and Interactive Techniques,cp83,accepted,f1963,2015,2015-03-31
s2439,p2439,APD: the Antimicrobial Peptide Database,"An antimicrobial peptide database (APD) has been established based on an extensive literature search. It contains detailed information for 525 peptides (498 antibacterial, 155 antifungal, 28 antiviral and 18 antitumor). APD provides interactive interfaces for peptide query, prediction and design. It also provides statistical data for a select group of or all the peptides in the database. Peptide information can be searched using keywords such as peptide name, ID, length, net charge, hydrophobic percentage, key residue, unique sequence motif, structure and activity. APD is a useful tool for studying the structure-function relationship of antimicrobial peptides. The database can be accessed via a web-based browser at the URL: http://aps.unmc.edu/AP/main.html.",c6,Americas Conference on Information Systems,cp6,accepted,f1964,2007,2007-05-23
s2440,p2440,"The PANTHER database of protein families, subfamilies, functions and pathways","PANTHER is a large collection of protein families that have been subdivided into functionally related subfamilies, using human expertise. These subfamilies model the divergence of specific functions within protein families, allowing more accurate association with function (ontology terms and pathways), as well as inference of amino acids important for functional specificity. Hidden Markov models (HMMs) are built for each family and subfamily for classifying additional protein sequences. The latest version, 5.0, contains 6683 protein families, divided into 31 705 subfamilies, covering ∼90% of mammalian protein-coding genes. PANTHER 5.0 includes a number of significant improvements over previous versions, most notably (i) representation of pathways (primarily signaling pathways) and association with subfamilies and individual protein sequences; (ii) an improved methodology for defining the PANTHER families and subfamilies, and for building the HMMs; (iii) resources for scoring sequences against PANTHER HMMs both over the web and locally; and (iv) a number of new web resources to facilitate analysis of large gene lists, including data generated from high-throughput expression experiments. Efforts are underway to add PANTHER to the InterPro suite of databases, and to make PANTHER consistent with the PIRSF database. PANTHER is now publicly available without restriction at http://panther.appliedbiosystems.com.",c108,International Conference on Information Integration and Web-based Applications & Services,cp108,accepted,f1965,2011,2011-11-06
s2441,p2441,An Electronic Lexical Database,Abstract content goes here ...,c67,Enterprise Application Integration,cp67,accepted,f1966,2002,2002-06-15
s2444,p2444,Carbohydrate-active enzymes : an integrated database approach,Abstract content goes here ...,c38,"IEEE International Conference on Software Analysis, Evolution, and Reengineering",cp38,accepted,f1967,2015,2015-01-30
s2445,p2445,Introducing the Global Terrorism Database,"Compared to most types of criminal violence, terrorism poses special data collection challenges. In response, there has been growing interest in open source terrorist event data bases. One of the major problems with these data bases in the past is that they have been limited to international events—those involving a national or group of nationals from one country attacking targets physically located in another country. Past research shows that domestic incidents greatly outnumber international incidents. In this paper we describe a previously unavailable open source data base that includes some 70,000 domestic and international incidents since 1970. We began the Global Terrorism Database (GTD) by computerizing data originally collected by the Pinkerton Global Intelligence Service (PGIS). Following computerization, our research team has been working for the past two years to validate and extend the data to real time. In this paper, we describe our data collection efforts, the strengths and weaknesses of open source data in general and the GTD in particular, and provide descriptive statistics on the contents of this new resource.",c72,Intelligent Systems in Molecular Biology,cp72,accepted,f1968,2003,2003-02-02
s2446,p2446,The Object Database Standard: ODMG-93,Abstract content goes here ...,c104,IEEE International Conference on Multimedia and Expo,cp104,accepted,f1969,2006,2006-05-21
s2447,p2447,MINT: the Molecular INTeraction database,"Protein interaction databases represent unique tools to store, in a computer readable form, the protein interaction information disseminated in the scientific literature. Well organized and easily accessible databases permit the easy retrieval and analysis of large interaction data sets. Here we present MINT, a database (http://cbm.bio.uniroma2.it/mint/index.html) designed to store data on functional interactions between proteins. Beyond cataloguing binary complexes, MINT was conceived to store other types of functional interactions, including enzymatic modifications of one of the partners. Release 1.0 of MINT focuses on experimentally verified protein-protein interactions. Both direct and indirect relationships are considered. Furthermore, MINT aims at being exhaustive in the description of the interaction and, whenever available, information about kinetic and binding constants and about the domains participating in the interaction is included in the entry. MINT consists of entries extracted from the scientific literature by expert curators assisted by 'MINT Assistant', a software that targets abstracts containing interaction information and presents them to the curator in a user-friendly format. The interaction data can be easily extracted and viewed graphically through 'MINT Viewer'. Presently MINT contains 4568 interactions, 782 of which are indirect or genetic interactions.",c43,ACM Symposium on Applied Computing,cp43,accepted,f1970,2001,2001-05-14
s2450,p2450,Third millenium ideal gas and condensed phase thermochemical database for combustion (with update from active thermochemical tables).,"The thermochemical database of species involved in combustion processes is and has been available for free use for over 25 years. It was first published in print in 1984, approximately 8 years after it was first assembled, and contained 215 species at the time. This is the 7th printed edition and most likely will be the last one in print in the present format, which involves substantial manual labor. The database currently contains more than 1300 species, specifically organic molecules and radicals, but also inorganic species connected to combustion and air pollution. Since 1991 this database is freely available on the internet, at the Technion-IIT ftp server, and it is continuously expanded and corrected. The database is mirrored daily at an official mirror site, and at random at about a dozen unofficial mirror and 'finger' sites. The present edition contains numerous corrections and many recalculations of data of provisory type by the G3//B3LYP method, a high-accuracy composite ab initio calculation. About 300 species are newly calculated and are not yet published elsewhere. In anticipation of the full coupling, which is under development, the database started incorporating the available (as yet unpublished) values from Active Thermochemical Tables. The electronic version nowmore » also contains an XML file of the main database to allow transfer to other formats and ease finding specific information of interest. The database is used by scientists, educators, engineers and students at all levels, dealing primarily with combustion and air pollution, jet engines, rocket propulsion, fireworks, but also by researchers involved in upper atmosphere kinetics, astrophysics, abrasion metallurgy, etc. This introductory article contains explanations of the database and the means to use it, its sources, ways of calculation, and assessments of the accuracy of data.« less",c112,Very Large Data Bases Conference,cp112,accepted,f1971,2018,2018-06-11
s2451,p2451,THE EXTRAGALACTIC DISTANCE DATABASE,"A database can be accessed on the Web at http://edd.ifa.hawaii.edu that was developed to promote access to information related to galaxy distances. The database has three functional components. First, tables from many literature sources have been gathered and enhanced with links through a distinct galaxy naming convention. Second, comparisons of results both at the levels of parameters and of techniques have begun and are continuing, leading to increasing homogeneity and consistency of distance measurements. Third, new material is presented arising from ongoing observational programs at the University of Hawaii 2.2 m telescope, radio telescopes at Green Bank, Arecibo, and Parkes and with the Hubble Space Telescope. This new observational material is made available in tandem with related material drawn from archives and passed through common analysis pipelines.",c15,International Conference on Conceptual Structures,cp15,accepted,f1972,2011,2011-11-27
s2452,p2452,The International Protein Index: An integrated database for proteomics experiments,"Despite the complete determination of the genome sequence of several higher eukaryotes, their proteomes remain relatively poorly defined. Information about proteins identified by different experimental and computational methods is stored in different databases, meaning that no single resource offers full coverage of known and predicted proteins. IPI (the International Protein Index) has been developed to address these issues and offers complete nonredundant data sets representing the human, mouse and rat proteomes, built from the Swiss‐Prot, TrEMBL, Ensembl and RefSeq databases.",j235,Proteomics,jv235,accepted,f1973,2006,2006-12-09
s2453,p2453,Providing database as a service,"We explore a novel paradigm for data management in which a third party service provider hosts ""database as a service"", providing its customers with seamless mechanisms to create, store, and access their databases at the host site. Such a model alleviates the need for organizations to purchase expensive hardware and software, deal with software upgrades, and hire professionals for administrative and maintenance tasks which are taken over by the service provider. We have developed and deployed a database service on the Internet, called NetDB2, which is in constant use. In a sense, a data management model supported by NetDB2 provides an effective mechanism for organizations to purchase data management as a service, thereby freeing them to concentrate on their core businesses. Among the primary challenges introduced by ""database as a service"" are the additional overhead of remote access to data, an infrastructure to guarantee data privacy, and user interface design for such a service. These issues are investigated. We identify data privacy as a particularly vital problem and propose alternative solutions based on data encryption. The paper is meant as a challenge for the database community to explore a rich set of research issues that arise in developing such a service.",c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f1974,2014,2014-12-29
s2454,p2454,"The InterPro Database, 2003 brings increased coverage and new features","InterPro, an integrated documentation resource of protein families, domains and functional sites, was created in 1999 as a means of amalgamating the major protein signature databases into one comprehensive resource. PROSITE, Pfam, PRINTS, ProDom, SMART and TIGRFAMs have been manually integrated and curated and are available in InterPro for text- and sequence-based searching. The results are provided in a single format that rationalises the results that would be obtained by searching the member databases individually. The latest release of InterPro contains 5629 entries describing 4280 families, 1239 domains, 95 repeats and 15 post-translational modifications. Currently, the combined signatures in InterPro cover more than 74% of all proteins in SWISS-PROT and TrEMBL, an increase of nearly 15% since the inception of InterPro. New features of the database include improved searching capabilities and enhanced graphical user interfaces for visualisation of the data. The database is available via a webserver (http://www.ebi.ac.uk/interpro) and anonymous FTP (ftp://ftp.ebi.ac.uk/pub/databases/interpro).",c111,International Society for Music Information Retrieval Conference,cp111,accepted,f1975,2001,2001-07-14
s2455,p2455,"Principles of database and knowledge-base systems, Vol. I",Abstract content goes here ...,c103,ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing,cp103,accepted,f1976,2021,2021-04-28
s2456,p2456,Query by humming: musical information retrieval in an audio database,"The emergence of audio and video data types in databases will require new information retrieval methods adapted to the specific characteristics and needs of these data types. An effective and natural way of querying a musical audio database is by humming the tune of a song. In this paper, a system for querying an audio database by humming is described along with a scheme for representing the melodic information in a song as relative pitch changes. Relevant difficulties involved with tracking pitch are enumerated, along with the approach we followed, and the performance results of system indicating its effectiveness are presented.",c49,International Symposium on Search Based Software Engineering,cp49,accepted,f1977,2012,2012-03-16
s2458,p2458,The serializability of concurrent database updates,"A sequence of interleaved user transactions in a database system may not be ser:ahzable, t e, equivalent to some sequential execution of the individual transactions Using a simple transaction model, it ~s shown that recognizing the transaction histories that are serlahzable is an NP-complete problem. Several efficiently recognizable subclasses of the class of senahzable histories are therefore introduced; most of these subclasses correspond to senahzabdity principles existing in the hterature and used in practice Two new principles that subsume all previously known ones are also proposed Necessary and sufficient conditions are given for a class of histories to be the output of an efficient history scheduler, these conditions imply that there can be no efficient scheduler that outputs all of senahzable histories, and also that all subclasses of senalizable histories studied above have an efficient scheduler Finally, it is shown how these results can be extended to far more general transaction models, to transactions with partly interpreted functions, and to distributed database systems",c7,European Conference on Modelling and Simulation,cp7,accepted,f1978,2015,2015-03-09
s2459,p2459,System R: relational approach to database management,"System R is a database management system which provides a high level relational data interface. The systems provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment.
This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.",c77,Networks,cp77,accepted,f1979,2019,2019-06-14
s2460,p2460,Online Predicted Human Interaction Database,"MOTIVATION
High-throughput experiments are being performed at an ever-increasing rate to systematically elucidate protein-protein interaction (PPI) networks for model organisms, while the complexities of higher eukaryotes have prevented these experiments for humans.


RESULTS
The Online Predicted Human Interaction Database (OPHID) is a web-based database of predicted interactions between human proteins. It combines the literature-derived human PPI from BIND, HPRD and MINT, with predictions made from Saccharomyces cerevisiae, Caenorhabditis elegans, Drosophila melanogaster and Mus musculus. The 23,889 predicted interactions currently listed in OPHID are evaluated using protein domains, gene co-expression and Gene Ontology terms. OPHID can be queried using single or multiple IDs and results can be visualized using our custom graph visualization program.


AVAILABILITY
Freely available to academic users at http://ophid.utoronto.ca, both in tab-delimited and PSI-MI formats. Commercial users, please contact I.J.


CONTACT
juris@ai.utoronto.ca


SUPPLEMENTARY INFORMATION
http://ophid.utoronto.ca/supplInfo.pdf.",c1,Technical Symposium on Computer Science Education,cp1,accepted,f1980,2002,2002-11-30
s2461,p2461,MCYT baseline corpus: a bimodal biometric database,"The current need for large multimodal databases to evaluate automatic biometric recognition systems has motivated the development of the MCYT bimodal database. The main purpose has been to consider a large scale population, with statistical significance, in a real multimodal procedure, and including several sources of variability that can be found in real environments. The acquisition process, contents and availability of the single-session baseline corpus are fully described. Some experiments showing consistency of data through the different acquisition sites and assessing data quality are also presented.",c99,Annual International Conference of the IEEE Engineering in Medicine and Biology Society,cp99,accepted,f1981,2015,2015-12-31
s2462,p2462,The Harmonized World Soil Database,"For more than 30 years the FAO/Unesco Soil map of the World has been the only harmonized source of global soil information. Recent updates and release of new soil information in all regions of the globe was an incentive to tackle the harmonization and integration of the new soil data. The task was undertaken by a consortium of institutes and organizations and resulted in a product with 30 arc second resolution that includes for each soil unit estimates for fifteen top- and subsoil properties. The data come with a viewer, are GIS compatible and are freely available on-line.",c47,International Symposium on Empirical Software Engineering and Measurement,cp47,accepted,f1982,2010,2010-01-03
s2463,p2463,Implementation techniques for main memory database systems,"With the availability of very large, relatively inexpensive main memories, it is becoming possible keep large databases resident in main memory In this paper we consider the changes necessary to permit a relational database system to take advantage of large amounts of main memory We evaluate AVL vs B+-tree access methods for main memory databases, hash-based query processing strategies vs sort-merge, and study recovery issues when most or all of the database fits in main memory As expected, B+-trees are the preferred storage mechanism unless more than 80--90% of the database fits in main memory A somewhat surprising result is that hash based query processing strategies are advantageous for large memory situations",c100,ACM SIGMOD Conference,cp100,accepted,f1983,2010,2010-08-23
s2464,p2464,AAindex: Amino Acid Index Database,"AAindex is a database of numerical indices representing various physicochemical and biochemical properties of amino acids and pairs of amino acids. It consists of two sections: AAindex1 for the amino acid index of 20 numerical values and AAindex2 for the amino acid mutation matrix of 210 numerical values. Each entry of either AAindex1 or AAindex2 consists of the definition, the reference information, a list of related entries in terms of the correlation coefficient, and the actual data. The database may be accessed through the DBGET/LinkDB system at GenomeNet (http://www.genome.ad. jp/dbget/) or may be downloaded by anonymous FTP (ftp://ftp.genome. ad.jp/db/genomenet/aaindex/).",c50,International Conference on Automated Software Engineering,cp50,accepted,f1984,2008,2008-03-29
s2465,p2465,The nucleic acid database. A comprehensive relational database of three-dimensional structures of nucleic acids.,Abstract content goes here ...,j381,Biophysical Journal,jv381,accepted,f1985,2019,2019-08-10
s2466,p2466,The Regulation and Supervision of Banks around the World: A New Database,"International consultants on bank regulation, and supervision for developing countries, often base their advice on how their home country does things, for lack of information on practice in other countries. Recommendations for reform have tended to be shaped by bias rather than facts. To better inform advice about bank regulation, and supervision, and to lower the marginal cost of empirical research, the authors present, and discuss a new, and comprehensive database on the regulation, and supervision of banks in a hundred and seven countries. The data, based on surveys sent to national bank regulatory, supervisory authorities, are now available to researchers, and policymakers around the world. The data cover such aspects of banking as entry requirements, ownership restrictions, capital requirements, activity restrictions, external auditing requirements, characteristics of deposit insurance schemes, loan classification and provisioning requirements, accounting and disclosure requirements, troubled bank resolution actions, and (uniquely) the quality of supervisory personnel, and their actions. The database permits users to learn how banks are currently regulated, and supervised, and about bank structures, and deposit insurance schemes, for a broad cross-section of countries. In addition to describing the data, the authors show how variables ay be grouped, and aggregated. They also show some simple correlations among selected variables. In a comparison paper (""Bank regulation and supervision: What works best"") studying the relationship between differences in bank regulation and supervision, and bank performance and stability, they conclude that: 1) Countries with policies that promote private monitoring of banks, have better bank performance, and more stability. Countries with more generous deposit insurance schemes tend to have poorer bank performance, and more bank fragility. 2) Diversification of income streams, and loan portfolios - by not restricting bank activities - also tends to improve performance, and stability. (This works best when an active securities market exists). Countries in which banks are encouraged to diversify their portfolios, domestically and internationally, suffer fewer crisis.",c21,Grid Computing Environments,cp21,accepted,f1986,2005,2005-09-11
s2467,p2467,New developments in the InterPro database,"InterPro is an integrated resource for protein families, domains and functional sites, which integrates the following protein signature databases: PROSITE, PRINTS, ProDom, Pfam, SMART, TIGRFAMs, PIRSF, SUPERFAMILY, Gene3D and PANTHER. The latter two new member databases have been integrated since the last publication in this journal. There have been several new developments in InterPro, including an additional reading field, new database links, extensions to the web interface and additional match XML files. InterPro has always provided matches to UniProtKB proteins on the website and in the match XML file on the FTP site. Additional matches to proteins in UniParc (UniProt archive) are now available for download in the new match XML files only. The latest InterPro release (13.0) contains more than 13 000 entries, covering over 78% of all proteins in UniProtKB. The database is available for text- and sequence-based searches via a webserver (), and for download by anonymous FTP (). The InterProScan search tool is now also available via a web service at .",c1,Technical Symposium on Computer Science Education,cp1,accepted,f1987,2002,2002-06-07
s2468,p2468,"ARAMEMNON, a Novel Database for Arabidopsis Integral Membrane Proteins1","A specialized database (DB) for Arabidopsis membrane proteins, ARAMEMNON, was designed that facilitates the interpretation of gene and protein sequence data by integrating features that are presently only available from individual sources. Using several publicly available prediction programs, putative integral membrane proteins were identified among the approximately 25,500 proteins in the Arabidopsis genome DBs. By averaging the predictions from seven programs, approximately 6,500 proteins were classified as transmembrane (TM) candidate proteins. Some 1,800 of these contain at least four TM spans and are possibly linked to transport functions. The ARAMEMNON DB enables direct comparison of the predictions of seven different TM span computation programs and the predictions of subcellular localization by eight signal peptide recognition programs. A special function displays the proteins related to the query and dynamically generates a protein family structure. As a first set of proteins from other organisms, all of the approximately 700 putative membrane proteins were extracted from the genome of the cyanobacterium Synechocystis sp. and incorporated in the ARAMEMNON DB. The ARAMEMNON DB is accessible at the URL http://aramemnon.botanik.uni-koeln.de.",j224,Plant Physiology,jv224,accepted,f1988,2020,2020-11-24
s2469,p2469,Human Immunodeficiency Virus Reverse Transcriptase and Protease Sequence Database,"The HIV RT and Protease Sequence Database is an on-line relational database that catalogues evolutionary and drug-related human immunodeficiency virus reverse transcriptase (RT) and protease sequence variation (http://hivdb.stanford.edu). The database contains a compilation of nearly all published HIV RT and protease sequences including International Collaboration database submissions (e.g., GenBank) and sequences published in journal articles. Sequences are linked to data about the source of the sequence sample and the anti-HIV drug treatment history of the individual from whom the isolate was obtained. The database is curated and sequences are annotated with data from 180 literature references. Users can retrieve additional data and view alignments of sequences sets meeting specific criteria (e.g., treatment history, subtype, presence of a particular mutation).",c16,Knowledge Discovery and Data Mining,cp16,accepted,f1989,2003,2003-05-30
s2470,p2470,World Ocean Database,"The U.S. National Oceanic and Atmospheric Administration's (NOAA) World Ocean Database 2009, released in November as an update to the 2005 version, provides about 9.1 million temperature profiles and 3.5 million salinity reports, with some information dating as far back as 1800. The updated database includes scientific information about the oceans that can be sorted in various ways, including geographically or by year. 
 
“There is now more data about the global oceans than ever before,” according to Sydney Levitus, director of the World Data Center for Oceanography, part of NOAA's National Oceanographic Data Center. “Previous databases have shown the world ocean has warmed during the last 53 years, and it's crucial we have reliable, accurate monitoring of our oceans into the future,” he said. The database is a part of the Integrated Ocean Observing System and the Global Earth Observation System of Systems.",c107,British Machine Vision Conference,cp107,accepted,f1990,2012,2012-09-07
s2472,p2472,PMRD: plant microRNA database,"MicroRNAs (miRNA) are ∼21 nucleotide-long non-coding small RNAs, which function as post-transcriptional regulators in eukaryotes. miRNAs play essential roles in regulating plant growth and development. In recent years, research into the mechanism and consequences of miRNA action has made great progress. With whole genome sequence available in such plants as Arabidopsis thaliana, Oryza sativa, Populus trichocarpa, Glycine max, etc., it is desirable to develop a plant miRNA database through the integration of large amounts of information about publicly deposited miRNA data. The plant miRNA database (PMRD) integrates available plant miRNA data deposited in public databases, gleaned from the recent literature, and data generated in-house. This database contains sequence information, secondary structure, target genes, expression profiles and a genome browser. In total, there are 8433 miRNAs collected from 121 plant species in PMRD, including model plants and major crops such as Arabidopsis, rice, wheat, soybean, maize, sorghum, barley, etc. For Arabidopsis, rice, poplar, soybean, cotton, medicago and maize, we included the possible target genes for each miRNA with a predicted interaction site in the database. Furthermore, we provided miRNA expression profiles in the PMRD, including our local rice oxidative stress related microarray data (LC Sciences miRPlants_10.1) and the recently published microarray data for poplar, Arabidopsis, tomato, maize and rice. The PMRD database was constructed by open source technology utilizing a user-friendly web interface, and multiple search tools. The PMRD is freely available at http://bioinformatics.cau.edu.cn/PMRD. We expect PMRD to be a useful tool for scientists in the miRNA field in order to study the function of miRNAs and their target genes, especially in model plants and major crops.",c26,PS,cp26,accepted,f1991,2010,2010-10-09
s2473,p2473,A common database approach for OLTP and OLAP using an in-memory column database,"When SQL and the relational data model were introduced 25 years ago as a general data management concept, enterprise software migrated quickly to this new technology. It is fair to say that SQL and the various implementations of RDBMSs became the backbone of enterprise systems. In those days. we believed that business planning, transaction processing and analytics should reside in one single system. Despite the incredible improvements in computer hardware, high-speed networks, display devices and the associated software, speed and flexibility remained an issue. The nature of RDBMSs, being organized along rows, prohibited us from providing instant analytical insight and finally led to the introduction of so-called data warehouses. This paper will question some of the fundamentals of the OLAP and OLTP separation. Based on the analysis of real customer environments and experience in some prototype implementations, a new proposal for an enterprise data management concept will be presented. In our proposal, the participants in enterprise applications, customers, orders, accounting documents, products, employees etc. will be modeled as objects and also stored and maintained as such. Despite that, the vast majority of business functions will operate on an in memory representation of their objects. Using the relational algebra and a column-based organization of data storage will allow us to revolutionize transactional applications while providing an optimal platform for analytical data processing. The unification of OLTP and OLAP workloads on a shared architecture and the reintegration of planning activities promise significant gains in application development while simplifying enterprise systems drastically. The latest trends in computer technology -- e.g. blade architecture, multiple CPUs per blade with multiple cores per CPU allow for a significant parallelization of application processes. The organization of data in columns supports the parallel use of cores for filtering and aggregation. Elements of application logic can be implemented as highly efficient stored procedures operating on columns. The vast increase in main memory combined with improvements in L1--, L2--, L3--caching, together with the high data compression rate column storage will allow us to support substantial data volumes on one single blade. Distributing data across multiple blades using a shared nothing approach provides further scalability.",c39,International Conference on Global Software Engineering,cp39,accepted,f1992,2020,2020-03-25
s2475,p2475,Chabot: Retrieval from a Relational Database of Images,"Selecting from a large, expanding collection of images requires carefully chosen search criteria. We present an approach that integrates a relational database retrieval system with a color analysis technique. The Chabot project was initiated at our university to study storage and retrieval of a vast collection of digitized images. These images are from the State of California Department of Water Resources. The goal was to integrate a relational database retrieval system with content analysis techniques that would give our querying system a better method for handling images. Our simple color analysis method, if used in conjunction with other search criteria, improves our ability to retrieve images efficiently. The best result is obtained when text-based search criteria are combined with content-based criteria and when a coarse granularity is used for content analysis. >",j79,Computer,jv79,accepted,f1993,2014,2014-03-16
s2476,p2476,"RWC Music Database: Popular, Classical and Jazz Music Databases","paper describes the design policy and specifications of the RWC Music Database , a music database (DB) that is available to researchers for common use and research purposes. Various com- monly available DBs have been built in other research fields and have made a significant contribution to the research in those fields. The field of musical information processing, however, has lacked a commonly available music DB. We therefore built the RWC Mu- sic Database which contains four original DBs: the Popular Music Database (100 pieces), Royalty-Free Music Database(15 pieces), Classical Music Database(50 pieces), and Jazz Music Database (50 pieces). Each consists of originally-recorded music compact discs, standard MIDI files, and text files of lyrics. These DBs are now available in Japan at a cost equal to only duplication, shipping, and handling charges (virtually for free), and we plan to make them available outside Japan. We hope that our DB will encourage further advances in musical information processing research.",c111,International Society for Music Information Retrieval Conference,cp111,accepted,f1994,2001,2001-01-13
s2477,p2477,Foundations of Preferences in Database Systems,Abstract content goes here ...,c112,Very Large Data Bases Conference,cp112,accepted,f1995,2018,2018-07-27
s2478,p2478,Exhaustive matching of the entire protein sequence database.,"The entire protein sequence database has been exhaustively matched. Definitive mutation matrices and models for scoring gaps were obtained from the matching and used to organize the sequence database as sets of evolutionarily connected components. The methods developed are general and can be used to manage sequence data generated by major genome sequencing projects. The alignments made possible by the exhaustive matching are the starting point for successful de novo prediction of the folded structures of proteins, for reconstructing sequences of ancient proteins and metabolisms in ancient organisms, and for obtaining new perspectives in structural biochemistry.",j97,Science,jv97,accepted,f1996,2012,2012-06-10
s2479,p2479,Human protein reference database as a discovery resource for proteomics,"The rapid pace at which genomic and proteomic data is being generated necessitates the development of tools and resources for managing data that allow integration of information from disparate sources. The Human Protein Reference Database (http://www.hprd.org) is a web-based resource based on open source technologies for protein information about several aspects of human proteins including protein-protein interactions, post-translational modifications, enzyme-substrate relationships and disease associations. This information was derived manually by a critical reading of the published literature by expert biologists and through bioinformatics analyses of the protein sequence. This database will assist in biomedical discoveries by serving as a resource of genomic and proteomic information and providing an integrated view of sequence, structure, function and protein networks in health and disease.",c107,British Machine Vision Conference,cp107,accepted,f1997,2012,2012-07-17
s2480,p2480,UBIRIS: A Noisy Iris Image Database,Abstract content goes here ...,c113,International Conference on Image Analysis and Processing,cp113,accepted,f1998,2002,2002-05-26
s2481,p2481,The Transporter Classification Database: recent advances,"The Transporter Classification Database (TCDB), freely accessible at http://www.tcdb.org, is a relational database containing sequence, structural, functional and evolutionary information about transport systems from a variety of living organisms, based on the International Union of Biochemistry and Molecular Biology-approved transporter classification (TC) system. It is a curated repository for factual information compiled largely from published references. It uses a functional/phylogenetic system of classification, and currently encompasses about 5000 representative transporters and putative transporters in more than 500 families. We here describe novel software designed to support and extend the usefulness of TCDB. Our recent efforts render it more user friendly, incorporate machine learning to input novel data in a semiautomatic fashion, and allow analyses that are more accurate and less time consuming. The availability of these tools has resulted in recognition of distant phylogenetic relationships and tremendous expansion of the information available to TCDB users.",c21,Grid Computing Environments,cp21,accepted,f1999,2005,2005-09-30
s2482,p2482,"PPDB, the Plant Proteomics Database at Cornell","The Plant Proteomics Database (PPDB; http://ppdb.tc.cornell.edu), launched in 2004, provides an integrated resource for experimentally identified proteins in Arabidopsis and maize (Zea mays). Internal BLAST alignments link maize and Arabidopsis information. Experimental identification is based on in-house mass spectrometry (MS) of cell type-specific proteomes (maize), or specific subcellular proteomes (e.g. chloroplasts, thylakoids, nucleoids) and total leaf proteome samples (maize and Arabidopsis). So far more than 5000 accessions both in maize and Arabidopsis have been identified. In addition, more than 80 published Arabidopsis proteome datasets from subcellular compartments or organs are stored in PPDB and linked to each locus. Using MS-derived information and literature, more than 1500 Arabidopsis proteins have a manually assigned subcellular location, with a strong emphasis on plastid proteins. Additional new features of PPDB include searchable posttranslational modifications and searchable experimental proteotypic peptides and spectral count information for each identified accession based on in-house experiments. Various search methods are provided to extract more than 40 data types for each accession and to extract accessions for different functional categories or curated subcellular localizations. Protein report pages for each accession provide comprehensive overviews, including predicted protein properties, with hyperlinks to the most relevant databases.",c55,Annual Workshop of the Psychology of Programming Interest Group,cp55,accepted,f2000,2014,2014-11-04
s2483,p2483,MyLifeBits: a personal database for everything,"Developing a platform for recording, storing, and accessing a personal lifetime archive.",c13,International Conference on Data Science and Advanced Analytics,cp13,accepted,f2001,2017,2017-03-20
s2485,p2485,Active Database Systems: Triggers and Rules For Advanced Database Processing,"From the Publisher: 
Active database systems enhance traditional database functionality with powerful rule-processing capabilities, providing a uniform and efficient mechanism for many database system applications. Among these applications are integrity constraints, views, authorization, statistics gathering, monitoring and alerting, knowledge-based systems, expert systems, and workflow management. This significant collection focuses on the most prominent research projects in active database systems. The project leaders for each prototype system provide detailed discussions of their projects and the relevance of their results to the future of active database systems. 
Features: 
A broad overview of current active database systems and how they can be extended and improved A comprehensive introduction to the core topics of the field, including its motivation and history Coverage of active database (trigger) capabilities in commercial products Discussion of forthcoming standards",c81,IEEE Annual Symposium on Foundations of Computer Science,cp81,accepted,f2002,2004,2004-12-19
s2486,p2486,The CMU Motion of Body (MoBo) Database,"In March 2001 we started to collect the CMU Motion of Body (MoBo) database. To date the database contains 25 individuals walking on a treadmill in the CMU 3D room. The subjects perform four different walk patterns: slow walk, fast walk, incline walk and walking with a ball. All subjects are captured using six high resolution color cameras distributed evenly around the treadmill. In this technical report we describe the capture setup, the collection procedure and the organization of the database.",c45,"IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies",cp45,accepted,f2003,2005,2005-10-29
s2487,p2487,The Columbia grasp database,"Collecting grasp data for learning and benchmarking purposes is very expensive. It would be helpful to have a standard database of graspable objects, along with a set of stable grasps for each object, but no such database exists. In this work we show how to automate the construction of a database consisting of several hands, thousands of objects, and hundreds of thousands of grasps. Using this database, we demonstrate a novel grasp planning algorithm that exploits geometric similarity between a 3D model and the objects in the database to synthesize form closure grasps. Our contributions are this algorithm, and the database itself, which we are releasing to the community as a tool for both grasp planning and benchmarking.",c114,IEEE International Conference on Robotics and Automation,cp114,accepted,f2004,2011,2011-08-04
s2488,p2488,The Pfam protein families database,"Pfam is a widely used database of protein families and domains. This article describes a set of major updates that we have implemented in the latest release (version 24.0). The most important change is that we now use HMMER3, the latest version of the popular profile hidden Markov model package. This software is ∼100 times faster than HMMER2 and is more sensitive due to the routine use of the forward algorithm. The move to HMMER3 has necessitated numerous changes to Pfam that are described in detail. Pfam release 24.0 contains 11 912 families, of which a large number have been significantly updated during the past two years. Pfam is available via servers in the UK (http://pfam.sanger.ac.uk/), the USA (http://pfam.janelia.org/) and Sweden (http://pfam.sbc.su.se/).",j102,Nucleic Acids Research,jv102,accepted,f2005,2002,2002-06-23
s2490,p2490,Database description with SDM: a semantic database model,"SDM is a high-level semantics-based database description and structuring formalism (database model) for databases. This database model is designed to capture more of the meaning of an application environment than is possible with contemporary database models. An SDM specification describes a database in terms of the kinds of entities that exist in the application environment, the classifications and groupings of those entities, and the structural interconnections among them. SDM provides a collection of high-level modeling primitives to capture the semantics of an application environment. By accommodating derived information in a database structural specification, SDM allows the same information to be viewed in several ways; this makes it possible to directly accommodate the variety of needs and processing requirements typically present in database applications. The design of the present SDM is based on our experience in using a preliminary version of it.
SDM is designed to enhance the effectiveness and usability of database systems. An SDM database description can serve as a formal specification and documentation tool for a database; it can provide a basis for supporting a variety of powerful user interface facilities, it can serve as a conceptual database model in the database design process; and, it can be used as the database model for a new kind of database management system.",c68,International Workshop on Software Engineering for Large-Scale Multi-Agent Systems,cp68,accepted,f2006,2016,2016-02-01
s2491,p2491,bioDBnet: the biological database network,"SUMMARY
bioDBnet is an online web resource that provides interconnected access to many types of biological databases. It has integrated many of the most commonly used biological databases and in its current state has 153 database identifiers (nodes) covering all aspects of biology including genes, proteins, pathways and other biological concepts. bioDBnet offers various ways to work with these databases including conversions, extensive database reports, custom navigation and has various tools to enhance the quality of the results. Importantly, the access to bioDBnet is updated regularly, providing access to the most recent releases of each individual database.


AVAILABILITY
http://biodbnet.abcc.ncifcrf.gov.",c63,IEEE International Software Metrics Symposium,cp63,accepted,f2007,2008,2008-10-06
s2492,p2492,Approximate String Joins in a Database (Almost) for Free,"String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data especially for more complex queries involving joins. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string joins directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string join capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on matching short substrings of length , called -grams, and taking into account both positions of individual matches and the total number of such matches. Our approach applies to both approximate full string matching and approximate substring matching, with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers. We demonstrate experimentally the benefits of our technique over the direct use of UDFs, using commercial database systems and real data. To study the I/O and CPU behavior of approximate string join algorithms with variations in edit distance and -gram length, we also describe detailed experiments based on a prototype implementation.",c112,Very Large Data Bases Conference,cp112,accepted,f2008,2018,2018-04-23
s2493,p2493,World Porifera Database,Abstract content goes here ...,c40,IEEE International Conference on Software Maintenance and Evolution,cp40,accepted,f2009,2013,2013-01-06
s2495,p2495,The RDP (Ribosomal Database Project) continues,"The Ribosomal Database Project (RDP-II), previously described by Maidak et al., continued during the past year to add new rRNA sequences to the aligned data and to improve the analysis commands. Release 7.1 (September 17, 1999) included more than 10 700 small subunit rRNA sequences. More than 850 type strain sequences were identified and added to the prokaryotic alignment, bringing the total number of type sequences to 3324 representing 2460 different species. Availability of an RDP-II mirror site in Japan is also near completion. RDP-II provides aligned and annotated rRNA sequences, derived phylogenetic trees and taxonomic hierarchies, and analysis services through its WWW server (http://rdp.cme.msu.edu/ ). Analysis services include rRNA probe checking, approx-i-mate phylogenetic placement of user sequences, screening user sequences for possible chimeric rRNA sequences, automated alignment, production of similarity matrices and services to plan and analyze terminal restriction fragment length polymorphism (T-RFLP) experiments.",c111,International Society for Music Information Retrieval Conference,cp111,accepted,f2010,2001,2001-10-14
s2496,p2496,DOOR: a database for prokaryotic operons,"We present a database DOOR (Database for prOkaryotic OpeRons) containing computationally predicted operons of all the sequenced prokaryotic genomes. All the operons in DOOR are predicted using our own prediction program, which was ranked to be the best among 14 operon prediction programs by a recent independent review. Currently, the DOOR database contains operons for 675 prokaryotic genomes, and supports a number of search capabilities to facilitate easy access and utilization of the information stored in it. Querying the database: the database provides a search capability for a user to find desired operons and associated information through multiple querying methods. Searching for similar operons: the database provides a search capability for a user to find operons that have similar composition and structure to a query operon. Prediction of cis-regulatory motifs: the database provides a capability for motif identification in the promoter regions of a user-specified group of possibly coregulated operons, using motif-finding tools. Operons for RNA genes: the database includes operons for RNA genes. OperonWiki: the database provides a wiki page (OperonWiki) to facilitate interactions between users and the developer of the database. We believe that DOOR provides a useful resource to many biologists working on bacteria and archaea, which can be accessed at http://csbl1.bmb.uga.edu/OperonDB.",c46,Brazilian Symposium on Software Engineering,cp46,accepted,f2011,2007,2007-03-17
s2497,p2497,On the Desirability of Acyclic Database Schemes,"A class of database schemes, called acychc, was recently introduced. It is shown that this class has a number of desirable properties. In particular, several desirable properties that have been studied by other researchers m very different terms are all shown to be eqmvalent to acydicity. In addition, several equivalent charactenzauons of the class m terms of graphs and hypergraphs are given, and a smaple algorithm for determining acychclty is presented. Also given are several eqmvalent characterizations of those sets M of multivalued dependencies such that M is the set of muRlvalued dependencies that are the consequences of a given join dependency. Several characterizations for a conflict-free (in the sense of Lien) set of muluvalued dependencies are provided.",c78,Neural Information Processing Systems,cp78,accepted,f2012,2012,2012-05-25
s2498,p2498,Lore: a database management system for semistructured data,"Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.",c25,International Conference on Contemporary Computing,cp25,accepted,f2013,2014,2014-08-11
s2499,p2499,Genetic association database,Abstract content goes here ...,j175,Nature reviews genetics,jv175,accepted,f2014,2007,2007-06-29
