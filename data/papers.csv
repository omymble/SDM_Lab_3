pId,paperUrlId,conferenceJournalUrl,conferenceJournal,pTitle,abstract
p0,0c2d3b28d48426b8b72f7214a7708ba8b4efa9d6,j0,Nature Biotechnology,"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2",
p1,fa5853fdef7d2f6bb68203d187ddacbbddc63a8b,None,None,High-Dimensional Probability: An Introduction with Applications in Data Science,"© 2018, Cambridge University Press Let us summarize our findings. A random projection of a set T in R n onto an m-dimensional subspace approximately preserves the geometry of T if m ⪆ d ( T ) . For..."
p2,4c6e31458b0b44c1e8bd6e58f7d7e0767f7fde44,j1,IEEE Transactions on Knowledge and Data Engineering,CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories,"CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics."
p3,7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6,c0,International Conference on Human Factors in Computing Systems,AutoDS: Towards Human-Centered Automation of Data Science,"Data science (DS) projects often follow a lifecycle that consists of laborious tasks for data scientists and domain experts (e.g., data exploration, model training, etc.). Only till recently, machine learning(ML) researchers have developed promising automation techniques to aid data workers in these tasks. This paper introduces AutoDS, an automated machine learning (AutoML) system that aims to leverage the latest ML automation techniques to support data science projects. Data workers only need to upload their dataset, then the system can automatically suggest ML configurations, preprocess data, select algorithm, and train the model. These suggestions are presented to the user via a web-based graphical user interface and a notebook-based programming user interface. Our goal is to offer a systematic investigation of user interaction and perceptions of using an AutoDS system in solving a data science task. We studied AutoDS with 30 professional data scientists, where one group used AutoDS, and the other did not, to complete a data science project. As expected, AutoDS improves productivity; Yet surprisingly, we find that the models produced by the AutoDS group have higher quality and less errors, but lower human confidence scores. We reflect on the findings by presenting design implications for incorporating automation techniques into human work in the data science lifecycle."
p4,3569c79cf90b203325dd7b8f6c30bacc60f5d30e,j2,SN Computer Science,"Data Science and Analytics: An Overview from Data-Driven Smart Computing, Decision-Making and Applications Perspective",
p5,d737e2d326519ab3ba5da17441e073ba1c7a3ef5,None,None,Computational Optimal Transport: With Applications to Data Science,"The goal of Optimal Transport (OT) is to define geometric tools that are useful to compare probability distributions. Their use dates back to 1781. Recent years have witnessed a new revolution in the spread of OT, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This monograph reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications. Computational Optimal Transport presents an overview of the main theoretical insights that support the practical effectiveness of OT before explaining how to turn these insights into fast computational schemes. Written for readers at all levels, the authors provide descriptions of foundational theory at two-levels. Generally accessible to all readers, more advanced readers can read the specially identified more general mathematical expositions of optimal transport tailored for discrete measures. Furthermore, several chapters deal with the interplay between continuous and discrete measures, and are thus targeting a more mathematically-inclined audience. This monograph will be a valuable reference for researchers and students wishing to get a thorough understanding of Computational Optimal Transport, a mathematical gem at the interface of probability, analysis and optimization."
p6,4ce46acc4a40038b02014c1bbce3e451586ebe05,j3,"Shanlax International Journal of Arts, Science and Humanities",Data Science,"Data science has become the most demanding task of the 21st century. All companies are looking for candidates with knowledge of data science. This topic provides an overview of data science. Includes data science duties, data science tools, data science components, applications and more."
p7,0b1d429110757c1a9deb27fb8568740182dc1679,j4,Genome Biology,Eleven grand challenges in single-cell data science,
p8,040d94340f742f522ceddb31f31fb9c9b4c23cfd,None,None,Computing competencies for undergraduate data science curricula,
p9,0b5b33b7ea1dc12f3e9252ac1852170a6a6775bf,j5,Nature Methods,Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl,
p10,dd45dc3767230b70197420e1523dc0f1d7930f80,None,None,Foundations of Data Science,"Computer science as an academic discipline began in the 1960’s. Emphasis was on programming languages, compilers, operating systems, and the mathematical theory that supported these areas. Courses in theoretical computer science covered finite automata, regular expressions, context-free languages, and computability. In the 1970’s, the study of algorithms was added as an important component of theory. The emphasis was on making computers useful. Today, a fundamental change is taking place and the focus is more on applications. There are many reasons for this change. The merging of computing and communications has played an important role. The enhanced ability to observe, collect, and store data in the natural sciences, in commerce, and in other fields calls for a change in our understanding of data and how to handle it in the modern setting. The emergence of the web and social networks as central aspects of daily life presents both opportunities and challenges for theory."
p11,d1c67a03ec85bdf6c4a4437120e89a349656580e,None,None,Data Science,
p12,c13147ef0b86d5ec833c272840f8f3bdacf96e7f,j6,International Journal of Data Science and Analysis,Data science: a game changer for science and innovation,
p13,1ba044d3d501dddd94b479aa9dbe55a93bfa9d5f,None,None,"QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science","We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science."
p14,ede0a8039a561905f40777ec2ae66c2010e3f2bc,j7,Journal of Big Data,Cybersecurity data science: an overview from machine learning perspective,
p15,0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24,j8,IEEE Transactions on Artificial Intelligence,Leveraging Data Science to Combat COVID-19: A Comprehensive Review,"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets."
p16,ab8ba0f2d290a8e56eb61e10027d0b2e57d2d544,None,None,"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions."
p17,8bba999de25bfb288b3f7f88e1d907aab02638b6,j9,Chemical Reviews,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years."
p18,72d3ddf1f7210d7e70144bbc09f770ec411fe909,None,None,"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward."
p19,2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,None,None,Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework."
p20,55bdaa9d27ed595e2ccf34b3a7847020cc9c946c,c1,International Conference on Software Engineering,Performing systematic literature reviews in software engineering,"Context: Making best use of the growing number of empirical studies in Software Engineering, for making decisions and formulating research questions, requires the ability to construct an objective summary of available research evidence. Adopting a systematic approach to assessing and aggregating the outcomes from a set of empirical studies is also particularly important in Software Engineering, given that such studies may employ very different experimental forms and be undertaken in very different experimental contexts.Objectives: To provide an introduction to the role, form and processes involved in performing Systematic Literature Reviews. After the tutorial, participants should be able to read and use such reviews, and have gained the knowledge needed to conduct systematic reviews of their own.Method: We will use a blend of information presentation (including some experiences of the problems that can arise in the Software Engineering domain), and also of interactive working, using review material prepared in advance."
p21,27e57cc2f22c1921d2a1c3954d5062e3fe391553,j10,Empirical Software Engineering,Guidelines for conducting and reporting case study research in software engineering,
p22,72910077a29caf411dbb03148997c72b47e65ab0,j11,IEEE Transactions on Software Engineering,Software Engineering Economics,"This paper summarizes the current state of the art and recent trends in software engineering economics. It provides an overview of economic analysis techniques and their applicability to software engineering and management. It surveys the field of software cost estimation, including the major estimation techniques available, the state of the art in algorithmic cost models, and the outstanding research issues in software cost estimation."
p23,d0bc1501ae6f54dd16534e651d90d2aeeeb1cfc1,c2,IEEE Aerospace Conference,Software engineering: What is it?,"In spite of many years of work by a multitude of organizations, a clear and simple standard for software engineering and management requirements and a method to assess their applicability to projects of various types and sizes remains elusive. From IEEE to CMMI to NASA's NPR 7150.2, there is no shortage of sources of information providing various types of requirements and standards for software engineering. Even a book on software project management for “dummies” approaches 400 pages. Wading through this information can dizzy the mind of even the most experienced software engineer; the newbie just trying to “do the right thing” will probably give up, open a text editor and start coding. This lack of clarity and simplicity perhaps goes a long way towards explaining why, in spite of this large body of work, there remains such an incredible variability in the knowledge and application of software engineering discipline not only from one organization to the next, but between groups within the same organization, or even between individual developers in the same group! Surely at least the basics of what should be done and why those things should be done can be conveyed in less than a novel-sized volume. There must be some timeless principles that cut across structured and object-oriented techniques, waterfall and agile methods, and CMMI and NASA standards. To properly interpret software engineering requirements and approaches and successfully (and selectively) apply them, one must first understand them at a fundamental level and how they can benefit the project. This paper will make an admittedly bold and brash attempt to boil it all down into something anyone can understand, hopefully resulting in a brief reference — a type of lens through which existing standards can be more practically viewed."
p24,81dbfc1bc890368979399874e47e0529ddceaece,None,None,Software Engineering: A Practitioner's Approach,
p25,f70b2f20be241f445a61f33c4b8e76e554760340,None,None,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations."
p26,0961e2650b3a62a1d198a046bef5f0700ab8c08f,c3,International Conference on Evaluation & Assessment in Software Engineering,Guidelines for snowballing in systematic literature studies and a replication in software engineering,"Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably.
 Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review.
 Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches.
 Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review.
 Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches."
p27,583da0826238d5d5e85d5c03a7334520a91733e3,None,None,Software Engineering,"Software engineering is as much about teamwork as it is about technology. This introductory textbook covers both. For courses featuring a team project, it offers tips and templates for aligning classroom concepts with the needs of the students' projects. Students will learn how software is developed in industry by adopting agile methods, discovering requirements, designing modular systems, selecting effective tests, and using metrics to track progress. The book also covers the 'why' behind the 'how-to', to prepare students for advances in industry practices. The chapters explore ways of eliciting what users really want, how clean architecture divides and conquers the inherent complexity of software systems, how test coverage is essential for detecting the inevitable defects in code, and much more. Ravi Sethi provides real-life case studies and examples to demonstrate practical applications of the concepts. Online resources include sample project materials for students, and lecture slides for instructors."
p28,0d091c5aaf3d2df349692b13758045368d1b9a37,j12,Informatik-Spektrum,Software Engineering,
p29,849a6be5adf1b9a2b6e59ba0290bca06692c0efd,j10,Empirical Software Engineering,Sampling in software engineering research: a critical review and guidelines,
p30,75f3726a39b563c9890fb8ed7dd393da15ad6594,c4,International Conference on Software Technology: Methods and Tools,Object-oriented software engineering - a use case driven approach,Part 1. Introduction 1. System development as an industrial process Introduction A useful analogy System development characteristics Summary 2. The system life cycle Introduction System development as a process of change System development and reuse System development and methodology Objectory Summary 3. What is object-orientation? Introduction Object Class andinstance Polymorphism Inheritance Summary 4. Object-oriented system development Introduction Function/data methods Object-oriented analysis Object-oriented construction Object-oriented testing Summary 5. Object-oriented programming Introduction Objects Classes and instances Inheritance Polymorphism An example Summary Part II. Concepts 6. Architecture Introduction System development is model building Model architecture Requirements model Analysis model The design model The implementation model Test model Summary 7. Analysis Introduction The requirements model The analysis model Summary 8. Construction Introduction The design model Block design Working with construction Summary 9. Real-time specialization Introduction Classification of real-time systems Fundamental issues Analysis Construction Testing and verification Summary 10. Database Specialization Introduction Relational DBMS Object DBMS Discussion Summary 11. Components Introduction What is a component? Use of components Component management Summary 12. Testing Introduction On testing Unit testing Integration testing System testing The testing process Summary Part III. Applications 13. Case study: warehouse management system Introduction to the examples ACME Warehouse Management Inc. The requirements model The analysis model Construction 14. Case study: telecom Introduction Telecommunication switching systems The requirements model The analysis model The design model The implementation model 15. Managing object-oriented software engineering Introduction Project selection and preparation Project development organization Project organization and management Project staffing Software quality assurance Software metrics Summary 16. Other object-oriented methods Introduction A summary of object-oriented methods Object-Oriented Analysis (OOAD/Coad-Yourdon) Object-Oriented Design (OOD/Booch) Hierarchical Object-Oriented Design (HOOD) Object Modeling Technique (OMT) Responsibility-Driven Design Summary Appendix A On the development of Objectory Introduction Objectory as an activity From idea to reality References Index
p31,130862d54894966552cb85d3ee6f739f885d4989,None,None,Model-Driven Software Engineering in Practice,"This book discusses how model-based approaches can improve the daily practice of software professionals. This is known as Model-Driven Software Engineering (MDSE) or, simply, Model-Driven Engineering (MDE). MDSE practices have proved to increase efficiency and effectiveness in software development, as demonstrated by various quantitative and qualitative studies. MDSE adoption in the software industry is foreseen to grow exponentially in the near future, e.g., due to the convergence of software development and business analysis. The aim of this book is to provide you with an agile and flexible tool to introduce you to the MDSE world, thus allowing you to quickly understand its basic principles and techniques and to choose the right set of MDSE instruments for your needs so that you can start to benefit from MDSE right away. The book is organized into two main parts. The first part discusses the foundations of MDSE in terms of basic concepts (i.e., models and transformations), driving principles, application scenarios and current standards, like the well-known MDA initiative proposed by OMG (Object Management Group) as well as the practices on how to integrate MDSE in existing development processes. The second part deals with the technical aspects of MDSE, spanning from the basics on when and how to build a domain-specific modeling language, to the description of Model-to-Text and Model-to-Model transformations, and the tools that support the management of MDSE projects. The book is targeted to a diverse set of readers, spanning: professionals, CTOs, CIOs, and team managers that need to have a bird's eye vision on the matter, so as to take the appropriate decisions when it comes to choosing the best development techniques for their company or team; software analysts, developers, or designers that expect to use MDSE for improving everyday work productivity, either by applying the basic modeling techniques and notations or by defining new domain-specific modeling languages and applying end-to-end MDSE practices in the software factory; and academic teachers and students to address undergrad and postgrad courses on MDSE. In addition to the contents of the book, more resources are provided on the book's website http://www.mdse-book.com/, including the examples presented in the book. Table of Contents: Introduction / MDSE Principles / MDSE Use Cases / Model-Driven Architecture (MDA) / Integration of MDSE in your Development Process / Modeling Languages at a Glance / Developing your Own Modeling Language / Model-to-Model Transformations / Model-to-Text Transformations / Managing Models / Summary"
p32,2bd576ce574df33c834b6032962cd5ae0be5299f,j13,Information and Software Technology,Guidelines for conducting systematic mapping studies in software engineering: An update,
p33,947b29eb3cde8ccb8df9342bb2384ec480ea3964,None,None,Experimentation in software engineering: an introduction,
p34,e28bdc373de80d7ec0e64631a89e64fbdcdae230,c3,International Conference on Evaluation & Assessment in Software Engineering,Systematic Mapping Studies in Software Engineering,"BACKGROUND: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. 
 
OBJECTIVE: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. 
 
METHOD: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. 
 
RESULTS: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. 
 
CONCLUSIONS: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis)."
p35,a963d05b9d4acd347ad528e7d098eb53d8f555a2,j13,Information and Software Technology,Systematic literature reviews in software engineering - A systematic literature review,
p36,21c6beb2a6df81f424e3d1283fbb9cc3157a3115,c5,International Conference on Agile Software Development,A Taxonomy of Software Engineering Challenges for Machine Learning Systems: An Empirical Investigation,
p37,967f4eb786aa143b7eb09f00d9ba8ddfe44e039f,c1,International Conference on Software Engineering,Sentiment Analysis for Software Engineering: How Far Can We Go?,"Sentiment analysis has been applied to various software engineering (SE) tasks, such as evaluating app reviews or analyzing developers' emotions in commit messages. Studies indicate that sentiment analysis tools provide unreliable results when used out-of-the-box, since they are not designed to process SE datasets. The silver bullet for a successful application of sentiment analysis tools to SE datasets might be their customization to the specific usage context. We describe our experience in building a software library recommender exploiting crowdsourced opinions mined from Stack Overflow (e.g., what is the sentiment of developers about the usability of a library). To reach our goal, we retrained—on a set of 40k manually labeled sentences/words extracted from Stack Overflow—a state-of-the-art sentiment analysis tool exploiting deep learning. Despite such an effort- and time-consuming training process, the results were negative. We changed our focus and performed a thorough investigation of the accuracy of these tools on a variety of SE datasets. Our results should warn the research community about the strong limitations of current sentiment analysis tools."
p38,247e2ee1a84e25cdc2d0d68811e2ee05ca0bc6a9,j10,Empirical Software Engineering,Software engineering in start-up companies: An analysis of 88 experience reports,
p39,f463018624b6f4b8dd576732b6cce36e31bac978,None,None,Software Engineering of Self-adaptive Systems,
p40,57e7a7323f58a35f5e2cc33bf17d4ac9cdcafdd4,j14,Nature Protocols,Systematic and integrative analysis of large gene lists using DAVID bioinformatics resources,
p41,fa60b6806050255a77699bd0f9f5d824884c5162,j15,Nucleic Acids Research,Bioinformatics enrichment tools: paths toward the comprehensive functional analysis of large gene lists,"Functional analysis of large gene lists, derived in most cases from emerging high-throughput genomic, proteomic and bioinformatics scanning approaches, is still a challenging and daunting task. The gene-annotation enrichment analysis is a promising high-throughput strategy that increases the likelihood for investigators to identify biological processes most pertinent to their study. Approximately 68 bioinformatics enrichment tools that are currently available in the community are collected in this survey. Tools are uniquely categorized into three major classes, according to their underlying enrichment algorithms. The comprehensive collections, unique tool classifications and associated questions/issues will provide a more comprehensive and up-to-date view regarding the advantages, pitfalls and recent trends in a simpler tool-class level rather than by a tool-by-tool approach. Thus, the survey will help tool designers/developers and experienced end users understand the underlying algorithms and pertinent details of particular tool categories/tools, enabling them to make the best choices for their particular research interests."
p42,fd495d6cf7c3169bc58550fdf32be6e16e2800f8,j4,Genome Biology,Bioconductor: open software development for computational biology and bioinformatics,
p43,90485e0ce54c1ad12a2d01362a007ab107d71063,None,None,Biopython: freely available Python tools for computational molecular biology and bioinformatics,"Summary: The Biopython project is a mature open source international collaboration of volunteer developers, providing Python libraries for a wide range of bioinformatics problems. Biopython includes modules for reading and writing different sequence file formats and multiple sequence alignments, dealing with 3D macro molecular structures, interacting with common tools such as BLAST, ClustalW and EMBOSS, accessing key online databases, as well as providing numerical methods for statistical learning. Availability: Biopython is freely available, with documentation and source code at www.biopython.org under the Biopython license. Contact: All queries should be directed to the Biopython mailing lists, see www.biopython.org/wiki/_Mailing_listspeter.cock@scri.ac.uk."
p44,8002fefd7bcc66c23a8d49aa9a7ae8d4a9885ad3,None,None,"Expasy, the Swiss Bioinformatics Resource Portal, as designed by its users","Abstract The SIB Swiss Institute of Bioinformatics (https://www.sib.swiss) creates, maintains and disseminates a portfolio of reliable and state-of-the-art bioinformatics services and resources for the storage, analysis and interpretation of biological data. Through Expasy (https://www.expasy.org), the Swiss Bioinformatics Resource Portal, the scientific community worldwide, freely accesses more than 160 SIB resources supporting a wide range of life science and biomedical research areas. In 2020, Expasy was redesigned through a user-centric approach, known as User-Centred Design (UCD), whose aim is to create user interfaces that are easy-to-use, efficient and targeting the intended community. This approach, widely used in other fields such as marketing, e-commerce, and design of mobile applications, is still scarcely explored in bioinformatics. In total, around 50 people were actively involved, including internal stakeholders and end-users. In addition to an optimised interface that meets users' needs and expectations, the new version of Expasy provides an up-to-date and accurate description of high-quality resources based on a standardised ontology, allowing to connect functionally-related resources."
p45,a41d8c4eddf4054ef080c7edec21b39c492892ee,j0,Nature Biotechnology,"Nanopore sequencing technology, bioinformatics and applications",
p46,34d405eaecab40a932108a7ff97e92fb8fd1ae4e,None,None,A review of feature selection techniques in bioinformatics,"Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications."
p47,1ff4bd599b950218f0517fb76ee49ad0599e1c53,j16,Journal of Molecular Biology,A Completely Reimplemented MPI Bioinformatics Toolkit with a New HHpred Server at its Core.,
p48,7d7735582cfa14efb00d967e9af4d725579e8746,None,None,The PATRIC Bioinformatics Resource Center: expanding data and analysis capabilities,"The PathoSystems Resource Integration Center (PATRIC) is the bacterial Bioinformatics Resource Center funded by the National Institute of Allergy and Infectious Diseases (https://www.patricbrc.org). PATRIC supports bioinformatic analyses of all bacteria with a special emphasis on pathogens, offering a rich comparative analysis environment that provides users with access to over 250 000 uniformly annotated and publicly available genomes with curated metadata. PATRIC offers web-based visualization and comparative analysis tools, a private workspace in which users can analyze their own data in the context of the public collections, services that streamline complex bioinformatic workflows and command-line tools for bulk data analysis. Over the past several years, as genomic and other omics-related experiments have become more cost-effective and widespread, we have observed considerable growth in the usage of and demand for easy-to-use, publicly available bioinformatic tools and services. Here we report the recent updates to the PATRIC resource, including new web-based comparative analysis tools, eight new services and the release of a command-line interface to access, query and analyze data."
p49,70b4af707b1bf5eebacaadcf994335feb3d7a43a,j17,bioRxiv,The digestive system is a potential route of 2019-nCov infection: a bioinformatics analysis based on single-cell transcriptomes,"Since December 2019, a newly identified coronavirus (2019 novel coronavirus, 2019-nCov) is causing outbreak of pneumonia in one of largest cities, Wuhan, in Hubei province of China and has draw significant public health attention. The same as severe acute respiratory syndrome coronavirus (SARS-CoV), 2019-nCov enters into host cells via cell receptor angiotensin converting enzyme II (ACE2). In order to dissect the ACE2-expressing cell composition and proportion and explore a potential route of the 2019-nCov infection in digestive system infection, 4 datasets with single-cell transcriptomes of lung, esophagus, gastric, ileum and colon were analyzed. The data showed that ACE2 was not only highly expressed in the lung AT2 cells, esophagus upper and stratified epithelial cells but also in absorptive enterocytes from ileum and colon. These results indicated along with respiratory systems, digestive system is a potential routes for 2019-nCov infection. In conclusion, this study has provided the bioinformatics evidence of the potential route for infection of 2019-nCov in digestive system along with respiratory tract and may have significant impact for our healthy policy setting regards to prevention of 2019-nCoV infection."
p50,d6425d11904920cf6fafe895e6073e2131978e60,c6,International Conference on Parallel Processing,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),
p51,17190dc2a51b4f0edd05753a4cea2a2540059933,j0,Nature Biotechnology,The nf-core framework for community-curated bioinformatics pipelines,
p52,766d3c1f67728737a255bd88e272f2bacb92d6e9,None,None,Protein Sequence Analysis Using the MPI Bioinformatics Toolkit,"The MPI Bioinformatics Toolkit (https://toolkit.tuebingen.mpg.de) provides interactive access to a wide range of the best‐performing bioinformatics tools and databases, including the state‐of‐the‐art protein sequence comparison methods HHblits and HHpred. The Toolkit currently includes 35 external and in‐house tools, covering functionalities such as sequence similarity searching, prediction of sequence features, and sequence classification. Due to this breadth of functionality, the tight interconnection of its constituent tools, and its ease of use, the Toolkit has become an important resource for biomedical research and for teaching protein sequence analysis to students in the life sciences. In this article, we provide detailed information on utilizing the three most widely accessed tools within the Toolkit: HHpred for the detection of homologs, HHpred in conjunction with MODELLER for structure prediction and homology modeling, and CLANS for the visualization of relationships in large sequence datasets. © 2020 The Authors."
p53,f22e039275da9512fd59109165b99abf7c6910f9,None,None,Snakemake - a scalable bioinformatics workflow engine,Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.
p54,bc54798db4962ec9ba6a5e87be3dce156454cfef,j18,Journal of Open Source Software,Augur: a bioinformatics toolkit for phylogenetic analyses of human pathogens,"Summary and statement of need The analysis of human pathogens requires a diverse collection of bioinformatics tools. These tools include standard genomic and phylogenetic software and custom software developed to handle the relatively numerous and short genomes of viruses and bacteria. Researchers increasingly depend on the outputs of these tools to infer transmission dynamics of human diseases and make actionable recommendations to public health officials (Black et al., 2020; Gardy et al., 2015). In order to enable real-time analyses of pathogen evolution, bioinformatics tools must scale rapidly with the number of samples and be flexible enough to adapt to a variety of questions and organisms. To meet these needs, we developed Augur, a bioinformatics toolkit designed for phylogenetic analyses of human pathogens."
p55,0ff76dd78e47f4534ee148b644f1f2707bc70df5,None,None,"Improvements to PATRIC, the all-bacterial Bioinformatics Database and Analysis Resource Center","The Pathosystems Resource Integration Center (PATRIC) is the bacterial Bioinformatics Resource Center (https://www.patricbrc.org). Recent changes to PATRIC include a redesign of the web interface and some new services that provide users with a platform that takes them from raw reads to an integrated analysis experience. The redesigned interface allows researchers direct access to tools and data, and the emphasis has changed to user-created genome-groups, with detailed summaries and views of the data that researchers have selected. Perhaps the biggest change has been the enhanced capability for researchers to analyze their private data and compare it to the available public data. Researchers can assemble their raw sequence reads and annotate the contigs using RASTtk. PATRIC also provides services for RNA-Seq, variation, model reconstruction and differential expression analysis, all delivered through an updated private workspace. Private data can be compared by ‘virtual integration’ to any of PATRIC's public data. The number of genomes available for comparison in PATRIC has expanded to over 80 000, with a special emphasis on genomes with antimicrobial resistance data. PATRIC uses this data to improve both subsystem annotation and k-mer classification, and tags new genomes as having signatures that indicate susceptibility or resistance to specific antibiotics."
p56,c67f7d629c4500bd47056b03bdfaf2df2f3c9346,j15,Nucleic Acids Research,Human Splicing Finder: an online bioinformatics tool to predict splicing signals,"Thousands of mutations are identified yearly. Although many directly affect protein expression, an increasing proportion of mutations is now believed to influence mRNA splicing. They mostly affect existing splice sites, but synonymous, non-synonymous or nonsense mutations can also create or disrupt splice sites or auxiliary cis-splicing sequences. To facilitate the analysis of the different mutations, we designed Human Splicing Finder (HSF), a tool to predict the effects of mutations on splicing signals or to identify splicing motifs in any human sequence. It contains all available matrices for auxiliary sequence prediction as well as new ones for binding sites of the 9G8 and Tra2-β Serine-Arginine proteins and the hnRNP A1 ribonucleoprotein. We also developed new Position Weight Matrices to assess the strength of 5′ and 3′ splice sites and branch points. We evaluated HSF efficiency using a set of 83 intronic and 35 exonic mutations known to result in splicing defects. We showed that the mutation effect was correctly predicted in almost all cases. HSF could thus represent a valuable resource for research, diagnostic and therapeutic (e.g. therapeutic exon skipping) purposes as well as for global studies, such as the GEN2PHEN European Project or the Human Variome Project."
p57,13226c692dd6908c64555fb095c4ee968a539a83,None,None,Trends in the development of miRNA bioinformatics tools,"Abstract MicroRNAs (miRNAs) are small noncoding RNAs that regulate gene expression via recognition of cognate sequences and interference of transcriptional, translational or epigenetic processes. Bioinformatics tools developed for miRNA study include those for miRNA prediction and discovery, structure, analysis and target prediction. We manually curated 95 review papers and ∼1000 miRNA bioinformatics tools published since 2003. We classified and ranked them based on citation number or PageRank score, and then performed network analysis and text mining (TM) to study the miRNA tools development trends. Five key trends were observed: (1) miRNA identification and target prediction have been hot spots in the past decade; (2) manual curation and TM are the main methods for collecting miRNA knowledge from literature; (3) most early tools are well maintained and widely used; (4) classic machine learning methods retain their utility; however, novel ones have begun to emerge; (5) disease-associated miRNA tools are emerging. Our analysis yields significant insight into the past development and future directions of miRNA tools."
p58,cc384cff27d8a609f89f9e915c26bf31c39749a1,None,None,Deep learning in bioinformatics,"In the era of big data, transformation of biomedical big data into valuable knowledge has been one of the most important challenges in bioinformatics. Deep learning has advanced rapidly since the early 2000s and now demonstrates state-of-the-art performance in various fields. Accordingly, application of deep learning in bioinformatics to gain insight from data has been emphasized in both academia and industry. Here, we review deep learning in bioinformatics, presenting examples of current research. To provide a useful and comprehensive perspective, we categorize research both by the bioinformatics domain (i.e. omics, biomedical imaging, biomedical signal processing) and deep learning architecture (i.e. deep neural networks, convolutional neural networks, recurrent neural networks, emergent architectures) and present brief descriptions of each study. Additionally, we discuss theoretical and practical issues of deep learning in bioinformatics and suggest future research directions. We believe that this review will provide valuable insights and serve as a starting point for researchers to apply deep learning approaches in their bioinformatics studies."
p59,053e5c1f175cf385fb8ab551e68446e24b3475a5,c7,International Conference on Intelligent Robotics and Applications,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),
p60,9de3f1ed6352262b24c63c3dad62e15a3ef5a653,None,None,Spectral graph theory,"With every graph (or digraph) one can associate several different matrices. We have already seen the vertex-edge incidence matrix, the Laplacian and the adjacency matrix of a graph. Here we shall concentrate mainly on the adjacency matrix of (undirected) graphs, and also discuss briefly the Laplacian. We shall show that spectral properies (the eigenvalues and eigenvectors) of these matrices provide useful information about the structure of the graph. It turns out that for regular graphs, the information one can deduce from one matrix representation (e.g., the adjacency matrix) is similar to the information one can deduce from other representations (such as the Laplacian). We remark that for nonregular graphs, this is not the case, and the choice of matrix representation may make a significant difference. We shall not elaborate on this issue further, as our main concern here will be either with regular or nearly regular graph. The adjacency matrix of a connected undirected graph is nonnegative, symmetric and irreducible (namely, it cannot be decomposed into two diagonal blocks and two off-diagonal blocks, one of which is all-0). As such, standard results n linear algebra, including the Perron-Frobenius theorem, imply that:"
p61,1ee0abcb8f0afd74d602255d529d7c2a036a8f02,None,None,Graph theory,
p62,b07c157e7d40e06a4f2d486b16d5180d8b24acb9,None,None,Algebraic Graph Theory,
p63,6bc77c4dc6075ee81c05f0f5f43e44b2a34a5876,None,None,Graph Theory with Applications,"When I first entered the world of Mathematics, I became aware of a strange and little-regarded sect of ""Graph Theorists"", inhabiting a shadowy borderland known to the rest of the community as the ""slums of Topology"". What changes there have been in a few short years! That shadowy borderland has become a thriving metropolis. International conferences on Graph Theory occur with almost embarrassing frequency. Journals on Graph Theory abound: I once counted the Editorial Offices of three of them in one of the mathematical departments of one of the Universities of one of the smaller cities of Canada. Any connection with Topology is likely to be firmly repudiated as soon as noted. I became aware of the burgeoning of Graph Theory when I studied the 1940 paper of Brooks, Smith, Stone and Tutte in the Duke Mathematical Journal, ostensibly on squared rectangles. They wrote of trees and Kirchhoffs Laws, of 3-connection and planarity, of duality and symmetry, of determinantal identities and coprime integers, ~ all in the Quest of the Perfect Square. I invariably recommend that paper to my students. ""Go to it"", I say, ""you will"
p64,54006d6fb211a090c98fee9d8103479b022c0db2,c8,Networks,Introduction to graph theory,
p65,c2f21a6b917286c7e904e0f168b53bbaa2bda4ba,j19,Frontiers in Neuroscience,Application of Graph Theory for Identifying Connectivity Patterns in Human Brain Networks: A Systematic Review,"Background: Analysis of the human connectome using functional magnetic resonance imaging (fMRI) started in the mid-1990s and attracted increasing attention in attempts to discover the neural underpinnings of human cognition and neurological disorders. In general, brain connectivity patterns from fMRI data are classified as statistical dependencies (functional connectivity) or causal interactions (effective connectivity) among various neural units. Computational methods, especially graph theory-based methods, have recently played a significant role in understanding brain connectivity architecture. Objectives: Thanks to the emergence of graph theoretical analysis, the main purpose of the current paper is to systematically review how brain properties can emerge through the interactions of distinct neuronal units in various cognitive and neurological applications using fMRI. Moreover, this article provides an overview of the existing functional and effective connectivity methods used to construct the brain network, along with their advantages and pitfalls. Methods: In this systematic review, the databases Science Direct, Scopus, arXiv, Google Scholar, IEEE Xplore, PsycINFO, PubMed, and SpringerLink are employed for exploring the evolution of computational methods in human brain connectivity from 1990 to the present, focusing on graph theory. The Cochrane Collaboration's tool was used to assess the risk of bias in individual studies. Results: Our results show that graph theory and its implications in cognitive neuroscience have attracted the attention of researchers since 2009 (as the Human Connectome Project launched), because of their prominent capability in characterizing the behavior of complex brain systems. Although graph theoretical approach can be generally applied to either functional or effective connectivity patterns during rest or task performance, to date, most articles have focused on the resting-state functional connectivity. Conclusions: This review provides an insight into how to utilize graph theoretical measures to make neurobiological inferences regarding the mechanisms underlying human cognition and behavior as well as different brain disorders."
p66,95d6ff6279fa0f92df6fae0e6bd4c259acfc8f09,None,None,Spectral Graph Theory,"Eigenvalues and the Laplacian of a graph Isoperimetric problems Diameters and eigenvalues Paths, flows, and routing Eigenvalues and quasi-randomness Expanders and explicit constructions Eigenvalues of symmetrical graphs Eigenvalues of subgraphs with boundary conditions Harnack inequalities Heat kernels Sobolev inequalities Advanced techniques for random walks on graphs Bibliography Index."
p67,1034712495a5d8f54d489674c3eac69250bf107e,None,None,Graph Theory,"Gaph Teory Fourth Edition Th is standard textbook of modern graph theory, now in its fourth edition, combines the authority of a classic with the engaging freshness of style that is the hallmark of active mathematics. It covers the core material of the subject with concise yet reliably complete proofs, while offering glimpses of more advanced methods in each fi eld by one or two deeper results, again with proofs given in full detail."
p68,7624a21a8d962b4e0aaa90d2c9b7b70f98dd008f,None,None,Introduction to Graph Theory,"1. Fundamental Concepts. What Is a Graph? Paths, Cycles, and Trails. Vertex Degrees and Counting. Directed Graphs. 2. Trees and Distance. Basic Properties. Spanning Trees and Enumeration. Optimization and Trees. 3. Matchings and Factors. Matchings and Covers. Algorithms and Applications. Matchings in General Graphs. 4. Connectivity and Paths. Cuts and Connectivity. k-connected Graphs. Network Flow Problems. 5. Coloring of Graphs. Vertex Colorings and Upper Bounds. Structure of k-chromatic Graphs. Enumerative Aspects. 6. Planar Graphs. Embeddings and Euler's Formula. Characterization of Planar Graphs. Parameters of Planarity. 7. Edges and Cycles. Line Graphs and Edge-Coloring. Hamiltonian Cycles. Planarity, Coloring, and Cycles. 8. Additional Topics (Optional). Perfect Graphs. Matroids. Ramsey Theory. More Extremal Problems. Random Graphs. Eigenvalues of Graphs. Appendix A: Mathematical Background. Appendix B: Optimization and Complexity. Appendix C: Hints for Selected Exercises. Appendix D: Glossary of Terms. Appendix E: Supplemental Reading. Appendix F: References. Indices."
p69,2e75dd6fb569ee917b6571d89afadd980af499c0,j20,Frontiers in Bioengineering and Biotechnology,A Guide to Conquer the Biological Network Era Using Graph Theory,"Networks are one of the most common ways to represent biological systems as complex sets of binary interactions or relations between different bioentities. In this article, we discuss the basic graph theory concepts and the various graph types, as well as the available data structures for storing and reading graphs. In addition, we describe several network properties and we highlight some of the widely used network topological features. We briefly mention the network patterns, motifs and models, and we further comment on the types of biological and biomedical networks along with their corresponding computer- and human-readable file formats. Finally, we discuss a variety of algorithms and metrics for network analyses regarding graph drawing, clustering, visualization, link prediction, perturbation, and network alignment as well as the current state-of-the-art tools. We expect this review to reach a very broad spectrum of readers varying from experts to beginners while encouraging them to enhance the field further."
p70,8851f2f404b3322170c3c660fb3cf30a2911d6ec,None,None,Graph Theory,
p71,88648d2ae7c18ee6b79d1d2754cf92724787f8ce,j21,Dialogues in Clinical Neuroscience,Graph theory methods: applications in brain networks,"Network neuroscience is a thriving and rapidly expanding field. Empirical data on brain networks, from molecular to behavioral scales, are ever increasing in size and complexity. These developments lead to a strong demand for appropriate tools and methods that model and analyze brain network data, such as those provided by graph theory. This brief review surveys some of the most commonly used and neurobiologically insightful graph measures and techniques. Among these, the detection of network communities or modules, and the identification of central network elements that facilitate communication and signal transfer, are particularly salient. A number of emerging trends are the growing use of generative models, dynamic (time-varying) and multilayer networks, as well as the application of algebraic topology. Overall, graph theory methods are centrally important to understanding the architecture, development, and evolution of brain networks."
p72,72744fb5963b168b4590e5eaa148c8d0e5eafe38,j22,Proceedings of the IEEE,"Electrical Networks and Algebraic Graph Theory: Models, Properties, and Applications","Algebraic graph theory is a cornerstone in the study of electrical networks ranging from miniature integrated circuits to continental-scale power systems. Conversely, many fundamental results of algebraic graph theory were laid out by early electrical circuit analysts. In this paper, we survey some fundamental and historic as well as recent results on how algebraic graph theory informs electrical network analysis, dynamics, and design. In particular, we review the algebraic and spectral properties of graph adjacency, Laplacian, incidence, and resistance matrices and how they relate to the analysis, network reduction, and dynamics of certain classes of electrical networks. We study these relations for models of increasing complexity ranging from static resistive direct current (dc) circuits, over dynamic resistor..inductor..capacitor (RLC) circuits, to nonlinear alternating current (ac) power flow. We conclude this paper by presenting a set of fundamental open questions at the intersection of algebraic graph theory and electrical networks."
p73,81b8dd00d832cc46bc72f11298b0839483c48d52,None,None,Graph theory approaches to functional network organization in brain disorders: A critique for a brave new small-world,"Over the past two decades, resting-state functional connectivity (RSFC) methods have provided new insights into the network organization of the human brain. Studies of brain disorders such as Alzheimer’s disease or depression have adapted tools from graph theory to characterize differences between healthy and patient populations. Here, we conducted a review of clinical network neuroscience, summarizing methodological details from 106 RSFC studies. Although this approach is prevalent and promising, our review identified four challenges. First, the composition of networks varied remarkably in terms of region parcellation and edge definition, which are fundamental to graph analyses. Second, many studies equated the number of connections across graphs, but this is conceptually problematic in clinical populations and may induce spurious group differences. Third, few graph metrics were reported in common, precluding meta-analyses. Fourth, some studies tested hypotheses at one level of the graph without a clear neurobiological rationale or considering how findings at one level (e.g., global topology) are contextualized by another (e.g., modular structure). Based on these themes, we conducted network simulations to demonstrate the impact of specific methodological decisions on case-control comparisons. Finally, we offer suggestions for promoting convergence across clinical studies in order to facilitate progress in this important field."
p74,8b7a2b2f4665be8b177abcf9a4d3634ef20eaefc,None,None,"Algebraic Graph Theory: Morphisms, Monoids and Matrices","This is a highly self-contained book about algebraic graph theory which iswritten with a view to keep the lively and unconventional atmosphere of a spoken text to communicate the enthusiasm the author feels about this subject. The focus is on homomorphisms and endomorphisms, matrices and eigenvalues. Graph models are extremely useful for almost all applications and applicators as they play an important role as structuring tools. They allow to model net structures -like roads, computers, telephones -instances of abstract data structures -likelists, stacks, trees -and functional or object oriented programming."
p75,ee4fd9cd27836870dd18eb2d81efac596a758fb1,j23,Journal of manufacturing science and engineering,Thermal Modeling in Metal Additive Manufacturing Using Graph Theory,"The goal of this work is to predict the effect of part geometry and process parameters on the instantaneous spatiotemporal distribution of temperature, also called the thermal field or temperature history, in metal parts as they are being built layer-by-layer using additive manufacturing (AM) processes. In pursuit of this goal, the objective of this work is to develop and verify a graph theory-based approach for predicting the temperature distribution in metal AM parts. This objective is consequential to overcome the current poor process consistency and part quality in AM. One of the main reasons for poor part quality in metal AM processes is ascribed to the nature of temperature distribution in the part. For instance, steep thermal gradients created in the part during printing leads to defects, such as warping and thermal stress-induced cracking. Existing nonproprietary approaches to predict the temperature distribution in AM parts predominantly use mesh-based finite element analyses that are computationally tortuous—the simulation of a few layers typically requires several hours, if not days. Hence, to alleviate these challenges in metal AM processes, there is a need for efficient computational models to predict the temperature distribution, and thereby guide part design and selection of process parameters instead of expensive empirical testing. Compared with finite element analyses techniques, the proposed mesh-free graph theory-based approach facilitates prediction of the temperature distribution within a few minutes on a desktop computer. To explore these assertions, we conducted the following two studies: (1) comparing the heat diffusion trends predicted using the graph theory approach with finite element analysis, and analytical heat transfer calculations based on Green’s functions for an elementary cuboid geometry which is subjected to an impulse heat input in a certain part of its volume and (2) simulating the laser powder bed fusion metal AM of three-part geometries with (a) Goldak’s moving heat source finite element method, (b) the proposed graph theory approach, and (c) further comparing the thermal trends predicted from the last two approaches with a commercial solution. From the first study, we report that the thermal trends approximated by the graph theory approach are found to be accurate within 5% of the Green’s functions-based analytical solution (in terms of the symmetric mean absolute percentage error). Results from the second study show that the thermal trends predicted for the AM parts using graph theory approach agree with finite element analyses, and the computational time for predicting the temperature distribution was significantly reduced with graph theory. For instance, for one of the AM part geometries studied, the temperature trends were predicted in less than 18 min within 10% error using the graph theory approach compared with over 180 min with finite element analyses. Although this paper is restricted to theoretical development and verification of the graph theory approach, our forthcoming research will focus on experimental validation through in-process thermal measurements."
p76,968ea337e15004a2ed3e1442d7f632a1214e2268,j24,Engineering computations,Novel reliable routing method for engineering of internet of vehicles based on graph theory,"
Purpose
The communication link in the engineering of Internet of Vehicle (IOV) is more frequent than the communication link in the Mobile ad hoc Network (MANET). Therefore, the highly dynamic network routing reliability problem is a research hotspot to be solved.


Design/methodology/approach
The graph theory is used to model the MANET communication diagram on the highway and propose a new reliable routing method for internet of vehicles based on graph theory.


Findings
The expanded graph theory can help capture the evolution characteristics of the network topology and predetermine the reliable route to promote quality of service (QoS) in the routing process. The program can find the most reliable route from source to the destination from the MANET graph theory.


Originality/value
The good performance of the proposed method is verified and compared with the related algorithms of the literature.
"
p77,56a13467a3cfb7a9ec00b7f3ed5e953324225233,None,None,Graph Theory with Applications,
p78,aec03b62900277709b10793a168058c5d05fd34f,j25,Bulletin of the Australian Mathematical Society,A GENERAL POSITION PROBLEM IN GRAPH THEORY,"The paper introduces a graph theory variation of the general position problem: given a graph $G$ , determine a largest set $S$ of vertices of $G$ such that no three vertices of $S$ lie on a common geodesic. Such a set is a max-gp-set of $G$ and its size is the gp-number $\text{gp}(G)$ of $G$ . Upper bounds on $\text{gp}(G)$ in terms of different isometric covers are given and used to determine the gp-number of several classes of graphs. Connections between general position sets and packings are investigated and used to give lower bounds on the gp-number. It is also proved that the general position problem is NP-complete."
p79,ecaa9d581d26cb23a759ca1310b0a5d8ce27b7b2,None,None,Topics in graph theory,"A graph is a system G = (V, E) consisting of a set V of vertices and a set E (disjoint from V ) of edges, together with an incidence function End : E → M2(V ), where M2(V ) is set of all 2-element sub-multisets of V . We usually write V = V (G), E = E(G), and End = EndG. For each edge e ∈ E with End(e) = {u, v}, we called u, v the end-vertices of e, and say that the edge e is incident with the vertices u, v, or the vertices u, v are incident with the edge e, or the vertices u, v are adjacent by the edge e. Sometimes it is more convenient to just write the incidence relation as e = uv. If u = v, the edge e is called a loop; if u 6= v, the edge is called a link. Two edges are said to be parallel if their end vertices are the same. Parallel edges are also referred to multiple edges. A simple graph is a graph without loops and multiple edges. When we emphasize that a graph may have loops and multiple edges, we refer the graph as a multigraph. A graph is said to be (i) finite if it has finite number of vertices and edges; (ii) null if it has no vertices, and consequently has no edges; (iii) trivial if it has only one vertex with possible loops; (iv) empty if its has no edges; and (v) nontrivial if it is not trivial. A complete graph is a simple graph that every pair of vertices are adjacent. A complete graph with n vertices is denoted by Kn. A graph G is said to be bipartite if its vertex set V (G) can be partitioned into two disjoint nonempty parts X,Y such that every edge has one end-vertex in X and the other in Y ; such a partition {X,Y } is called a bipartition of G, and such a bipartite graph is denoted by G[X,Y ]. A bipartite graph G[X,Y ] is called a complete bipartite graph if each vertex in X is joined to every vertex in Y ; we abbreviate G[X,Y ] to Km,n if |X| = m and |Y | = n. Let G be a graph. Two vertices of G are called neighbors each other if they are adjacent. For each vertex v ∈ V (G), the set of neighbors of v in G is denoted by Nv(G), the number of edges incident with v (loops counted twice) is called the degree of v in G, denoted deg (v) or deg G(v). A vertex of degree 0 is called an isolated vertex; a vertex of degree 1 is called a leaf. A graph is said to be regular if its every vertex has the same degree. A graph is said to be k-regular if its every vertex has degree k. We always have"
p80,46c0b71ceea130bead6c9120cf6d32b2835db831,None,None,The Cambridge Structural Database,"This paper is the definitive article describing the creation, maintenance, information content and availability of the Cambridge Structural Database (CSD), the world’s repository of small molecule crystal structures."
p81,948fd800ecdd3c99488dde36b41480ca1b8acce3,None,None,The PRIDE database and related tools and resources in 2019: improving support for quantification data,"Abstract The PRoteomics IDEntifications (PRIDE) database (https://www.ebi.ac.uk/pride/) is the world’s largest data repository of mass spectrometry-based proteomics data, and is one of the founding members of the global ProteomeXchange (PX) consortium. In this manuscript, we summarize the developments in PRIDE resources and related tools since the previous update manuscript was published in Nucleic Acids Research in 2016. In the last 3 years, public data sharing through PRIDE (as part of PX) has definitely become the norm in the field. In parallel, data re-use of public proteomics data has increased enormously, with multiple applications. We first describe the new architecture of PRIDE Archive, the archival component of PRIDE. PRIDE Archive and the related data submission framework have been further developed to support the increase in submitted data volumes and additional data types. A new scalable and fault tolerant storage backend, Application Programming Interface and web interface have been implemented, as a part of an ongoing process. Additionally, we emphasize the improved support for quantitative proteomics data through the mzTab format. At last, we outline key statistics on the current data contents and volume of downloads, and how PRIDE data are starting to be disseminated to added-value resources including Ensembl, UniProt and Expression Atlas."
p82,95cd83603a0d2b6918a8e34a5637a8f382da96f5,j26,Scientific Data,"MIMIC-III, a freely accessible critical care database",
p83,da692ee969d9c33986196372c3f7cb87fa6b6f8f,None,None,Database resources of the National Center for Biotechnology Information,"Abstract The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 39 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. Resources that were updated in the past year include the genome data viewer, a human genome resources page, Gene, virus variation, OSIRIS, and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov."
p84,98128fd412ebfa90201a276f2c59020ccc696a75,None,None,DrugBank 5.0: a major update to the DrugBank database for 2018,"Abstract DrugBank (www.drugbank.ca) is a web-enabled database containing comprehensive molecular information about drugs, their mechanisms, their interactions and their targets. First described in 2006, DrugBank has continued to evolve over the past 12 years in response to marked improvements to web standards and changing needs for drug research and development. This year’s update, DrugBank 5.0, represents the most significant upgrade to the database in more than 10 years. In many cases, existing data content has grown by 100% or more over the last update. For instance, the total number of investigational drugs in the database has grown by almost 300%, the number of drug-drug interactions has grown by nearly 600% and the number of SNP-associated drug effects has grown more than 3000%. Significant improvements have been made to the quantity, quality and consistency of drug indications, drug binding data as well as drug-drug and drug-food interactions. A great deal of brand new data have also been added to DrugBank 5.0. This includes information on the influence of hundreds of drugs on metabolite levels (pharmacometabolomics), gene expression levels (pharmacotranscriptomics) and protein expression levels (pharmacoprotoemics). New data have also been added on the status of hundreds of new drug clinical trials and existing drug repurposing trials. Many other important improvements in the content, interface and performance of the DrugBank website have been made and these should greatly enhance its ease of use, utility and potential applications in many areas of pharmacological research, pharmaceutical science and drug education."
p85,6e1e6afb314f9c5a24d744252a30aa5efc313571,None,None,"The STRING database in 2017: quality-controlled protein–protein association networks, made broadly accessible","A system-wide understanding of cellular function requires knowledge of all functional interactions between the expressed proteins. The STRING database aims to collect and integrate this information, by consolidating known and predicted protein–protein association data for a large number of organisms. The associations in STRING include direct (physical) interactions, as well as indirect (functional) interactions, as long as both are specific and biologically meaningful. Apart from collecting and reassessing available experimental data on protein–protein interactions, and importing known pathways and protein complexes from curated databases, interaction predictions are derived from the following sources: (i) systematic co-expression analysis, (ii) detection of shared selective signals across genomes, (iii) automated text-mining of the scientific literature and (iv) computational transfer of interaction knowledge between organisms based on gene orthology. In the latest version 10.5 of STRING, the biggest changes are concerned with data dissemination: the web frontend has been completely redesigned to reduce dependency on outdated browser technologies, and the database can now also be queried from inside the popular Cytoscape software framework. Further improvements include automated background analysis of user inputs for functional enrichments, and streamlined download options. The STRING resource is available online, at http://string-db.org/."
p86,51da1eab2d350b5aa0eeebf83fba7caae3a3bc29,j27,International Journal of Systematic and Evolutionary Microbiology,Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies,"The recent advent of DNA sequencing technologies facilitates the use of genome sequencing data that provide means for more informative and precise classification and identification of members of the Bacteria and Archaea. Because the current species definition is based on the comparison of genome sequences between type and other strains in a given species, building a genome database with correct taxonomic information is of paramount need to enhance our efforts in exploring prokaryotic diversity and discovering novel species as well as for routine identifications. Here we introduce an integrated database, called EzBioCloud, that holds the taxonomic hierarchy of the Bacteria and Archaea, which is represented by quality-controlled 16S rRNA gene and genome sequences. Whole-genome assemblies in the NCBI Assembly Database were screened for low quality and subjected to a composite identification bioinformatics pipeline that employs gene-based searches followed by the calculation of average nucleotide identity. As a result, the database is made of 61 700 species/phylotypes, including 13 132 with validly published names, and 62 362 whole-genome assemblies that were identified taxonomically at the genus, species and subspecies levels. Genomic properties, such as genome size and DNA G+C content, and the occurrence in human microbiome data were calculated for each genus or higher taxa. This united database of taxonomy, 16S rRNA gene and genome sequences, with accompanying bioinformatics tools, should accelerate genome-based classification and identification of members of the Bacteria and Archaea. The database and related search tools are available at www.ezbiocloud.net/."
p87,0f5c63182b5d40850c741888a89e6c055a3593af,None,None,The Pfam protein families database: towards a more sustainable future,"In the last two years the Pfam database (http://pfam.xfam.org) has undergone a substantial reorganisation to reduce the effort involved in making a release, thereby permitting more frequent releases. Arguably the most significant of these changes is that Pfam is now primarily based on the UniProtKB reference proteomes, with the counts of matched sequences and species reported on the website restricted to this smaller set. Building families on reference proteomes sequences brings greater stability, which decreases the amount of manual curation required to maintain them. It also reduces the number of sequences displayed on the website, whilst still providing access to many important model organisms. Matches to the full UniProtKB database are, however, still available and Pfam annotations for individual UniProtKB sequences can still be retrieved. Some Pfam entries (1.6%) which have no matches to reference proteomes remain; we are working with UniProt to see if sequences from them can be incorporated into reference proteomes. Pfam-B, the automatically-generated supplement to Pfam, has been removed. The current release (Pfam 29.0) includes 16 295 entries and 559 clans. The facility to view the relationship between families within a clan has been improved by the introduction of a new tool."
p88,788b43b7c62b497cf69b31544c6f81c6f4856d42,None,None,Pfam: the protein families database,"Pfam, available via servers in the UK (http://pfam.sanger.ac.uk/) and the USA (http://pfam.janelia.org/), is a widely used database of protein families, containing 14 831 manually curated entries in the current release, version 27.0. Since the last update article 2 years ago, we have generated 1182 new families and maintained sequence coverage of the UniProt Knowledgebase (UniProtKB) at nearly 80%, despite a 50% increase in the size of the underlying sequence database. Since our 2012 article describing Pfam, we have also undertaken a comprehensive review of the features that are provided by Pfam over and above the basic family data. For each feature, we determined the relevance, computational burden, usage statistics and the functionality of the feature in a website context. As a consequence of this review, we have removed some features, enhanced others and developed new ones to meet the changing demands of computational biology. Here, we describe the changes to Pfam content. Notably, we now provide family alignments based on four different representative proteome sequence data sets and a new interactive DNA search interface. We also discuss the mapping between Pfam and known 3D structures."
p89,b204970b0503a923359bff532726666f5e0e971b,None,None,The SILVA ribosomal RNA gene database project: improved data processing and web-based tools,"SILVA (from Latin silva, forest, http://www.arb-silva.de) is a comprehensive web resource for up to date, quality-controlled databases of aligned ribosomal RNA (rRNA) gene sequences from the Bacteria, Archaea and Eukaryota domains and supplementary online services. The referred database release 111 (July 2012) contains 3 194 778 small subunit and 288 717 large subunit rRNA gene sequences. Since the initial description of the project, substantial new features have been introduced, including advanced quality control procedures, an improved rRNA gene aligner, online tools for probe and primer evaluation and optimized browsing, searching and downloading on the website. Furthermore, the extensively curated SILVA taxonomy and the new non-redundant SILVA datasets provide an ideal reference for high-throughput classification of data from next-generation sequencing approaches."
p90,93d5369a0be3134c6018373d5290923f3d718815,None,None,The Molecular Signatures Database Hallmark Gene Set Collection,
p91,7f19972754ac0c15329666b3a6efbf569b27d8d5,None,None,The Pfam protein families database in 2019,"Abstract The last few years have witnessed significant changes in Pfam (https://pfam.xfam.org). The number of families has grown substantially to a total of 17,929 in release 32.0. New additions have been coupled with efforts to improve existing families, including refinement of domain boundaries, their classification into Pfam clans, as well as their functional annotation. We recently began to collaborate with the RepeatsDB resource to improve the definition of tandem repeat families within Pfam. We carried out a significant comparison to the structural classification database, namely the Evolutionary Classification of Protein Domains (ECOD) that led to the creation of 825 new families based on their set of uncharacterized families (EUFs). Furthermore, we also connected Pfam entries to the Sequence Ontology (SO) through mapping of the Pfam type definitions to SO terms. Since Pfam has many community contributors, we recently enabled the linking between authorship of all Pfam entries with the corresponding authors’ ORCID identifiers. This effectively permits authors to claim credit for their Pfam curation and link them to their ORCID record."
p92,d2c733e34d48784a37d717fe43d9e93277a8c53e,None,None,ImageNet: A large-scale hierarchical image database,"The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond."
p93,02613d6e3ecf67ed9ae8ce67a35a92f3986bc4cf,j15,Nucleic Acids Research,Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.,"The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSI-BLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily."
p94,0e5bccdedb82fbafece8ca71d64b16ff05ec9145,None,None,The carbohydrate-active enzymes database (CAZy) in 2013,"The Carbohydrate-Active Enzymes database (CAZy; http://www.cazy.org) provides online and continuously updated access to a sequence-based family classification linking the sequence to the specificity and 3D structure of the enzymes that assemble, modify and breakdown oligo- and polysaccharides. Functional and 3D structural information is added and curated on a regular basis based on the available literature. In addition to the use of the database by enzymologists seeking curated information on CAZymes, the dissemination of a stable nomenclature for these enzymes is probably a major contribution of CAZy. The past few years have seen the expansion of the CAZy classification scheme to new families, the development of subfamilies in several families and the power of CAZy for the analysis of genomes and metagenomes. This article outlines the changes that have occurred in CAZy during the past 5 years and presents our novel effort to display the resolution and the carbohydrate ligands in crystallographic complexes of CAZymes."
p95,f986968735459e789890f24b6b277b0920a9725d,j28,IEEE Transactions on Pattern Analysis and Machine Intelligence,Places: A 10 Million Image Database for Scene Recognition,"The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems."
p96,57dfc18815bba1c3737dbc2e5497fd1fc595edb5,j27,International Journal of Systematic and Evolutionary Microbiology,Introducing EzTaxon-e: a prokaryotic 16S rRNA gene sequence database with phylotypes that represent uncultured species.,"Despite recent advances in commercially optimized identification systems, bacterial identification remains a challenging task in many routine microbiological laboratories, especially in situations where taxonomically novel isolates are involved. The 16S rRNA gene has been used extensively for this task when coupled with a well-curated database, such as EzTaxon, containing sequences of type strains of prokaryotic species with validly published names. Although the EzTaxon database has been widely used for routine identification of prokaryotic isolates, sequences from uncultured prokaryotes have not been considered. Here, the next generation database, named EzTaxon-e, is formally introduced. This new database covers not only species within the formal nomenclatural system but also phylotypes that may represent species in nature. In addition to an identification function based on Basic Local Alignment Search Tool (blast) searches and pairwise global sequence alignments, a new objective method of assessing the degree of completeness in sequencing is proposed. All sequences that are held in the EzTaxon-e database have been subjected to phylogenetic analysis and this has resulted in a complete hierarchical classification system. It is concluded that the EzTaxon-e database provides a useful taxonomic backbone for the identification of cultured and uncultured prokaryotes and offers a valuable means of communication among microbiologists who routinely encounter taxonomically novel isolates. The database and its analytical functions can be found at http://eztaxon-e.ezbiocloud.net/."
p97,9f7626c7af925b7b69f1ba86ceb916d21bc03dbe,None,None,Pfam: The protein families database in 2021,"Abstract The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/."
p98,16b0744424f02e01fe2f01b3ea03e2862f1359fc,None,None,"Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation","The RefSeq project at the National Center for Biotechnology Information (NCBI) maintains and curates a publicly available database of annotated genomic, transcript, and protein sequence records (http://www.ncbi.nlm.nih.gov/refseq/). The RefSeq project leverages the data submitted to the International Nucleotide Sequence Database Collaboration (INSDC) against a combination of computation, manual curation, and collaboration to produce a standard set of stable, non-redundant reference sequences. The RefSeq project augments these reference sequences with current knowledge including publications, functional features and informative nomenclature. The database currently represents sequences from more than 55 000 organisms (>4800 viruses, >40 000 prokaryotes and >10 000 eukaryotes; RefSeq release 71), ranging from a single record to complete genomes. This paper summarizes the current status of the viral, prokaryotic, and eukaryotic branches of the RefSeq project, reports on improvements to data access and details efforts to further expand the taxonomic representation of the collection. We also highlight diverse functional curation initiatives that support multiple uses of RefSeq data including taxonomic validation, genome annotation, comparative genomics, and clinical testing. We summarize our approach to utilizing available RNA-Seq and other data types in our manual curation process for vertebrate, plant, and other species, and describe a new direction for prokaryotic genomes and protein name management."
p99,68c03788224000794d5491ab459be0b2a2c38677,c9,Human Language Technology - The Baltic Perspectiv,WordNet: A Lexical Database for English,"Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4]."
